<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>a t-SNE adventure | The SMC Blog</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="a t-SNE adventure" />
<meta name="author" content="Ulrik Halmøy" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A system for interactive exploration of sound clusters with phone sensors" />
<meta property="og:description" content="A system for interactive exploration of sound clusters with phone sensors" />
<link rel="canonical" href="https://smc-aau-cph.github.io/smc-cph.github.io/motion-capture/2020/05/20/tsne-adventure.html" />
<meta property="og:url" content="https://smc-aau-cph.github.io/smc-cph.github.io/motion-capture/2020/05/20/tsne-adventure.html" />
<meta property="og:site_name" content="The SMC Blog" />
<meta property="og:image" content="https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2020_05_21_ulrikah_tsne_cover.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-20T22:00:10+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2020_05_21_ulrikah_tsne_cover.jpg" />
<meta property="twitter:title" content="a t-SNE adventure" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ulrik Halmøy"},"dateModified":"2020-05-20T22:00:10+00:00","datePublished":"2020-05-20T22:00:10+00:00","description":"A system for interactive exploration of sound clusters with phone sensors","headline":"a t-SNE adventure","image":"https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2020_05_21_ulrikah_tsne_cover.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://smc-aau-cph.github.io/smc-cph.github.io/motion-capture/2020/05/20/tsne-adventure.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2022_07_20_stefanof_SMC_logo_grey.png"},"name":"Ulrik Halmøy"},"url":"https://smc-aau-cph.github.io/smc-cph.github.io/motion-capture/2020/05/20/tsne-adventure.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://smc-aau-cph.github.io/smc-cph.github.io/feed.xml" title="The SMC Blog" /></head>
<body><header class="site-header" role="banner"><div class="hamburger-menu-container">
  <nav>
    <ul>
      
      <li class="page-link-hamburger">
        

        <a href="#" class="drop-btn-hamburger" onclick="handleMenuClick(this)"
          >Topics</a
        >
        <ul class="dropdown-hamburger">
          
          <li><a href="/interactive-music/">New Interfaces for Musical Expression (NIME)</a></li>
          
          <li><a href="/machine-learning/">Machine Learning for Media Experiences</a></li>
          
          <li><a href="/motion-capture/">Embodied Interaction</a></li>
          
          <li><a href="/networked-music/">Networked Music</a></li>
          
          <li><a href="/sonification/">Sonification</a></li>
          
          <li><a href="/sound-programming/">Sound Processing</a></li>
          
          <li><a href="/spatial-audio/">Spatial User Interfaces</a></li>
          
          <li><a href="/other/">Other</a></li>
          
          <li><a href="/alltopics/">All Topics</a></li>
          
        </ul>

        
      </li>
      
      <li class="page-link-hamburger">
        

        <a href="#" class="drop-btn-hamburger" onclick="handleMenuClick(this)"
          >Projects</a
        >
        <ul class="dropdown-hamburger">
          
          <li><a href="/mini-projects/">Course Mini Projects</a></li>
          
          <li><a href="/applied-projects/">Semester Projects</a></li>
          
          <li><a href="/masters-thesis/">Master's Theses</a></li>
          
          <li><a href="/projects/">All Projects</a></li>
          
        </ul>

        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/people/">People</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/about/">About</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/Guides/">Guides</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/search/">Search</a>
        
      </li>
      
    </ul>
  </nav>
</div>
<div class="wrapper">
        <div class="title-and-logo-wrapper">
            <a href="/">
                <img class="site-logo" src="/assets/image/2022_07_20_stefanof_SMC_logo_grey.png" />
            </a>
            <p class="site-title">
            The SMC Blog
            </p>
        </div>

        <nav class="site-nav">
        <ul>
            
            <li class="page-link">
            

                <a href='#' class="drop-btn" onclick="handleMenuClick(this)">Topics</a>
                <ul class="dropdown">
                
                <li><a href="/interactive-music/">New Interfaces for Musical Expression (NIME)</a></li>
                
                <li><a href="/machine-learning/">Machine Learning for Media Experiences</a></li>
                
                <li><a href="/motion-capture/">Embodied Interaction</a></li>
                
                <li><a href="/networked-music/">Networked Music</a></li>
                
                <li><a href="/sonification/">Sonification</a></li>
                
                <li><a href="/sound-programming/">Sound Processing</a></li>
                
                <li><a href="/spatial-audio/">Spatial User Interfaces</a></li>
                
                <li><a href="/other/">Other</a></li>
                
                <li><a href="/alltopics/">All Topics</a></li>
                
                </ul>

            
            </li>
            
            <li class="page-link">
            

                <a href='#' class="drop-btn" onclick="handleMenuClick(this)">Projects</a>
                <ul class="dropdown">
                
                <li><a href="/mini-projects/">Course Mini Projects</a></li>
                
                <li><a href="/applied-projects/">Semester Projects</a></li>
                
                <li><a href="/masters-thesis/">Master's Theses</a></li>
                
                <li><a href="/projects/">All Projects</a></li>
                
                </ul>

            
            </li>
            
            <li class="page-link">
            
                <a href="/people/">People</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/about/">About</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/Guides/">Guides</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/search/">Search</a>
            
            </li>
            
        </ul>
        </nav>
        <nav class="hamburger-icon-nav"><div class="hamburger-icon-container" onclick="handleHamburgerClick(this)">
  <div class="hamburger-icon-div hamburger-bar1"></div>
  <div class="hamburger-icon-div hamburger-bar2"></div>
  <div class="hamburger-icon-div hamburger-bar3"></div>
</div>
</nav>
    </div>

  <script src="/utils/jquery.js"></script>
  <script src="/utils/hamburger-nav.js"></script>
  <script src="/utils/nav-dropdown-click-handler.js"></script>
</header>
<main class="page-content" aria-label="Content">

      <div class="wrapper">
        <article
  class="post h-entry"
  itemscope
  itemtype="http://schema.org/BlogPosting"
>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline" align="left">
      a t-SNE adventure
    </h1>
    <p class="post-meta">
      <time
        class="dt-published"
        datetime="2020-05-20T22:00:10+00:00"
        itemprop="datePublished"
      >May 20, 2020 •
      </time>
         
      <a href="/authors/ulrikhalmoy.html">Ulrik Halmøy</a>
        </p>
  </header>

  <div class="post-content" itemprop="articleBody"><h2 id="idea">Idea</h2>

<p>The goal of this project was to develop a system for interactive exploration of sound clusters. More specifically, I wanted to extract features from sounds, cluster them together in a virtual 3D space with t-SNE <sup>[1][2]</sup>, and make an interactive environment for exploring these sounds.</p>

<p>The idea was initially sketched out with an optical tracking system in mind,i.e. Optitrack. Due to the circumstances of Covid-19, the available technologiesfor tracking motion were either mobile phone sensors or web camera by usingcomputer vision algorithms to extract motion data. As one of the main conceptsof the project was to make use of a 3D space, the choice quickly landed onphone sensors, since they are readily available to extract data from in threedimensions.</p>

<h2 id="result">Result</h2>

<p>The video below is a quick demonstration of the end result.</p>

<iframe src="https://player.vimeo.com/video/418516597" width="640" height="482" frameborder="0" allow="autoplay; fullscreen" allowfullscreen="">
</iframe>

<p>The Max patch controls the visual and auditory elements. It communicates with a Python script that receives sensor values from the phone as OSC, and then processes the data.</p>

<p><strong>Source code</strong>: <a href="https://github.com/ulrikah/tsne-adventure">https://github.com/ulrikah/tsne-adventure</a></p>

<h2 id="method">Method</h2>

<p>The initial plan for the interactive environment was to create a 1:1 room-scale virtual mirror of an actual room, e.g. a living room. By using a phone to trigger samples through measurements of the jerk (derivative of acceleration), the user would then be able to move around in the room and explore the sounds. With an optical tracking system, there would be no need for a phone to do this. Instead, one could use a suitable physical object, e.g. a drumstick, attach a reflective marker to it, and then use the jerk of that marker to determine a hit.</p>

<p>Extracting position from the phone sensors (by double integration of the accelerometer values) turned out to be more difficult than expected. The complications were mainly related to noisy accelerometer values and corresponding drifting. After a couple of failed attempts to try to obtain a smooth experience by going through resources provided by the course professor, I decided to use the gyroscope in the phone to control the position instead. The way I ended up doing this was by mapping the pitch, yaw and roll to the velocity vector of a virtual <em>controller object</em>, inspired by <a href="https://en.wikipedia.org/wiki/Ball-in-a-maze_puzzle">ball-in-a-maze puzzles</a>. In the figure below, the orange object represents this controller object. The grey spheres are samples.</p>

<figure>
    <img src="/assets/image/2020_05_21_ulrikah_tsne_cover.jpg" width="70%" align="center" />
    <figcaption>Virtual model with the controller in the middle</figcaption>
</figure>

<p>To trigger the samples, I used the jerk value from the accelerometer sensor. Jerk is the rate of change of acceleration, and can be derived in both x, y and z axis. By performing an abrupt movement that causes a jerk value over a certain threshold in any of the axis, the user triggers the sample to which the controller (see the orange blob in the figure) is closest to. Intuitively, this is similar to a typical percussive instrument, where the musician hits a surface to make sound. This method of triggering samples was inspired by <a href="https://SMC-master.github.io/motion-capture/2019/04/24/How-music-related-motion-capture-can-sound.html">last year’s motion tracking project</a>. The jerk threshold was set by trial and error.</p>

<p>To determine which sample to trigger, I picked the sample with the lowest euclidian distance to the controller object.</p>

<p>To avoid that successive jerk values triggers the same sample too frequently, I also added a cooldown of 200 ms between each sample trigger.</p>

<figure align="middle">
   <img src="https://www.uio.no/english/studies/programmes/SMC-master/blog/assets/image/2020_05_21_ulrikah_tsne_jerk.gif" width="auto" height="auto" />
   <figcaption>Jerk movement</figcaption>
</figure>

<h2 id="reflections-and-future-work">Reflections and future work</h2>

<p>The most apparent problem for interacting with the system is the way the position is currently being controlled by the orientation of the phone. This would naturally be solved by replacing the control method with an optical motion tracking system.</p>

<p>By only using the euclidian distance to determine which sample to be triggered, I realised that I often found myself triggering the same samples over and over again. To be able to better explore a sound cluster, I would look into adding some randomness into the sample selection. One way of doing that could be to do some weighted random selection of the closes <em>N</em> samples. It is also possible that this issue would resolve itself by using the optical tracking system, as it allows for finer control of position.</p>

<p>In terms of interaction and user feedback, I would like to improve the visual aspects of the patch as well. In particular, coloring the currently playing audio samples should be a top priority.</p>

<p>Due to the multifaceted nature of the project, the auditory output of the system was not the main priority. I would like to experiment a lot more with different ways of manipulating the sound. Further work on the project should also focus on finding a better way of triggering samples. The current solution is rendering the audio buffers at runtime, which is not ideal at scale. Additional gain normalization, filters and envelopes should also be implemented.</p>

<p><strong>Clustering</strong></p>

<p>To cluster the files, I used an implementation of the t-distributed Stochastic Neighbor Embedding algorithm (t-SNE) from <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">scikit-learn</a>. t-SNE’s strength is visualising high-dimensional data <sup>[1]</sup>. For this project, I extracted features based on root mean square value of the audio signal, mel-frequency cepstrum, spectral contrast and spectral centroid.</p>

<p>The end result, as seen in figure of the virtual model, does not contain many distinct clusters, which was a bit disappointing. Perhaps it was due to the relatively sparse feature extractors. Even though I experimented with a range of different perplexity values to get the best result (see figure below), I would like to try other methods of clustering in the future. One possibility that I thought of at the very end of the project was to choose a set of <em>exactly three</em> features, and to map the samples according to the mean values of those features, each representing an axis in the 3D space. In that way, the sample would be distributed linearly on the axes by some known feature extraction method. This could potentially make it more understandable for the user why the samples are placed where they are. Future work could also include looking into ways of letting the user change the feature extractors themselves.</p>

<figure>
    <img src="/assets/image/2020_05_21_ulrikah_tsne_perplexity.png" width="70%" align="center" />
    <figcaption>Perplexity values [5, 10, 20]</figcaption>
</figure>

<h2 id="references">References</h2>

<ol>
  <li>Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. <em>Journal of machine learning research</em>, (Nov):2579–2605, 2008</li>
  <li>Wattenberg, M., Viégas, F., &amp; Johnson, I. (2016). How to use t-SNE effectively. Distill, 1(10), e2. <a href="https://distill.pub/2016/misread-tsne/">https://distill.pub/2016/misread-tsne/</a></li>
</ol>
</div>

  

  <a class="u-url" href="/motion-capture/2020/05/20/tsne-adventure.html" hidden></a>
</article>

<script src="/utils/slideshow.js"></script>
<script src="https://unpkg.com/wavesurfer.js@5.0.1/dist/wavesurfer.js"></script>
<script src="/utils/waveform.js"></script>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The SMC Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The SMC Blog</li><li><a class="u-email" href="mailto:cer@create.aau.dk">cer@create.aau.dk</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/smc-aau-cph"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">smc-aau-cph</span></a></li><li><a href="https://www.twitter.com/smc-aau-cph"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">smc-aau-cph</span></a></li><li><a href="https://youtube.com/c/smc-aau-cphr/videos"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#youtube"></use></svg> <span class="username">SMC_master</span></a></li><li><a href="/feed.xml"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#rss"></use></svg> <span>RSS feed</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>The student-led blog of the Aalborg University Copenhagen (AAU-CPH) master&#39;s programme in Sound and Music Computing (SMC).</p>
      </div>
    </div>

  </div>

</footer>


    <!-- include comments here -->

  </body>

</html>
