

[
  
  
    
    
      {
        "title": "Benjamin Melvin Stein",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/ben.html"
      },
    
      {
        "title": "Cumhur Erkut",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/cumhurerkut.html"
      },
    
      {
        "title": "Eirik Dahl",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/eirikdahl.html"
      },
    
      {
        "title": "Elias Andersen",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/eliasandersen.html"
      },
    
      {
        "title": "Emin Memis",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/eminmemis.html"
      },
    
      {
        "title": "Espen Wik",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/espenwik.html"
      },
    
      {
        "title": "Fabian Stordalen",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/fabianstordalen.html"
      },
    
      {
        "title": "Gaute Wardenær",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/gautewarden%C3%A6r.html"
      },
    
      {
        "title": "Guy Sion",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/guysion.html"
      },
    
      {
        "title": "Henrik Sveen",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/henriksveen.html"
      },
    
      {
        "title": "Hugh Alexander von Arnim",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/hughalexandervonarnim.html"
      },
    
      {
        "title": "Iosif Aragiannis",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/iosifaragiannis.html"
      },
    
      {
        "title": "Jack Hardwick",
        "author": "\n",
        "excerpt": "All blog posts authored by this student.\n",
        "content": "\n",
        "url": "/authors/jackhardwick.html"
      },
    
      {
        "title": "Jackson Goode",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/jacksongoode.html"
      },
    
      {
        "title": "Jakob Høydal",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/jakobhoydal.html"
      },
    
      {
        "title": "Jarle Steinhovden",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/jarlesteinhovden.html"
      },
    
      {
        "title": "Joachim Poutaraud",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/joachimpoutaraud.html"
      },
    
      {
        "title": "Joni Mok",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/jonimok.html"
      },
    
      {
        "title": "Joseph Clemente",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/josephclemente.html"
      },
    
      {
        "title": "Juliana Bigelow",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/julianabigelow.html"
      },
    
      {
        "title": "Jørgen Varpe",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/j%C3%B8rgenvarpe.html"
      },
    
      {
        "title": "Karenina Juarez",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/kareninajuarez.html"
      },
    
      {
        "title": "Karolina Jawad",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/karolinajawad.html"
      },
    
      {
        "title": "Kristian Eicke",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/kristianeicke.html"
      },
    
      {
        "title": "Kristian Wentzel",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/kristianwentzel.html"
      },
    
      {
        "title": "Leigh Murray",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/leighmurray.html"
      },
    
      {
        "title": "Lindsay Charles",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/lindsaycharles.html"
      },
    
      {
        "title": "Mari Lesteberg",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/marilesteberg.html"
      },
    
      {
        "title": "Masoud Niknafs",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/masoudniknafs.html"
      },
    
      {
        "title": "Oliver Getz",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/olivergetz.html"
      },
    
      {
        "title": "Paul Koenig",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/paulkoenig.html"
      },
    
      {
        "title": "Pedro Lucas",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/pedrolucas.html"
      },
    
      {
        "title": "Rayam Luna",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/rayamluna.html"
      },
    
      {
        "title": "Sam Roman",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/samroman.html"
      },
    
      {
        "title": "Sepehr Haghighi",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/sepehrhaghighi.html"
      },
    
      {
        "title": "Shreejay Shrestha",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/shreejayshrestha.html"
      },
    
      {
        "title": "Simon Sandvik",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/simonsandvik.html"
      },
    
      {
        "title": "Sofía González",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/sofiagonzalez.html"
      },
    
      {
        "title": "Stephen Gardener",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/stephengardener.html"
      },
    
      {
        "title": "Thibault Jaccard",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/thibaultjaccard.html"
      },
    
      {
        "title": "Thomas Anda",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/thomasanda.html"
      },
    
      {
        "title": "Tom Ignatius",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/tomignatius.html"
      },
    
      {
        "title": "Tom Oldfield",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/tomoldfield.html"
      },
    
      {
        "title": "Trym Bø",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/trymboe.html"
      },
    
      {
        "title": "Ulrik Antoniussen Halmøy",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "Interested in most things that have to do with sound and computers.\n\ngit me up\n\n\n\n",
        "url": "/authors/ulrikhalmoy.html"
      },
    
      {
        "title": "Wenbo Yi",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/wenboyi.html"
      },
    
      {
        "title": "Willie Mandeville",
        "author": "\n",
        "excerpt": "All blog posts authored by this student\n",
        "content": "\n",
        "url": "/authors/williemandeville.html"
      },
    
  
    
    
      {
        "title": "Portal Jam 23 January 2019, Documentation from Oslo Team",
        "author": "\n",
        "excerpt": "On the 23 of January, we were testing out to jam together through the Portal.\n",
        "content": "Portal Jam 23 January 2019, Documentation from Oslo Team\n\n\n\nPreparing for the experiment\n\nOn the 23 of January, we were testing out to jam together through the Portal. We had a meeting on 21 January to make a plan for the jam. We suggested that we should minimize the amount of instruments, and focus on bass &amp; guitar in Trondheim and drums &amp; vocal in Oslo. We also had a technical team, and we documented the jam by recording it with video and audio.\n\nWe chose to use multitrack Lola for the jam. On 21 January we also chose which song we wanted play during the jam, so that all musicians could be prepared and so that we didn’t have to waste time on discussing which songs we wanted to play on the day of the experiment. So we decided on playing the jazz standard “All of Me” by Gerald Marks.\n\nThis is how we delegated the tasks:\n\nOslo side:\n\nMixing: Ashane\n\nDocumentation: Guy &amp; Sam\n\nDrums: Elias\n\nVocal: Mari.\n\nTrondheim side:\n\nDocumentation: Karolina\n\nBass: Eirik\n\nGuitar: Jonas or Sepehr.\n\n\n\nRecording Audio from Midas Mixer to PC:\n\n\n\nConnecting to Lola:\n\n\n\nPreparing the Portal for the jam:\n\n\n\nVideos from the performance\n\nIn those two videos, you see the Oslo Side of the performance. The Trondheim side of the performance was documented in this blog post.\n\nFirst try, without click:\n\n\n\nSecond try, with click:\n\n\n\nThe line up (Oslo side):\n\nSaxophone: Guy Sion\n\nDrums: Elias Sukken Andersen\n\nVocals: Mari Lesteberg\n\nLine up (Trondheim side):\n\nBass: Eirik Dahl\n\nGuitar: Jonas Bjordal\n\nPiano: Daniel Formo\n\nReflections\n\nAs you can see and hear in the first video, we very quickly started to decrease in tempo. There is a huge difference in BPM at the beginning and in the end. It’s likely that this decrease in tempo happened because of the transmission latency in the LOLA system.\n\nTo solve the problem of decreasing in tempo, we decided to let Elias listen to click, while the other were following his tempo. This might sound like a good idea, but it was not as simple as it might sound. Since Elias is listening to the click, the musicians in Oslo and the musicians in Trondheim at once, he get three different tempos or more, since the different musicians might play in a slightly different tempo. That means that Elias have to try follow his click, regardless of how the others play. And since there are delay from Trondheim, this got quickly quite confusing. After trying this a couple of time, with not the best result we came up with the next idea - let us all listen to the same click.\n\nAs you can hear in the second video, we are now listening to the same click (even though Trondheim got some delay, since the click was played from Oslo). This helped a lot, but we were still struggling somehow to keep up in sync with the click. For rhytmic music like jazz, it seemes that playing with click is mandatory to be able to keep the time in the telematic performance. However, for the audience, listening to a click track is not very pleasant. An interesting future task could be to figure out a nice soloution to let the performers only hear the click track during a performance (and not the audience).\n\nAnother idea for future telematic jam sessions, would be to experiment with different types of music than jazz, with less strict tempo preferences. Maybe more ambient types of music would fit the setting better.\n",
        "url": "/networked-music/2019/01/23/Portal-Jam-Oslo-Side.html"
      },
    
      {
        "title": "The Sounds We Like",
        "author": "\n",
        "excerpt": "For our first lesson in the Sonification and Sound Design course, we were asked to give a short presentation of a sonification or a sound design of choice. It was interesting to see the variety of examples among our classmates. Each of us brought a unique example and explained what is it about? why did they choose it? and how does it relate to our work at the SMC program?\n",
        "content": "For our first lesson in the Sonification and Sound Design course, we were asked to give a short presentation of a sonification or a sound design of choice. It was interesting to see the variety of examples among our classmates. Each of us brought a unique example and explained what is it about? why they did choose? and how does it relate to our work at the SMC program?\nThis is an overview of the examples presented:\n\nKarolina Jawad - Particles in the air\nAn installation which discovers the subtle presence of the dust in a room through a sensor and turns it into a generative musical composition and an animation of dust particles. Karolina talked about how it would be interesting the connect the idea of particle sonification in an ocean context, and presented us with another example of a sound installation that utilize the sea, the sea-organ.\n\n   \n\n\nJonas Bjordal - Musical Fractals\nA video by Adam Neely presenting the musical equivalent of a fractal, the mathematical concept often represented in geometric art that explores recursion and self-reference.\n\n   \n\n\nShreejay Shrestha - The Defining Rise of Carbon Pollution\nIs a song which is based on carbon dioxide measurements since 1957 where one can hear the rise of carbon pollution, caused by the burning of fossil fuels and deforestation. Lower notes equal lower carbon dioxide measurements.\n\n   \n\n\nÇağrı Erdem - Myogram\nAn 8 channel sonification of muscular corporeal states by Atau Tanaka. Two Myo armbands are measuring the electrical activity of voluntary and involuntary  muscle contractions. Erdem talked about his interest in the interactive sonification and interactive systems.\n\n   \n\n\nJørgen Nygård Varpe - No Country for Old Men\nIs an Hollywood movie released in 2007. Jørgen described how much he liked the soundtrack of the film, the idea that less is more and that the details in sound design throughout this film really made a strong impression on his viewing experience.\n\n   \n\n\nEigil Aandahl - Red Dead Redemption Soundtrack\nIn this video we are being exposed to the behind the scenes of the recording and creation of the game Red Dead Redemption. The soundtrack, which is based mainly on stems, captivate and reimagine the old American west and successfully enhance the gaming experience.\n\n   \n\n\nAshane Silva - The Pulse Oximeter\nIs a noninvasive method for monitoring a person’s oxygen saturation. This is an example of a medical device which present pulse with sound. Ashane talked about the importance of the sound design one should consider when creating a device meant be used by people.\n\n   \n\n\nQichao Lan - Breaking Bad sound layering\nA scene from the TV series Breaking Bad with a sound layer breakdown containing Sound FX, Music and Dialog. The scene features an anxious person in a panic state. Qichau talked about how the soundtrack serves the scene, and how important are the artistic choices sound designers make.\n\n   \n\n\nElias Andersen - Sauden\nA sonification of a newspaper article read by Siri with added sounds and beats. Elias mentioned how he liked the idea of sharing songs instead of sharing news articles among friends and presented us with this example made by his friend.\n\nSepehr Haghighi - Stream Line\nAn interactive project that explores the interaction between the subject and the sonification of his/her movement. The subject could be a dancer or an ensemble. The sonification is going to be done via motion tracking and shall be played back to the subject real time.\n\n   \n\n\nGuy Sion - Nissan Leaf\nIn 2011 Nissan was the first to introduce an artificial car sound for its electric vehicles. Electric vehicles are so quiet at low speeds that they can go unnoticed by pedestrians. In 2017 Nissan debuted the ‘Canto’ (derived from sing in Latin) which changes the sound Nissan cars make while driving by varying in pitch and tone depending on weather the vehicle is accelerating, decelerating or reversing.\n\n   \n\n\nSam Roman - Examples of practical use of sonification\nTwo examples of basic sonification, a heart rate monitor and a truck reversing, that are quite similar sonically but mean different things when are in different settings were presented by Sam.\n\n   \n\n\nMari Lesteberg - Miles Beyond\nA track by the Mahavishnu Orchestra which features a synth sound in the first few seconds of the song. Mari mentioned how much she enjoys this soothing synth sound which actually continued to be played in the background throughout the whole song. Mari expressed her desire to learn how to create this kind of sounds which are always there but you don’t really hear them all the time and incorporate them in her artwork.\n\n   \n\n\nEirik Dahl - Sonification of Hurricane Sandy\nA satellite video of hurricane Sandy together with a sonification of the meteorological parameters. Eirik discussed his interest in creating a similar sonification of meteorological data from the North Cape.\n\n   \n\nFinal Remarks\nOverviewing the examples presented, we can sort them into three main categories, sonification (The Pulse Oximeter, Nissan Leaf, Reversing Truck, Heart Rate Monitor), sound design (Musical Fractals, No Country for Old Men, Red Dead Redemption Soundtrack, Breaking Bad sound layering, Miles Beyond), and sound interaction design (Particles in the air, The Defining Rise of Carbon Pollution, Myogram, Sauden, Stream Line, Sonification of Hurricane Sandy). It was a nice experiance to hear what other students find interesting when it comes to sonification and sound in general. Sonification and sound design is such a broad field and I am very much looking forward to learning more about conveing information through sound throughout this course.\n",
        "url": "/sonification/2019/01/23/The-Sounds-We-Like.html"
      },
    
      {
        "title": "How to stream content from the portal",
        "author": "\n",
        "excerpt": "In this blogpost, we will try to explain in more detail how these streams have been set up using OBS, Open Broadcaster Software and Youtube Live while being connected between Trondheim and Oslo. This can be of use for anyone looking to set up a co-located stream of a speaker or performance.\n",
        "content": "\n\n\n\n\nIn 2006, Time Magazine names “You” as the person of the year. This was because of the explosion of community based content and the impact people on the internet could have. Since then, the accessibility of creating and sharing content online has increased tremendously, and the ability to broadcast sound and video has been similarly democratised. We at SMC have also been exploring ways of broadcasting some of what goes on in The Portal to a wider audience (NTNU/Trondheim, UiO/Oslo and online viewers, particularly in the context of WoNoMute and the monthly talks by women in music technology.\n\nIn this blogpost, we will try to explain in more detail how these streams have been set up using OBS, Open Broadcaster Software and Youtube Live while being connected between Trondheim and Oslo. This can be of use for anyone looking to set up a co-located stream of a speaker or performance.\n\nHow to use OBS and Youtube Live\n\nFirst things first, we will take a look at how to set up a live stream using OBS and Youtube, as this information will be crucial to understand how we have been using these technologies.\n\nThe end goal of our streams has been to broadcast talks and events in the portal to whomever wants to watch and listen anywhere in the world. To do this we have been using Youtube, which is the most popular video-sharing website and supports live streaming. Since we want to use many different cameras, microphones, presentation slides and so on when broadcasting, we use a streaming encoder software called OBS. This software lets us compose scenes with different elements in them that go out as a single video feed. OBS can be linked to for example a Youtube channel using a streaming key that is generated by the website to send the stream where we want it to go. It is also possible to use OBS to do screen recording of a computer or local recordings from video cameras.\n\nWhen setting up a scene in OBS, it is possible to add many different sources and move them around, such as text, images, video, and even audio that can be mixed within the software. In studio mode, it is possible make edits to scenes without the edits going out before a transition is made. Furthermore, the sources can be modified, chroma keyed (made transparent using for example a green-screen), filtered and cropped. Below is a screenshot of OBS in studio mode with two scenes. The image on the right is what would be the output if we were to click “start streaming” to a live feed. The image on the left is a preview of what the output would be when transitioning, and changes made to the composition here is not streamed.\n\n\n\nOBS in studio mode with two scenes.\n\n\nFor more information on how to use OBS and setting up live streaming on Youtube, the following links lead to external Youtube video tutorials on the subject:\n\n\n  How to Use OBS Studio (Beginners Guide)\n  How To Live Stream On YouTube With OBS | Fast Start Guide\n\n\n\nA couple of notes on streaming with Youtube. Compared to some of the other solutions we have been using to communicate between Oslo and Trondheim, streaming is very slow. See the blogpost Mutual Concert Between Oslo and Trondheim - Personal Reflections, about the network performance we did using LoLa. To ensure the best possible quality of image and sound, the streaming services generally have a long buffer which adds up to several seconds of latency. It is therefore not very useful for real-time two-way communication with sound or video, but these kinds of services usually have a text chat where questions from the audience can be read and responded to.\n\nHow our setup has been for WoNoMute\n\nHere we present how we have setup streaming from The Portal. It should be noted that this is only one of many possible solutions, and might not be exactly how we will do this in the future.\n\nIn order to have a proper communication, two computers are needed in the place that the presenter is and one on the other side of The Portal. The computers in Trondheim are used for streaming, setting up LOLA for audio sharing and video sharing (presentation and live videos), using Zoom video conference. The third computer (in Oslo) is used for receiving and sending the sound and image, via LOLA and Zoom video conference. Also a mixer is required to mix the sounds in the presenter’s place. Below we shall explain the signal chain and the way that audio/visual aspect of this presentation could be set up. See also the flowchart below for an overview of the different audio and video signals going between Trondheim, Oslo and streaming.\n\n\n\nFlowchart for the streaming setup between Oslo and Trondheim.\n\n\nFirst we will look more thouroughly at the audio setup.\n    There could in this case be three audio sources in Trondheim, where the presenter is, and one in Oslo to pickup questions from the audience.\n    The audio sources in Trondheim consist of:\n\n\n  1 - Presenter Mic\n  2 - Presentation material (presenter’s laptop)\n  3 - Audience Mic\n\n\nThose sources are going to the mixer and then sent to Oslo and played back via LOLA system. The same thing happens with the sound from  Oslo. The audience mic in Oslo is going to be received in Trondheim via LOLA and then played back.\n    In the end, the sound from the mixer (consisting of all of the sound sources from the two sides) goes to the streaming computer and is played back in the stream.\n\nWhen it comes to the video, the process gets more complicated. Starting from the Trondheim’s side, there are three video sources.\n\n  1 - Presenter camera\n  2 - Presentation material (presenter’s laptop)\n  3 - Audience camera\n\n\nAlso there is one camera in Oslo to capture the audience. For video communication, Zoom video conference software is used.\n    Starting from the presenter camera in Trondheim, It goes directly to the streaming computer. When it comes to audience cameras, the Trondheim one’s signal goes into a Blackmagic web presenter, which captures the video so it can be used in OBS, alongside the audience camera signal coming from Oslo. Then both are getting into the streaming computer and go into the stream. Zoom is being used in the streaming computer in order to share the presenter video with Oslo.\n    The presentation material’s video is directly going into the Zoom computer and going to be shared. That shall allow the Zoom software in the streaming computer to receive the video for streaming.\n    In regards of the displays, both places use two big screens. In Trondheim, one screen is receiving the audience signal from Oslo via Zoom and the other is showing the presentation material, again coming from the Zoom. In Oslo both screens receive their picture from Zoom as well and one is dedicated to presenter’s picture (since the presenter is not in Oslo) and one to the presentation material.\n\nThat sums up the technical setup we have been using to stream talks from the Portal between Trondheim, Oslo and the wider web through Youtube.\n\nFinal Remarks\nAs stated, the setup described here is only how we have done it lately for the WoNoMute talks, but it is perhaps not the optimal solution, and more trials to improve on the setup are certainly ongoing. It is anyway indicative of the the complex but rewarding activity of connecting places and people, letting them take part in events when they are happening. Hopefully this has been an illuminating read, but if not, we can probably be reached for answers to your questions.\n",
        "url": "/networked-music/2019/01/30/portalvideotalkstreaming.html"
      },
    
      {
        "title": "Documentation and recommendations from the latest Portal Jam",
        "author": "\n",
        "excerpt": "As the Portal is still in its infancy, pushing and exploring its technical possibilities is an ongoing process. We still encounter different issues while actually seeking a smooth and standardized setup of the signal routing and performance space. At the end it is about optimizing the telematic experience but not getting stucked in technicalities at the same time.\n",
        "content": "\n\n  \n  Your browser does not support video tag.\n\n\n\nAs the Portal is still in its infancy, pushing and exploring its technical possibilities is an ongoing process.\nFor different scenarios like jamming with acoustic instruments cross-campus, we still encounter different issues while actually\nseeking a smooth and standardized setup of the signal routing and performance space. A teaching-workshop set-up requires\nfor example a different solution than a lecture-performance, so in each of those scenarios we try to find out how to optimize\nthe telematic experience.\n\nIn one of our latest test-jams we were testing the current semester set-up and tried to apply many of these considerations.\nWe also tried to implement a screen layout, since the visual cross-campus communications helps to increase the sense of immersion.\nThis was the first time we putted up lightning in Trondheim in order to recognize the faces better and remove visual noise from the screens.\nOur current system still leaves much to be desired when it comes to the visuals. For instance, the faces in Oslo are unclear.\nThis is due to the fact that the performers in Oslo have to be very far back in the frame due to the position of the mixer.\nThis, combined with visual noise in the frame makes it hard to immerse visually with the performers in Oslo. However, it would have been\noptimal to install them on the racks from above in a 65 degree angle. By now we’re moving more and more towards a film-studio-/ theatre stage like situation, but without the 4th wall.\n\nIn terms of latency, on an audible level it was low enough to perform together. Visually, the latency was so\ndistracting that the performers chose not to look at the screens in order to be able to focus on the sound and stay in synch.\nFor some parts of the jam we chose to use the click track from the drums in Oslo. This was sent and recieved on the same channels as the drum sounds themselves, which of course was not ideal in terms of sound mixing. The click was also heard in Trondheim later than in Oslo, because of the latency added in the signal chain. By playing “on click” you are thereby playing the system’s latency behind Oslo.\n\n\n\n\n\n\nTechnicalities\n\nBoth sides agreed to use headphones for monitoring, which eliminated the chance of feedback.\nIn Oslo, drums, vocals and saxophone were sent through LOLA. In Trondheim, bass, piano and guitar were routed to Oslo. Some instruments took longer time to connect, like the bass guitar. The output of the bass amp which was bought to be directly connected to the mixer, created noise. To solve the noise issue, the bass was instead connected to a DI box, connected to the Midas mixer.\nAll other instruments were connected directly to the mixer, like the guitar through an amp. For documentation we used several video sources, like Sony camcorder, Zoom audio-visual capturing and mobile phones.\n\nStereo mix/ monitor mix\nIn Trondheim we recieved audio from Oslo on 4 channels through LOLA (Two for drums). These channeles were sent together with the local audio channels to output bus 9 and 10, as you can see illustrated in the signal flow chart above. These channeles were physically connected to a headphone amplifier, which distributed the signal to the local participants in Trondheim. Since only two channels (left and right) were used, everyone got the same monitor mix. This mix was set by adjusting the amount of each instrument sent to the monitor busses. Later on we will add several busses, to be able to send different mixes to each participant.\n\nGeneral thoughts on the documentation, it would be practical to develop a standard procedure for them.\nTemplates that can be executed and documented in a standardized way. To achieve that we need reliable equipment which is always accessible.\n",
        "url": "/networked-music/2019/02/10/Documentation-and-recommendations-from-the-latest-Portal-Jam.html"
      },
    
      {
        "title": "The Wavesynth",
        "author": "\n",
        "excerpt": "During the first workshop week in the course Audio Programming, I have been working on a project which I have called “The Wavesynth”. I have called it this because I have chosen to use wavetables to shape the output of an oscillator. I have not made a wavetable synthesizer like for instance Ableton’s Wavetable, where you can interpolate between waves. instead I use some wavetables created by Google Chrome Labs to make it sound like “real” instruments. The synth is played by using the computer keyboard, and the user can choose the output sound, and adjust three different effects to shape the it the way they want. The synthesizer is made using web technologies, including HTML, JavaScript, Web Audio API, and more.\n",
        "content": "During the first workshop week in the course Audio Programming, I have been working on a project which I have called “The Wavesynth”. I have called it this because I have chosen to use wavetables to shape the output of an oscillator. I have not made a wavetable synthesizer like for instance Ableton’s Wavetable, where you can interpolate between waves. instead I use some wavetables created by Google Chrome Labs to make it sound like “real” instruments. The synth is played by using the computer keyboard, and the user can choose the output sound, and adjust three different effects to shape it the way they want. The synthesizer is made using web technologies, including HTML, JavaScript, Web Audio API, and more.\n\nKeywords: Web Audio API, JavaScript, HTML5 Canvas, Wavetable, Synthesizer, DSP, NexusUI\n\nEarly process\n\nLuckily for me I have experience working with javaScript and the Web Audio API. This meant that I could use my previous knowledge to help others that were new to the field of programming and digital signal processing. Helping others also helped myself refresh my knowledge, which have been laying in the back of my head for some time. It’s crazy how quickly one can forget!\n\nDuring class we went through the basics of HTML, javaScript and the Web Audio API, some of the web technologies required to create a website and to be able to control audio on the web. I quickly decided that my project should be a keyboard-controlled “sound generator”. With that in mind, I created a piano keyboard layout, where I made computer keyboard presses correspond to each note.\n\nThe keyboard\n\n\n  \n  Synth piano keys\n\n\n\n  \n  Computer keyboard\n\n\nAbove you can see images of how the computer keyboard is mapped to the piano keys.\n\nThe piano keys are created using HTML5 Canvas, a way of creating graphics on the web using javaScript. Two canvas elements area created in the html file, one for drawing the white keys on, and the other for drawing the black keys on. The actual drawings are programmed in javaScript, and the canvases are redrawn for every time a key is pressed, with the corresponding key turning blue.\n\nWhere is the sound?\n\nAfter a day without making any sounds, only working with on making the keyboard and the key triggering logic, I felt I had to start producing sounds. Day 2 was on, and I wanted the synthesizer to be polyphonic. I wrote, tested, edited, tested, looked for examples, copied examples, edited, tested, and so on. It did not work, and stress came along…\n\n\n  \n  \n\n\nTwo hours maximum of coding after the seminars was not enough for my brain to come up with a solution, so I put the idea of making it polyphonic on the top shelf. It will be taken down later.\n\nMid process\n\nI had decided to go over to a monophonic synth, and during day 3 it was working. for every time I pressed a key, I created a new oscillator, passing the frequency as an argument, and for every release of the key, I stopped the oscillator. I wrote logic code to prevent multiple triggering when holding down keys and used envelopes to prevent “click” noise when stopping sounds.\n\nAt the end of day 3 I got the custom wavetable to work. I was originally most interested in getting a Wurlitzer sound, and at Google Chrome Labs’s Github page I got a wavetable for this. I copied the raw file, and made a JSON (JavaScript Object Notation)-file out of it. JSON is a syntax made for storing and exchanging text-data. In javaScript I can then get the json-file using a XMLHttpRequest object and convert it to a javaScript object using the built-in function JSON.parse(). I then made a function to set the wavetable to the oscillator, which you can see in the script.js file in the repository, which I’ve linked to at the bottom of the blog post.\n\nLate stage\n\nDay 4 had come, and I had not gotten as far as I had hoped. Maybe I prioritized wrong; beginning with the keyboard layout, helping others, and getting in over my head in complexity, but I ended up with an ok result.\n\nI used the last hours to implement a lowpass filter into the signal chain, which the user could change using a slider. The slider was made with a library shown to us by our teacher Anna Xambó, called NexusUI. “NexusUI is a collection of HTML5 interfaces and javaScript helper functions to assist with building web audio instruments in the browser.” (https://nexus-js.github.io/ui/). This is a very nice and easy to implement library that makes it relatively easy to build a good-looking web interface.\n\nThe weekend\n\nNot being so happy with the result, I decided to use some of my weekend on cleaning up my spaghetti code, optimizing it, and implementing some new functionality. First, I went away from making a new oscillator with every keypress, to connecting and disconnecting it from the output (destination). When working with the functionality of the filter and a newly added delay effect, I encountered some logic errors with the signal chain. I then decided to instead turn the oscillator on and off by setting the gain to 0.5 and 0 respectively.\n\nThe effects\n\nI ended up with three effects. A lowpass-filter with adjustable cut-off, a delay with adjustable delay time, and a tremolo effect with adjustable tremolo speed. The user can activate and deactivate these effects with a corresponding button and use sliders to adjust the effects.\n\n\n  \n  The Wavesynth\n\n\nMore waves, octaves, and optimization\n\nAs you can see in the picture above, I also added a dropdown where you can choose between different instruments! I added this by retrieving more wavetables from Google Chrome Labs, making several json-files, and running a XMLHttpRequest every time the user chooses an option from the list, and then updating the wave of the oscillator. In addition, you can also choose which octave you want to play in. This is quite useful, since the keyboard has a limited number of keys in a row.\n\nI chose to start the audio context, create every node, connect every audio node, and set a default configuration with one big button. This button initiates all this, and the code runs only on the first press. The synthesizer will not work before you press the big button.\n\nLastly, I have worked a bit with CSS to make it look alright.\n\nValuable learning and future work\n\nEven though I have previous experience as mentioned, I have learnt a lot during this week. I was more than a bit rusty on the syntax, the routing and the Web Audio API functionalities. This workshop has definitely helped me grasp these concepts more clearly.\n\nI will continue to explore the libraries we have been shown, which I believe will make life a lot easier :smiley: I want to continue to work on how I have routed the effects, adding user ability to change dry/wet ratio, and I want to add reverb. I may choose to use some libraries in the future for faster building, but still I see the absolute need for having the foundation of how to do DSP with only the Web Audio API. One last important thing is that I need to make it responsive. For now, the GUI (Graphical User Interface) is not scalable, so it would work poorly on a small device.\n\nIf you want to see how I have built my application, look at my repository! I have tried to comment the most important lines of code and divided it into sections for easy reading.\n\nWant to try it? Try it here\n",
        "url": "/sound-programming/2019/02/10/The-Wavesynth.html"
      },
    
      {
        "title": "The Mono Synth",
        "author": "\n",
        "excerpt": "This blog post outlines the production of the MonoSynth. The Mono Synth is drawn by Jørgen N. Varpe, who also wrote a lot of the code. The objective of this prototype was to improve my familiarity with coding, and at the same time be able to have a working chromatic instrument. Working with a cromatic instrument is interesting because it allows me to have a less abstract understanding of what happens in the code - behind the scenes if you will.\n",
        "content": "\n\n\n\n\nMy previous knowledge with programming before starting the SMC course was virtually nonexistent, much unlike the error codes I have come across during the creation of this project. Still, I feel like my understanding of programming has grown alot since the previous semester’s programming course.\n\nFinding A Project\n\nIt was somewhat difficult to decide what i would be able to do. Working with sonification or making a simple synth was my two options and I even thought this could be challenging, but after a short discussion with Anna, i\nlanded on the latter.\n\nFrom there i tried to implement code from the web. I find it very helpful to use exsisting code when starting out. That helps one to get an overview of the functions rather than having to guess all the names and commands\noneself. As a beginner i would advise people that is starting out to do the same. Anna also gave me another good tips, and this was to make code that does not work, or need improvement into comments, so you can segment the code to isolate problems.\n\nTo concretize, my goal was to make a mono synth that was routed to the keyboard on my laptop. My overarching goal was simply to gain more insight into coding through being more exposed to it.\nThat might not sound very impressive, but for me, with my startpoint, I think it was an ambitious goal.\n\nTechnologies used\n\n\n  \n    Visual studio Code\nVisual studio code was the code editor i used. I do not have too much to compare it to but it seems to be a powerful tool, and from what i gather from my peers, it seems like a very good place to start with coding. I also like the live share function, and the ease of importing folders. Drag and drop is my style!\n  \n  \n    Java Script\n  Most of the code and functions are written in Java Script. This includes the keyboard, the oscillator, and all the mappings\n  \n  \n    GitHub\n  GitHub was used for repositories and the sharing.\n  \n\n\nWorkflow and progress\n\nAfter going through the code examples of the day, we were encouraged to use the group rooms. I felt more comfortable staying in the portal so as to get help from Anna and some of the more experienced students in the class.\nWhile doing programming, it is not always intuitive to find out what is the issue. For example, a word in a line could be right, but missing a capital letter. That is really not easy to figure out if you are just starting out, such as i am.\n\nIt is very helpful to have your peers and teachers looking over the code and teach you some debugging tricks. For late night work, google is your friend. Googeling everything will in many cases give you an asnwer, because\nsomeone else has probably experienced a similar issue before. Finding the right keyword, and knowing what to google is still an issue that is left to oneself.\n\nRouting the keys was actually the easiest and most pleasurable thing to do. I am not really a huge fan of repeating tasks, but here, it provided me with a sense of progress and independency which was very motivating.\n\nTimeline\n\n  \n    Day 1\nI familiarized myself with the code, and got the keyboard code from Jørgen. Most of the day went with just understanding how that went.\n  \n  \n    Day 2\nDay two was used to connect and test the Oscillator. From here i had sound, which is an achievement in itself. i Was very happy with getting sound even though the piano was not playable.\n  \n  \n    Day 3\nDay three was used to make the routings of the keyboard. This was satisfying since i could do it on my own without any help.\nI also had to work on the functionality of the keyboard. The keys did not work as a piano. I needed to implement an keyup event to make the keyboard play a sustained note and stop it when i lifted the key.\n  \n  \n    Day 4\nThis day was used to add some visual features through CSS. I was pleased with how it worked and how i could change the visual apperarance.\n  \n\n\nCode snippets\n\nKeydown function:\n\nThis is the keydown function routed to black and white keys on my keyboard:\n\n  //window.addEventListener(\"keyup\", function (event) {\n      this.console.log(\"up\");\n      drawWhite(20, 14, \"up\");\n      drawBlack(55, 10, \"up\")\n      switch (event.key) {\n        case \"a\":\n        case \"s\":\n        case \"d\":\n        case \"f\":\n        case \"g\":\n        case \"h\":\n        case \"j\":\n        case \"k\":\n        case \"l\":\n        case \"ø\":\n        case \"æ\":\n\n        case \"w\":\n        case \"e\":\n        case \"t\":\n        case \"y\":\n        case \"u\":\n        case \"o\":\n        case \"p\":\n        case \"å\":\n\n            stopSound();\n            break\n    }\n\n\nKeyup function:\nThis is the function i used to stop the oscillator form playing when i lifted the key.\n\n//window.addEventListener(\"keyup\", function (event) {\n    this.console.log(\"up\");\n    drawWhite(20, 14, \"up\");\n    drawBlack(55, 10, \"up\")\n    switch (event.key) {\n        case \"a\":\n        case \"s\":\n        case \"d\":\n        case \"f\":\n        case \"g\":\n        case \"h\":\n        case \"j\":\n        case \"k\":\n        case \"l\":\n        case \"ø\":\n        case \"æ\":\n\n        case \"w\":\n        case \"e\":\n        case \"t\":\n        case \"y\":\n        case \"u\":\n        case \"o\":\n        case \"p\":\n        case \"å\":\n\n        stopSound();\n          break\n    }\n\n\nHappy Birthday\n\nAt the day of the presentation, one of my peer’s had a birthday. I choose to use the oppurtunity to play him the birthday song.\n\n\n\n\n\nEnd Product\n\nAlbeit not the most complex project, I am proud of what i was able to produce. Making this synth gained me more understanding than i initially tought i would get from the first week of Audio Programming.\nIt was very helpful to be able to follow a sectioned piece of code like what was produced in the teaching materials.\n\nI ended up with a workable MonoSynth that I was able to complete with a CSS color scheme. As seen in the video, a previous part of the code was\nable to produce Happy birthday, allthough the performance is littered with human errors.\n\nFor now, i am done with the messy spaghetti code that eventually became this project, and I am looking forward to start anew with a shared project next week.\n\nFuture possibilities\n\nRight now, the synth is mono. An obvious improvement would be to make it polyponic. That could be solved with adding more oscillators - for instance one per key, which would improve the playability considerably. Another aspect that could be changed is the sound of the oscillator itself. For instance one could make a button to change the waveform. There are multiple possibilities that could make this synth a very interesting product to use when a midi keyboard is not available.\n\nFor anyone interested, here is a link to my GitHub repository where i keep the code\nhttps://github.com/EirikDahl/Code\n",
        "url": "/sound-programming/2019/02/10/MonoSynth.html"
      },
    
      {
        "title": "Freak Show",
        "author": "\n",
        "excerpt": "As my first experience working with Web Audio API, utilizing JS, HTML and CSS; it was quite a challenge, but a pleasant one that lead to the outcome that I wanted and also broadened my perspective, in regards of my future plans.\n",
        "content": "Starting Point\nAt the start, I have to mention that these seven days, were quite challenging to me. As a person who has done no coding before (only a couple of times before, in the same program), this project has been a little bit hard for me to finish.\nThe idea of the project is to have a basic interactive experience. There is a Troll picture on the screen and a silly laugh, associated to it. They amount of delay time and size of the troll picture is related to the placement of the mouse and with the cursor movement, they shall change as well.\n\n\n\n\n\nWith the movement of the mouse, from left to right, the value of the elements - delay time and the size of the picture – increase and also there is a slider placed in the page to control the amount of the delay itself (dry/wet). The whole project is based on utilizing JS for running and also CSS is used in order to make it more appealing.\nIn here you can observe a presentation of the project:\n\n\n        \n        \n        Your browser does not support the video tag.\n\n\n\n\nWork Process\n\nDay 1:\nRegarding the first day, I only added a picture and linked it to the mouse position on the page and managed to play a laughing sound effect, but the problem regarding the picture was that the aspect ratio of it wasn’t fixed. I also tried to add a reverb effect to the picture and map the reverb time to the mouse curser as well; but did not succeed in the end of the day.\n\nDay 2:\nIn the second day I decided to fix the issues I faced yesterday; but I could not manage to do it. In order to do that different libraries were installed, so they could be a resource in order to create the reverb. Libraries such as p5.js, Tuna and a couple of others which I forgot their name. But none of them basically worked. Also there were several solutions I found online, in order to fix the Aspect Ratio, but couldn’t implement them in the code. I was not present during the presentation of the project in this day. Therefore, Anna did the favor of playing back a short video of the basic idea of my project for me.\n\nDay 3:\n\nDay 4:\nIn the last day, with the help of Jørgen the Aspect Ratio issue was fixed and the picture was put in the center, top of the screen. I have to mention that in the previous days he was being really helpful regarding troubleshooting and building up the code structure. This was done by dividing the normalized width and height and multiplying it to a defined scale. But still the audio problem didn’t get solved. But with learning how to create different files and connecting them to the index.html file, I managed to create a .js file with all the main codes in them and connect it to the index.html. Also another interesting event that happened in this day was the introduction of CSS to the class. I created a CSS file for the project and by utilizing it I made the background. Afterwards I inserted a slider in the page, and tried to get a sound being played back, with clicking on the Troll picture. But actually at the end of the day I didn’t manage to connect the slider to the delay level and make the click playback sound work. Therefore the bright side of my presentation mostly included the visual improvements in the project. Also as same as the rest of the class, I didn’t receive any feedbacks or questions. The same thing happened to me in the previous day as well, since my project was not developed that much.\n\nExtra Days :D (Weekend):\nAlthough things didn’t go that much well during the week days, in the weekend with putting a lot of time and effort, with help from a friend of mine - Hamed Kazemi - who is back in Iran, finally the following issues were fixed:\n\n\n  having the delay being engaged with the mouse movement, by creating a function at the beginning and calling it later on in the code.\n  linking the slider to the delay level and creating a fader value field in the page, so the delay level could be observed as well.\n  playing the specified sound, while clicking on the picture.\n  cleaning up the code and spreading the work load between the various platforms - CSS, JS and HTML; mostly between JS and HTML. For example, instead of everything being happened in JS, the image is being called in HTML and then everything happening to it is being programmed in JS.\n  Making the code more organized aesthetically with “Prettier” plugin for Visual Studio Code.\n\n\nBut still one last problem remained. The code needed to create a new buffer node after being stopped. For starting again it needed to create a new node and in order to fix that, I used Eigil’s help and finally the project was completed and was working without a flaw.\n\nConclusion:\nIn conclusion, this project was quite challenging for me; but also helped me a lot to figure out how web audio works in practice and made me aware of new possibilities that could come in handy for my future projects. That include both practical aspects and the first ideas regarding those projects. Since the laughing sound used in the project, in combination with the delay and the troll picture, created a really freaky atmosphere; I chose to name the project “Freak Show”.\nIn the end, I have to thank Anna who helped a lot in regards of various issues that happened during this week. You can access to the whole project’s codes on this Github page.\n\nCheers!\n",
        "url": "/sound-programming/2019/02/11/FreakShow.html"
      },
    
      {
        "title": "Reese da Alien!",
        "author": "\n",
        "excerpt": "The project I have developed on over the first week of web audio based programing is called Reese da Alien - a web based synth of sorts with mouse functionality. The idea is that the program presents a relatively novel way of producing a reese, by the user moving around the mouse on the the page to find different sweet spots as they affect the pitch and amplitude of two oscillators with the movements. The persona of the application came after early in development I likened the sounds to an alien talking – I felt it a fitting title for the weird, abrasive sounds that the program creates.\n",
        "content": "\n\n    \n\n\n\nTechnologies Used\n\n\n  Visual Studio Code\n\n\nCode editor used to write and compile code. Seems to be ideal for the job, but relatively new. Opinion from from some experienced coders “I use …… - but you shouldn’t, if I started again I would use visual studio code”. It also has a live share function that may prove useful later when collaborating.\n\n\n  HTML\n\n\nfor the basic user interface (UI), including the buttons, the main picture and the placement of all the UI elements.\n\n\n  JavaScript (JS)\n\n\nThe bulk of the code and all action and sound based commands are run through JavaScript. This includes the creation of oscillators, and routing the audio signal (which is how the delay functionality is created). Also mapping of all controls – buttons, mouse and keyboard.\n\n\n  CSS\n\n\nUsed on the last day to add some beatification to the basic HTML. Was used for the “Reese da Alien” title and main background colour. A slider from the nexus library was also added.\n\n\n  Github\n\n\nUsed for sharing code, and where the repositories of this blog and the code will be stored and shared.\n\nTimeline\n\n\n  Day 1\n\n\nGot accustomed to JavaScript and basic HTML. Created two oscillators in visual studio code. Experimented with the provided code.\n\n\n  Day 2\n\n\nAdded functionality to the mouse movement, and set it up to affect one oscillators pitch and another’s volume. Gave the project the name “Reese da Alien” and found the background image. Experimented with audio FX with little result due to inexperience of routing.\n\n\n  Day 3\n\n\nAfter slow progress during the day, managed to organise the variables and routing on the code. Through routing added a delay effect to the oscillator mix (OSCMix).  Created a player looping an audio sample – routed to the output running parallel to the MainMix. Added a third oscillator as FM. Mapped the keyboard with hotkeys.js\n\n\n  Day 4\n\n\nAdded mouse functionality to the FM oscillator, and remapped the keyboard to use all of the lettered keys. Used CSS to improve visuals, and tweaked audio mapping variables and UI elements to get ready for the final presentation.\n\nThe Project &amp; Code\n\nOn day 3 when asking for help with my project I was told to possibly start from scratch – and pay attention to the routing and organisation of the signal flow. This helped me immensely. Although I didn’t start from scratch (í was determined to find the problem in the code)\nthe one point that I started to feel more comfortable coding was when I took control of the routing. One of the challenges I faced was that in the first couple of days in the workshop I was trying to achieve results by copying and pasting various examples of code – with mixed or no results. This technique of trial and error in the end helped me formulate my idea, and it was satisfying eventually (although stressful before!) once I worked out how to organise the routing of nodes properly. The original routing was simple like this:\n\n\n\n“Routing day 1 &amp; 2”\n\n\nThe aspect that I was confused about was the volume node. With experience in using DAW’s, I didn’t take volume to be a node, rather as a parameter that is attached to in this case, the oscillators. It is a small difference – but not knowing that it was a node in its own right confused me when I tried to expand on the project. The flow diagram of the project currently looks like this:\n\n\n\n“Routing day 3 &amp; 4”\n\n\nEvent - Talk Button\n\nMost of the functionality of this project is located in this event – as can be seen from the flow diagram it includes the first two oscillators, the delay and most of the routing.\n\nThe first action I took was to split the routing of the oscillators to various volume nodes (volume, volume 2, volume 3 and volume FM) so they are distinguishable from each other, and can be routed and affected differently if need be. I then combined the oscillator 1 &amp; 2 volume nodes into OSCMix, which was then routed to MainMix into the context.destination  or output. Although there is little reason to do this amount of volume nodes for the project in its current state, I wanted to have options for routing if needed – and it helped me visualise the signal flow.\n\n//oscillator.connect(volume);\n    oscillator2.connect(volume2);\n\n    volume.connect(OscMix);\n    volume2.connect(OscMix);\n\n    OscMix.connect(MainMix);\n    MainMix.connect(context.destination);\n\n\nDelay\n\nThe delay was used as an example in class, and I have lifted most of the code from there. It is created using just JavaScript and routing. The delay is only affecting one volume node, and is short time at 0.1 sec and at half intensity of 0.5. This combination I found to soften the sound as well as adding some richness – without sacrificing all of the harshness.\n\n// create a delay effect node\n    delay = context.createDelay();\n    delay.delayTime.value = 0.1; // lenght of the delay\n\n     // create a gain effect node\n    delayAmount = context.createGain();\n    delayAmount.gain.value = 0.5; // amount of the effect\n\n\n // connect the different nodes\n    oscillator.connect(delay);\n    oscillator2.connect(delay);    \n    delay.connect(delayAmount);\n    delayAmount.connect(volume);\n    delayAmount.connect(delay);\n\n\nEvent - FM Probe (Fm Oscillator)\n\nOne of the aspects of this project I am proudest of is the addition of the FM Probe event that routes the third oscillator to the OSC2 frequency. This is done with the line:\n\n//volumeFM.connect(oscillator2.frequency);\n\n\nFirstly, the OSC FM is routed to VolumeFM – then that is routed to the frequency of OSC2.\nThis was because the first oscillators frequency was already mapped to the mouse movement, so now there would be ability to affect the pitch of both oscillators. This worked in conjunction with the mouse mapping:\n\n    if (playing == 1) {\n      //oscillator.frequency.value = x;\n      oscillator.frequency.value = widthnor*1000;\n      volume2.gain.value = 1 - heightnor;\n      oscilatorFM.frequency.value = heightnor*1000;\n\n\nThis shows the three mappings to the mouse. Osc1 pitch is affected by the x – axis. Osc2 volume (volume 2) is affected by the Y – axis. Lastly the OSC FM frequency is adjusted by the Y axis. The aim was to create an increasingly complex sound as the mouse moves around.\n\nEvent - Space (Audio Player)\n\nThis is an audio player that runs alongside the main patch, routed through volume 3 to the output. Although one of the undeveloped parts of the code, it allowed some creativity in adding recorded sound, and can be experimented further at a later date (a beat possibly?) It was adapted from the code given in SMC repository.\n\n    player = context.createBufferSource();\n    player.loop = true;\n\n    volume3 = context.createGain();\n    volume3.gain.value = 1;\n\n    player.connect(volume3);\n    volume3.connect(context.destination);\n\n    loadSound(\"alien.mp3\");\n\n\nKeyboard Mapping\n\nI found an online library called hotkeys.js that allowed me to map my keyboard. Found on Github. I declared which keys to use (list after the yellow “hotkeys”), and then below created a large list mapping each character key to frequency of OSC2. I used a webpage (link at bottom) as a reference to frequencies of notes, and mapped just over two octaves (not in a piano roll formation). This allowed for another way to affect the pitch of the patch – adding depth to the playability.  I found when used it overrode the mouse mapping of the volume. Not intentional or wanted, however I found when the mouse was off the screen this was reset – which could be used as a feature when played. I would prefer if there was a more elegant way of harnessing both these controls at the same time, an aspect to improve in the future.\n\nUI Elements - HTML &amp; CSS\n\nI wanted one centred, background image. I found this code online to centre the image using HTML, and found the alien picture I wanted and applied it in code, and the jpg same folder:\n\n    body, html {\n      height: 100%;\n      margin: 0;\n    }\n\n    .bg {\n      /* The image used */\n      background-image: url(\"alienwareplus.jpg\");\n\n      /* Full height */\n      height: 100%;\n\n      /* Center and scale the image nicely */\n      background-position: center;\n      background-repeat: no-repeat;\n      background-size: cover;\n\n\nCSS &amp; Beautification\n\nCSS was used for the title in the bottom right corner, separated in a CSS file. This code defined the font, border and colour, as well as the positioning permanently in the bottom right.\n\nposition: fixed;\nbottom: 0;\nright: 0;\nwidth: 300px;\nborder: 3px solid #73AD21;\n\n\nThe same technique is applied to the text, and the fader that went unused. The fader came from the nexus.js library, I was able to implement the fader, but not attach it to any command as of yet.\n\nOther tips I have learned throughout the workshop – organisation is key. Having events on the leftmost lane, and having its contents further tabbed on the right helped a lot in understanding the code, and made it easier on the eye at a glance. Writing notes and toggling line comment was picked up on the second day – but vital for any beginner.\n\nFuture Development\n\nThere are many ideas and avenues I feel this project could go down. I would like some more feedback on the project, but for now I have some thoughts on the directions it could go.\n\nOne of the issues of the patch is that not all areas of the screen sound “good”. This was alleviated by the inclusion of a delay – but I believe more FX could be added to sweeten the corners especially. These FX ideally would be sensitive to mouse placement, and programed in a way so that they would be applied only with the mouse on a certain part of the page. I installed Tuna.js for more FX, but did not manage to get it implemented in the code and ran out of time to troubleshoot.\n\nDuring day 3 I spent some time trying to implement canvas.js, after being inspired by another project. I managed to create a box, but quickly realised if I wanted the sound to be the focus it was a little out of my depth or timescale for the project. The idea was to create more interactivity in regards to the synth, more of a hybrid from the X/Y axis that the mouse mapping produced. Although I didn’t manage any meaningful results, I would like to explore this further. I would like the page to work well in a smart phone environment, and a canvas feature would aid in this I feel.\n\nTalking of visual components, having the alien pull various facial expressions during certain moments would be a nice touch, giving more character to the potential app. As I mentioned I would like the program to be smart phone friendly (have no experience in how that is implemented, but the concept) – using a finger rather than a mouse may be preferable for the final product.\n\nIf anyone would like to use or view the projects code it will be located at this repository for reference:\n\nhttps://github.com/cheeserage/ReeseAlien/tree/master/ReeseAlien\n\nSources used below -\n\nNote Frequencies\n\nhttp://pages.mtu.edu/~suits/notefreqs.html\n\nSMC Github\n\nhttps://github.com/axambo/sound-programming-workshop/\n\nCanvas.js\n\nhttps://canvasjs.com/\n\nNexus.js\n\nhttps://github.com/nexus-js/ui\n\nHotkeys.js\n\nhttps://github.com/jaywcjlove/hotkeys\n",
        "url": "/sound-programming/2019/02/11/Alien-Reese.html"
      },
    
      {
        "title": "SineWave Pad",
        "author": "\n",
        "excerpt": "It was a wonderful journey we had for a week getting hands-on experience with Web audio API and JavaScript. In the beginning, I was tensed about the way that I will handle coding with zero prior experience. But, at the end of the week, I was happy about what I have managed to achieve. I was lacking ideas to start a project for the week but after getting introduced to oscillators, I thought of making a synthesizer or a drum pad that works on the browser. So it was either to work with Oscillators or sound loops.\n",
        "content": "SineWave Pad\n\nHow it started\nIt was a wonderful journey we had for a week getting hands-on experience with Web audio API and JavaScript. In the beginning, I was tensed about how I will be able to handle coding with zero prior experience. But, at the end of the week, I was happy about what I have managed to achieve. I was lacking ideas to start a project for the week but after getting introduced to oscillators, I thought of making a synthesizer or a drum pad that works on the browser. So it was either to work with Oscillators or sound loops.\n\n\n\nWork Progress\n\nOn the first day, my first step was to set up an oscillator that works with a click of the mouse. This was a great way to understand how JavaScript works with HTML elements. Then I extended it to six oscillators and additionally, Volume up and Volume Down buttons to control the global gain of the Oscillators. I prefered to have “Sine” as my Oscillator Wave Type.\n\nOn the second day, I was trying to implement exponential and linear ramps to the oscillators to create smooth volume increment. However, it wasn’t working as I supposed and maybe I’ll come back to this In the future.\n\nOn day three and four, my workflow was to map the keyboard to start and stop the oscillators. And also to implement CSS into my code and make the synth look good and interactive. I was struggling for so many hours to map the keyboard to the buttons that I had in my interface. Then I got help from Eirik and I managed to grab some ideas from his code. I used a CSS and HTML code from code pen and tried to adapt it to my synthesizer to make it look more exciting.  I changed the color attributes to my preferences and tried to map the pads to the oscillators that I have built.  Additionally, I managed to use reverb impulse responses and include it in my signal chain to add reverb to the synth. I used the reverb.js library for that purpose.\n\nI was so much happy at the end of the week about my progress where I managed to build my own synthpad without having any prior knowledge at the beginning. I managed to understand the concept of using audio context and its methods to generate and change characteristics of sounds. Also managed to grasp the ideas about creating effects like delay and Ping-Pong delay with the code.\n\nWatch the video SineWave Pad\n\nChallenges\n\nIt was challenging to work with the timeline I had. I had to decide which elements I should be included and which not. I was struggling most of the time to make the interaction between the sound and the interface.  It was challenging to learn the concepts and adapt them to my own creation with limited time.I faced the most difficulties with mapping the HTML and CSS elements to work with javascript. I spend so many hours working to changes the colors of the pads with a keypress. Still, that is something I have to work on.  I was working on applying delay attribute to my oscillators but I wanted it to be controlled by a slider. The delay seemed working but the trouble was to map it to a controller. personally, i felt that I need more knowledge on working with CSS and HTML for mapping parameters and to place my elements on the page where ever I want.\n\nFuture Plans\n\nCurrently, I have plans to explore more musical possibilities with the oscillator sounds and also to implement envelops to make them sound smooth. and also to add a player with drum loops to make the application more fun to play. Additionally, I hope to experiment with different JavaScript libraries and add controllers to handle reverb and other parameters.\n\nClick to find the Code\n",
        "url": "/sound-programming/2019/02/11/SineWave-Pad.html"
      },
    
      {
        "title": "The Pointilator Sequence Synthesizer",
        "author": "\n",
        "excerpt": "The Pointilator sequence synth is an experimental instrument that can be played directly from a web browser! It is tested to work with Opera and Chrome, but does not work in Safari. It is based around entering a sequence of notes as points on a Canvas that registers each click and draws a circle where the note was put. It can then play back the notes from left to right with the height of the click translating to pitch. The result is a sequencing synthesizer that has a finely detailed scope in both time and pitch, although it is not easy to control based on traditional musical scales or rhythmic time.\n",
        "content": "\n\n\n\nIntroduction\nDuring the first week of the sound-programming workshop, we were tasked with making an individual project incorporating some of the skills and tools learned as we progressed through the week. Having had some experience with web technologies, HTML, CSS and JavaScript, I was quite confident, but not necessarily that sure of what I wanted to do.\n\nAfter experimenting a bit with the Web Audio API and how to achieve playback of audio samples, I went back to things I knew from before using the HTML5 Canvas element to draw and log clicks on a digital surface.\n\nThis led me to make The Pointilator sequence synth, which I will describe in further detail below.\n\nDescription\nThe Pointilator sequence synth is an experimental instrument that can be played directly from a web browser! It is tested to work with Opera and Chrome, but does not work in Safari.\n\tIt is based around entering a sequence of notes as points on a Canvas that registers each click and draws a circle where the note was put. It can then play back the notes from left to right with the height of the click translating to pitch. This is done by pressing the ‘Play’ button. The sequence will loop, and a ‘reset’ button can stop the audio by muting it and then reset the Canvas.\n\tFurther control is added by using controllers from NexusUI, a library for making graphical user interfaces for JavaScript. These include changing the values of three sliders for ‘Timescale’, ‘Envelope Level’ and ‘Note Length’. The ‘Timescale’ refers to the length of the sequence in seconds. The volume envelope of each note can be somewhat controlled by a ten-values multi slider that creates a curve for note’s volume over time.\n  The result is a sequencing synthesizer that has a finely detailed scope in both time and pitch, although it is not easy to control based on traditional musical scales or rhythmic time.\n\nI will come back to how this was achieved, but first I will provide a quick timeline for the project development.\n\nTimeline\n\n\n\n\n\nWhen I started out on the first day of the workshop, I worked on a sampler instrument that would use a Convolver node to create reverb, but this proved less inspiring than I had thought, and after just achieving sound and some improvements in the program structure, I abandoned this angle for the project.\n\nOn the second and third day, I worked on what would become The Pointilator, focusing mostly on the graphics and interactivity on day 2 and adding sounds on day 3. Much of the work was put into how the sequence of notes was generated and controlled.\n\nThe last day was spent updating the visuals of the instrument to reflect its somewhat refined state having gotten more control of the notes using Nexus UI.\n\nAchievements\n\nThe main achievements of the project as I see them are connected to how I was able to make the information from the Canvas element and clicks translate into a playing sequence of notes. The signal processing and sound of the instrument is however quite basic, being a collection of single oscillators per note. This gives the instrument a somewhat boring sounding timbre and audio quality.\n\nI will now give a more detailed description of some of the code to explain how each part works.\n\nClicks on the Canvas element are registered by the code and draws based on the coordinates. Since the vertical range of the Canvas starts at 0 at the top, this value has to be reversed.\n//Eventlistener for click on Canvas\nc.addEventListener('click', function(event) {\n  let x = event.pageX - c.offsetLeft, y = event.pageY - c.offsetTop;\n  //Draw dots\n  drawDot(x, y);\n// Add note event by click coordinate to array\nnoteEvents.push(new noteEvent(x/width, reverseRangeOfCanvasScale(y, 200, 2000)));\n\nfunction drawDot(x, y) {\n  console.log(x, y);\n  ctx.beginPath();\nctx.arc(x, y, 10, 0, 2 * Math.PI);\nctx.stroke();\n}\n\n\nA note event is then created as an object with the coordinates of the clicks and added to an array.\n//Note events\nlet noteEvent = class {\n  constructor(x, y){\n    this.time = x;\n    this.pitch = y;\n  }\n};\nvar noteEvents = [];\n\nThe array can then be iterated through using a for loop which generates the oscillators and sounds each time the loop starts.\n\nfunction scheduler() {\n  running = true;\nplayedOnce = false; // to make it work in no looping mode\nwhile ((nextLoopTime &lt; context.currentTime+0.1) &amp;&amp; (!playedOnce || loopingbool) &amp;&amp; !stopper) // Keep playing if loop.\n  {\n    for (event in noteEvents)  // play note events\n      {\n      playTone(noteEvents[event]);\n      }\n    loopStartTime = context.currentTime;\n    playedOnce = true;\n  }\n  timerID = window.setTimeout(scheduler, 100.0);\n}\n\n\nMore of the code can be found in my Github repository.\n\nChallenges\n\nMaking the sequencer play in the way I wanted to proved to be a challenge, because of the time-sensitive nature of audio programming.\nI mostly did not use prebuilt libraries to handle the scheduling of all audio events, which meant some things were happening asynchronously and before real-time. That meant adding notes while the sequence was playing would only manifest after the current loop was finished.\nI was however able to overcome this by calculating when the note should play based on the current loop’s start and when the Canvas was clicked.\n\nThe way I did it was however mostly a workaround than revorking how the scheduling basically functioned, which could have been better, but as I was working on it, I became somewhat intrigued by the limitations and challenges this entailed.\nAnother thing was that I locked myself in using basic elements from HTML and Web Audio compared to using premade GUI and audio libraries since I would have had to just redo a lot of what I had done to switch over to the libraries.\nThis might have been a poor decision since it made much of the programming more cumbersome.\n\nFurther development\n\nThere are still many bugs and issues with the instrument, but I am satisfied with what I achieved during the week. Further improvements could be made to the audio quality, scheduling of note events and control over pitch. Quantizing the pitch and time would also be interesting to explore, as well as functionality for editing and undoing steps added to the sequencer.\n\nThe working prototype is hosted at my NTNU web space and the source code is available at my repository on Github.\n",
        "url": "/sound-programming/2019/02/11/The-Pointilator.html"
      },
    
      {
        "title": "Catch the wave – First week's dive into web audio programming",
        "author": "\n",
        "excerpt": "It is possible to create simple, but effective applications on the web browser, even without prior knowledge. However, it took way longer to implement those ideas, but luckily there was always someone around to ask.\n",
        "content": "Getting familiar with some of the basic JavaScript syntax was the first day’s work.\nWe did small practices and learned about the interplay of different web technologies like\nHTML, CSS and JavaScript which shed some light. Apart from the nice things one can do with web audio as we\nlearned through different online examples the main possibilities for me have been opened by skimming some of the\npapers from the web audio conferences and the mini-presentations of some selected papers introduced by the fellow students.\nIt is possible to create simple, but effective applications on the web browser, even without prior knowledge.\nHowever, it took way longer to implement those ideas, but luckily there is always someone around to ask.\n\nMini-Logbook and achievments\n\nIt took some time to decide what I want to go for more in-depth since there are so many options.\nThe first day I tried to work with audio spatialization online. The code I found was pretty advanced and\nI did not want to spent to much time understanding it, so I rolled back to something more simple as a start.\nInspired by the tone.js and its examples on audio sound synthesis with sample files I got the idea to create something similar. Finally I chose to work with audio samples, uploading them and processing them on the web browser like here. I spent a lot of time finding out how to ask for content. Gradually things became more clear. Signal flow chart helped a lot to visualize what is happening.\n\n\n\n\n\n\nDifferent tasks require different workflows, to adapt within a different environment turned out to be useful. It was essential\nto start with templates and then move on and research more examples online and then copy paste different parts of the code\nelements. One of the main challenges was to not despair and keep sticking to the matter. The spaghetti code will eventually\nmake sense at some point. Of course in the long run skills like creativity and looking for unconventional paths are of use\nsince there are different ways to get to the same goal. And that works best with trial and error and letting yourself get inspired and motivated by more interesting things than you’re working at the beginning.\n\nMake the argument function\n\nEver returning questions were, how can I recognize the difference between a function, an argument and other means of code?\nWhen will I finally learn when to use what and why. It is recommended expecting to achieve the full spectrum of patience and endurance more than once. And it happens to not see the forest for the trees. I had to overcome that idea of doing something complex which blocked me at the beginning, in spite of the fact that it can get quite complex at some point. What it basically is as I realized equals more building abstract blocks and pieces, elements and trying to keep them together.\nThe good thing is with audio programming that you immediately are able to prove what you have been coding\nsimply by listening – is there any sound? It keeps you in the flow and motivates to explore further.\n\nWhat’s next?\nI hope to create something collaboratively in the group project and have the possibility to develop the project further and dive deeper.\n",
        "url": "/sound-programming/2019/02/11/Catch-the-sound-First-week's-dive-into-web-audio-programming.html"
      },
    
      {
        "title": "Odyssey",
        "author": "\n",
        "excerpt": "Odyssey is a simple prototype of a Web Audio API envisioned to immerse users into a misty jungle environment. Besides soundscape of a jungle, the application adds bits of flavour of few domestic animals and mix them all together with a piece of jazz music. The web audio application is developed using HTML5 and javascript.\n",
        "content": "\n  \n\n\nIntroduction\nOdyssey is a simple prototype of a Web Audio API developed as a mini project during the first week of workshop in the SMC4048 Audio Programming course. The application is envisioned to immerse users into a misty jungle environment. Besides soundscape of a jungle, it adds bits of flavour of few domestic animals and mix them all together with a piece of jazz music. It may not give you a profound immersion into the environment at the moment though. But, I am hopeful that you will enjoy and have fun playing with it. It is written in HTML5 and javascript and the sounds and picture are taken from freesound.org and selfgrowth.info respectively.\n\nKeywords: Web Audio API, JavaScript, HTML5\n\nHow to play Odyssey?\nThe start button triggers a soundscape from a jungle environment and fills in a jazz piece in about 10 seconds. But, you can also press the buttons named ‘Dog’, ‘Cat’, ‘Chicken’ or ‘Elephant’ to play their sounds at any time. Infact, there is no any particular sequence of pressing the buttons. So you can actually experiment with a permutation of 5x5! i.e. in 600 unique ways. However, you can repeat it as many times as you like 😜.\n\nWant to try my Odyssey? Check the Live Demo here.\nYou can also have a look at the code and related files in my github repository\n\nRouting of Different Audio Nodes\n\n  \n  \n\nThe application is built upon ‘example 3’ presented in the first and the third day of the Audio Programming workshop called ‘play sound’ and ‘delay node’ respectively. All of the sounds have been individually routed following the figure above except for the environment sound and the jazz piece, which have been directly sent to the output without delay or any kind of envelope.\n\nLearning from the individual mini-project\nIt was my first time working with HTML and javascript. With no doubt, their was a lot of frustration in the first few days of the workshop. I did have some experience working with MATLab where codes would normally follow line by line. But unlike MATLab, I experienced that the process in javascript is a bit different. Nonetheless, the overall structure of the workshop, environment of working everyone together and later in split groups made it comforting and possible to ask for help in no time. It was not so difficult for me to grasp the concepts presented by our teacher, Anna. Moreover, I am also lucky to have such a friendly and helpful peers both in Trondheim and Oslo who I could run to whenever I have questions and problems. I am so glad that I can do some code and create similar kind of web audio API. And I am hopeful to learn more and also incorporate some css in near future!\n\nFuture Plans\nI wish to make my Odyssey a bit more immersive by applying some binaural/ambisonic techniques and adding varieties of soundscape. Besides, to make it more interactive, features like mouse interaction can be applied that we exercised during Day 1 of the Audio Programming Workshop in ‘example 6’ called the ‘Mouse Interaction’. And ofcourse it would be intresting to add some fancy looking stuffs incorporating CSS.\n\nAcknowledgement\nMy heartfelt gratitude to Anna xambo for her valuable insights, guidance and patience in teaching us the basics of HTML5, javascripts, CSS and create a Web Audio API. Prior to the first week of workshop, I had zero knowlede in these areas. Besides, I would also like to thank my peers, Jørgen, Mari, Guy and everyone for helping me out with programming, giving suggesitions and sharing codes. Thank you all.\n",
        "url": "/sound-programming/2019/02/11/Odyssey.html"
      },
    
      {
        "title": "The Spaghetti Code Music Player",
        "author": "\n",
        "excerpt": "The Spaghetti Code Music Player is a simple music player that is loaded with one of my own tracks. The player allows you to play and stop the tune, turn on and off a delay effect and control a filter with your computer mouse. The player also has a volume control.\n",
        "content": "\n\nThe Spaghetti Code Music Player 2.2\n\nThis has been an intense week with a steep learning curve for me, but I have enjoyed it so much. HTML and web design was a huge hobby for me back in the 90’s when I was little, and this week I have been reminded of how fun it is to be creative with code. To try and fail, endlessly googling for solutions when nothing works, troubleshooting for hours and hours, getting no sleep. And the rush you get when it finally works!\n\nTake a look at the evolution of the Spaghetti Player\n\n\nThe Spaghetti Code Music Player is a simple music player that is loaded with one of my own tracks. The player allows you to play and stop the tune, turn on and off a delay effect and control a filter with your computer mouse. The player also has a volume control.\n\nResearch Journal, Mini Project, day 1\n\nI started by googling “Web Audio API” to get some inspiration for my project. I found the Web Audio API samples page with a collection of examples of Web Audio API applications.\n\nJust to get started, I quickly chose the filter sample and figured out that I could use this as a base for my own project. To gather the necessary files, I went into the sources of the web page and found the repository there. The repository contained a lot of different files, both js-files, an html index file, and a css file. I copied everything into my Visual Studio Code workspace and started on studying the files - to see what I could get out of it.\n\nThe code was rather complex, and the most of it was way beyond my level of javascript knowledge. But I started with the easier stuff that I could understand, like editing the colours in the css-file and some of the text in the html-file, and then I replaced the sound file from the original js-code with one of my own sound files.\nThen I figured out that I wanted to add another parameter in addition to the original filter parameter. I thought that adding a volume control would be an easy task to do. I copied the slider in the HTML-file, and then I was going to edit the js-code to make it actually work. For the same Web audio API samples page, I found a volume control.\n\nOne of the js-files was the same for these two applications (“shared.js”). I later figured out that this is the file where the the web audio object is applied. To make it work the way that I wanted, was not an easy thing; I used several hours after I got home that evening to actually get it work. But I also learned a lot during this process. When I included the gain-variable in the code, I think I from the beginning understood where to copy-paste it in the code, but some of the brackets were implemented the wrong way. The volume control did something with the sound, but it wasn’t really controlling the volume. Later that day, after a lot of trying and failing, I figured out that it was the routing on connecting the volume to the destination I had done wrong:\n\nI had just connected both filter and gainNode to destination, and that wasn’t working. But this way around worked:\n  source.connect(filter);\n  filter.connect(gainNode);\n  gainNode.connect(context.destination);\n\n\nIt might look like basics if you know Web Audio API from before, but for me, dealing with a lot of Spaghetti code, it was not that obvious before I actually noticed it. The spaghetti factor also gave me inspiration for the title of the player …\n\nBy the end of this day there were still some issues with my code:\n\n\n  when I turn off and on the filter toggle, also the volume control is also effected. I only want the filter effect to be effected by this toggle.\n -when i turn off and on the filter toggle, the volume control stops working.\n\n\nFor the next couple of days, I also wanted to try to add more parameters to my player, and also include more of what I had learned in this course so far.\n\nAt the end of the day, my player looked something like this:\n\n\nResearch Journal, Mini Project, day 2\n\nI quickly found out what was the problem with the volume control from yesterday. I used “if” and “else” to fix this problem:\n\n  };\nFilterSample.prototype.toggleFilter = function(element) {\n  this.source.disconnect(0);\n  this.filter.disconnect(0);\n  this.gainNode.disconnect(0);\n  // Check if we want to enable the filter.\n  if (element.checked) {\n    // Connect through the filter.\n    this.source.connect(this.gainNode);\n    this.gainNode.connect(this.filter);\n    this.filter.connect(context.destination);\n  } else {\n    // Otherwise, connect directly.\n    this.source.connect(this.gainNode);\n    this.gainNode.connect(context.destination);\n  }\n};\n\n\nI find that I’m learning new things all the time. Along the way, I am also taking an introductory course to JavaScript in a mobile app, that helps me a lot to learn the basics.\n\nThis day I used hours on trying to implement a visualizer in music player; the intention was to make it look like something like this:\n\n\n\nThe visualizer code was also taken from the Web Audio Samples Page, and this screenshot is taken during the endless trying and failing sessions when I tried to merge the visualizer code into my code. From the screen shot, it looks like it’s working, but when the visualizer was working, my effects didn’t work and vice versa. I couldn’t succeed with implementing the visualizer into my player, unfortunately, because coders also need to sleep. I did several attempts during the week and weekend, but it seemed unpossible.\n\nResearch Journal, Mini Project, day 3\n\nThe mission for this day has been:\n\n-try to clean up my code, build it from the beginning.\n\n-implement some things i’ve learned through the week\n\nToday I felt that I learned a lot about from the tutorial lecture about the nodes that I’d already been using. To enhance my understanding I aimed to build a new player with the same effects that I’d been using before. I started by adding the mouse move effect from the first day tutorial to be able to control the filter node with the mouse. This was working perfectly!\n\n        window.onload = function() {\n          this.addEventListener('mousemove', function(e) {\n          var x = e.clientX;\n          var y = e.clientY;\n          var width = window.innerWidth;\n          //adding height\n          var height = window.innerHeight;\n          //console.log(width);\n          var widthnor = (x/width); // 0-1 range\n          var heightnor = (y/height);\n          // console.log(widthnor*1500);\n          // console.log(heightnor*50);\n          //console.log(x, y);\n          if (playing == 1) {\n            //oscillator.frequency.value = x;\n            filter.frequency.value = widthnor*1500;\n            filter.Q.value = heightnor*50;\n          }\n          });\n        }\n\n\nSo from this day my project developed in a new direction, but it was definitely related to my previous player. The difference between the two players was that the previous one used sliders to control filter, the new one used the mouse. The new player also had a delay effect, controlled by on and off buttons.\n\nOne can say that I’ve almost started on building my code up from the beginning, but not entirely. I think I’ve learned a lot today, crossed a threshold perhaps. While yesterday I understood the if/else statement and was able to implement it in my own code, today I understood how to assign different buttons to different effects and make it work.\n\nI used a lot of time to try to make a volume knob to my “new” project. By this point, I still hadn’t figured out how, and I was wondering if I had to give it up and start on merging my two projects together to one project.\n\nThis was how my player looked by now:\n\n\n\n90’s nostalgia!\n\nResearch journal, Mini Project, day 4 and rest of the weekend\n\nThis was the last day of the first Audio Programming workshop week, and I had to try to finish my project for the presentation and also use some time to prepare my presentation. I decided on cleaning up the code of my “new” player and at least make the volume slider work. With step by step help from Anna, we finally made it! The problem was probably due to signal chain routing.\n\nDuring the weekend I have mainly been working on the css file and the design of the music player. I should also mention that I replaced the mp3-file with one of my own tunes, composed and produced by me. I have also been fiddling with the idea of replacing the music player with one or more oscillators, to make an instrument out of it instead of a music player. I could still use my filter and delay effect. It turned out that I didn’t have enough time during the weekend to realize this idea, even how much I wanted to try. I once again used too much time failing to make the visualizer work (I thought it would be a really nice feature to my player), with help from different tutorials (this one for instance). Why is it so hard!\n\nIn the .css, I changed the colourful theme with a black/pink/purple palette, and made the buttons look rounder. I also added some nice hover effects for the buttons and the slider:\n\n  button:hover {\n    opacity: 1;  }\n  button:active {\n    background: rgb(83, 8, 46); }\n\n\nI wanted my player to look good in browsers for both phones and computer, and to do that I used percentages instead of pixels for my measurements. Take a look at my container css for instance:\n\n#container {\n    margin: 0 auto;\n    width: auto;\n    max-width: 500px;\n    height: auto;\n    background: rgb(0, 0, 0);\n    padding: 20px;\n    border: 10px rgb(68, 7, 38) solid;\n    background-image: url(\"bg5.jpg\");\n  }\n\n\nThis is the container that my whole player is placed within. When I use margin: 0 auto;, width: auto; and max-width: 500x, I say that the margins of container should adjust to the window. But I chose a maximum-width of 500 px, to prevent it from looking enormous on web browsers. There were still some problems with the margins of the container. It looked symmetrical at this point, but I had placed the buttons in containers inside the container, and when I shrunk the browser window enough, the buttons ended on top of each other. Didn’t look good at all. I thought a solution could be to place the buttons inside of a table instead, but then it didn’t look symmetrical any more. Wow, what a dilemma:\n\n\n\nLater I took a close-up photo of my colourful sock and used that as a background. Then I used too much time on trying to make the text look good on top of the background picture. I could have used weeks on trying to make it look better, but I had to draw the line somewhere and declare the player finished, give it a try if you want: https://fractionmari.github.io/SpaghettiPlayer/. And here is the link to the gitHub repository: https://github.com/FractionMari/SpaghettiPlayer\n\nBy the way, this is how the player looks on a mobile phone:\n\n\n",
        "url": "/sound-programming/2019/02/11/SpaghettiPlayer.html"
      },
    
      {
        "title": "The Giant Steps Player",
        "author": "\n",
        "excerpt": "As part of the SMC master program we are being introduced to a variety of technologies for creating music and sounds. We have just finished a week long workshop learning about Audio programing and web audio API. The benefits of this technology are helpful and relevant in areas like art, entertainment or education. We were introduced to several ways for creating and manipulating sound, follow tutorials and experiment on our own during the days. I must admit that I do not have intensive knowledge in programing in general and javaScript in particular. Many failures accrued while trying, from simple syntax errors to flawed design. But understanding the idea behind each process and striving towards the wanted result was an important progress.\n",
        "content": "Background\nAs part of the SMC master program we are being introduced to a variety of technologies for creating music and sounds. We have just finished a week long workshop learning about Audio programming and web audio API. The benefits of this technology are helpful and relevant in areas like art, entertainment or education. We were introduced to several ways for creating and manipulating sound, follow tutorials and experiment on our own during the days. I must admit that I do not have intensive knowledge in programing in general and javaScript in particular. Many failures accrued while trying, from simple syntax errors to flawed design. But understanding the idea behind each process and striving towards the wanted result was an important progress.\n\nFor some unknown reason, I managed to get stuck on a tutorial from our second day. The tutorial was dealing with timing of an oscillator events. I wrote the melody notes of John Coltrane’s tune ‘Giant Steps’ with this frequency to midi note converter.\n\nLater on, I have tried to develop this into a player that will play the same melody with an option to choose the waveform (Sine, Sawtooth, Triangle and Square). I wanted to add a pre-recorded percussion soundwave that would sync with the melody of each oscillator. And of course, the last but not least, a functioning stop button that would let me stop this mad experiment.\n\n\n\n\n\nCode Rundown\n\nThis is a short description of the code. A much more detailed description can be viewed within the code itself which is available at my Github repo.\nI added the Tone.js to my source code and created the buttons in the body of the page. Later, I have added a CSS file to set the style of my document. I Added the variables for context, Audio buffering for the audio file, 4 oscillators and 4 players. In the stop button function I’ve used an if statement to ensure the stop of all oscillators playing at any time. Setting the initialization of the audio context to include the buffer initialization, decoding the audio data and initializing the player was not an easy task but thanks to friends, teachers and students I managed to do so. Next came the actual melody set awkwardly with parameters for frequency value (instead of note names) and time value in seconds (instead of using a transport time). This meant that the player plays the melody in 120 bpm. Four Oscillator buttons were then created with the functionality of initializing the player and oscillator together.\n\nWhat I’ve Learned\nBesides the fact that almost everything was new to me, the most important points that will continue with me are:\n\n  It is extremely important to plan your design before jumping into implementation mode. Sketching it out on a piece of paper is a helpful way.\n  Learn how to find your mistakes in the code, use the console.\n  Don’t be afraid to (miserably) fail, it’s part of the process.\n  Keep your friend close but your programmer friends even closer.\n\n\nResources\nDuring the week I was introduced to many great sites, tutorials and videos which really helped me understand the potential of Web Audio and the implementation of objects within my code. Using the web-browser’s view source option is another helpful tool for figuring out the coding behind many of the available examples online.\n\nFor Web Audio API I recommend checking out the API itself and this Web Audio API tutorial.\nThis JavaScript website was very useful as well as this site for the Tone.js API, examples and demos.\n\nWork in Progress\nNow that my giant player is working properly I can continue experimenting with applying filters, a fancier UI with NexusUI.js, learning how to use notes instead of frequencies, change the way I handle the timing and scheduling of the melody and add a change tempo slider. The Giant Step player in itself is a non-interesting piece of code in my opinion, and I would have loved to invest more time in creating a more interactive system with a much more interesting result. I am happy that I got introduced to the Tone.js library because I could utilize this library together with several Max/MSP patches I am currently working on.\n",
        "url": "/sound-programming/2019/02/11/Giant-Player.html"
      },
    
      {
        "title": "Convolverizer",
        "author": "\n",
        "excerpt": "Convolverizer, Real-time processing of ambient sound, voice or live instruments, utilizing the convolution effect.\n",
        "content": "\n\n\n\nIntroduction\nReal-time processing of ambient sound, voice or live instruments. We are several string instrument players and sound artists, but would like to prototype something modular that extends our artistic expressions, which is always at hand and instantly available. Create hidden gems that don’t require additional expensive equipment. Get your laptop, convolve it, GO!\n\nDevelopment team, why are we doing this:\n\n  Eirik: wants to do/learn something new\n  Sepehr: likes to extend his guitar and other sonical productions\n  Karolina: has little space in her room and therefore wants a fancy mobile application\n  Shreejay: loves live visualization\n\n\nTechnologies used\nWorking tools\n\n  Visual studio code\n  p5.js\n  Zoom\n  Discord\n\n\nPerformance\n\n  Sound card / audio interface\n  Guitar\n  Shure SM57 Microphone\n  Prototype of our Code\n\n\nResearch and development journal\nDAY 1\nOn the first day, all groups have formed up, different from other groups our final product was not 100 percent defined. We had a working idea, developed a concept and agreed on an application model:\n\n  Two independent audio sources that influence each other in real-time\n  Mobile application to extend the prototype in another environment\n  Having a modular application that is accessible which does not require a bulk of technological hardware and software\n\n\n\n\n\n\nSince the level of programming expertise was more or less equally low distributed throughout the group, we left the division of the roles open. Sepehr suggested right after we formed as a group to work with the p5.js library, and showed some examples. Shreejay again suggested going for web audio API. Karolina and Eirik assisted in research for both ways. We agreed to try building the model with the Web Audio API library first.\n\nDAY 2\nDuring our research, we realized that we would have to trim our concept a little bit more and take it step by step. We were struggling to get two audio inputs as two separate channels using Web Audio API. After that, we thought about recording one input and obtain the convolution reverb from the recorded piece. Finally, the idea was to mix the resulting output with another live audio input and send it to the main output. We spent a lot of time to find the right code snippets to make the convolver listen to the microphone. Soon after we found a code that gets the media stream, enabling audio and video. We then manipulated the code so as not to include video.\nIn the first feedback round, Anna recommended us to build a more modular system for situations where the live input cannot be activated, and would then activate a pre-recorded audio sample.\n\nDAY 3\nWe took the advice to heart and divided the workload into sub-tasks:\n\n  Getting the sound from the mic and having the sound file as the convolver input\n  Getting the sound from the mic recorded and save it → being implemented on the real-time mic\n  Load the sound file and implement it on another sound file (as an effect)\n  Creating visuals with p5.js or CSS\n\n\n\n\n\n\nWe were a bit overwhelmed by the complexity of our initial idea and the backup solution we wanted to implement proved to be a project in itself. Shreejay suggested going for something else instead, to simplify things and would make it suitable for our level of expertise. In the end, we all agreed to continue our project but within the p5.js library, step by step. We were reassured it would make things easier since many examples were given and we would only have to find out how to connect them in the code. We considered at some point to work without the mic and only load two sounds that could be used as an effect on one another. We agreed on two scenarios for the application, each divided into different sub-steps. Sepehr and Eirik worked on fusing the impulse response to the live input and Karolina and Shreejay worked on integrating the recording function.\n\nDAY 4\nAfter we had some guided cleaning of the code and more guided coding sessions, we were able to finish the prototype and make it work both visually and sonically. In the process, we replaced the idea of using live recorded sound with a preloaded soundfile. Fig 3 shows the final audio signal flow diagram of the prototype. We were able to perform together with almost all developers and everything went well, fortunately. You can read about our performance below.\n\n\n\n\n\nPerformance\nThe performance was set-up in Trondheim. We used a guitar and a microphone for the live input. As far for the guitar, the notes played began with low notes and gradually changed to high notes. In our experience, high pitches can lead to a better result for this convolution effect.  In the convolution process, the low notes are overemphasized and produce a muddy sound profile. Therefore the sounds produced by the microphone were lost in the mix and could not be convoluted properly.\nAlso, sustained notes should let the effect be more prominent, since the sound coming out of the convolution process goes on in a loop. Using sustained notes could lead to more control over the generated sound. Although, if anyone is interested in a chaotic atmosphere, there would be no limits to the playing style. Also, that could potentially involve any other sound sources as well, not only instruments.\nSince the sound that we used for convolution was an intense Drums solo, the dynamics of the effect were pretty intense as well. This has led us to a clearer and more visible convolution effect. Therefore, in any case, if you intend to have an intense and very dynamic convolution effect on the input sound; try to use a more vivid and high dynamic source for the convolution as well.\n\nHowever, we missed the opportunity to make our performance (including positioning us within more camera resources, a clear announcement of the start of the event) more demonstrative for lack of time, which ultimately affected our performativity. In the feedback round the audience reported a glitch in sound during the performance, we suspect that in connection with the process of buffering and dry gain. In addition, the connected (old) computer was no longer in good condition during the performance after a few hours of coding. When we performed again for the live demo, which you can see below, the glitch was almost not audible anymore.\n\n\n\n\n  \n  Your browser does not support video tag.\n\n\n\nWorking Style\nResearch, design and programming were carried out in collaboration so that the prototype developed in a constant discussion and joint evaluation of the application. We established a main hub in one of the group rooms and put the code from Visual Studio Code on the screen. From there, we brainstormed and prototyped together.\nThe strategy we used has the advantage of being a very open and collaborative working style. With our group dynamics in terms of previous knowledge, we could risk that one person did most of the work and the others did less. With a shared screen and open dialogue, we prevented that.\n\n\n\nThe Team Working in the Group Room \n\n\nDecoding Convolvorizer\nThe code of Convolverizer has mainly two parts. First, the toggle button operation and slider function, which provides the user to control the application. Second, the role of canvas and creation of live visualization. We have tried to comment on each line of the code so that you could easily figure out what is what. You can check the full code with relevant files and libraries in the github repo.\n\n// Part I - Eventlistener for click on Canvas\n    function togglebutton() {\n      if (playing == 0) {\n        // start getting live audio input\n        mic = new p5.AudioIn(); // this function is setting mic as mic input\n        mic.start(); // mic input can now play sounds\n        cVerb = createConvolver(\"recorded/drumsolo.mp3\"); // This function takes the Impulse Response\n        // from the soundfile and uses it to recreate sound of that space. The sound is convolved\n        // with different impulse response every time the start/stop button is pressed.\n\n        mic.connect(cVerb);\n        mic.disconnect();\n        cVerb.process(mic);\n        playing = 1; // stating that its now playing\n\n        // Following 3 lines are for visualization\n        // Fast Fourier Transform (fft) function analyses individual audio frequency in a waveform\n        fft = new p5.FFT(0.9, 1024); // 0.9 refers to smoothing &amp; 1024 refers to bin array length.\n        w = width / 350; // w = width of each rectangle in the visualization\n        fft.setInput(cVerb.process(mic)); // cVerb.process(mic) is being used for visualization\n\n      }\n      else if (playing == 1) {\n        // stop playing sounds\n        mic.stop();\n        cVerb.disconnect();\n        playing = 0;\n        start = 1;\n      }\n    } i\n\n\n// Part II - Drawing Canvas &amp; Creating Visualization    \nfunction draw() {\n\n    if (playing == 0 &amp;&amp; start == 1) {\n    //show the image when stop button is pressed\n    drawimage(); // it is a function defined in other part of the code which simply shows the image on canvas\n\n    } else if (playing == 1) {\n    //stop showing the image and start the visualization\n    createCanvas(1275, 350); // created fo the panasonic monitor in the group room at Trondheim campus\n    background(255); // white background\n\n    var val = slider.value();\n    valnorm = val / 100; // defining intensity of the slider\n    cVerb.drywet(valnorm); // setting the slider as the drywet gain controller\n\n    noStroke(); // setting lines of each rectangles as normal\n    var spectrum = fft.analyze(); // define spectrum as array of amplitude values across the frequency spectrum\n    //console.log(spectrum.length);\n\n    for (var i = 0; i &lt; spectrum.length; i++) { // loops for i = 1 to spectrum length\n      var amp = spectrum[i]; // gets amplitude of each spectrum\n      var y = map(amp, 0, 255, height, 0);// defining value of y-coordinate as amplitudes\n      rect(i * w  , y, w - 2, height); // rect(x-coordinate, y-coordinate, width, height)\n      fill(255, i, 180); // filling each rectangle with color\n\n      // x-axis refers frequency from low in the left to high to the right\n      //y - axis refers to the aplitude\n    }\n      }\n }\n\n\nDesign and interactivity\nWhen it comes to design and interaction, the most important thing is that the user interface is easy to use and understand. Therefore one button for starting and stopping the process would do the job. Also, we have the visualization in the middle of the page to have the visual balance maintained. To regulate the level of the convolution effect, we’ve added a slider next to the Start/Stop button to give the user more control over the effect.\n\nChallenges\nGroup challenges:\nOur group was put together of people who did not have any previous knowledge in programming prior to joining the SMC program. This proved to be a very big challenge since we had to research the very basics of the programming syntax for us to be able to progress. Programmers talk about “Languages” when referring to the syntax that is used when writing computer programs. We had to Talk when we, in all honesty, struggled to stutter.\n\nConceptualizing and deciding on the final product:\nOur plan was always to use the Convolver, but how to actually achieve it was much discussed within our group. We first wanted to run a live audio signal into the convolver (e.g. voice), and make that affect another audio signal (e.g. guitar). We found out that it was hard, and for this prototype, we decided to use a recording of a Drum solo as the audio input that the convolver would be fed with.\n\nFamiliarizing ourselves with Libraries and code:\nWe decided to use p5.js as an additional library. That involved working with a new environment. This was a bit challenging, but luckily there are a lot of videos and tutorials online that are tied to the p5.js community.\n\nAchievements\nAll in all, the most significant achievement of ours is the gained exposure and understanding of coding in music. We were hitting a near vertical learning curve these two weeks, and to end up with a working prototype after this course is very pleasing. There was a bit of frustration tied with the difficulty of the task we had at hand, but through that, we managed to have a good working relationship and good teamwork. We also achieved a greater understanding of the use and implications of technologies used in coding, and also with the p5.js Library.\n\nLesson Learned:\nWhile aiming to get two live audio input from two separate channels, we noticed that both p5.js and Web Audio API do not have functions in order to support two parallel inputs or more. For example, by using the MediaStreamAudioSourceNode &amp; getUserMedia functions, it is only possible to get one audio and one video input channels. Similarly, by using the p5.js AudioIn &amp; mic.start() functions, it is only possible to get one audio input channel.\n\nFuture development / What is next\nThe next step would be developing the whole project for different platforms. It is a Web Audio API project, browser-based and we can use it the way it is in a smartphone right now. But with further development, which is going to be mentioned further on, it would be more challenging to present it on smartphones as well. Also, since none of us in the group is a professional developer, we didn’t manage to reach our ideal result. As it was mentioned, below are the future steps and prototypes that we may want to achieve later: \nDeveloping the project so that the user can choose between several different sounds, so the project can use the selected sound to create a convolution effect.\nDeveloping the project in a way that it would record the surrounding or any sound source that the user intends to record; save it and use it as the source for convolution process for the live input.\nRegarding the first option, it is not going to be a challenging matter. But when it comes to the second option, since the project should be able to save the recorded sound in a server and then download it and use it as the convolution source, the whole process gets more complicated. In other words, we have to develop the server side code in order to achieve this goal; and that requires more practice and experience in developing.\nIn terms of interactivity, we overall we tried to create a simple, user-friendly interface, in order to create a more pleasurable experience for the user. The next step is to make it adaptable for various devices and screen sizes.\n\nAcknowledgement\nOur heartfelt gratitude to Anna Xambo for guiding us all through the process from Day 1 to Day 4. Thank you so much for your support and patience which helped us learn and practice coding in Web Audio API, Html, javascript, CSS and p5.js. Similarly, thanks to all our peers and audience for their constructive feedbacks and suggestions.\nAlso, we would like to take this opportunity to thank Hamed Kazemi wholeheartedly for heping us to solve and own his code for our complex plan shown in Fig.2 above. His creative code could record live audio input in cache memory and rout it as an input to the convolution process using Node.js. Following live demo shows the collaborative work with Hamed:\n\n\n        \n        \n\n\n\n\n And last but not the least, a big thanks to shiffman from\n The Coding Train\nfor sharing his huge collection of tutorials on YouTube. The\nSound Visualization tutorial in particular,  gave us insights in creating a similar kind of visualization for this project.\n\n\n",
        "url": "/sound-programming/2019/02/19/Convolverizer.html"
      },
    
      {
        "title": "The Magic Piano",
        "author": "\n",
        "excerpt": "During our second week learning about Audio programing and web Audio API we were divided into groups and had to come up with an idea for a final project. The main challenges were to find an idea that is doable within 4 days, to code collaboratively and to prepare for the presentation of our project. Guy had an Idea for building a piano keyboard that will help beginners play a simple melody and Ashane and Jørgen agreed to collaborate and join forces in creating ‘The Magic Piano’.\n",
        "content": "\n\nDuring our second week learning about Audio Programming and Web Audio API we were divided into groups and were asked to come up with an idea for a final project. The main challenges were to find an idea that is doable within 4 days, to code collaboratively and to prepare for the presentation of our project. Guy had an Idea for building a piano keyboard that will help beginners play a simple melody and Ashane and Jørgen agreed to collaborate and join forces in creating “The Magic Piano”.\n\nThe Idea\n\n\"I was inspired by my daughter's attraction to music when I came up with the idea for the magic piano. I remember asking her to play a melody on the piano for a song she knows well and can sing in pitch. My daughter does not know how to play the piano, but when she tried to perform the task she managed to play the right melodic rhythm (but with the wrong notes). That sparked the idea for creating a piano that will play the right notes of the chosen melody regardless if the player hits the right or wrong key.\"-Guy\n\nWhy Web technologies?\n\nBuilding the Magic Piano using web technologies has several advantages that fit our purpose. The application is available for anyone with internet access, and there is no need for installation and no dependence on a specific operating system. All you need is a web browser, speakers and a MIDI controller to start playing. After a short discussion, we came up with additional features that we would like to implement:\n\n  Choosing a song from dropdown menu.\n  Adding a simple player with Play and Stop buttons to hear the melody.\n  Highlighted piano keys showing the next correct note of the song.\n  Having the option to scale the app for various screen sizes.\n\n\nTimeline\n\n\nDeveloping the Magic Piano\n\nMIDI Messages\n\nAs this project was to be developed using web technologies, we wanted to use some libraries to make the build process easier and faster. We decided to use NexusUI’s piano keyboard interface, and the framework Tone.js to generate sound. We knew we wanted to trigger sound using a MIDI-keyboard, and luckily for us, our teacher Anna Xambó had already provided us with a JavaScript code snippet which accepts MIDI-data. We could then create a function which handled this data, to be used for triggering a note of a melody. See the code snippet below.\n\n//if (navigator.requestMIDIAccess) {\n  navigator.requestMIDIAccess({\n      sysex: false // this defaults to 'false' and we won't be covering sysex in this article.\n  }).then(onMIDISuccess, onMIDIFailure);\n  } else {\n  alert(\"No MIDI support in your browser.\");\n  }\n\n  // midi functions\n  function onMIDISuccess(midiAccess) {\n  // when we get a succesful response, run this code\n  console.log('MIDI Access Object', midiAccess);\n      // when we get a succesful response, run this code\n      midi = midiAccess; // this is our raw MIDI data, inputs, outputs, and sysex status\n\n      var inputs = midi.inputs.values();\n      // loop over all available inputs and listen for any MIDI input\n      for (var input = inputs.next(); input &amp;&amp; !input.done; input = inputs.next()) {\n          // each time there is a midi message call the onMIDIMessage function\n          input.value.onmidimessage = onMIDIMessage;\n      }\n  }\n\n  function onMIDIFailure(e) {\n  // when we get a failed response, run this code\n  console.log(\"No access to MIDI devices or your browser doesn't support WebMIDI API. Please use WebMIDIAPIShim \" + e);\n  }\n\n\nThe Melody Player\n\nBefore adding a function which would play through a melody with each piano keystroke, we branched out from the main idea, and started developing the functionality of playing through a melody using a play and stop button. Adding the player function would give the user the possibility to listen to the song before playing it. This feature would be very helpful for beginners who are not familiar with the song or just would like to refresh their memory.\n\nFrom MIDI to JSON\n\nAs we were going to use the same melody for playback as well as for the user interaction, we first came up with an idea to play through a MIDI-file. It ended up being challenging to get JavaScript to handle MIDI-data. We tried to implement a library called MIDIPlayerJS to get it to work, but the library itself seemed to have some fault, preventing it from working.\n\nAs JSON (JavaScript Object Notation) is a file format used for storing and transporting text data, and we knew it was easy to handle for JavaScript, we started looking for ways to convert MIDI-files to JSON-files. It didn’t take long before we found an easy way to convert, using a website called Visipiano. Using Ableton, Guy created the MIDI files for the two songs we were about to use (ABCD and Alle Fugler). The only thing we had to do, was to drag it into this website, and then download it as a JSON-file, ready for implementation.\n\nExtracting JSON Data\n\nThe code below shows 32 of 321 lines of the JSON-file containing all the data needed for playing the ABCD-song. The syntax is identical to JavaScript code for creating objects, which makes it easy for JavaScript to work with. The file is constructed in an object, with more nested objects, where every object has key-value pairs. In the code snippet you can see the key called “notes”, which has an array of all the note information of the melody as its value.\n\n//{\n\"header\": {\n  \"PPQ\": 96,\n  \"timeSignature\": [\n    4,\n    4\n  ],\n  \"bpm\": 120,\n  \"name\": \"ABCD\"\n},\n\"startTime\": 0,\n\"duration\": 24,\n\"tracks\": [\n  {\n    \"startTime\": 0,\n    \"duration\": 24,\n    \"length\": 42,\n    \"notes\": [\n      {\n        \"name\": \"C5\",\n         MIDI\": 72,\n        \"time\": 0,\n        \"velocity\": 0.7952755905511811,\n        \"duration\": 0.5\n      },\n      {\n        \"name\": \"C5\",\n         MIDI\": 72,\n        \"time\": 0.5,\n        \"velocity\": 0.7952755905511811,\n        \"duration\": 0.5\n      },\n\n\nThe code below shows how the JSON-files are retrieved with an XMLHttpRequest and parsed into a JavaScript object. We’ve chosen to loop through the notes, and put the MIDI-values, note names, note durations and start time of each note, to arrays. We did this to make the workflow later in the process “easier”. On the first line, you can see that we build the URL based on “songSelect.value”, and this value is chosen by the user with a dropdown menu.\n\n// url = \"sounds/AlleFugler.json\";\n  var ourRequest = new XMLHttpRequest();\n  ourRequest.open('GET', url);\n  ourRequest.onload = function(){\n  var songChoice = JSON.parse(ourRequest.responseText);          \n  for(var i = 0; i &lt; songChoice.tracks[0].notes.length; i++){    MIDINotes[i] =  songChoice.tracks[0].notes[i] MIDI;            noteNames[i] =  songChoice.tracks[0].notes[i].name;\n  noteDurations[i] =  songChoice.tracks[0].notes[i].duration;\n  noteStart[i] =  songChoice.tracks[0].notes[i].time;\n  }\n  piano.toggleKey MIDINotes[currentNoteIndex], 0); //Toggling color of first key to be pressed, to show user where to begin\n  };\n  ourRequest.send();\n\n\nThe Dropdown Menu and the Melody Player\n\nThe dropdown menu was made by using the NexusUI library with this function:\n\n//var songSelect = new Nexus.Select('song',{  \n'size': [200,30],\n'options': ['AlleFugler','ABCD'] //List of sounds to chose from\n});\n\nsongSelect.size = [200,30];\nsongSelect.colorize(\"accent\",\"#ffd106\");\nsongSelect.colorize(\"fill\",\"#ffd106\");\n\n\nThe code below shows the function for playing back the melody to the user when the user presses the play button. We receive the arrays made from the chosen JSON-file and loop through them, making several instances of  “synth.triggerAttackRelease”, which is a method from the Tone.js library, for triggering a synth.\n\n//function PlayMelody(midiNotes, noteNames, noteDurations, noteStart){              //Now we just need some Tone.js magic where we can enter these values and let it play :)\n    var now = context.currentTime;\n\n    for(var i = 0; i &lt; noteNames.length; i++){\n        synth.triggerAttackRelease(noteNames[i], noteDurations[i], now + (noteStart[i])); //noteStart[i]\n    }\n}\n\n\nThe synth that is triggered is defined as you can see in the picture below.\n\n//synth = new Tone.Synth({\n        oscillator: {\n          type: 'sine',\n        },\n        envelope: {\n          attack: 0.001,\n          decay: 0.8,\n          sustain: 0.2,\n          release: 0.1\n        }\n\n\nHaving the function this way caused a problem with the melody not starting at the beginning every time we pressed the play button. This is because the audio context clock starts when the page loads, and when we say with our “noteStart”-array, that the first note should start at 0 time, 0 has already past. We made an attempt to offset the melody with “now = context.currentTime”, but this did not work. (We will come back to how we fixed this, later.)\n\nMelody Triggering with MIDI Input\n\nTriggering the melody with each MIDI key stroke was a bit easier than playing it back, since we did not have to account for the clock. As you can see in the code below, it is done in a similar fashion, but here it was important to make it loop. That is why you can see line with the code: “(currentNoteIndex + 1) % noteNames.length”. By Using modulo (%), it will go back to index 0 when it has counted to the length of the array (ex. 42%42 = 0). One more addition, is the “piano.toggleKey” which lights up the next correct key to be played on the piano.\n\n//function o MIDIMessage(message) {\ndata = message.data; // this gives us our [command/channel, note, velocity] data.\nif(data[0] != 254){\n    //console.log( MIDI data', data); // MIDI data [144, 63, 73]\n    if(data[0] === 144 &amp;&amp; weLikeItMono == true){    //Preventing multiple key presses with weLikeItMono\n        TriggerMelody MIDINotes, noteNames, noteDurations);\n        weLikeItMono = false;\n    }\n    if(data[0] == 128){\n        weLikeItMono = true;\n        removeColor MIDINotes); //Remove color of keyboard when note off message comes\n    }\n  }\n}\n\n\nHere you can also see how we are running the TriggerMelody-function on each piano key press (144 means “on”, 128 means “off”), and running removeColor on key release, which is a function we made to toggle the color off. We had a problem with our TriggerMelody-function being triggered by multiple simultaneous key presses, but we later removed this by adding a Boolean called “weLikeItMono”. Then a note-off message has to be registered before another note-on message is received.\n\nKilling Bugs\n\nAs you can see below, some changes had to be made to make the playback-function better. We chose to use a scheduler method from the Tone.js library, called “Transport”. This abstracts away audio context time, and makes it possible for us to always start at 0. “Tone.Transport.scheduleOnce(play, noteStart[i])” triggers the function “play” on each value of “noteStart[i]”. This made the melodies play back correctly…most of the time. There is a bug somewhere, sometimes making the notes play in the wrong order. If you refresh the website and try again, it is gone. We don’t know why, but that will be part of the research going forward. At least it works most of the time!\n\n// function PlayMelody MIDINotes, noteNames, noteDurations, noteStart){              \nvar currentNoteIndex2 = 0;  // Start from 0 each time PlayMelody is called\n\nfor(var i = 0; i &lt; noteNames.length; i++){\n    Tone.Transport.scheduleOnce(play, noteStart[i]);    //Using Tone scheduler to trigger the function \"play\" on the times from chosen song\n}\nTone.Transport.start();                                 //Abstracting away audioContext time, and always playing from when Tone Transport is played\nTone.Transport.bpm.value = 120;\n\nfunction play(time){\n    synth.triggerAttackRelease(noteNames[currentNoteIndex2], noteDurations[currentNoteIndex2], time);\n    currentNoteIndex2 = (currentNoteIndex2 + 1) % noteNames.length;\n    if(currentNoteIndex2 == 0){                                         //This is for making it possible to press play after song is finished, not having to press \"stop\" first\n        Tone.Transport.stop();\n        Tone.Transport.cancel();\n        connect = false;\n        playing = false;\n    }\n  }\n}\n\n\nThis solution enables the user to start the song again when it is finished playing, without having to press the stop button first. The code for the start and stop buttons are at the end of the “app.js” script in the GitHub repository. Please have a look at the code if you want to see how it all fits together. We have left comments on most parts of the code to make it more understandable.\n\nFuture Development\n\n\n  Fix the bug that sometimes causes the melody to not play in a correct sequence.\n  Fix the bug that causes clicks on “Alle Fugler”.\n  Improve web usability:\n    \n      Add option for changing between different languages.\n      Make it more intuitive to use.\n      Improve scalability to different devices/screen sizes.\n    \n  \n  Add possibility to play on the graphical piano by click or touch (as suggested to us during the Q&amp;A session by Alexander Refsum Jensenius).\n  Add additional songs to the library.\n  Add functionality for the user (or parent/teacher of the user) to upload own JSON-file with a melody. (Thanks to Anna Xambó for the suggestion). Since there are more websites with MIDI-files, like Bitmidi, we should let the user upload a MIDI-file. This will require adding a script that converts MIDI files to JSON.\n  Add different instrument sounds (currently just a sine wave oscillator).\n  Add function where the user gets notified if they press the wrong note (as suggested by Anna Xambó).\n  Add notification popups if user presses correct notes many times in a row, to givesupport and motivation.\n  Optional scoring system, where the user gets a score overview when done playing (ex. 30 of 42 notes correct).\n\n\nThe Workflow\n\nWe worked in a collaborative way where we share screens with each other and work on the same document and files. We set up the VS Live Share to view the same code in real time and to discuss the code. We used GitHub for sharing the code among us when we worked offline. We used Zoom to communicate and share screen.\n\nJørgen Nygård Varpe - Collaborative Coding, UI Design, Troubleshooting.\nAshane Silva - Collaborative  Coding, UI Design, Troubleshooting.\nGuy Sion - Collaborative  Coding, Max/MSP Demo, MIDI Files, Troubleshooting.\n\nUser Demonstration of The Magic Piano\n\n\n\nFinal Reflections\n\nWe have learned a lot during these two weeks of workshop in the Audio Programming course. We have gained experience in several web technologies, like HTML, CSS, JavaScript and additional libraries. Collaborative coding was a great challenge, but we feel like we managed to have good workflow as a group. Our idea for making an educational tool for beginners to learn playing a simple melody on the piano, has developed into a prototype that we are proud of.\n\nThanks\n\nWe would like to thank Arthur Hureau who was with us on day 4, observing how we worked, helping with making the design of the piano responsive, and keeping an eye on the clock, reminding us to go for lunch!\n\nWe would like to thank Anna Xambó, Alexander Refsum Jensenius, Kristian Nymoen, Anders Tveit, Daniel Buner Formo for great feedback and support. A last thanks to our great classmates for making it a fun workshop!\n\nPlease visit our GitHub repository and try out our Magic Piano!\n\nGitHub repository\n\nThe Magic Piano\n",
        "url": "/sound-programming/2019/02/22/The-Magic-Piano.html"
      },
    
      {
        "title": "Touch the Alien",
        "author": "\n",
        "excerpt": "The web audio synth ‘Touch the Alien’, a project by Eigil Aandahl, Sam Roman, Jonas Bjordal and Mari Lesteberg at the master’s programme Music, Communication and Technology at Aalborg University and Norwegian University of Science and Technology. The application offers touchscreen functionality, Oscillators, FM Oscillator &amp;  Delay, phaser, Chorus &amp; Filter on Dry/wet slider,Canvas UI with follow visual FX\tand it’s alien themed for your pleasure!\n",
        "content": "\n\n\n\nThe web audio synth “Touch the Alien”, a project by Eigil Aandahl, Sam Roman, Jonas Bjordal and Mari Lesteberg at the master’s programme Music, Communication and Technology at Aalborg University and Norwegian University of Science and Technology. The application offers:\n\n\n  Touchscreen functionality\n  Oscillators, FM Oscillator &amp;  Delay\n  Phaser, Chorus &amp; Filter on Dry/wet slider\n  Canvas UI with follow visual FX\n\n\nAnd it’s alien themed for your pleasure!\nYou can try the synth yourself on this page.\n\nIn the developing of the synth, we have used tools as:\n\nJavascript with Web Audio API, CSS and HTML5\n\n  Javascript to make the “brain” of the synth, such as the audio features, the canvas function with mouse and touch functionality. HTML and CSS for the page UI elements.\n    Tuna\n  \n  An audio effects library for the Web Audio API.\n    Visual Studio Code\n  \n  A very convenient source code editor that makes coding a dream!\n    Github\n  \n  We used Github to share our code with each other during the process.\n\n\nConsultation with extraterrestrial races (coders)\n\nThe team\n\nEigil \n\nEigil used his experience with web technologies and coding to make the canvas work as the interactive centerpiece of the instrument, and then he made the connections to the audio functions, making the instrument playable. He was also responsible for the live demo.\n\nJonas \nDuring the first day was bug fixing and generally advising and cleaning up the code from Sam’s Sam’s Reese the Alien synth. Unfourtunatly Jonas was unwell for the later part of the project development.\n\nMari \n\nMari contributed with her acquired Javascript knowledge from last week, about filter nodes, the interactivity between the sound and faders and buttons, and the responsive design with CSS. During the week, her main responsibility in the team was to research the  touch event functionality , that would allow our final product to work on touch screen devices. She also made a prototype that implemented a canvas with drawing function into the sound- and aesthetic design of Sam’s Reese the Alien synth.\n\nSam \n\nSam took the role of expanding and improving the functionality of the audio code for the synth. This included utilising the Tuna.js\n library for new effect nodes, expanding the audio parameters from the original Sam’s Reese the Alien synth prototype made a week ago.\n\nTuna.js\n\nTimeline\n\nDay\t1\n\nThe first day of work was on Tuesday, February 12th, and during our Audio Programming class we had decided upon an idea that combined some visuals with sound. Mari’s initial idea was to make some sort of a ‘sonified’ drawing pad, inspired by   Eigil’s Pointilator Sequencer synth. The Pointilator allowed you draw ‘sonified’ dots on a canvas, and play it over and over again as in a sequencer. Mari’s idea was to make something similar, but instead of drawing dots, you could draw continuous lines (like in MS Paint), with different colours, and maybe also with different textures. The different textures and colours could represent different instruments or timbres.\n\nSam’s  Reese da Alien Synth  from the previous week was ideal for the sonification part of our new project. It already used mouse movement to control the pitch of the synth, so we decided that we wanted to use his code as a base for the new project. We also agreed that we wanted our new project to work on touch devices. Then we delegated the tasks: Mari was going to research the touch functionality,\n\nDay\t2\n\nWorking side-by-side on our different tasks;\nMari: touch-function\nSam: sound design gibber\nEigil made the Canvas draw lines as you click and drag on it, even making the line fade out over time. This process took a lot of experimentation to get right, as it essentially was making a user interface with relatively low level code compared to using something like Nexus UI.\n\nExcerpt from Mari’s Research Diary:\n\nI did a lot of work on my own after I got home because I knew that the next day I had to be at the doctor and would lose a lot of time in class. I started by trying to implement the touch function with my Spaghetti code player from Audio Programming workshop week 1. I found a tutorial on how to use touch event and canvas functionality on the Mozilla developer pages. By the end of the day I had been able to also add this function into Sam’s code.\n\nFirst, I made the touch events work together with the Spaghetti Code Player that I had made in my individual project. We had not planned to use any of the code from my Spaghetti Player, but since I was more familiar with this code, I thought that this was a good exercice just to increase my understanding of the touch event code.\n\nI got this working pretty soon. Anna showed me in class how I could use the Chrome developer tools to use the toggle device toolbar function to preview how the page would look on touch devices. The difference from the original code was that I now was able to control the filter effect using touch screen, that before was only possible to do with a computer mouse.\n\nNow, when I sort of had a concept of how the touch event function was working, next step was to try it out together with Sam’s  Reese the Alien synth. Sam had shared the code with me on Github. Since I’m still a beginner in JavaScript coding, it’s not an easy job, and the code turned out more and more spaghetti. First, I was able to implement a canvas in the Alien synth, and surprisingly, it worked! At this point, one was able to make a drawing, just like in MS Paint, and the sound was following the line you drawed. But the problem now, was that when you first had hitted the canvas and activated the sound, the sound wouldn’t stop before you were hitting the “stop” button. I also included a volume control in the application, that hadn’t been there before. In Sam’s original synth, there were two oscillators, but only one of them was controlled by the mouse movements, the other one was controlled by keys on the keyboard. But for two reasons I made the two oscillators being controlled by moving x and y axis instead: 1: since the synth was supposed to be used on touch devices, it wouldn’t work to make it dependent of a PC keyboard. 2: It seemed that there had to be two oscillators, one for each direction, to make the canvas sonification work at all.\n\nSo the next step now, was to also make the sound stop when you were lifting your finger from the touch device. I solved this by moving the code that handled the “stop” function of the sound from the “shutup”-button to the function handleEnd(evt) function. This was working perfectly fine, except from that now, the volume control and the FM effects didn’t work anymore! However, I had at least got the main aspects of my idea working, and now it was time to go to bed …\n\nUnder the “Prototype section” of this blog post, you can see a video demonstration of how Mari’s prototype looked by this point.\n\nDay\t3\nThe third day was also spent on working with our individual tasks, but in a more organised way. We divided our scripts, CSS and HTML into separate files to make the different components of the app more clearly defined. In the end, we started combining our codes by linking the files and calling functions from the script concerned with interactivity to the script audio. When this was done, we could start focusing on expanding the audio functionality with effects from the Tuna JavaScript library.\n\nDay\t4\nOn the last day, we finished the prototype by combining all codes together and made final touches on design and functions. This meant tidiying up some of out code, bugfixing and making each intended function work properly. The final part of the day was spent on the presentation, where we got positive feedback from the class and guests, Alexander, Daniel and Arthur. The questions we got were mostly focused on the issues we faced, and how we could further develop the instrument.\n\nIdeas and prototypes\n\nReece da Alien\n\n\n        \n        \n        Your browser does not support the video tag.\n\n\n\n\nThe original inspiration for the project was Sam’s Alien synth. Originally made for experimenting with phasing and clashing frequencies, it also utilised the computer keyboard to effect the frequency of the second oscillator. The idea started as a reese bass creator, however the move to use canvas and touchscreen functionality was a natural progression for the project – it makes the page more accessible and easy to use, as well as a more modern incarnation to present this project.\n\nPointilator synth\n\n\nEigil’s Pointilator synth from the first week was the basis and inspiration for using Canvas to draw visual feedback when playing.\nInstead of having continous sound when clicked, it played note events based on the static and stored position of clicks in sequence.\n\nHello there human.\n\n\n   \n   \n\n\nThis was the prototype for the canvas pad after the first day, and as you can see, the canvas draws a line after the mouse pointer which fades over time. Some features for the drawing was still not implemented at this point, but the main style and functionality was already working.\n\nPrototype: - Reece Da Alien with sonified drawing pad\n\n\n\nThe process of developing this prototype was described earlier in this blog post, in the excerpt from the day 2 of Mari’s research journal. But briefly summed up, the prototype featured:\n\n\n  \n    Implementation of the touch events code into Sam’s Alien Synth\n  \n  \n    Two oscillators controlled by X and Y movements in the canvas\n  \n  \n    The buttons and volume slider didn’t work by this point\n  \n\n\nFlowchart\n\n\n\nThis is the signal chain for the prototype’s audio code. It includes the 2 saw oscillators, FM oscillator and audio player brought over from the alien synth from last week. New additions include everything located right of the output node on the graph – the 3 audio FX and the dry/wet routing.\n\nAudio code &amp; Tuna.js\n\nThis is a sample of the code of how the Tuna.js code was implemented. The library has to be in the same folder as the code (as with all the external libraries and plugins used) and then opened in the code:\n\n&lt;script src=\"tuna.js\"&gt;&lt;/script&gt;\n\n\nEach FX node had similar layout, and examples were found online. Each node the FX of choice is declared, and then each has parameters that are used for each, different depending on the FX (threshold for compression, feedback for delay etc.). The code below shows the options for the chorus – rate, delay, feedback and bypass. The aim for these FX is to have a simple user friendly use of multiple FX in one action – so the parameters of the FX were generally modest and not too intense. This is due to the fact that the synth design creates crazy sounds already – the ethos of the FX addition to the project was to sweeten the sound (the FM function messes it up enough already!). Here are some Tuna.js examples used.\n\ndocument.querySelector(\"#button1\").addEventListener('click', function() {\n  var tuna = Tuna(context);\n\n  var chorus = new tuna.Chorus({\n    rate: 1.5,\n    feedback: 0.2,\n    delay: 0.0045,\n    bypass: 0\n  });\n\n\nThe Tuna.js effects Sam found needed to be inside a function to work. At first a dedicated button for named tuna was created to activate the FX, in the end once a slider was introduced the button was triggered by the talk button along with the rest of the sound. This was because now the user had the choice to activate the tuna.js FX using the slider. The solution to using the slider for all FX at once was dealt with via routing a dry and a wet path in the signal flow – and then the fader used volume between the different nodes to crossfade between the “wet” path and the “dry”.\n\nDry/Wet Functionality\n\nvar slider2 = new Nexus.Slider('#slider2',{\n        'size': [120,20],\n        'mode': 'relative',  // 'relative' or 'absolute'\n        'min': 0,\n        'max': 1,\n        'step': 0,\n        'value': 0\n      })\n\nslider2.on('change',function(filterValue) {\n  wet.gain.value = filterValue             // map the wet gain to the filter value\n  dry.gain.value = 1 - filterValue          // doing the opposite of wet\n\n\nHere ‘FilterValue’ is the fader variable. Lines below it attaches this variable to the ‘dry’ and ‘wet’ nodes gain. The -1 on the dry line inverts the 0-1 range so it does the opposite gain fade to the wet value. This creates a crossfade effect – the fader in the middle it will be 0.5 and 0.5 each, or 0.7 and 0.3. the value of both nodes together will always be 1, wherever it is on the slider. This code was referenced from W3.\n\n  dry = context.createGain();\n  dry.gain.value = 1;\n\n  wet = context.createGain();\n  wet.gain.value = 0;\n\n  MainMix.connect(wet)\n  MainMix.connect(dry)\n\n  wet.connect(chorus);\n\n  chorus.connect(phaser)\n  phaser.connect(filter);  \n  filter.connect(context.destination);\n\n  dry.connect(context.destination);\n\n\nThis is the code for the routing of the dry/wet nodes.\n\nAchievements\n\nSo, for the the workshop we materialised a working prototype!\n\nFeaturing:\n\n\n  Touch interaction on both canvas, buttons and sliders\n  Sounds made with samples, synthesis and interactive effects\n  A styled website\n  Responsive design\n\n\nWe managed to combine our three branches of code. Working as a group and moving to produce code individually during the midweek development was a decision we made. In doing this we could produce more code, but we had to be able to compile the results (on the last day!). One of our achievements was to successfully implement all the code for the presentation – it showed that our organisation of labour worked out in the end!\n\nChallenges\n\nWorking on each other’s code together was certainly both challenging and rewarding, highlighting the importance of a good project structure and management to not make a big mess when putting the pieces together. This was defiantly a challenge;\non the first days it could be hard to know what work process each member had. This also hindered communication somewhat in the first days, although we showed each other our code, it was hard to understand their whole process after each day’s work. As its been stated, we compiled and worked together on the last day, demystifying what the other members of the group had been working on.\n\nSam’s Challenges - Gibber me Timbers!\n\nSam was very inspired by the online JavaScript based live coding environment introduced to the SMC workshop early in the week. The way the platform dealt with FX inspired Sam to get it working with the Alien synth, the audio manipulation on the mouse movements were a particularly cool feature, as well as simple beat generation (ringo a code for drums!). After downloading the library for use in the project, two days of trial and error, problem scanning and asking for advice ensured – to no avail.\n\nFrom what Sam found was that gibber didn’t want to play ball inside the JavaScript audio context that was used to compile and output the audio. There is very little online to help with the external library for Gibber, and so this is Sam’s guess after findings testing the library. It works easily with its own examples, and sets up the audio context differently. So after coming to this rough conclusion – Sam realised that there was too little time to get Gibber to work – and instead used Tuna.js to achieve a similar goal.\n\nIssues &amp; Further Development\n\nEven with all that we achieved during the four days, there are some issues.\nAfter the app is loaded, the app seems to slow down after a while. This is probably due to a memory leak where part of the code keeps piling up information over time without freeing itself.\n\nThe instrument is a one trick pony and could have more parameters for a more complete app.\n\nFor a more responsive design, the Canvas could be adjusted to fit the screen size of any device, but this was not implemented, so it does not take full advantage of for example a smartphone in portrait mode.\n\nIf we had the time, we would have liked to make it more like an actual sonified drawing pad (a bit like MS Paint), adding a function that allows you to play and store the drawing, so you can play it over and over again, like in the Pointilator.\nExpanding on this, we could make a function that allows you to draw with different colours and textures. Different colours represented by different instruments or timbres.\n\n\n\nClosing remarks\nIn the end, we are pleased with the result and experience we got from Anna’s workshop. Perhaps this project or something based on it could be presented by us as a paper or workshop during the 2019 Web Audio Conference in December, who knows?\n",
        "url": "/sound-programming/2019/02/22/TouchTheAlien.html"
      },
    
      {
        "title": "Ambisonics!",
        "author": "\n",
        "excerpt": "On 27 February 2019, we had a workshop on Ambisonics in the portal course. Anders Tveit gave us a lecture on how to encode and decode sound inputs from Lola, using the AIIRADecoder in Reaper.\n",
        "content": "Portal workshop on Ambisonics, 27 February 2019\n\nOn 27 February 2019, we had a workshop on Ambisonics and the SoundField microphone in the portal course. Anders Tveit gave us a lecture on how to encode and decode sound inputs from Lola, using the AIIRADecoder in Reaper. He talked about different formats, and that we had to convert A format to B format with software. And then convert it to Ambic format.\n\nWe made a session in Reaper with bus tracks for the purpose of encoding and decoding the signals.  The inputs from the soundfiled SPS200 microphone was converted from A-format to B-format using the “surroundZone2 plugin” which is completely free to use.\n\n\n\nThe next was to convert the B-format to B-format (ambix) by using the “soundfield plugin” by “RODE”\n\n\n\nLater in the chain we used the AIRADecoder to decode the audio in to the speaker setup.\n\n\n\nWe learned that you can have as many speakers as you like, but at least 4 speakers. Then we discussed what microphone shape/pattern to use, like omni, heart-shape etc.\n\nBinaural Decoding.\n\nWe managed to tryout the “MultiEncoder” from IEM plugin suite to encode multiple input and also used the “binauraldecoder” to decode and listen to the audio with headohones.\n\n\n\n\n\nThis was the chain flow: Midas mixer - &gt; Computer - &gt; mixer - &gt; Lola - &gt; Speakers\n\nAfter the lecture, we started to set up the portals with eight speakers, that had to be placed with the right angle relative to each other. In Oslo, we chose not to mount the speakers in the sealing, but to set up a non-permanent with speaker stands instead. It is important to set the speakers at the correct spot with right angle and the same distance from where we set the middle point. It is also important to set the same volume for all the speakers. The reason why this is important is that the volume and the distance to the speakers will interfer with Ambisonic, since volume is the main factor to make this immersive space.\n\n\n\nExperimenting with E4L Reverb\n\nIn Oslo we tried out using some of the binaural effects included with the E4L plugin suite. One of the more striking effects we tried was the E4L B-format convolution reverb. This reverb used to the extreme, when paired with the binaural setup had a strong, alien washed out effect on the voices spoken into the microphone. This by itself ws pretty spooky, and falls into the catagory of augmented reality. (no natural reverb can ever last that long!)The settings on the reverb size was a massive 300% and higher!\n\nChallenges about the set-up:\n\nNext time we should clear the floor completely before setting up all the equipment. As you can see from the picture above there was a lot of cables laying around. It might have been easier if we had a proper speaker setup in the sealing. Since this was a short time solution and was lacking speakers mounts for the sealing, we went for the floor option.\n\nTo get the angle and the distance between the speakers was not as easy as we first thought. Since every time you move one speaker, the other once have to be moved to. In case you do not set it at the right position from the start, which we had trouble with. Here a laser meter would have been useful. Since we could then print out a paper which we lay in the middle of the setup. On this paper we could have eight direction, one for each speaker, and then point the laser in the right dirction for each speaker and then the measure the right distance.\n\nTrying to measure up the dictance between the speakers in the middle of a cable jungle\n\n\n",
        "url": "/networked-music/2019/02/27/Ambisonics.html"
      },
    
      {
        "title": "Repairing scissors and preparing the portal for talks",
        "author": "\n",
        "excerpt": "\n",
        "content": "Rearranging and setup the portal for seminars and talks\n\nOn 20 March 2019, the SMC team were tidying up the portal and rearranging the Portal for talks. We have collected some pictures from the day, enjoy the slideshow:\n\n\n\nAshane working on the best camera position\n\n\n\nFinal camera solution! We figured out in the end that the roof is not an optimal position for cameras.\n\n\n\nTo move an electrical drum kit is not the easiest task. It needs to be disassembled and everything.\n\n\n\nHow is it even possible to break a pair of scissors?!\n\n\n\nNeed an extra XLR cable?\n\n\n\nOr a power cable?\n\n\n\nAll microphones are now labeled with numbers from 1-4. No more confusion!\n\n\n\n    \n\n\n\nLook what we found in the storage room!\n",
        "url": "/networked-music/2019/03/20/PortalWorkshop.html"
      },
    
      {
        "title": "Composition and mapping in sound installations “Flyndre” and “VLBI Music”",
        "author": "\n",
        "excerpt": "Øyvind Brandtsegg talks about the creation and life cycle of two art installations in this inspiring talk. Follow the link to read about the first lecture in a series about sonification.\n",
        "content": "\n\nFlyndre, Nils Aas (With a Sound installation by ØB)\n\n\nThis blog post is a presentation of Øyvind Brandtsegg and his guest lecture for SMC held the 26th of March. The lecture is the first  part of the series of lecutres given in the course in sonification, led by Anna Xambó. The author is the facilitator of this talk.\n\nØyvind Brandtsegg\nØyvind Brandtsegg (born 1971) is a composer and performer working in the fields of algorithmic improvisation and sound installations. Adding effects to his instrument, the vibraphone, was his entry into electronic music. In 2008 he completed his PhD at NTNU where he made a computer program that he could improvise with.\nSonification is the translation of data values to sound, and Øyvind is interested in creating sounds that represent that data in a meaningful way, and the sound can relate to the origin of the data. When he works with sonification installations, Øyvind always starts with visiting the research area to get inspired by the place the data comes from. His job is then to tie the art-piece to the place of origin, to facilitate the relation between the art and the field.\nHe presented us with two installations, and talked about the creation and life cycle of these.\n\nFlyndre\n\nFlyndre is a sculpture by Nils Aas placed in Inderøy in Trønderlag. It is inspired by the golden flounder, which is a flat fish that was a valuable commodity in Trondheim. Øyvind has made a sonification installation of this sculpture. This is a good example of an algorithmic composition, as it maps time and weather data to different sound parameters to mirror the environment. For example, the parameters Øyvind uses in this composition includes tidal data, moon phases, weather(temperature/light), weekdays, times and months. All these parameters produce a composition that changes over time. The first cycle was for 10 years, from 2006 to 2016, but when it was completed, the project got renewed for another ten years, which is a very impressive lifespan for a sound installation. The Boros Bunker in Berlin only allows for four years.\n\nLouisa Wood Ruby, in “Layers of seeing and seeing through layers” states that you cannot experience art in the same way online as you can in being on site, and Øyvind seems to agree with this. He says that the true form of the sonification is first experienced when it is coupled with the sounds on site. The massive metal sculpture has a sound of its own, and it resonates with the wind. Couple that with both the sonification and the ambient sounds of the weather and wildlife, and you get the intended sound of “Flyndre”.\nStill, an online real time sonification of the work can be found here.\n\nVLBI\n\nVery long baseline interferometry, or possibly the world’s most expensive sine tone instrument is a sonification of the data produced by triangulation measurements created when measuring the relative distance of quasars that are a billion lightyears ago. The origin of the light we observe when looking at quasars are from a time when the earth was nothing like what it is today.\nThe VLBI technique is used in GPS navigation as well as map making, where two antennas on earth observing the same point in the sky (a quasar) creates a triangulation.\nTo do sonification properly one needs to understand the field one is working in. Øyvind has used a lot of time traveling reading and talking to scientists to understand the complex scientific field that these data originate in, and upon completion, he was hailed by the scientists for making a great way of presenting this data to the non-scientific community. That is the quintessence of sonification.\n\nThe projects home page can be found here.\n\nHere is a video of a TED talk that Øyvind did on this project. A TED talk is an inspirational film that is made to hit the sweet spot between entertainment and teaching. Here he explains how it is possible to translate processes in nature into sound, so as to get a new view on nature.\n\n\n\n\n\nExerpt from the Q&amp;A section\n\nØyvind was asked about his thoughts about the cycle of “Flyndre”, and how it would feel when the project finally ends in 2026. Originally the project was meant to last for ten years and end in 2016, and he was very happy to see it renewed, Øyvind Told us. But for now, the epquipment starts to show signs of wear and upkeep is getting harder and harder. He talked about how weather and the elements not only affected the sound of the installation, but also the wiring and electronics as well. Some of the epquipment is about 15 years at this stage. But now, he sees that it is natural to end the cycle. When he visits the live stream, he can tell that it “sounds like what i did in 2006”. As an artist it is always necesarry to look forward.\n\nClosing thoughts\n\nMy background is among other a master in fine arts, and as a person interested in art, i find it very interesting how sonification seems to be able to bridge the gap between science and art. It is appartent that Øyvind does with integrety when one looks at all the trouble he goes through to set the data into context. The sonification itself is the technique and the craft, but the realtion it has to the world we percieve around us is what makes it art. Sound can be a powerful converyer of meaning, but only if it is put into prorper context. It was very inspiring to see how Øyvind is able to do this in his professional artistic work.\n\nIt is highly relevant to have a speaker such as Øyvind in the sonification course. It is very relevant to see how this field can be implemented into society in an artistic way. If you were not able to be there, the TED talk above is a good source of information to get you interested in the work and style of Øyvind Brandtsegg\n",
        "url": "/sonification/2019/03/27/%C3%98yvind.html"
      },
    
      {
        "title": "MoCap Recap - Two weeks recap of a Motion Tracking workshop",
        "author": "\n",
        "excerpt": "During weeks 10-11 we attended the Music related motion tracking course (SMC4043) as part of the SMC program. The week started with the installation of the OptiTrack system in Oslo, placement of cameras, connecting wires to hubs and software installation and setup. we got familiar with the Motive:Body software and was able to run calibrations, set and label markers, record motion data, export it in a correct way and experiment with sonifying the results with both recorded and streamed motion capture data.\n",
        "content": "\n\nDuring weeks 10-11 Sam &amp; Guy attended the Music related motion tracking course (SMC4043) as part of the SMC program.\nThe week started with the installation of the OptiTrack system in Oslo, placement of cameras, connecting wires to\nhubs and software installation and setup. We got familiar with the Motive:Body software and was able to run\ncalibrations, set and label markers, record motion data, export it in a correct way and experiment with sonifying\nthe results with both recorded and streamed motion capture data.\n\nWeek 1\n\nThe first week was building up towards the Nordic Championship of Standstill. This gave us a good overview on all aspects of a thorough research, both in terms of preparation (calibration, paperwork, division of labor, attracting participants, testing etc.), conducting the experiment and data analysis. The SMC students have been rotating\nbetween the different responsibilities and between us have managed to take part and be involved with all tasks during the\nweek. We also have attended the lectures of the NordicSMC Winter School at RITMO, and was presented with several\ntutorials (mocap toolbox, Musical Gesture Toolbox) on research and project presentations concerning motion and\nmusic. The experience was a bit overwhelming at times because this subject and the level of the material seemed\nto be more advanced for our level, but we are glad to be exposed to the tools and the work whithin the motion capture field.\n\n\nElias is getting the maximum best score for using the calibration wand\n\nWeek 2\n\nIn the second week we were divided into groups and focused on a project to be presented on the last day of the\ncourse. The group (including Elias and Sam) developed a Virtual DJ that utilizes traditional and nontraditional\nmovements of a DJ, for manipulating parameters a musical track. Our objective was capturing DJ body movements\n(recorded and streamed) into motion capture data and to sonify these movements.\n\n\n\nOur method was to use the OptiTrack/Motive system to record the motion tracking data of a DJ standing by a table\nimitating a controller deck. We exported the mocap data to a text file that can be read by a Max/MSP patch that\nwas developed initially by Alexander Refsum Jensenius (in PD), converted to Max (by Anders Tveit) and further developed by Georgios Sioros. We mapped several markers from the mocap data (hand, head, sliders) via the Max4Live patch to different parameters in the Ableton music project. We scaled (in Max) the row mocap data to\nthe exact range of our Ableton musical parameters and were successfully controlling the chosen mapped\nparameters. We used our recorded mocap data to produce video clip using the Mokka toolkit. We then synchronized the video to the music we created and manipulated.\n\n\nMusic by DJ Dr. Roman\n\nDuring the last day we mostly concentrated on creating a live performance of our project as well as presenting the\nproject in its entirety. With the help of Victor and Georgios we managed to live stream the mocap data to our max\npatch (OSCNatNetClient) and were able to run several tests before reaching the desired result. Although our live\npresentation was not functioning properly (presenting as the second group in Oslo without re-calibration), I feel\nthat our objectives were met and that we successfully managed at the end to sonify mocap data in a convincing\nway.\n\n\nMusic by DJ Dr. Roman\n\nThe limitations and problems we faced were mainly regarding the lack of knowledge in Max/MSP and with getting\nfamiliar with the systems that were introduced to us. But being aware of our group limitations and getting the\nneeded support from both teachers and classmates, we overcame all difficulties.\n\nIf we had more time to work on the project, we would of liked to explore more expressive forms of modulation. The medium allows for unique relationships between movement to create and morph sound. Another parameter that could of been harnessed was acceleration data, similar to the process used for the standstill research event done on the first week. It could be used as another unique factor to map.\n",
        "url": "/motion-capture/2019/04/01/MoCap-Recap-My-two-weeks-recap-of-a-Motion-Tracking-workshop.html"
      },
    
      {
        "title": "Advanced collaborative spaces, tele-immersive systems and the Nidarøs Sculpture",
        "author": "\n",
        "excerpt": "Leif Arne Rønningen introduced us to ‘Advanced Collaboration Spaces, requirements and possible realisations’ and to the ‘Nidarø Sulpture’, a dynamic vision and audio sculpture. In both parts Leif’s main research areas on tele-immersive collaboration systems and low latency networks are at the forfront.\n",
        "content": "\n\nNidarø-Sculpture, collage created by Leif Arne Rønningen\n\n\nIn the SMC elective course ‘Sonification and Sound Design’ curated by Anna Xambó we got exposed to many different approaches\nwithin the context of the sonification field from the arts and sciences. Sonification is concerned with research functions, e.g. the uncovering of patterns that lie outside our usual sphere of consciousness and are difficult to grasp visually. But sonification also functions in artistic and playful auditory forms. The lectures were in that sense elaborative, innovative and visionary without claiming an ownership on sonification and it’s ‘real’ purpose. Even though some academic practices certainly attach more importance to firm definitions other practices don’t give it such a prominent place even though the approach is the same.\n\nThe art of science or the science in art\nI was facilitating the talk of Leif Arne Rønningen, our second guest lecturer, a professor emeritus from the department of telematics at NTNU (today the department of information security and communication technology). Not only is he drawing knowledge from academia but he also held several job positions, from being a diving instructor- and marketing manager to research director at the Department of Signal Processing and System Design at SINTEF. Leif Arne Rønningen consulted also for one of the biggest Norwegian tele-communication companies and was involved in digital TV solutions for Europe. Topics, that are on the radar in the moment are for example audio-visual installations in the context of music-drama and advanced city planning.\n\nIn his guest lecture he introduced us to ‘Advanced Collaboration Spaces, requirements and possible realisations’ and to the\n‘Nidarø Sulpture’, a dynamic vision and audio sculpture. In both parts Leifs main research areas on tele-immersive collaboration systems and low-latency networks are on the forfront. Leif Arne provides us an in the nutshell overview on camera lense constructions required for immersive experiences through multi-perspective image transmissions and showed areas where it could be applied: office space, travel scenarios among other examples.\nThe Nidarøs Sculpture is an architectural piece and encompasses a lot of Leif Arne’s technical concepts. Presented in a artistic manner it includes the sonification of a short clip he made to demonstrate this architectural idea.\n\n\n\nVisions on advanced collaborative spaces, created by Leif Arne Rønningen\n\n\nGenerating sound from vision, method of sonification\nThe sonification process Leif chose was generated from changes in the image in the short demo-clip on the Nidarø sculpture. Up to that time point we have not come across that method in the course. He explains how changes in a sequence of video images can be temporal or spatial. The sounds he mapped to the sequences are inspired by an opera, three music instruments are used for melody, chord and drums. To reduce processing which as explained is very heavy in this undertaking, Leif focussed to measure a subset of pixels. The changes in the intensity  for the selected set of pixels were measured. When switching from one image frame to the next he then was drawing different variables from those in a RGB format. The input from the image frames was then mapped to operational if-then rules.\n\nQuestions, answers and reflections\nThe Q&amp;A’s from the students though were mainly addressed to his visions and ideas of tele-immersive systems and how distant they are\nfrom now, in years. One question was if mixed reality and collaborative spaces would then compete in some way, but Leif stresses their\ncomplementary role towards each other. It is about studying and finding the connecting dots, a collaboration of different systems and approaches. Another questions was addressed to one of the applied examples of these so called ‘multi-view-architectures’, it is the concept of the travel bureau: Beside the near-natural-virtual communication environment through a multi-view perspective on the landscape and  sensoral integration into the happenings of an touristic event, it would of course also include a corresponding soundscape.\nFrom an engineering perspective Leif-Arne Rønningen’s ‘hands-on’ concepts are so advanced but practical at the same time that he is usually at least 20 years ahead of the current practice. But they are technically already possible he explained, only too expensive for a mass market. External factors could eventually accelerate research, and the days in which these “vacation scenarios” become real are most likely closer than we think and want.\n\nTo classify as a contribution for the portal the article should have been much longer, it has a lot to do with what is trying to be established in our study program. But already in the late 1980ties Leif-Arne Rønningen started researching on low latency image and video transmission in collaborative environments. What seems new and innovative to a lot of us SMC people in terms of telematic practices was thought of quite before our time and remained often a lot of distant theory. Therefore it was an honor to be exposed to an overview of in depth on Advanced Collaboration Spaces. It could be very interesting to extend the possibilities of telematic musicking with Leifs technical concepts and see where it goes. Especially to learn about the perceptual aspects that would accompany such technical developments. The approach to sonification was very refreshing and new to me, as I like to work with visuals and video, Leifs method is a nice inspiration.\n",
        "url": "/sonification/2019/04/03/Advanced-collaborative-spaces-and-tele-immersive-systems.html"
      },
    
      {
        "title": "Using Speech As Musical Material",
        "author": "\n",
        "excerpt": "As a part of a three-week workshop in the course Sonification and Sound design at SMC, we were lucky to have Daniel Formo as a guest speaker.\n",
        "content": "\n\n\n\nDaniel Formo\n\nDaniel Formo (born 1978) works as a musician, composer and researcher within a broad range of music, and is a lecturer at the Department of Music at NTNU. In 2018, Daniel Formo finished his doctoral degree from the Norwegian Artistic Research Program with the project The Music of Language and Language of Music, which resulted in a performance concept called The Orchestra of Speech. This performance concept was the topic of Daniel’s talk.\n\nThe Orchestra of Speech\n\nThe Orchestra of speech explores the close relationship between music and speech, with the focus being on improvised music and everyday conversation. Daniel talks about the immense complexity of human speech, which he wants to translate into other sounds, using his unique instrument, created in Max. He is not interested in the semantic content of speech, but rather the utterances as an act – as vocal gestures, and looks for speech genres as musical expression. Speech genres being a common social language, like for instance; Smalltalk, interrogation, quarrel, baby talk etc.\n\n\n\nThe Orchestra of Speech system\n\n\nWith his instrument he can extract a range of musical features from speech (pitch, amplitude, tempo, voice quality and articulation) and orchestrates it into layers of sound and explores this in real time. Daniel compares his instrument to a microscope, where he explains that he can zoom into the speech signal, and then extract the characteristic feature of the voice.\n\nDaniel did not want his instrument to sound like “early electroacoustic music”, so in a performance setting he utilizes transducers to be able to play back his translated sounds through different acoustic instruments, like a drum, organ, guitar etc. During performance he uses a computer with a midi controller to control the speech translation, and often accompanies with a piano and/or his own vocal, improvising music all together.\n\nQ&amp;A\n\nBecause of Daniel’s very interesting topic, there were a lot of questions, which is great! Many of the questions addressed his use of speech and language; how he chose a specific language or recording, what he looked for, and for what reason. Daniel expressed that semantics were something he tried looking away from and would only focus on the utterances and musical features of a speech. During his research he was surprised of how similar different languages are when looking away from semantics. He did mention that recordings from Reality-TV is maybe the best source, because of the big range of emotions which are genuine because of the cameras and microphones being hidden. His goal was to extract the significant characteristics of the speech, where even the slightest difference means something, and translate the underlying meaning of the speech into a musical improvisation and find a middle ground between speech and music.\n\nFinal Reflections\n\nI think Daniel Formo has become a great inspiration to the students of the sonification course with his Orchestra of Speech instrument. His talk has displayed how sonification can be used as both a tool for research and as a musical instrument for use in a performance setting. His presentation goes through an entire process of a sonification in detail, where he starts with the raw data of speech, and by analysis and feature extraction translate it to musical sounds with the use of mapping and digital signal processing.\n\nIf you want to learn more about Daniel’s instrument, take a look at this website. You can even download the Max patch!\n",
        "url": "/sonification/2019/04/03/Daniel-Formo.html"
      },
    
      {
        "title": " An Overview of Sonification by Thomas Hermann",
        "author": "\n",
        "excerpt": "It was my privilege and honour to facilitate a guest lecture and introduce one of the ‘Gurus’ in the field of sonification, Dr. Thomas Hermann. He shared his enormous knowledge on sonification with hands on exercises for two days (March 28, and 29, 2019) through the SMC portal in Trondheim. I am quite excited to share my notes and will try to cover the summary of his talks in this blog.\n",
        "content": "\n\n  Dr. Thomas Hermann\n  Pic Source: https://sonification.de/\n\n\nIt was my privilege and honour to facilitate a guest lecture and introduce one of the ‘Gurus’ in the field of sonification, Dr. Thomas Hermann. He shared his enormous knowledge on sonification with hands on exercises for two days (March 28, and 29, 2019) through the SMC portal in Trondheim. I am quite excited to share my notes and will try to cover the summary of his talks below. But first, let me give a short introduction of the guest speaker.  Dr. Thomas Hermann is a computer scientist working in the field of Cognitive Interaction Technology. He earned his master’s degree in Physics in 1997 from Bielefeld University, Germany. He also achieved his Ph.D. in Computer Science in 2002 from Bielefeld University with his thesis on ‘Sonification for Exploratory Data Analysis’. His research focus is on sonification, data mining, human-computer interaction and ambient intelligence. You can explore his publications and know more about him here.\n\nDay 1\nIn the first day, Thomas introduced the taxonomy and definition for sonification and auditory display, making us aware that sonification itself is relatively at its developing phase. While sound has a long tradition in science e.g. in the use of Stethoscope, Geiger Counter, machine diagnostics etc., visual elements have been dominating the processing and dissemination of information in the digital age. Nonetheless, we use all our senses to perceive the world. Thus, sonification can play a significant role in the enhancement of our perception in this era of computers by using the neglected modality of sound.\n\nSonification\n\n\n\nDiagram reproduced from T. Hermann (2008), TAXONOMY AND DEFINITIONS FOR SONIFICATION AND AUDITORY DISPLAY,http://www.icad.org/Proceedings/2008/Hermann2008.pdf\n\n\nSonification is the use of non-speech audio to convey information (Kramer et.al, 1999). Hermann(2008) has given a relatively modern definition of sonification as a technique, that uses a data set as input and produces sound signals,  if it meets the\nfollowing conditions: (1) the sound should reflect objective properties or relation in the input data, (2) the transformation is\nsystematic, (3) the sonification is reproducible, and (4) the system can be used with different data sets.\n\nOverview of Sonification Techniques\n\n\n\nFigure adapted from https://sonification.de/media/SonTechniques.png\n\n\nAudification\n\nAudification is the process of direct translation of data into sounds. It is applicable only for structured data sources e.g. time/space etc. The following example is an audification of earthquake in Tohoku, Japan, 2011/03/11 - 2011/03/12.\n\n\n\n\nYour browser does not support the audio element.\n\nSound example by www.sonifyer.org taken from https://sonification.de/handbook/chapters/chapter12/#S12.3\n\n\nEarcon\nEarcons are structured sounds that serve as index for abstract messages. For example, those who are using windows operating system in their computer, may be aware of the system sounds when the computer starts or gets some error messages etc., earcons are these types of sounds. Below is another similar kind of example.\n\n\n\n  \n  Your browser does not support the audio element.\n\n  Sound example by T. Hermann, A. Hunt and J.G. Neuhoff taken from https://sonification.de/handbook/chapters/chapter14/#S14.2\n\n\nAuditory Icons\nIt can be defined as acoustic event markers whose meaning can be understood via an association process. It is easier to understand than earcons because unlike earcons, auditory icons do not reflect abstract meaning. Auditory icons have an existing relationship between the sound and the information that it is providing.\n\n\n\n  \n  Your browser does not support the audio element.\n\n\nSound example by T. Hermann, A. Hunt and J.G. Neuhoff taken from https://sonification.de/handbook/chapters/chapter13/#S13.2 \n\n\nParameter Mapping Sonification (PMSon)\nIn this technique, a data set is mapped with different parameter of sound synthesis. Below is an example of PMSon which shows a continuous change of rising water temperature with five data points.\n\n\n\n  \n  Your browser does not support the audio element.\n\n    Sound example by T. Hermann, A. Hunt and J.G. Neuhoff taken from https://sonification.de/handbook/chapters/chapter15/#S15.3 \n\n\nDay 2\nIn the second day, Thomas talked about sonification techniques beyond parameter mapping. These include Model based sonification (MBS) and Wave Space Sonification (WSS).\n\nModel Based Sonification (MBS)\nIn MBS, a data set is used to construct a virtual data object in a state of equilibrium. The object is then allowed to be interfered by human interaction which transfers energy over the object and produces a temporal signal that can be sonified. It is very different from PMSon and has many benefits over it. For instance, PMSon is about mapping certain sound sequence to data and user has no control over sounds produced. In contrast, MBS does not work at all without interference of a user. Moreover, user can control the sound depending upon how he/she interacts with the model. MBS can be designed in a very physically plausible way, for instance by respecting Newton’s third law of motion: For every action there is an equal and opposite reaction.\n\n\n\nDiagram reproduced from Hermann, T., Hunt, A., &amp; Neuhoff, J. G.(2011), The Sonification Handbook, https://sonification.de/handbook/chapters/chapter16/\n\n\nWave Space Sonification (WSS)\nWSS is a newly established novel class of sonification technique for time-space indexed data.  Hermann (2018) defines WSS as the transformation of data into sound signal by sampling wave space function while moving along the embedded trajectory by means of chosen time-advancement morphing. It provides a system for defining (1) geometric objects (trajectories) from time-sequenced data and (2) a wave space of equal dimensionality. In essence, the time series navigates a high-dimensional wave space and probes it’s sound to create the sonification. For more detail, please follow Thomas’s paper on WSS.\n\n\n\nDiagram reproduced from T. Hermann (2018), WAVE SPACE SONIFICATION, https://pub.uni-bielefeld.de/record/2919707\n\n\nApplication of Sonification\nApart from Earcons and Auditory Icons, rest of the techniques can be used for time-series sonification. Besides, the application of sonification is quite diverse. It can set the eyes free as a replacement of visual information and serves as a powerful medium for visually impaired people. It can be useful in data mining enabling to see hidden patterns/information. Similarly, it enables better monitoring of complex processes for e.g. in analysis of epileptic brain activity as sonification of EEG helps to perceive dynamic changes in brain activity. Likewise, vocal EEG sonification helps to learn concepts from experience, measuring EEG of a person while sleeping etc. Likewise, interactive sonification can substitute or amplify sensory information which can be used in rehabilitation, therapy, skills learning, sports performance etc.\n\nQ&amp;A Sessions\n\nThe talks were very interesting and thus there were plenty of questions from both Oslo and Trondheim. One question was about the adverse effect of sonification that asked if it adds in noise pollution instead. In reply, Thomas said that it’s a frequently raised issue which is totally untrue. He explained it giving an example of a supermarket where numerous scanners produce high frequency (~1000Hz) sounds. He argued that sonification could result in better sound design that could rather change the harmful soundscape into healthier, aesthetically more pleasing or entertaining. It could even be designed to serve multiple purposes like notifying users with various offers etc. Similarly, some questions were posed regarding sound design and reasons for selecting particular type of sound. Thomas clarified that sonification designs are meant for specific tasks under specific situation with specific audio context in mind. He added that, at times there could be conflicts or even trade-offs between aesthetic qualities and informative aspects. Therefore, he suggested that one should maintain a check and balance as its nature is often subjective and open ended.\n\nMy Final Reflections\n\nI was very inspired by Thomas’s ideas and his works in the field of sonification. I had goose bumps when he showed the example of EEG of a boy showing his brain activity. It was breath-taking to see how well sonification communicated the state of his mind which is, in my opinion, far more superior than any other visual display. Besides, I have been dazzled with whole new facets of music since the beginning of the SMC program. And the talks in this SMC4046 Sonification and Sound Design course have driven me to a next level. It’s a great experience being exposed to professionals working with sounds/music technology in many different ways. I feel so proud to be a part of the SMC program.\n\nReferences\nHermann, T. (2018). WAVE SPACE SONIFICATION. 24th International Conference on Auditory Display, Michigan Technological University June 10 - 18, 2018. Retrieved from: https://pub.uni-bielefeld.de/record/2919707.\n\nHermann, T. (2008). TAXONOMY AND DEFINITIONS FOR SONIFICATION AND AUDITORY DISPLAY. 14th International Conference on Auditory Display,      Paris, France June 24 - 27, 2008. Retrieved from: http://www.icad.org/Proceedings/2008/Hermann2008.pdf.\n\nHermann, T., Hunt, A., &amp; Neuhoff, J. G. (2011). The Sonification Handbook (1st ed.). Berlin: Logos Publishing House. Retrieved from:        https://sonification.de/handbook/.\n\nKramer, G., Bargar, R., Barrass, S., Berger, J., Evreinov, G., Tecumseh Fitch, W., … Tipei, S. (1999). Sonification Report: Status of      the Field and Research Agenda. International Community for Auditory Display. Retrieved from: http://www.icad.org/websiteV2.0/References/nsf.html.\n",
        "url": "/sonification/2019/04/05/Thomas-Hermann.html"
      },
    
      {
        "title": "Presentation by Pamela Z",
        "author": "\n",
        "excerpt": "As part of our Sonification and Sound Design course (SMC4046), we were fortunate enough to host scholars and artists which are well established within the sonification and sound design field. Pamela Z is a composer, performer and a media artist who is known for her work of voice with electronic processing. Pamela arrived in Norway for several workshops and performances, and we were lucky enough to have her for a short presentation on April 4th. After a brief introduction by Tone Åse who has been a long-time fan of Pamela’s work, Pamela started the session with a 10 minutes performance of a live improvised mashup of several existing pieces she often performs. While performing, Pamela is being circled by several self-made sensory devices that are connected to her laptop. On her hands, she wears sensors that send signals to her hardware setup. She sings and makes sounds with her voice, hands, and body and manipulates all that with hand gestures.\n",
        "content": "\n\nAs part of our Sonification and Sound Design course (SMC4046), we were fortunate enough to host scholars and artists which are well established within the sonification and sound design field. Pamela Z is a composer, performer and a media artist who is known for her work of voice with electronic processing. Pamela arrived in Norway for several workshops and performances, and we were lucky enough to have her for a short presentation on April 4th. After a brief introduction by Tone Åse who has been a long-time fan of Pamela’s work, Pamela started the session with a 10 minutes performance of a live improvised mashup of several existing pieces she often performs. While performing, Pamela is being circled by several self-made sensory devices that are connected to her laptop. On her hands, she wears sensors that send signals to her hardware setup. She sings and makes sounds with her voice, hands, and body and manipulates all that with hand gestures. In the video below Pamela Z is using more or less the same setup she presented during her talk in Oslo.\n\n\n\nPamela started describing the basics she’s been using on any given performance. She sings and records her voice in real-time. Both the recorded voice samples and the live singing can be processed and manipulated by her during the live performance. Most of the processing is based on digital delay. She captures and samples her voice, creates layers and processes the sampled and real-time audio. Pamela processes her voice through Max/MSP and can layer, loop and alter her live vocal sound. Some of her most used plug-ins are Shuffling and Freeze by GRM tools.\nIn her earlier years, she has used hardware racks full of digital delay units, multi-effects processors and samplers. Her first delay pedal was recommended to her after a concert where Jaco Pastorius was performing with effects. Pamela described her first experience realizing the artistic potential of layering and how she immediately was immersed in the experience of building textures and rhythms from her voice. It was a moment when all her artistic output was changed “I found my voice as an artist when I started playing with processing” she added. Earlier work by Pamela Z can be seen in the video below.\n\n\n\n\n\nShe developed her skill using effects, mostly delay, and began to learn more about out-of-phase delays which allow for more textures and timbres. In the early 1990s, she introduced a gesture control in her work with the \"Body Synth\", a wearable MIDI controller that allowed her to control sound through gestures. It includes a belt with sensor sensitivity adjustment knobs. The suite and the CPU unit translates muscular effort into numbers. The numbers are being mapped to different musical parameters and triggers depending on the musical piece. Check out Pamela's performance at TEDxStanford from 2012.\n\n\n\n\nPamela described her learning process when using gestures in her performance. She realized that one should “be really still in your body,” she continued by saying that “the quality of my movements in performance completely changed from using this instrument.” Gesturing is an integral part of Pamela Z’s performance even in those years when she only used her voice and manipulated sound.\n\n\nWith the years it became impossible to carry so much equipment, and with the help of friends and colleagues, she migrated to the laptop and specifically to Max/MSP. Pamela developed her arsenal of effects within Max and later worked to develop gesture controllers to incorporate with her performance.\n\nAfter showing some of her unique self-made devices, Pamela’s presentation concluded with some questions from both teachers and students. Once available here, I recommend listening to her performance and talk because it tells a personal story of an artist that is adapting, incorporating and finding her artistic voice through and with technology.\n\n",
        "url": "/sonification/2019/04/07/Pamela-Z.html"
      },
    
      {
        "title": "Ole Neiling Lecture",
        "author": "\n",
        "excerpt": "I had the pleasure of introducing Ole Neiling, a extradisciplinary ‘life-style’ artist. This was part of a series of lectures held over the portal in the sonification and sound design module.\n",
        "content": "SMC Sonification Lecture - Ole Neiling\n\nhttp://www.ole.wtf/\n\nI had the pleasure of introducing Ole Neiling, a extradisciplinary ‘life-style’ artist. This was part of a series of lectures held over the portal in the sonification and sound design module.\n\n\n\nSam Roman (me) introducing Ole over the portal\n\n\nI firstly met Ole over the portal, as his talk was being hosted in Trondheim. I was in Oslo. This was an experience; he was very polite when I probably pronounced his name incorrectly - lost in portal translation!  After doing the introductions and a little delay, the lecture began. There was a lot of content – So as well as writing about who Ole Neiling is, I will also touch on some of the core and interesting elements of the lecture.\n\nBackground and Bio\n\nOle playfully researches the tension amongst the fields he is expert in: music, visual art, performance, new media and craftsmanship. These mixtures result in a diverse body of work with as common denominator the mentality that: Instead of accepting predetermined notions (the famous “It’s just the way it is”), one could also wonder why things are the way they are and act upon the findings.\n\nOle’s has achieved a lot in his career – studying in the UK, Holland and Norway, and winning three awards in 2018 including the Bcademy Graduate Award.\n\nOne of his main influences has been with recorder building. In November 2017 Ole started an apprenticeship at the world renowned workshop of Adriana Breukink. This allowed a daily insight in traditional instrument-building alongside an artistic practise – this theme of interdisciplinary and extradisciplinary practices is important to Ole’s work, and is central to the talk he gives over the portal to us lucky participants.\n\nOle’s themes he discusses are; Music and language, what makes you human, what makes trends and also how to brainwash children! The first half he shows how he thinks about his work and some examples, second half into how he prepares and works on the projects themselves – as well as a brief look into the instruments he has created for children.\n\nProject Examples\n\n\n\n\n\n\nThe first project shown touches on Ole’s core ideals an an artist, so was a great way to start of the talk. Skauhytt is a cabin project created by Ole Nieling that has been traveling around Europe. It is an experiment how to apply artwork in a secondary level. Ole Gave 9 artists the experience of being in the cabin staying in the forest, and they were asked to produce art during this experience (artwork on a secondary level). This explores themes such as what makes us human? Ole recollects about talking to himself when alone at the cabin. He relates this to keeping ones’ humanity. The area for language use in the brain is also the same as tool preparation. This issue of human learning is a reoccurring theme in Ole’s talk and work. He also explains how he had to gain many skills, from PR to survival to achieve this project, linking to the extradisciplinary practices Ole promotes as well.\n\nAnother project mentioned was Esoterrorism - mobile FM radio pirating, with all the equipment on a backpack. He mentions the future of a possible “analog” open network of the future after FM has been disused, like a retro open internet of sorts. Interestingly he looked for FM stations around Norway whilst travelling through, hearing only a few local stations on his way to Trondheim.\n\nA good proportion of the talk was discussing how we, and children learn. One project could be seen as quite controversial - “my first fire”. This is a surprising twist to the “kids do not play with fire” social norm. For Ole, this kit is a way to get into a discussion about how mans relationship with fire has changed, and especially children in cities that could benefit from some teaching on how to start - and control a fundamental part of human history - fire. As Ole states at least some discussion may come from it.\n\n\n\n\n\n\nOLME - Instrument Prototype for Kids.\n\nChildren learn a universal language early on, and learn far deeper than an adult. Ole shows many examples proving this - coca cola’s pushing their brand on children, and Genie Wiley a dutch girl who couldn’t learn grammar after being isolated from human society.\nDuring Ole’s preparation for this project he derived some inspiration. Children are almost continually in a state off flow (1975 Mihaly Csikszentmihalyi) and humans can have a ‘eureka’ moment (Pythagoras, Kairos). It should look inviting for exploration - a space station console is shown as an example of what a child would love to play with!  Lift buttons also are great for excitement with children- always responsive, lights up and moves the lift instantly!\n\n\n\nOLME prototypes\n\n\nOle wanted to produce a learning instrument for children that could be more like playing with clay – a musical sandbox of sorts. This will differ from the plastic, fake instruments regularly on the market today.\n\n\n\nOLME prototypes\n\n\nLater in the talk after asked during questions, he shows his large, wooden boxes that he has created. A labour of love – he explains how he needed to learn how to solder properly to create it.\n\nQuestions and Conclusions\n\nAs part of the questions I asked what makes being extradisciplinary in practice help in a sonification career? Ole has many disciplines, from music to gardening. As he states, fishing and fine arts do not normally relate to each other - but when both are combined possibly a specialist in fine art may find inspiration from fishing, and vice versa. This crossover of knowledge and techniques can result in interesting ideas, and seems to be more and more relevant in todays ever increasing changing world – including in a subject such as sonification, which is rather new and can involve many methods.\n\nAnother takeaway I get from this talk is the importance of introducing of exploring the world through sound. I found a connection between the lack of sound as a medium to portray data as a parallel to the lack of children learning sound as exploration experience. As a child we are told to draw what we want – explore through sight in many ways but to a lesser extent make whatever sound we like – explore through sound. Ole Neiling by creating these sandbox (what about the name “soundbox”?) tools for children is in some way addressing this fact – and I find that very inspiring.\n\nPortal Experience\nLastly a mention of the Portal. It can feel a little jarring speaking over a string of multiple screens to two or more audiences. This was a worthy experiment though, I felt that the introductions worked, not just because of the novelty factor of being in another city, but on an aesthetic level I thought the discussion was framed well in the portal, and the setting works thematically with the sonification subject matter we are studying.\n\nhttp://www.ole.wtf/\n",
        "url": "/sonification/2019/04/12/Ole-Nieling-Portal-Lecture.html"
      },
    
      {
        "title": "MuX -playground for real-time sound exploration",
        "author": "\n",
        "excerpt": "It was so fascinating to have Edo Fouilloux in the SMC sonification seminar series. Edo is a visual innovator and a multidisciplinary craftsman of graphics, sound, computing, and interaction. He co-founded Decochon in 2015 to work with Mixed Reality (XR) technologies. MuX is a pioneering software in the field of interactive music in virtual reality systems. Edo demonstrated the concepts and philosophies inside Mux, where it is possible to build and play new instruments in the virtual space.\n",
        "content": "What new challenges and opportunities arise while programming and playing sound inside 3D space?\n\nEdo Fouilloux\n\n\n\n\n\nIt was so fascinating to have Edo Fouilloux in the SMC sonification seminar series. Edo is a visual innovator and a multidisciplinary craftsman of graphics, sound, computing, and interaction. He co-founded Decochon in 2015 to work with Mixed Reality (XR) technologies. MuX is a pioneering software in the field of interactive music in virtual reality systems. Edo demonstrated the concepts and philosophies inside Mux, where it is possible to build and play new instruments in the virtual space.\n\nMuX\n\n\n\n\n\nFrom his own words “Mux is for build sound. It is a playground for real-time sound exploration”. The user can explore the principles of sound by combining the various sound modules provided in the system. As an example, the VR module for the speaker actually visualizes the concept of a real speaker.\nIn the designing process, the very first idea was to define how the components can be separated or combined and to what extent it is possible. Started with a speaker and an oscillator with using arithmetic operations like multiplication, addition to generate different sounds.\nThere are discrete and continues data flows inside MuX. They created analogies to describe the data types. The bulbs represent the discrete data and cogs are to represent the continuous data. Additionally, there is another type of components called “interfaces” which are buttons, sliders, etc.  The interfaces will allow the user to control the information is preferred.\nThe user can walk around the virtual Space and be surrounded by all the electronic sound components. The components can be created from the “sound palette”. There are Oscillators (with multiple wave patterns), Noise generators, Buttons, Decay envelops, and Filters. Inside “event components”, there is a metronome, random generator, step sequencer and much more. By using these event components it is possible to create loops. Edo showed another way of creating loops inside MuX. That is by using a “marble generator”. These items will produce marbles and “wooden pallets” are used to catch them and the slope of the palettes can be used to define the tempo to modulate any oscillators. “Frequency converter” component helps to modulate the frequency according to the input from the marble generator.\nThe “laser board” can be used to create melodies and the marble generator will create the loop of the melody.  The components have a very mechanical nature in design. Also, MuX is an expressive instrument in live performances. This opens up new concepts of using gestures to create music inside a virtual reality platform.\n\nQ &amp; A\nAfter a very fun and informative presentation, students had a lot of questions. There were questions regarding the design of the MuX, live performances and future expectations. There are about 100 modules in the system which is fully flexible for users to play with. It is a great way to explore the fundamentals and theories of generating sound and people have given positive reviews. The MuX has a huge potential of using it as an educational tool for audio concepts and sound design. The live performance is an interesting challenge for MuX. The person who uses the VR system becomes the center of the performance and other musicians will follow. Development of augmented reality systems in the future will be beneficial to optimize the system for live sessions. As for future development, there are several discussions on implementing multi-player options. Right now the system has the ability to use OSC signals or have multiple screens. The community is about 500 people and it’s growing. As for further future plans, there is an idea to have an assets store to share and download components and also to make the system make playable outside the Virtual reality platform.\n\nPersonal reflections\nI think MuX is a revolutionary instrument for the future music industry that has the potential in creating a new musical culture around virtual reality. MuX can be also seen as a great learning tool to understand and explore fundamentals in sound design and has great flexibility towards it. Edo presented very helpful ideas and concepts to consider playing inside the MuX.\n\nYou can try it out by visiting the website\nhttps://www.playmux.com/\n",
        "url": "/sonification/2019/04/13/Edo-Fouilloux.html"
      },
    
      {
        "title": "The Sound of Traffic - Sonic Vehicle Pathelormeter",
        "author": "\n",
        "excerpt": "Is it possible to transmit complex data-sets within an instance of a sound, so the content gets revealed? As communication and dissemination of information in our modern digital world has been highly dominated by visual aspects it led to the fact that the modality of sound got neglected. In order to test the hypothesis, the project presents a model for sonification of temporal-spatial traffic data, based on principle of Parametric Mapping Sonification (PMSon) technique.\n",
        "content": "\n\n  Figure1: The Sound of Traffic\n\n\nIntroduction\nDuring the sonification Lecture series, we were exposed to fascinating concepts and theories behind sonification, sound design and their applications. Our primary goal was to explore the techniques that we learned and put them into practice in order to understand a real-world problem. After searching and referring so much available data we finally decided to work on traffic vehicle data from 3 regions in England measured over 17 years. Is it possible to transmit complex datasets within an instance of a sound, so the content gets revealed? As communication and dissemination of information in our modern digital world has been highly dominated by visual aspects it led to the fact that the modality of sound got neglected. In order to test the hypothesis, the project presents two prototypes for the sonification of temporal-spatial traffic data. While looking for similar projects, it was striking that ‘sonification of traffic’ is mainly applied for internet- and network traffic. In order to better comprehend data-streams and detect abnormal patterns for example. There is one project though with an approach very close to our experiment, found on LA-Listen by Steven Kemper. It extracts data from field recordings of moving vehicles and uses SuperCollider to generate Midi files aiming to use sound to further articulate patterns in the sonic data.\n\nAs stated earlier, we had been introduced with various facets of sonification from artistic and or qualitative approach like Øyvind Brandtsegg’s Flyndre Project or Daniel Formo’s Orchestra of Speech to a combination of both artistic and quantitative approach like sound examples by T. Hermann, A. Hunt and J.G. Neuhoff taken from chapter15 ( e.g. example #S15.3 ) from , The Sonification handbook (Hermann, T., Hunt, A., &amp; Neuhoff, J. G., 2011)\nand so on. Our idea was quite like the latter approach. Hence, we decided to go for Parameter Mapping Sonification (PMSon) technique. In this technique, a data set is mapped with different parameter of sound synthesis which produce sound signals. Further, as per the definition of Sonification by Thomas Hermann (2008), a true sonification should have the following four attributes; (1) the sound should reflect objective properties or relation in the input data, (2) the transformation is systematic, (3) the sonification is reproducible, and (4) the system can be used with different data sets. Through PMSon technique, both of our prototyes meet these four criterias.\n\nData set\n\n\n  Figure 2: Method for Selecting Data\n\n\nThe data has been collected from The Department of Transport, U.K. It includes Annual Average daily Flow (AADF) of buses/coaches, cars/taxies and motorbikes for three regions of England from 2000 to 2017. The three regions include the North East, the West Midlands &amp; the South East which have been chosen based upon their population as shown in Figure 2. The primary data includes number of vehicles, but they are segregated into numerous divisions of road types for each region. Shreejay made a matlab script to refine the data as per our requirement. The script reads the raw data of each region of choice at a time and writes them into an excel sheet by calculating the total number of vehicles in the region in each year and exports them as in CSV file format.\n\nSonification &amp; Sound design\n\nPrototype 1 with Javascript and P5.js\n\n\n\n Your browser does not support the audio element.\n\nAudio 1: Prototype-1, Sonification of Buses &amp; Coaches in the three regions\n\n\nWe chose to work with JavaScript to create our first prototype. The number of buses in each particular region was directly mapped into a frequency of an oscillator by using the “map ()” method in P5.js. Figure 3 below gives an overview of the mapping method. Here, for example, the minimum to maximum range of number of bus in each region is mapped to the minimum to maximum range of frequency.\n\n\n\nFigure 3: Mapping method in Prototype 1\n\n\nSound was generated through three unique oscillators that correspond with the data for each region. Additionally, three non-overlapping frequency ranges were chosen in the mapping to further differentiate between the three sounds. Moreover, the “settimeout()” method states the durations of a single frequency that corresponds to a certain year.\n\nCode Snippet\nThe following code snippet shows how the three oscillators are defined and highlights the mapping method and set timeout () method applied in the prototype 1.\n\nfunction preload(){\n  loadJSON(\"Buses.json\", dataReady1);\n}\n\nfunction dataReady1(data1){\n  osc1 = new p5.Oscillator();\n  osc2 = new p5.Oscillator();\n  osc3 = new p5.Oscillator();\n\n  osc1.setType('sine');\n  osc1.freq(0);\n  osc1.amp(0.4);\n  osc1.start();\n\n  osc2.setType('triangle');\n  osc2.freq(0);\n  osc2.amp(0.5);\n  osc2.start();\n\n  osc3.setType('sawtooth');\n  osc3.freq(0);\n  osc3.amp(0.2);\n  osc3.start();\n\n  figures1 = data1.figures;\n  for (var i = 0; i &lt; figures1.length; i++) {\n    append(arrayBus, [figures1[i].AADF,figures1[i].NE,figures1[i].SE,figures1[i].WM]);\n};\n\n  // for three cities\n  for (var i = 0; i &lt; arrayBus.length;  i++) {\n  setTimeout(function(y) {\n\n    freq1 = map(arrayBus[y][1],140235, 183753, 200, 600);\n    freq2 = map(arrayBus[y][2],227700, 377048, 700, 1000);\n    freq3 = map(arrayBus[y][3],328924, 490877, 1100, 2000);\n\n    //console.log(freq);\n\n    osc1.freq(freq1);\n    osc2.freq(freq2);\n    osc3.freq(freq3);\n\n    }, i * 500, i);\n     // we're passing i  \n    };\n  }\n\nPrototype 2 with Python and Supercollider\n\n\nFigure 4: Prototype 2, System Diagram\n\n\nThe first prototype was further developed with Python and Supercollider. It is inspired by Thomas Hermann’s lecture on topics of Sonification and hands on exercise on Parameter Mapping Sonification during a series of talks in the SMC4046 Sonification and Sound Design course in this spring semester. The python code for this prototype is based upon two examples; Example-1: the code of the hands-on exercise provided by Thomas and Example-2: the code shared by Thomas through his GitHub repository. The code for the prototype has been further developed to adapt a different structure of data, create different synth definitions and apply different forms of mapping by exploring various example of synths in Supercollider. For instance, Example-1 and Example-2 have used synth definitions like SinOsc.ar, DynKlank.ar, Dust.ar etc while prototype 2 applies Saw.ar, LFPulse.ar and RLPF.ar along with combination of SinOsc.ar and so on. Moreover, the synths have been designed to meet our objective of combining both artistic and scientific approach, crafted in Supercollider, as shown in the code snippet below;\n\n// ----------------Bus -----------------Amplitude &amp; Frequency to be used in Mapping\nSynthDef (\"bus\", {arg out=0, freq= 50, mul=0.7,amp = 1;\n    var f;\n    f = Saw.ar(freq,mul,0);\n\tOut.ar(out,f*amp);\n}).add;\n\ng= Synth.new(\\bus)\ng.free\n\n// ----------------Car -----------------Amplitude &amp; Frequency to be used in Mapping\nSynthDef (\"formula1\", {arg out=0, freq = 2.5, mul2= 10;\n    var f;\n    f = LFPulse.ar(LFPulse.kr(freq, 0, 0.3, mul2, 200), 0, 0.8, 0.1);\n    Out.ar(out,f);\n}).add;\n\nf= Synth.new(\\formula1)\nf.free\n\n// ----------------Motorbike -----------------Amplitude &amp; Frequency to be used in Mapping\nSynthDef(\"motogp\", { arg out=0, freq= 10, mul3 = 100, amp = 2;\n    var m;\n    m = RLPF.ar(LFPulse.ar(SinOsc.ar(0.2, 0, 0, 10),0.5, 0.2),freq,0.5,100);\n    Out.ar(out, m*amp);\n}).add;\nm = Synth.new(\\motogp)\n\nSimilarly, Figure 5 below highlights the mapping of different parameters for the prototype 2. Here, for example, the minimum to maximum range of number of bus in each region is mapped to both of the minimum to maximum range of frequency and the minimum to maximum range of amplitude and so on. Therefore, for buses, the change in frequency and amplitude gives the idea of increase or decrease in number of buses in the selected region with respect to time. The speed of oscillation and change of amplitude signify the rise or fall of number of car/taxies in the region of choice with time. Similarly, for the motorbikes, the change in frequency and amplitude gives the information of change in number of motorbikes with respect to time in the selected region.\n\n\n\n Figure 5: Mapping method in Prototype 2\n\n\nCode Snippet\n\nimport pandas as pd from matplotlib import pyplot as plt\n\n\nd = pd.read_csv(\"Data_WM.csv\")\n\n\nimport time, random, os\nimport sc3nb as scn\n\n\nsc = scn.startup()  # optionally use argument sclangpath=\"/path/to/your/sclang\"\n\n\n%%sc\nSynthDef (\"bus\", {arg out=0, freqb= 50, mul=0.7, ampb = 0.2;\n    var f;\n    f = Saw.ar(freqb,mul,0);\n    Out.ar(out,f*ampb);\n}).add;\n\nThe code snippet above imports various python modules in the Jupyter notebook, required to make the sonification. Besides, it reads the data, boots supercollider using the sc3nb module and creates synth definition for buses and coaches using the Saw.ar synth in supercollider.\n\nSimilarly, the code snippet below sets TimedQueue function, which is required for creating sonification with precise timing. Synths are then initiated with a delay of 0.2 seconds and preparation are made for recording. Similarly, with iteration of the data, the algorithm maps the minimum and maximum range of the data to certain specified range of frequency and amplitude of the corresponding synth as explained in Figure 5. Finally, the sonification of each set of vehicle for each region is generated one at a time in wav file format.\n\nqueue = scn.TimedQueue()\n\nt0 = time.time()\ndelay = 0.2\n\n# instantiate synths\n#queue.put(t0+delay, sc.msg, (\"/s_new\", [\"bus\", 1234, 1, 1]))\n#queue.put(t0+delay, sc.msg, (\"/s_new\", [\"car\", 1235, 1, 1]))\nqueue.put(t0+delay, sc.msg, (\"/s_new\", [\"motorbike\", 1236, 1, 1]))\n\nsc.prepare_for_record(0, \"my_recording.wav\", 99, 2, \"wav\", \"int16\")  # buffer 99 will be used\nsc.record(t0+0.1, 2001)  # recording starts in 200 ms\n#sc.bundle(0.2, \"/s_new\", [\"car\", 1234, 1, 1])\nsc.stop_recording(t0+10) # and stops in 1 seconds\n\n# modulate with data while playing through time\nfor i in range (len(d)):\n    onset = scn.linlin(i, 0, 18, 0, 9)\n    #b_freq= scn.linlin((d.iloc[i][4]), min(d.BusCoaches),max(d.BusCoaches), 10, 100)\n    #print(b_freq)\n    #b_amp = scn.linlin((d.iloc[i][4]), min(d.BusCoaches),max(d.BusCoaches), 0.2, 10)\n    #print(b_amp)\n    #c_freq= scn.linlin((d.iloc[i][3]), min(d.CarTaxies),max(d.CarTaxies), 0.2,15)\n    #c_amp= scn.linlin((d.iloc[i][3]), min(d.CarTaxies),max(d.CarTaxies), 0.2,10)\n    m_freq = scn.linlin((d.iloc[i][2]), min(d.MotorCycle),max(d.MotorCycle), 30,300)\n    m_amp = scn.linlin((d.iloc[i][2]), min(d.MotorCycle),max(d.MotorCycle), 0.2,10)\n    #queue.put(t0 + delay + onset, sc.msg, (\"/n_set\",\n        #[1234,'freqb', b_freq,'ampb',b_amp]))\n    #queue.put(t0 + delay + onset, sc.msg, (\"/n_set\",\n     #   [1235, 'freqc', c_freq,'ampc',c_amp]))    \n    queue.put(t0 + delay + onset, sc.msg, (\"/n_set\",\n        [1236, 'freqm',m_freq,'ampm', m_amp]))\n\n\n# shut down synth when finished\n#queue.put(t0 + delay + onset, sc.msg, (\"/n_free\", 1234))\n#queue.put(t0 + delay + onset, sc.msg, (\"/n_free\", 1235))\nqueue.put(t0 + delay + onset, sc.msg, (\"/n_free\", 1236))\n\n\n\n\n Your browser does not support the audio element.\n\nAudio2: Prototype-2, Sonification of Buses &amp; Coaches in the North East Region in England\n\n\n\n\n\n  Your browser does not support the audio element.\n\nAudio3: Prototype-2, Sonification of Buses &amp; Coaches in the South East Region in England\n \n\n\n\n  Figure 6: Prototype-2, Buses &amp; Coaches in the North East &amp; South East Region\n\n\n\n\n\n  Your browser does not support the audio element.\n\nAudio 4: Prototype-2, Sonification of Cars &amp; Taxies in the North East Region in England\n\n\n\n\n\n  Your browser does not support the audio element.\n\nAudio 5, Prototype-2, Sonification of Cars &amp; Taxies in the South East Region in England\n\n\n\n\n  Figure 7: Prototype-2, Cars &amp; Taxies in the North East &amp; South East Region\n\n\n\n\n\n  Your browser does not support the audio element.\n\nAudio 6: Prototype-2, Sonification of Motorbikes in the North East Region in England\n\n\n\n\n\n  Your browser does not support the audio element.\n\nAudio 7: Prototype-2, Sonification of Motorbikes in the South East Region in England\n\n\n\n\n  Figure 8: Prototype-2, Motorbikes in the North East &amp; South East Region\n\n\nWe bet you would also like to listen to the sonification of the third region of our second prototype and see full codes. Great! Just follow the GitHub repository and you will get access to all the sonification, codes, scripts, data files and other files related to this project.\n\nProject Timeline &amp; Contributions\n\n\n\n  Figure 9: Project Timeline\n\n\n\n\n  Figure 10: Division of labour\n\n\nMain findings in brief\n\nPrototype 1 was not revealing the actual content of the data. Although we chose non-overlapping frequency ranges in the mapping to further differentiate between the three sounds, it did not meet our objective of combining both artistic and scientific approach. Thus we decided to improve it and developed prototype 2. The informative qualities of the sounds, now standalone and representing each region/vehicle, could increase immensely. The structural elicitors like amplitude and frequency are creating again rhythm and tempo among others, which could reveal patterns in the content. In doing so it affirms our hypothesis.\n\nAdditional reflections\n\nOur approach to perceptualize the data was from the beginning complementary, using sonification and visualisation. Especially together they have the power to convey information in ways also non-experts can comprehend. Going through the development processes of two sonification prototypes it posed questions how sound can be designed in order to become a standalone mediator of the sonified processes. Finally, the context of sonification and target audience refines the final outcome as well.\n\nFuture Work\nCurrently, the prototype 2 covers sonification of data for individual vehicle for a given period of time only. Running the sonification for the three vehicle types can mask each other and hide potential patterns of data. So, the sound design can be improved to make the sonification distinct as much as possible.\nSimilarly, additional features can be introduced to the system such as the ability to select a preferred time period, to select certain transportation methods and regions etc. A data visualization tool can also be introduced to support the sonification model with multiple methods of visualizations to choose from.\nAnother interesting suggestion appeared to build an API for sonification of traffic data of England, or any country with options for selecting different regions and different vehicles. The API could be extensively used for analysis and or for understanding the growth of traffic in the chosen region via sonification.\n\nAcknowledgement\nWe are very grateful to Anna Xambo, our teacher for the SMC4046 Sonification and Sound Design course for exposing us to many experts and professionals working in the areas of sonification and sound design through this course. We would also like to thank her for guiding us in developing prototype 1. Similarly, we would like to extend our heartfelt gratitude to Thomas Hermann for sharing his plethora of knowledge on sonification. Likewise, we would also like to thank Thomas for providing us with his slides and python script and credit him for the prototype 2. All of them helped us practice, learn and expand our knowledge on sonification.\n\nThank you!\n\nReferences\n\nHermann, T. (2008). TAXONOMY AND DEFINITIONS FOR SONIFICATION AND AUDITORY DISPLAY. 14th International Conference on Auditory Display,      Paris, France June 24 - 27, 2008. Retrieved from: http://www.icad.org/Proceedings/2008/Hermann2008.pdf.\n\nHermann, T., Hunt, A., &amp; Neuhoff, J. G. (2011). The Sonification Handbook (1st ed.). Berlin: Logos Publishing House. Retrieved from:        https://sonification.de/handbook/.\n",
        "url": "/sonification/2019/04/19/The_Sound_of_Traffic.html"
      },
    
      {
        "title": "Brexisonification",
        "author": "\n",
        "excerpt": "The goal in the project is to sonify Brexit, in a way that the audience could interpret new insight from the data through audio.\n",
        "content": "\n  \n  \n\n\nBrexit\nWikipedia article on Brexit.\n\n“Fuck knows. I’m past caring. It’s like the living dead in here.”\n\nAn unnamed UK Cabinet minister’s texts BBC Newsnight editor Nicholas Watt when asked why May was holding another vote on her deal. In doing so, the pretty much summed up an entire nation. (The Journal, 2019)\n\nInspired by the controversy and charade that was, and is Brexit. This project is an attempt to make sense of the data that came from the UK’s National European referendum - by translating it into sound. Named “Brexsonification”, our aims were to try and portray a new outlook on the data from the results, whilst keeping to the tongue in cheek style both social and regular media have often to come to cover the issue.\n\nInspiration &amp; Other works\nWhen looking for other sonifications on the subject of Brexit we found very little, one piece that was heavily artistic, and not directly communicating the brexit data. So we found inspirations from other places. The Sonification &amp; Sound Design lecture series we attended and the SMC blog especially helped explain us understand the concept of sonification, in particular the Thomas Hermanns lecture. Aside from this, one tutorial helped inspire our Max patch. It also helped us troubleshoot as we worked with the prototype. It was the only tutorial using similar technologies to ours:\n\n\nThe Data\nThe dataset that we used is a publicly available archive of statistics on the EU referendum, providing some of the voter and region information data from the results.\n\nWhen we looked at the referendum results we noticed how the different counties that make up England happened to vote quite differently, depending on the location. In London and the south for instance, there was a high proportion of voters who wanted to remain in the EU, whereas in the northern part of England it was common to have a majority to leave.\n\nThese inconsistencies have been debated over the media, and sheds some light into how the Brexit decision was come to. This, including other factors such as voter turnout, were used to create a sonification. We wanted to see if we could portray this in a different light and understand better through sound. At very least it can add to the over bloated media steamroller that is Brexit, in a somewhat original way.\n\nLastly, going in to the project we had a clearer idea on what parts of the data we could highlight, and a possible tone for the project, and the rest often came down to experimentation and trial and error.\n\nOur system\n\n\n  \n  Technologies Used\n\n\nTechnologies Used\n\n  ExcelEd - Editing and adjusting the data set\n  MaxMSP - Main patch for translating the data into parameter values, midi &amp; importing and processing video\n  Max for Live - imports the patch into ableton\n  Ableton Live - Map parameter values to audio and composition\n\n\nWhen deciding how to approach creating the sonification system, we decided to stick to our strengths. We had previously been using Max4Live to process Motion capture data earlier in the academic year. When we came with this objective, we thought we could use a similar technique to process this type of code.\n\nOne advantage to this method is that if we use M4L for different sonifications, we become more adept to the program. Eventually we might be more adept at M4L  and advance to creating auditory displays that can be fed various data sets.\nAnother bonus is that we have experience with ableton, that made the mapping part a lot easier after processing the data - we could then foreseeably get a prototype up in the limited time we had.\nThe Patch\nHere is our completed audio patch! Details can be found in this text file. You are very welcome to download the patch to look in more detail or try out! Just copy the code and paste it into a new patch.\n\n  \n  Audio Patch\n\nAudio Patch\nIn our prototype we used three techniques to process the audio:\n\n  Sampled - Sampled political speech\n  Synthesized - Low drone\n  Effects - Rave Generator (digital sampler), Overdrive, Reverb, Delay\n\n\nEach of these processes were applied to the sonification mapping as well.\n\nMapping &amp; Reasons\nOur projects lead, as you will - came from vocal cuts from Theresa May  (“Brexit means Brexit” - May, 2016) and opposition. This was inspired by the mashup of media that surrounds brexit, and how iconic in particular the prime ministers voice is, in regard to the issue. From here we painted a picture, finding other mappings along the way:\nLead mapping - Remain or Leave?\n\n  Leave EU above 50% - “Brexit means, Brexit!”\n  Remain EU above 50% - “Remain in the European Union!”\n\n\nThe reason behind the choice of sounds is two-layered. In the foreground, we have the “brexit” and “remain” statements. These obviously represent if there is a majority vote to remain or leave in that area, but also have an aesthetic quality that portrays the value of the data.\n\nIf the majority to leave is high in percentage, Theresa May will be higher in pitch, sounding more angry and intense, trying to represent the mood of those voters. If the area has a majority to remain the “remain in the European Union” also plays high in pitch as the majority increases\nBoth of these can sound quite silly in their extreme, however we feel that this tone fits the mood of brexit, and hopefully makes it a little more entertaining.\nDrone\nOn the other, deeper level, we have the ominous drone sound. This is mapped to area (north, south and so on) to create a feeling of dread. It is staccato in nature, a constant deep rumble that represents the intensifying mood among the british population as brexit progressed.\n\nAs each area progressed, the drone would rise a semitone - acting as a information as to when the next area is being played, but also giving a narrative to the whole composition, intensifying the mood further with each rising tone.\n\n\n  \n  Drone sound in ableton live sampler\n\n\nBleep\nWe then synthesised a “bleep” sound, that we used to represent the overall Electorate (people) amount in each area went to vote - up to 2 million! The bleep would rise in pitch as the amount that voted in the area went up. This gave some context, the counties in england can have very different sized populations, so this represented that factor in the demo. (Synthesized sonification).\n\n\n  \n  The use of synthesized sound in Ableton Live\n\n\nOther mappings\n\n  The voter percentage turnout of each area was also mapped to the volume of the voice, which got louder in higher percentages - representing the loudness of the “voice” of that area.\n  Reverb was also mapped to the whole percentage who voted as a counter mapping. It was done in a way so that fewer votes created more reverb. (Effects sonification)\n\n\n\n  \n  The use of effects in Ableton Live\n\n\nVideo (Failure!)\nAlso as an afterthought, we decided to try and make a data reactive video that would enhance the sonification. To do this we got a looped GIF file that we felt fitted the tone of the prototype, and applied FX from the data parameters using M4L. The patch is using the amplitude of samples/synthesized sounds in order to control the amount of the distortion affecting the M4L patch.\n\nTo accompany this, we also ported the area names to the M4L patch in presentation mode, so that on screen this information could help reinforce the sonification, and make sense of the video image. On another note, it was very useful when checking to see if the sonification was working properly!\n\n\n  \n  Mapping the visuals in Ableton Live\n\n\nHere was the “presentation” view from ableton, that showed the relevant text data as the sonification played.\nBoth these visual elements never made it to the final prototype, for reasons we will go into later (although there is an example).\n\nDemos (Audio/Visual)\nYoutube link and final publication of the sonification:\n\nPlease leave any comments that you have, in regards to the sonification, in the comments section.\n\n\nVisuals demo:\n\n  \n        \n         The unused visual elements of the project         \n        Your browser does not support the video tag.\n  \n\n\nReflective notes\nOverall impression\n“Guys! This project has been going really well! We have yet to have a crisis!” - Sam, late into the project.\n\nOur team have progressed in a remarkably steady pace and it has been a pleasure. On the whole the decisions we made to create our prototype have been a success. The tone and choice of the audio is fitting.\n\nAs noted in our report we managed to implement Symbolic, Metaphoric, and Iconic sounds representing data in our sonification. It was important for the aesthetics. Thomas Hermann introduced us to this way of thinking sonification, and we wanted to implement it in our project to familiarize us with sonication as a practice.\n\nMaybe too much? At least for us we were exploring the possibilities that this craft has to offer, while at the same time doing some experimentation!\n\n\n  \n  \n\n\nIn this portrayal of serious, political data we originally tried to remain unbiased. However, with a overarching feeling of “dread” as part of our sonification, we didn’t exactly achieve neutrality. So our mapping and choice of sounds could be seen as somewhat biased in terms of the brexit standpoint. Maybe it begs the question, isn’t the majority of media biased when reporting on brexit? Or even in general? (let’s not go down THAT road)\n\nChallenges &amp; Achievements\nContrary to what is common with these kinds of group projects, we did not have notable challenges with planning the concept. As soon as we decided upon working with Brexit, we were pretty unanimous with how to proceed. We did come into some challenges though:\n\n  Difficulty in MAX reading the data, we solved this by editing the data set in EdExcel\n  The audio part of the patch were being developed in Oslo and the Visual part in Trondheim. While merging them, there was had a synchronization problem. The tempo of the data being read by the Max patch was different for the video than for the audio. Remains unfixed.\n  Also, alongside the region text being displayed in the max patch presentation mode, we wanted to have it displayed on the video as well. But we couldn’t fix that issue as well.\n  Patch would reset all mappings in Ableton Live each time on reopen - big pain in the posterior!\n\n\nThe main failure was the inability to use the video element we had spent a lot of time working on. We attempted to fix the issue after the presentation of the prototype, to no avail. We managed to implement the region text in the video. That was being done by using “jit.op” to merge the text and video in the output display window (success!) but could still not fix the tempo issue - rendering it useless for the project as a whole.\n\n\nOverall we managed to make a sonically interesting audible rendition of the Brexit votes, so the audience perhaps could find him/herself comfortable to interpret some meaning from it. Although, since we have not performed the sonification for any audiences, we have little feedback after the final outcome, aside from the presentation of the prototype in SMC class.\n\nLearning outcomes and future work\nWe may want to create an interactive sonified map of England, using Javascript, in which by hovering over, you get a sound that informs you about the votes in the area.\nAlso, we could create a side project, called Spacexit, which may use our patch to sonify data from spacecrafts.\n\nFurther developments\nThe plan we had in mind for the publication and use of this project was to feed it back to the inspiration that started it - the media that follows Brexit. As with the subject itself, the media has been an influence on the situation, not merely covering it. So, we wanted to complete the feedback loop and show our sonification to the world. One of the comments we received after the presentation was what we wanted to do with our project. We have uploaded it to Youtube, and posted it on our social media. In our minds, we may get feedback and further collaboration opportunities, that could help propel the project further into the future.\n\nOther possible future ideas:\n\n  Create an interactive sonified map of England, using Javascript, in which by hovering over, you get a sound that informs you about the votes in the area.\n  Create a side project called “Spacexit”, which may use our patch to sonify data from the position of spacecrafts.\n\n\nAcknowledgements\nWe wholeheartedly thank Daniel Buner Formo and Anna Xambó Sedó for their support.\n\nRefrences\nRónán Duffy, for The Journal “F**K knows. I’m past caring’ - The week in Brexit quotes”\nRetrieved from URL: https://www.thejournal.ie/brexit-quotes-3-4567342-Mar2019/\n\nProducer: RT UK. (2016, 07.11)  “Brexit means Brexit” - May. Retrieved from URL: https://www.youtube.com/watch?v=KMek1okqphs\n\nKeywords: Sonification, Max for Live, Ableton Live, Brexit, Visuals, Audio\n",
        "url": "/sonification/2019/04/19/brexisonification.html"
      },
    
      {
        "title": "How music related motion tracking can sound",
        "author": "\n",
        "excerpt": "During the course ‘Music related Motion Tracking’ there were several approaches among the students to realize their ideas. The Opti-Track system, new to all of us consists of infrared-cameras, markers and a software with calibration tools. We were exploring the functions from scratch during the first week when hosting the ‘Nordic-stand-still-championship’ on both campus.\n",
        "content": "\n        \n        \n        Here we are recording motion data from Eirik. The video was send through a Mathlab code provided by Alexander Refsum Jensenius        \n        Your browser does not support the video tag.\n        \n\nHere we are recording motion data from Eirik. The video was sent through a Matlab program provided by Alexander Refsum Jensenius\n\nFirst week\nDuring the course ‘Music related Motion Tracking’ there were several approaches among the students to realize their ideas. The Opti-Track system, new to all of us consists of infrared-cameras, markers and a software with calibration tools. We were exploring the functions from scratch during the first week when hosting the ‘Nordic-stand-still-championship’ on both campus’. With the mocap system we were measuring how music affects micro motion while the participants were standing still and being exposed to beat intense music. The winner was the person who was able to move the least.\nIt was a good opportunity to familiarize with the procedure and work flow of motion-tracking including setting up the required items.\n\nSecond week\nEven though most of us were not familiar with methodological concepts of sonification, it was what we ended up doing in the second week. Different skills, interests and backgrounds in our group formed an idea that turned out to be very feasible within the given time frame: Mapping an impulsive sound to an impulsive action in order to create a virtual/air percussion instrument.\n\nForming ideas\nThe speed of the mocap cameras meant that they were very good at producing the kinds of data that we could work with. We recorded data with the motion capture system and would ‘sonify’ it afterwards. First the system needed to be calibrated and set with a ground plane. We then attached markers on the tip of two drumsticks. A recording was made, where Eirik hit a table placed in the recording space, at different positions on the x-axis, and with variable force. We used the table as a reference to a “natural” position on the y-axis (height), to be able to set the sound-trigger point. This data was exported as a CSV (comma separated data) text file and converted to a tabular separated text file in excel to be able to handle the file better in MAX.\n\nPractical Steps\nAfter some reflection on the possible ways of triggering sound, Andreas Bergsland came with the suggestion to use jerk as the trigger point and helped us develop it. Jerk, in physics, is the rate of change in acceleration, and in mathematics this is the derivative of acceleration. When used as a percussive tool, the drumsticks would trigger one of the 4 sounds in the Max patch.\n\nReflections\nOriginally, we were going to use Pure Data as the tool for analysing the data and doing the sonification. Because of lacking documentation, and several bugs were occurring (ex. floating frames in the program window), we chose to switch over to Max 8 since the community is bigger, and it was easier to get help from tutorials/forums. Also Jørgen and Jonas had experience from working with Max from previous projects. With regard to the goal of the week it was possible for us to formulate an idea and gradually implement it to the extent that was initially intended. The steps that needed to be taken were clear to all of us beforehand. We also tried to work together as much as possible in each of them.\n\n\n        \n        \n        Live-Demo: SMC-Students performing in the Portal and Eigil on the Aircussion instrument        \n        Your browser does not support the video tag.\n\n\nLive-Demo: SMC-Students performing in the Portal and Eigil on the Aircussion instrument\n",
        "url": "/motion-capture/2019/04/24/How-music-related-motion-tracking-can-sound.html"
      },
    
      {
        "title": "A generic overview to the 'Sound in Space' exhibition at KIT",
        "author": "\n",
        "excerpt": "Marking the final event before Easter and from our Sonification and Sound design course I was tasked to visit the group exhibition ‘Sound in Space’ that took place on the 11th of April at Gallery KIT, Trondheim. It was also the closing event for the sound art course in which music-technology and fine art academy students (NTNU) could participate.\n",
        "content": "Marking the final event before Easter and from our Sonification and Sound design course I was tasked to visit the group exhibition\n‘Sound in Space’ that took place on the 11th of April at Gallery KIT, Trondheim. It was also the closing event for the sound art course\nin which music technology and fine art academy students (NTNU) could participate. Beside the exhibition there have been also screenings,\nconcerts and happenings. Among the performers were Ellen Lindqvist, Jeremy Welsh, Tijs Ham/Craig, Øyvind Brandtsegg and Michael Duch.\nIt was a a nice afternoon with many events going on and things to explore, however I will introduce just two installations that\nwere created under the umbrella of ‘sonification and interactivity’.\n\nPlayful sonification: Interactive and complicit\n\nBeforehand all students had a short workshop on physical computing. Within a few days they prepared different interactive installations.\nSome participants had never seen an Arduino before nor had any programming experience. Others collected everything they had been doing so\nfar into their work as Øyvind Brandsegg and Daniel Formo told me (teachers). The first part of the sound art course would work with field\nrecordings, composing stories in sound with environmental sounds, which would go hand in hand with the digital composition course,\nhaving audio processing techniques and algorithmic composition. With Arduino again they could quickly protoype their ideas. The sonification process again should reveal itself through a creative\naccess and control of the installation pieces with sensors or other activation tools.\nThe context of interactive sound art can challenge new experiences with music and knowledge also for non-experts:\n\n“[W]orking with sound is an active multisensory experience which bridges the gap between perception and action. Sound making is considered to be a meaningful aesthetic experience not only for musicians but also for users who do not posses expert musical skills. This shift from reception-based to performance-based experience brings new challenges to sound design and sonification practices. .” (Hermann, Hunt, Neuhoff)\n\n\n\n  Talking fox made by a fine art academy student\n\n\nMost of the items in the sound art exhibition were indeed so designed that the visitors could directly\nengage with the interactive installations. The talking fox above in the first figure was made by Mina Paasche, a\nstudent from the fine art academy. A proximity sensor established the interactive part, when approaching\nto the fox he would start talking to you and the closer you get the more distinct the sample would get.\nToo bad I couldn’t get a hold of her so she could have might explained to me what the fox was actually preaching.\n\nAspects on sound art\n\nWhat is sound art though? There is no specific definition out there since it is produced by\ndifferent actors from different disciplines like visual art, composition, sculpturing just to name a few.\nWith digital technology sound art has undergone transformations, especially through the low threshold accessibility of sound tools.\nOften features overlap with ‘electronic art’ e.g the use of technology. Like a visual artwork sound art has no specified timeline;\nit can be experienced over a long or short period of time, without missing the beginning, middle or end (Alan Licht), as sound\nis visceral and emotive it can define a space at the same time as it triggers a memory (Susan Philipsz).\n\nOne of the students I met there was Joel Hynsjö (MuTek). Joel’s ‘Do you remember’ (see below) actually plays with that sense of memory and space in a very practical way. He collected speech in news reports from the worst natural disasters world wide in the last 6 years.\nOn a world map one could see six circles in different regions, the piece is silent until one of the circles is being pushed.\nThere is a voice which interrupts randomly the news-reports, every time you push there is a chance that “do you remember” is played instead of the report.\n\n\n\n  'Do you remember?'\n\n\nThe idea he explains evolved from thinking about an interactive method to recall weather reports but gradually\nmoved into some other sort of information transmission. ‘I just wanted some kind of interface that wouldn’t be that technical, basically it\nis just a paper with rings on it and I liked that. Together with the noise-cancelling headphones I wanted to create an intimate moment\nwithin the noisy environment where one could get more introverted as the reports are getting quite close to you.’ The rings on the paper\nlying on top of 6 pressure sensors which are connected to an Arduino board. The Arduino sends messages to Max/Msp if a sensor gets\npressed which then triggers the radio samples. Joel had some programming experience from the first semester in Max/Msp but most\nof the workshop days he spent learning how the Arduino works.\n\nAnother student I met was Emiel Huijs. He developed an ultrasonic distant sensor that controls different musical parameters.\nA pressure sensor for LFO grain modulation and a little duck gave it an interesting twist.\nListen here:\n\n\n\n\n  Your browser does not support the audio element.\n\nShort excerpt from Emiel Huijs prototype\n\n\n\n\n  Emiel Huij's prototype\n\n\nReflections\n\nIt seems to be good practice to explore sonification by physical computing.\nThe development of an instrument that works with playful and interactive functions looks like a rewarding and entertaining way to learn and take up the subject. It is also committed to understanding each step during prototyping.\nBut it also allows us to experience and reflect on how we normally interact with computers.\nIn the context of art it also has the possibility to be not only useful, but also meaningful. At the same time, the useful skills that are developed can later be integrated into a variety of contexts.\nInteractive sound art installation are a great opportunity to invite non-experts exploring the matter playfully.\n\nReferences\n\nHermann, T., Hunt, A., &amp; Neuhoff, J. G. (2011). The Sonification Handbook (1st ed.). Berlin: Logos Publishing House. Retrieved from: https://sonification.de/handbook/.\n\nAlan Licht: Sound Art - Origins, development and ambiguities\n\nSusan Philipsz, citation retrieved 30.04.2019\nhttps://www.tate.org.uk/art/art-terms/s/sound-art\n",
        "url": "/sonification/2019/04/30/Sound-In-Space-Exhibition-A-Generic-Overview.html"
      },
    
      {
        "title": "Augmented Reality",
        "author": "\n",
        "excerpt": "In the final Portal workshop of this semester we were looking at Ambisonics as a potential way to create an augmented auditory space from the perspective of sound.\n",
        "content": "Creating a virtual/augmented immersive space with Ambisonics in the Portal\n\n\n\n\n\nOn the 24 of April, we went into groups to solve a task handed out by Anders Tveit. Our group’s (Karolina, Sam, Sepehr, Jørgen and Mari) task was to create a virtual/augmented immersive space, using Polycom or Zoom for video and a DAW of any choice together with MIDAS mixer and LOLA for sound. We chose to use Ableton Live together with Envelope for Live plugins (E4L), and a custom OSC M4L (Max for live) device.\n\nWhat constitutes a natural immersive space and in what way can a immersive virtual space be created?\n\nTo design an immersive experience in a way that makes it indeed more natural to spend time ‘together’ is a multi-layered matter. In the final Portal workshop of this semester we were looking at Ambisonics as a potential way to create an augmented auditory space from the perspective of sound. It could be a way of dissolving the playback system from being in the foreground as it is now at any time (along all the other technological means) and getting closer to the idea of immersive practice.\n\nFirst we were kicking off the workshop session by discussing on a more higher level what an natural immersive space might be and what therefore an augmented immersive space could or should do. Does it imitate a natural space by means of technology? As elaborated in other posts, spending many hours in a conference system that is not rendered for the human sensory system, can create fatigue symptoms and also some sort of exhaustion. In ‘The road to immersive communication’ immersive communication is defined as exchanging natural social signals with remote participants, as in face-to-face meetings, and/or experiencing remote locations. Moving towards a playback system that imitates the way ears perceive sounds in a ‘natural’ environment might be a step in the right direction.\n\nPlan and create an efficient routing setup between the technology, what type of hardware is needed?\n\nWe decided to create an immersive space that included both a natural space and also an augmented space which could be experienced both in Trondheim and Oslo.\n\nIn regards to the natural sounds, we used a waterfall and a birds singing sound. For the augmented reality part we used two microphones, each being placed in a corner of the room. These 4 sound sources were sent to Ableton, imported into the E4L Source Panner. This device takes a stereo input and encodes it into 16 channels of high order ambisonics. We chose to place one sound source in each corner of the augmented space. All four sound sources were then sent to a E4L Master Bus, which decoded the sources to binaural. In the end we had an immersive space which included both a natural and an augmented.\n\n\n\n\n\nFor the next step we wanted the user to be in control of his/her placement in the space. The output of the DAW was 2 channel binaural, which the participant could hear by a pair of headphones. The output sound was also being sent to Oslo over LOLA and the audience in Oslo could use a pair of headphones to experience this space in parallel to Trondheim.\n\nIn order to have the user in control of his or her placement in the immersive space, we used a mobile app by the name of OSC Controller. We chose to use 4 sliders that could send OSC messages to our Max for Live device called OSC Device Control. Each of these sliders were then used to control the radius of our four sound sources. In this way, the user could control his/her distance from the sound sources in the space.\n\n\n\n\n\nVideo:\n\n\n\n\nFuture use of the custom “OSC Device Control” plugin\n\nThe M4L device is built using the M4L Live API. With this you can use any application that can send OSC messages to control device parameters in Ableton Live.\n\n\n\n\n\nIn the plugin you choose a port you want to use for communication over UDP (9999 is default). The plugin is set to receive OSC messages from four sliders from the Osc Controller app by default. These four addresses can be changed to fit your OSC controller of choice, and you can choose which device and parameter each address will control.\n\nIn the portal in Trondheim we have now available both an optical tracking system called OptiTrack and a radio tracking system called Pozyx. Both of these systems can send tracking information as OSC messages over UDP. By connecting any of these two tracking systems to the OSC Device Control plugin, you can control any device with positional data. If you are walking around the portal with a wireless microphone, you can for example use the E4L Source Panner plugin and let the azimuth and radius of your microphone signal be controlled by this movement data. This could then be decoded to binaural and sent to Oslo. A setup like this in both Oslo and Trondheim might improve the immersiveness of the portal space. At least it would be intersting to experiment with.\n\nChallenges\n\nWhen testing our system, we were facing a few issues that made it difficult to get enough time to make the system work exactly like we first have imagined. At least on the Oslo side, we first couldn’t get the OSC controller to work on both phones. But eventually, we got it working on one of the phones, and we could start testing out the system. To calibrate the MAX patch with the OSC controller and with Ableton, we had to use a scale object in the MAX patch. This was working fine with one fader, controlling one parameter, but a problem occurred when we wanted to add an extra dimension to the system, with a second fader.  Unfortunately, we had too little time to solve this problem before the final testing of the system.\n\n\n\n\n",
        "url": "/networked-music/2019/05/10/augmented-reality.html"
      },
    
      {
        "title": "Scenarios in the Trondheim Portal during the spring semester-2019",
        "author": "\n",
        "excerpt": "We have had numerous scenarios set up in the portal in the period of January-May 2019. Each of the scenarios are unique and therefore serve specific functions. This blog presents four of such scenarios with a bit of discussions on the advantages and challenges with the set ups.\n",
        "content": "\n  \n  Vår Portaltog i Trondheim\n\n\nIntroduction\n\nThere are many blogs in The blog of SMC explaining various technologies and audio set up that we have been exploring in the portal. However, only few of them have addressed the overall physical layout of the portal with technical equipment using various technologies to meet specific tasks, for example, A-team-First Week, M.A.D.E - setting up Tico and Group A - Setting up for a joint group presentation. While completing one whole year of the SMC Studies and spending one semester in the new portal in Trondheim, we have had different scenarios in the portal for different events like lecture, seminar, conference meeting and so on. We also hosted a series of talks, workshops and started both running and streaming of the WoNomute from the portal. This blog is an attempt to fill the gaps and documents how the portal in Trondheim has been used to facilitate different modes of communication in various events during the spring semester from January-May 2019, highlighting on the physical layout and technical solutions opted during those occasions.\n\nLogistics &amp; Modes of Communication\nWe have been working with three display screens for visual solutions in the portal in Trondheim. As shown in Layout 1 below, Screen 1 is a unit combined with 4 separate screens (each of approx. 140cm x 79 cm) which links with each other and works as a single big screen (approx. 280cm x 157 cm). Screen 2 is a 65-inch TV mounted on a mobile stand and Screen 3 is a full length wall finished with special paints for making it a screen suitable for projectors. So far, the physical layouts of the portal have been created based upon the choice of these display units.\n\nFurther, we have been using Zoom as a primary mode of communication between the two campuses in Trondheim and Oslo. Although it has a bit of issue with latency, it has been working quite well. Moreover, we have been having a stable and uninterrupted connection via zoom all through the semester. There is a dedicated computer which is only used to run the zoom in the portal. In addition to Zoom, we have been using LOLA to transmit the audio between the two campuses. It also has its own dedicated computer which is more powerful than the Zoom pc. But most of the times, we are using Zoom only for video conference while LOLA for the audio as the latter delivers a low latency high quality audio solution. However, we have used ZOOM for both audio and video solution on few occasions when LOLA did not work. Among other technologies, we have TICO and Polycom available for both audio and visual modes of communication. However, we never worked with TICO in this semester and Polycom was just used for some testing couple of times. Besides, for managing the audio signals we have been using the Midas ‘m32’ mixer. There are of course a lot of other equipment like different types of microphones, cables, cameras, switch boxes and so on and details regarding them have not been covered in this blog.\n\nScenario-1: Lectures / Conference meeting\n\nPhysical Layout\n\n\n  \n  Layout-1: V-shaped lecture facing Screen 1\n\n\n\n  \n  Layout-2: V-shaped lecture facing Screen 1 with frontal space for teacher\n\n\nWe have tested different seating layouts to hold lectures and meetings in the portal in Trondheim, for example U-shape, V-shape, L-shape and so on. As we were only 7 students here, 8 seats were enough for lectures with one seat for our teacher. The Layout-2 above has an additional feature than the Layout-1. The standing height working table placed in front of the students in Layout-2 gives the teacher an advantage of facing directly towards the students in Trondheim in front and the students in Oslo through the projection on the wall (Screen 3) behind the Trondheim students. Both layouts have been used for holding lectures here in Trondheim during the SMC4048 Audio Programming - Spring 19 course while the Layout-1 has also been used for holding initial Monday meetings in the SMC4022 Physical-virtual communication and music 2 - Spring course this semester.\n\nTechnical solutions, Advantage &amp; Challenges\nLayout-1 requires only one camera and it makes it very easy and simple to connect a USB supported camera (in our case the logitech ptz camera) to the zoom pc. All we need to do is open a zoom meeting through the zoom tablet that connects wirelessly to the zoom pc. Then adjust the camera zooming in and out as per preference while waiting for participants to join in. Zoom does not require any other systems for the audio but as we are interested in achieving one of the best solutions, we are using LOLA instead. And while using LOLA, we mute the audio signals from Zoom coming in and or going out from the portal via the mixer. We do not require multi-channel audio for these scenarios and only send and get sterio signals to and from the Oslo portal. The LOLA pc is also connected to ADAT interface that connects the mixer to the soundcard of the LOLA pc and thereby makes AD/DA conversion work. Thus, we can choose either to send digital or audio signal to Oslo using the RME TotalMix software in the LOLA pc. Oslo can do the same but they are having some other interface.\n\nFor Layout-2 the audio part is the same, but it requires an additional camera to capture the video footage of the presenter. This has been solved mostly in two ways. Either by connecting the camera to LOLA pc and joining in the zoom meeting or using some other laptops and using its own camera or connecting any other external camera to the laptop and then joining in the zoom meeting.\n\nWhen it comes to screens, Screen-1 has been mostly used to display the presentations. Similarly, Screen-2 has been used mostly to display the Oslo portal while the Screen-3 has been mostly switched off in these two scenarios. However, it would be ideal to have the Oslo portal also being displayed on the Screen-3 as per the Layout-2 since the teacher/presenter would be facing to the Screen-3. And ideally the cam-2 should be moved towards the Screen-3 so that it will help in establishing eye contact between the presenter and the portal in Oslo.\n\nScenario-2: Lecture, Workshops and Series of Talks from Experts and Professionals\n\nPhysical Layout\n\n\n  \n  Layout-3: L-shaped lecture facing Screen 1\n\n\n\n  \n  Layout-4: L-shaped presentation facing Screen 1 with presenter in front\n\n\nWe had to try a new set up for hosting a series of talks from experts and processionals in the SMC4046 Sonification and Sound design - Spring 19 course this semester. Layout-4 came out to be perfect because it was also flexible to be used as a scenario for workshop or lecture with a very little re-arrangement. For example, with just unplugging the presenter’s laptop and removing the cam-2 gives the Layout-3 which was used for all the lectures during the SMC4046 course in the semester. In this case, the teacher gave her lectures while seating among the students.\n\nTechnical solutions, Advantage &amp; Challenges\n\nThe audio solution for both Layout-3 and Layout-4 is, more or less the same in comparison to previous scenarios. Similarly, the visual solution for Layout-3 and Layout-4 is also similar to that of Layout-1 and Layout-2 respectively. However, there is an advantage in Layout-4 for visual solution over the Layout-2 mainly because of the location of the Screen-1, Screen-2 and position of the presenter. Here the presenter can look at the Screen-2 and at the same time maintain an eye contact with the Oslo portal as the cam-2 is closer to the Screen-2. The presenter’s position also makes it easy to communicate well with the Trondheim students and again the Trondheim students can also communicate well with the portal in Oslo as Screen-2 is just in front of them. However, the two campuses are still at around 60 degrees to the presenter. That means he/she can only focus on one campus at a time. In order to have an eye contact with both campuses at the same time, Layout-2 could be a promising solution with Oslo being projected on the Screen-3 and cam-2 being moved closer to the Screen-3.\n\nScenario-3: WoNoMute\n\nPhysical Layout\n\nAlthough all the scenarios are different from one another, but most of them have only few dissimilarities. In my experience, the layout of WoNomute is quite similar to Layout-2. The only big difference is the V-shape is changed into a linear array of seating for the audience, cam-2 is located at far end corner close to Screen-3 and there is an additional camera, the cam-2. However, it is an open talk show where the number of audienceS taking part can go from 20-25 at each end. Perhaps, the portal in Oslo can house a bit more than that. But the most interesting part is, the talk show is streamed live online. It adds in some kicks to the crew members sometimes because we have seen many strange technical difficulties that make it challenging to do the job. Nonetheless, right from the beginning of the semester, we have been able to make a stable layout (Layout-5) for hosting the talks. It also involves a bit of physical works on removing all tables out and bringing in enough chairs and vice versa after the end of the talk.\n\n\n  \n  Layout-5: Wonomute facing Screen 1\n\n\nTechnical solutions, Advantage and Challenges.\nThe audio solution is again more or less the same in this scenario as well. The only difference is the WoNomute uses OBS as an intermediate interface for the live streaming of the talks. As such, the OBS requires audio inputs from the mixer and video inputs from\nthe presentation, cam-3 and video footage of both audiences in Oslo and Trondheim. The live streaming has been working quite well except for the last talk where we faced some super weird problem that we never faced before. However, when it comes to the usage of the screen, Screen-1 has always been used to display the presentation (the presenter logs into the zoom meeting room and share the presentation for this purpose), Screen-2 has been used to display the WoNomute logo while Screen-3 has been used to display the Oslo audience. This set up is quite o.k, but the only challenge is to have the Oslo come into play on Screen-1 during the ‘Question Answer’ session of the talk. It is possible to do so by making the presenter aware beforehand to stop sharing her screen while presenting during the Q &amp; A session, but this has not been not a proper solution. Besides, getting video feeds from the same Zoom meeting (audience of Trondheim and Oslo) into the OBS using window capture technique in the OBS seems both tiresome and unstable as it gets changing every time a new participant joins the zoom meeting or if any participant leaves the meeting. If this happens during the time of streaming of the audience’s scene then the stream may also be affected. This might be prevented though using another separate zoom meeting between the LOLA PC and any other computer in the portal in Oslo which should be connected to a high-resolution camera and sending the audience’s feed from Oslo to the LOLA pc. This has not yet been practiced but could be a probable solution worth testing out soon.\n\nAnd regarding the challenge with getting the Oslo audience in both of the screens (Screen-1 and Screen-3) an easy solution would be to connect one of the screens to a different computer and pin the desired video feed onto the screen. Because, zoom seems to be restricted when it comes to pin the same video on two or more screens.\n\nAcknowledgement\nWe have been greatly successful in organizing and re-organizing different scenarios in the portal as per the requirement of the set ups. However, this would not have been possible without the help of our dedicated teachers and all of us 💪🏽 hard working students😜. Thus, I would like to take this opportunity to thank our teachers: Robin Støkert, Daniel Formo, Andreas Bergsland, Anna Xambo and Anders Tveit for their guidance and support in creating different audio-visual scenarios during the entire semester. I would also like to extend my heartfelt gratitude to all of my batchmates for being out there and working together in solving problems and issues in setting up the portal. Thank you all!\n",
        "url": "/networked-music/2019/05/12/Our-Portal-Train.html"
      },
    
      {
        "title": "Blue sky visions on sonic futures",
        "author": "\n",
        "excerpt": "Pedro Duarte Pestana was the final guest speaker who joined us virtually with his presentation ‘Career Management in Music Technology / Knowledge Engineering in Music Technology and Sonic Arts’ from Porto. Pedro was the first student in Portugal who held a PhD in computer music. He works as a researcher and consulter for emerging audio technologies and intelligent systems.\n",
        "content": "Pedro Duarte Pestana was the final guest speaker who joined us virtually with his presentation\n‘Career Management in Music Technology / Knowledge Engineering in Music Technology and Sonic Arts’ from Porto.\nThe Sonification seminar series was part of the SMC Spring semester 2019, curated by Anna Xambo. This session was facilitated by Eigil Aandahl.\n\nPedro Duarte Pestana (Lisbon, 1975) has a background in engineering and worked professionally as an sound engineer and music producer\nfor 15 years before entering academia full-time. It is a pleasure and a very interesting challenge to introduce\nhis complex curriculum vitae along his thoughts and projects he was and is still involved in. As they do mainly gravitate around intelligent audio technologies in musical and non-musical context, he works as a researcher and consulter (intertwined with industry) for emerging audio technologies and intelligent systems.\n\nPedro was the first student in Portugal who held a PhD in computer music. He teaches currently at the UCP in Porto\ninteractive sound design. Recently Pedro worked in media art installations and with high profiled artists such as Bill Fontana. In one of his other ‘previous lifes’ as a sound engineer, he worked for figures in the classic acoustical portugese scene like Teresa Salgueiro, Pedro Joia, Mario Laginha among many others. Pedro was mostly known for bringing consistently new and unconventional approaches into the field, his mindset was always that of a tinkerer and hacker. Frustrated by the iterative processes that came along the profession of an audio engineer like microphone placing, he started to develop and research on automated mixing procedures. Pedro would for example build a robot that could position the microphones for him. He also designed a multi-touch interface in the 1990s, long time before they were commercially available. Following everything that seemed interesting to him between his MSc and PhD like teaching or building recording studios, he could gave us pretty precious, interdisciplinary advices for pursuing careers in music technology and its related fields.\n\n\n  \n  Biographical overview. Retrieved from Pedro Pestana's presentation\n\n\nAutomated Mixing systems\nPedro’s vision and that of his colleagues during his PhD studies was to artificially replicate what a human mixing engineer could do.\nAn algorithm performs with more ease and makes it respond to different heuristics at a click of a button, so sais Pedro.\nMany aspects of his research in automated mixing systems sparked wider interest and attracted also the industry. The community and industry around this approach grew quite fast. It challenged him to push his research in a certain direction that were more adapted to marketplace driven questions.\n\nAs a Gulbenkian Professor he currently researches interactive sound design and audio-content analysis. The novel approach here said Pedro, is that interactive sound functions as a social mediator of engagement between human actors. A machine, always equipped with some sort of intelligent hearing, either acts as an an antagonist or as a ‘helper’ within human interaction.\n\nSonification of care -  or healthy sonic futures\nIn the Gulbenkian project Pedro’s research group focused on 3 case studies based on generative algorithms either being agents of\nfacilitation or blockers of everyday interaction between humans. For the context of the SMC sonification course there is\nespecially one interesting case study among the presented. In that, sound design is being applied for aural feedback in live support machines and EGG systems by health care professions. How can the sound be designed in a way that it has an informative character to care givers and at the same time enables to soothe those in need of help? He calls the system behind the process curated sonification as there is no one-to-one mapping of features to sonic parameters. Apparently most of the studies for this project were undertaken with offline data which poses questions to real-world results. However the analysis is still ongoing.\nBased on his own career, he pointed out possible career options and sees great potential in “audio analytics” in particular, as reflected in the figure below.\n\n\n  \n  Paths and directions in music technology. Retrieved from Pedro Pestana's presentation.\n\n\nQuestions and answers\nOne important question was tackling the elaborations on corporate audio analytics. How can privacy be sustained in an unregulated field where data collection is so easy and profitable with almost no safeguards?\nBased on the fact that audio analytics are about to enter the internet of things with extra-ordinary precision, machines around us will get better at their listening performance likewise. The fluctuations of our own environment in terms of mood and emotional prediction are getting increasingly transparent through utterances, footsteps and breathing intensity for example. Predicting future happenings would come with ease and great accuracy. The concept of privacy, he confirms, will alter irreversibly within the next decades.\nAnother questions pointed out on the academic establishment, especially the dilemma of publishing (‘publish or perish’) he illuminated.\nThe common patterns of recognition trouble would have led into a situation in which many researchers face the\n(on the long run economic) pressure of publishing for the sake of being perceived rather than out of the need to contribute with new knowledge. Consequently there would not only be a certain predictability in some areas of research, but also having too much similar information at hand potentially risks dissolving the essential information, to summarize it briefly.\n\nReflections\nThere were a lot of interesting aspects, aynyhow the most relevant from a societal point of view I think were Pedro’s elaboration on audio analytics which were mentioned in the Q&amp;A. Having a a clear opinion and a statement from a scientific researcher with audio engineering background who basically has experiences with machines acting as ‘human antagonists’ gives it even more weight. It poses furthermore new challenges for societies and thereby potential opportunities and new tasks. From a personal aspect however I found it helpful to become aware of how a field in academic research can spark also into the market place. Even I don’t see myself working with automated mixing technologies for example, it could apply for any new and relevant field. Also the possibilities that come along with emerging technology research. While ‘classical’ scientific academic disciplines would require ruthlessness to achieve a certain degree of interesting and valuable work, a novel area would act much more forgiving towards mistakes.\n",
        "url": "/sonification/2019/05/17/Blue-sky-visions-on-sonic-futures.html"
      },
    
      {
        "title": "Experiencing Ambisonics Binaurally",
        "author": "\n",
        "excerpt": "During the SMC2022 Physical-virtual communication course we have had two sessions where we explored multichannel and ambisonic decoding. The first session, on February 27th, was mainly about recording a four channel A-format stream with an Ambisonic 360 sound microphone. We converted the A-format mic signal into a full-sphere surround sound format which was then decoded to 8 loudspeakers layout we placed evenly across the portal space. We have used the AIIRADecoder from the free and open source IEM plug-in suite.\n",
        "content": "\n\nDuring the SMC2022 Physical-virtual communication course we have had two sessions where we explored multichannel and ambisonic decoding. The first session, on February 27th, was mainly about recording a four channel A-format stream with an Ambisonic 360 sound microphone. We converted the A-format mic signal into a full-sphere surround sound format which was then decoded to 8 loudspeakers layout we placed evenly across the portal space. We have used the AIIRADecoder from the free and open source IEM plug-in suite. Although the setup was not ideal, mostly because of the LOLA connection between the Trondheim and Oslo portals, and the fact that the loudspeakers were not at the same level of the listener’s head, we did manage to get a 3D spatial sound stream between the two campuses.\n\nFor our second session on April 24th, we focused on decoding binaurally the input from four large diaphragm condenser microphones placed in a square formation. It required us to use headphones for a better perception of the sound stream and enhanced immersive environment. The decoding was done this time with E4L (Envelop for Live) free collection of spatial audio production tools. We created four audio channels and placed the E4L Source Panner device on each of the audio tracks. On a fifth audio track, we set the E4L Master Bus device and directed all output from the four microphones into that channel. You can see the setup we have used in Ableton Live in the pictures below.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe were given the time to experiment with this setup. Several class members have started to whisper, talk loudly or sing into their dedicated microphones, trying to create unique sonic material for the listeners who wore headphones in Trondheim and Oslo. I seized on the opportunity to play the saxophone with this ambisonic setup and resorted to the Giant Steps melody which seemed to be a recurring theme whenever I am being asked to play something (have you checked out my (Giant Steps Player already?).\n\nI stood in the middle of the four-microphone setup and played into each of the mics at a time. During the first round of the melody, I played segments of the song into each different mic, and for the second round of the tune, I played (almost) each note into a different mic. That resulted in an audio clip where you can hear me play parts of the melody in four positions (Front-Right, Front-Left, Back-Right, Back-Left). I would recommend listening to this audio clip with headphones to be able to clearly identify the direction I am playing from (from the listeners perspective).\n\n\n   \n     \n     Your browser does not support audio tag.\n   \n\n\nWhen listening to it again in a quiet environment at home, I now wish I had more time to experiment with playing longer tones while moving between the microphones (maybe a ballade would fit better next time?). I wanted to hear the movement of my sound in the spatial environment.\n\nIn any case, I decided to continue the extermination at home with several sound effects like footsteps, a clock ticking and writing marker. I placed all sound samples on an audio track containing the E4L Source Panner device and mapped the Azimuth knob to a physical knob on my midi controller.\n\n\n\n\n\n\n\nGive it a listen (with headphones of course) and see what you think! The Envelop for Live free tool pack has some great collection of devices which can really utilize space as a compositional element, and it would be interesting to see how this type of devices can elevate my artistic output in the future.\n\n",
        "url": "/networked-music/2019/05/19/Experiencing-Ambisonics-Binaurally.html"
      },
    
      {
        "title": "Sonification of Near Earth Objects",
        "author": "\n",
        "excerpt": "As a part of a two-week workshop in the Sonification and Sound design course, we worked on the development of a self-chosen sonification project. For three days we explored how to design and build an auditory model of Near-Earth Objects (NEO) with data collected from NASA.\n",
        "content": "\n\n\n\nIntroduction\n\nHow can we sonify data about objects in space passing by Earth over a chosen timespan, in a way that conveys characteristics of the objects?\n\nAs a part of a two-week workshop in the Sonification and Sound design course, we worked on the development of a self-chosen sonification project. For three days we explored how to design and build an auditory model of Near-Earth Objects (NEO) with data collected from NASA.\n\nWe intend to make future NEO events accessible for people by turning it into an auditory composition. As they orbit the Sun, NEOs can occasionally approach close to Earth. A “close” passage astronomically can be very far away in human terms: millions or even tens of millions of kilometers. These immense distances in space are hard to grasp for us living on earth. We hope that our auditory display will provide a unique (sonic) view on near-earth objects as they pass by earth. To do this, we need to overcome the challenges of getting data from an online source, parse the data in a way that creates a sound at the right time and design a sound that conveys the characteristics of each data point in a clear manner. Ideally, our auditory composition should be self-explanatory upon listening, but we are aware that without having any background describing the composition and its elements, listeners might experience difficulty knowing what they are listening to.\n\nThe user of the system will be able to select a timespan into the future, and then listen to a composition generated from the data. As the selected timespan could be a year or more, we decided to scale one year of events down to 6 minutes (30 seconds per month). As a user of the system, you will be listening to the events happening over time with physical parameters like velocity, magnitude and distance of the NEO shaping the sound of the object.\n\nTheory, background and similar work\n\nSonification is described by Gregory Kramer and colleagues (2010) as “the use of non-speech audio to convey information”, and the sonification criteria specified by Thomas Hermann (2008) are met by our project:\n\n\n  Sound has to reflect properties and/or relations in the input data.\n    \n      Magnitude → tone quality\n      Nominal approach data + velocity → amplitude/doppler effect\n    \n  \n  A precise definition of how interactions and data cause the sound to change must be provided.\n    \n      Our blog post provides this additional information.\n    \n  \n  Sonification has to be reproducible so that the same data and interactions/triggers must result in structurally identical sound, which does not imply sample-based identity.\n    \n      Our model can be reproduced and is not sample-based.\n    \n  \n  The system can intentionally be used with different data, as well as in repetition with the same data.\n    \n      Our model is meant to be used for any time frame extracted from the API\n    \n  \n\n\nOur work was inspired by a sound installation made by Øyvind Brandtsegg called VLBI Music which also uses astronomical observations as a source for sonification. Please take a look at this blog, written by our fellow student Eirik Dahl, which describes Brandtsegg’s installation.\n\nSystem\n\nFor our project, we chose to work with data provided by the SBDB Close-Approach Data API. This API contains a database with all the future NEOs, and we are able to sort and filter the data with query parameters when building the URL.\n\n\n\nFig. 1 - System flow\n\n\nWe decided to use Max/MSP together with Node for Max. Node for Max was used for data retrieval and extraction, and Max/MSP for data processing and sound generation. We have been able to retrieve the selected data from the API using HTTPS requests. The parameters we extracted from the data were time and date, velocity, magnitude and distance (Fig. 2). We then sent them as arrays to Max/MSP for further processing (Fig. 3).\n\n\n\nFig 2.  - Data to arrays with JavaScript\n\n\n\n\nFig 3.  - Data processing in Max/MSP\n\n\nThe sound from our system gets generated as each entry in the dataset gets passed to a synth voice in Max/MSP. Here, the distance and velocity gets used to create a model of the sound passing by, creating a doppler effect, much like the sound of a car going past at high speed. The magnitude of the object gets mapped to the pitch of an oscillator, creating a single tone for each object.\n\nTimeline\n\n\n\nFig 4. Timeline\n\n\nProgress after the end of the course\n\nThe last day of the course, we presented an example of our system using randomly generated mock data played at slightly varying intervals. We still had not cracked how to read each entry of data at the correct time, so the sounds were only slightly varying and were not based on real world data. After the end of the course, we continued to develop a solution that would create a sort of timestamp based on the date of each object. With this we could calculate the delay between each entry to the next and pass the data to the sound generation in Max/MSP at those intervals.\n\nIn the end, we managed to use a naive conversion of the date in Max/MSP (Fig. 5) to get the number of minutes after the start of each year and then scale it to get the delay in milliseconds. The solution isn’t generally applicable, meaning it only works for 2019 and 2020, but it is possible to improve the function to make it usable with any year and give the correct time, even with leap years.\n\nThis solution made it possible to use the correct data for sonification and is the key for the end result presented in this blogpost. In addition, the scaling of data to sound parameters was tuned to create more distinct sounds based on the object’s characteristics.\n\n\n\nFig 5 - Date conversion\n\n\nHere you can hear an example of how the NEO data is sonified by our system:\n\n\n\n  \n  Your browser does not support the audio element.\n\n  Audio demo of NEO sounds\n\n\nReflection\n\nThe main challenges for the project were the use of the unfamiliar tool Node for Max and the data file (JSON) format which made it difficult to work within Max/MSP. It is important to mention that the decisions we made regarding the relations between the NGO parameters and the sound parameters, were based on a short discussion we conducted within our group. We decided to go with what seemed, at least to us, to be a clear and obvious connection of sound elements and NGO parameters. We view the presentation of a clear dataset, being able to implement the data with Node.js and developing the sound generator within Max/MSP, as our main achievements. In the end, we made the system work mostly as intended, resulting in sound from data.\n\nPlease take a look at our project files in the Code repository.\n",
        "url": "/sonification/2019/05/20/NEO.html"
      },
    
      {
        "title": "Seismerssion: Retrospectives on an Audio-visual installations",
        "author": "\n",
        "excerpt": "Seismerssion is the title we gave our Applied Project in the context of the SMC spring semester 2019. This audio-visual installation is dedicated to the widely unknown issue of sound pollution in the ocean. In collaboration with NTNU Oceans, an intership was established to develop and implement a public installation concept for 2 different venues.\n",
        "content": "\n     \n\n\nIn the context of the SMC Applied Project 1 our group collaborated with NTNU Oceans to design an Ocean Sensory Room within an internship project. Maria Azucena Gutierrez, our external supervisor tasked us to create an aesthetic environment that optimizes the perception of scientific content. The only requirement we got was to create an immersive space that would result in a public installation for the NTNU Ocean week in May 2019 and for “The Big Challenge” Science Festival in June 2019. We dedicated the Ocean Sensory Room to noise pollution in Norwegian waters, backed up by a scientific report from the Marine Research Institute. There was no explicit mentioning of Acoustic Pollution during the ‘Breakfast Ocean Meeting” in January, therefore we decided to raise awareness about the important issue.\n\nPlanning period Seismerssion\n\nOur project, called ‘Seismerssion’ is an audio-visual installation that enables participants to explore the issue of noise pollution through an interactive soundscape. The title of our installation, ‘Seismerssion’ is a word-game and derives from the term reflection seismology and immersion.\n\nInspired by Bruno Latour’s lecture at HKW Berlin, where he talks about the blue planet satellite perspective that is taken by the media when talking about anthropocentric issues, enhances the personal feeling of being tiny and too distant from the anthropocentric issues. We were keen to avoid a “global and general perspective” but rather to approach empathy through a local attachment, so that the participants are able to relate more to their immediate environment. In other words, we were lucky to find new research about the topic in a report from January 2019, specifically addressed to Norwegian endeavours in the ocean. The HFI report called ‘Advice form the Institute of Marine Research of anthropocentric noise in the sea: seismic,electromagnetic surveys and underwater explosions’ advices industries and other institutions operating in the ocean in how to avoid harm to marine life by noise. We extracted especially the information on reflection seismology for our project.\n\nIn brief, one of the main acoustic polluters in the ocean are seismic airguns explosions that are being deployed in search for gas and oil. These explosions are reflecting back from geological layers in the bedrock while releasing almost uninterrupted short and powerful low frequency sound waves. As sound travels 4 times faster in the water, it is especially for creatures who process their environment mainly with their acoustic organs an harmful event with vast impacts (Clark). The report, as many other sources affirm that acoustic pollution has not yet been fully explored.\n\nIt was clear that we would need an artistic approach to achieve our vision of an immersive, experience based perception of a scientifically proven topic. Other than plastic pollution of the ocean, sound is neither visible nor tangible, we had to find adequate means to communicate this quite complex topic. By aestheticizing the experience of the seismic air guns explosion we could make it comprehensible and expand the sonic event to other media. As many resources went in the beginning to think and concept visual narration, we soon realized our restrictions in time and decided to work only with audio to communicate the issue. From there we gradually added visual elements to it. At the end we were able to engage all possible sensations like touch (the rumbler), sound, vision to navigate in a darkened room. The spatial framework was quite important in order to engage the participant as best as possible and absorb them from any distraction.\n\nTechnicalities\n\nThe participant tracking and soundscape automation feature was the main engine to realize many aspects of the installation concept. The soundscape, dramatised with suspense and surprising elements would change according to the position of the participants in the room.\nWe intended to “submerge” the participants into the ocean with sound. Equipped with wireless headphones, participants would be able to walk freely around the room, where an “underwater” ambience would always be heard. The room would then be divided “sonically” into different zones, with varying soundscapes ( marine mammals and sonars, fish and whales, ships and a vibrating platform with a seismic boom) depending on their position. The elements would consist of recordings of ocean life and noise pollution sources, with each zone having its own topic. In the centre of the room we would have a vibrating platform, with the intention to communicate the impact of seismic activity.\n\nOriginally it was meant to be used for both head orientation and participant localisation, but because of limitations in hardware and time for implementation, we only chose localisation. Pozyx turned out to be the tracking system alternative for our purpose. For running Pozyx we used a Python script, which enabled tracking of multiple participants. All data was then sent to Max for Live, programmed for controlling the automation of soudscapes in Ableton Live.\n\nThe overall process of creating the Max for Live device can be summed up in these points:\n\n\n  Retrieval of OSC messages over UDP (Figure 2).\n  Extraction of x and y coordinates from tag 1 (Figure 2).\n  Defining grids for zone 1 (Figure3).\n  Using the Live API (https://help.ableton.com/hc/en-us/articles/209071389-Controlling-Live-using-Max-for-Live-Tutorial-) and the Live Object Model (LOM) for controlling Ableton Live parameters. (Figure3).\n  Copying zone 1 and using the logic for making the rest of the zones.\n  Copying the sub-program called “zones” to work for all 4 tags (Figure 2).\n  Adding video playback for two projectors, and making videos turn red when “stage rumbler platform” is activated, using Jitter (Figure2).\n\n\n\n\nFigure 2\n\n\n\n\nFigure 3\n\n\nIn Ableton Live every zone is organized in session view, each on its own track (Figure4). Audio is sent to the different participants on the return tracks, with an aggregate device (Figure 5) as output, where all Bluetooth headphones has its own stereo channel. The Max for Live device controls the sends on each track/zone, where “send A”is participant A, “send B” is participant B and so on. Whether the sends are “on” or “off” depends on where the participants are in the room. The device also triggers the “entering” sound and the “seismic boom”. The seismic boom is not only sent to the participants, but also to the “stage rumbler platform”\n\n\n\nFigure 4\n\n\nEvaluation\n\nDuring the Ocean Week we handed out questionnaires to the participants, asking them questions about the experience regarding immersion, noise pollution, audio and visuals. We got 28 participants to answer, and the response suggested that it was well received in all aspects. We forgot to hand out questionnaires for the “Big Challenge” science festival in Trondheim, but were always in dialogue with the visitors and got overly positive feedback as well.\n\nHere are some impressions from the preparation process in the Portal up to the Big Challenge Festival.\n\nAcknowledgements\n\nBeside the scientific facts on reflection seismology provided by the HFI and the Marine Research Defence Institute (FFI) who are collaborating on this particular issue, they generously shared various recordings from marine life with us. The sound files from their (Lise Doksætter Silve, Petter H. Kvadsheim) excursions were extensively used in our installation. From Robin Støckert, not only a technical wizard and great consulter during our planning period, we could borrow equipment. We were also able to borrow a Pozyx system from Øystein Kjørstad Fjeldbo,which we used for learning how to place the system in a room and making tracking work with one person. We are also very grateful to Maria Azucena Gutierrez who was supporting us in every possible way and Daniel Formo, our internal supervisor who would always be around if we needed advice.\n\nAs a group we have managed to combine our different knowledge and skills, combining visuals, audio and technology to create an installation that communicated the real-world issue of noise pollution in a new and interesting way. We have worked well together as a team and have divided roles to be able to get the installation finished in time.\n\n\n\n\n",
        "url": "/applied-project/2019/07/14/Seismerssion.html"
      },
    
      {
        "title": "It's not just phase, mom!",
        "author": "\n",
        "excerpt": "Why phase coherency in loudspeakers matters.\n",
        "content": "Buzzwords\nThere is a lot of abstract terms and expressions being tossed around in Pro Audio. Buzzwords like “floating point”, “time alignment”, “pattern control”, “crossfired, dual endfire, cardioid subwoofer array”, “holographic stereo imaging”- the list goes on and on - dominate our lingua when we try to one-up each other in conversations about sound reinforcement. But there is one term that I hear being misrepresented or poorly understood more than others, and that is phase.\n\nWhat is phase?\nIn short, phase is the relationship between two waveforms. Say that you have two matching loudspeakers, each is playing a sine wave of 440Hz. The loudspeakers are placed close together and have matching amplitude, so that the total sum of the signal produced is +6db of a single source.\n\nNow, imagine that we flip the polarity of the signal going into one of the loudspeakers. The two loudspeakers are now 180 degrees out of phase and should theoretically cancel each other out perfectly, so that the result is total silence at the listening position.\n\nThere is also another way we can get the same result without flipping the polarity of any loudspeaker, and that is with distance.\n\nInstead of flipping the polarity of one of the speakers, we are now going to move said speaker back half a cycle of 440Hz:\n\n342 (the speed of sound through air at sea level in 22 Celsius) divided by 440 equals one period or 0.777 meters. Divide that by two and we get 0.3886 meters.\n\nBy moving one of the speakers back 0.3886 meters and compensating for the loss of amplitude by distance, we should achieve the same result of total cancellation at the listening position.\n\nConsequences in the real world\n\n“But, Gaute; I never listen to loudspeakers placed in that way, this is nonsense!”\n\nYes, you do, all the time.\n\nImagine a typical loudspeaker, say the Yamaha NS10\n\n  \n  The most iconic studio monitor\n\nWhile true that you can obtain almost perfect time alignment with the drivers in these speakers by sitting dead center of the stereo pair, with equal distance to the drivers in both the horizontal and vertical plane - in reality you can’t, but crossover distortion and the like is a whole nother story -, but let’s be realistic here, when do you ever sit exactly like this? Not at a concert, that’s for sure.\n\nThe problem occurs when you have loudspeakers with multiple drivers that does not sum perfectly, and it is MULTIPLIED when you have arrays or clusters of imperfect (hint: no loudspeaker is perfect) loudspeakers interfering with the other imperfect loudspeakers, trying to reproduce the sound of imperfect microphones picking up the same source at different distances, summed by a mixer with uneven latency on a channel basis…\n\nThe result of this is a blurry stereo image, lack of clarity, impact and dynamics and poor separation in the listening experience.\n\nConclusion\n\nSadly, I do not have the time to address all these issues in this blog post. The main goal has been to introduce you, the reader, to the concept of phase and how it effects your listening experience.\n\nFor you engineers out there: Before you reach for the EQ knob in order to give the kick drum a little more “oomph”, have you considered that it might just be out of phase somewhere?\n\nFor the audiofiles: Instead of buying gold plated connectors for your HIFI, have you considered checking if the drivers in your 100 000$ speakers really sum that well in your listening position?\n\nFor further reading, here are some useful links:\n\n- Tom Danley’s Synergy Horn explained\n\n- Merljin van Veen’s explanation of phase\n\n- Robert Scovill explains time alignment between tracks in a digital mixer\n",
        "url": "/sound-programming/2019/08/24/Gaute-testpost.html"
      },
    
      {
        "title": "Reflections on diversity",
        "author": "\n",
        "excerpt": "First year students in Trondheim reflects on diversity in SMC\n",
        "content": "This post is a collection of thoughts on how to bring more diversity into the domain of SMC. It was written by the first years students in Trondheim during the first week.\n\n\n  Dear ladies, do you really expect that any of you would end up working in the music industry in the future? Female engineers? Do you picture yourself running around with amplifiers bigger and heavier than yourselves? Do you think that any acoustic company really hire women? It’s all men’s work. It requires strength and analytical thinking. I don’t want to destroy your plans but forget it. It will just not happen.\n\n\nMagda:\n\nIt came as a surprise when starting my bachelor’s in Acoustic Engineering some years ago, it appeared that the majority of students in my faculty wasn’t men. Close to half of us were women. The very first lecture with the professor who was the mentor of our faculty quickly dashed our hopes that it would be a fair time in our - girls’ - life in terms of any gender equality.\n\nThe first paragraph is the words of a person who was supposed to lead all of us through the next 3,5 years of getting to know technology in terms of music and acoustics in general.\nEven though we all treated each other equally within the group of students, those words stayed somewhere at the back of our heads until the very end.\n\nCollective thoughts:\n\nOur first point on why we believe there is a lack of diversity in music technology resulting from education and recruitment. From our observation of the available courses found in universities and schools around the world, there is a void of music technology and sound technology related courses compared to other fields of study. Along with students often having to choose between music or technology courses individually and commit to either one but seldom both at the same time.\n\nFrom the experiences of our team, we also see the lack of quality when it comes to the education in the schools that have courses related to our field. Partially to blame is probably the speed at which technology is advancing, which often leads to institutes struggling to fund equipment to keep it as current as possible. To add on, due to the nature of how new the field of study is, we don’t see a lot of highly skilled mentors and educators as well.\n\nAlso, we traditionally see that women in higher education favor social studies over technological studies. Although over the years there has been an increase in women in technological studies, there is still a substantial gender gap. The gender ratio seems to vary internationally. Looking at India and Malaysia the female participation is higher compared to the west, suggesting that biological differences are less important than cultural and social factors.\n\nWhen computers first became mainstream in the 80’s, it was marketed as a toy for boys, creating a social stigma. In 2018 within humanities and esthetic studies, there was a female ratio of 60% in Norway. In technological studies however, only 34% of students were female. Overall in Norway, more women get higher education compared to men, according to SSB.\n\nWhich relates to our next point, recruitment.\nWith the absence of diverse highly skilled technologist coming out of the woodworks of education, we are lacking in numbers. With the lack of numbers, we see the lack of exposure. With the lack of exposure, we naturally see the lack of diversity as well.\n\nWe also see a void of mainstream “heroes” with women. There are the likes of Chris Lord Alge’s and Deadmau5’s for the male population to look up to but with women, one of the only few people that comes close to any mainstream fame is Sylvia Massy. She of course is a great role model however her videos are only occasionally viral. She is nowhere close to the amount of male EDM producers with billions of views on social media. This might be one of the bigger reasons for the lack of women of any racial background in the industry.\n\nOne of the current solutions is to promote a diverse range of heroes in popular culture, not only in music technology. An example where the lack of diversity becomes obvious is in festival lineups. The last few years, a number of festivals have received critique for their lack of female artists in their lineups (such as in Moldejazz 2017).\n\nAs a reaction to the imbalance in terms of gender equality, several artists and actors in the industry are speaking up to address the problem. Several well-known artists and DJ’s, both female and male, have been publicly shaming festivals for underrepresenting female artists in their lineups. It is crucial for the public awareness of such issues that key actors are highlighting the issues within their scene.\n\nThe New York based collective Discwoman is working to promote female-identified talent in the electronic music scene. “Exclusively representing women is a political act that is tied to the hope of permanently changing the music industry,” says Frankie Decaiza Hutchinson, one of the founders of Discwoman. A local example of this phenomenon would be Feminalen, a festival in Trondheim which main purpose is the promotion of female talent in the Nordic music scene.\n\nTo study the gender and race of the industry’s core professions, the researchers looked at the 700 top songs on Billboard’s year-end Hot 100 chart between 2012 and 2018. Across the three creative roles highlighted in the study, women make up 21.7 percent of artists, 12.3 percent of songwriters and 2.1 percent of producers (source).\n\nNot even mentioning any other jobs related to music industry like sound engineering, music production etc. Vast majority of people responsible for the technical side of the sound in theaters, during live concerts, on the radio or in recording studios are men. This doesn’t result from the fact that there are no women whatsoever already working, or trying to get into this field. They are just simply not as welcome as men there: mainly because of the stereotypes evolving in everyone’s (yes, also women’s!) heads since forever.\n\nLuckily, those stereotypes are slowly but effectively changing nowadays. Self-confidence can be the first and, honestly, the best of the steps that can be taken here. Even though it’s not always that easy to be self-assured about your own skills in a world where all the time males are perceived as those who are stronger and more independent, without trying to prove your achievements and strong sides, nothing will ever change. We are on a good way to accomplish gender equality in the music industry now though - all we need is to get open to possibilities and forget about the differences. All in all, it’s contrast what makes music beautiful.\n\nCritics will say that gender-specific organizations and events are not the way to go to promote diversity in the music industry. The state of today is still that women are underrepresented in most domains of the industry. Our opinion is that organizations and events that exclusively represent female talents should be warmly welcomed, with the acknowledgement that gender imbalance is still a fact.\n",
        "url": "/other/2019/08/27/Diversity.html"
      },
    
      {
        "title": "Meet SMC Group B 2019",
        "author": "\n",
        "excerpt": "Our group spans across Norway, Poland and the United States with a wealth of experience between us. We are funny too!\n",
        "content": "Hello from the B Team!\n\nJackson - UiO\n\n\n\nI grew up in Charlotte, North Carolina, moved to Los Angeles for college, where I received an undergraduate degree in Cognitive Science.  Most of my free time since them has been filled with guitar, digital environments, and a desire to begin creating through these mediums. I believe novel applications of technology may be able to help us reach a better understanding of our own relationships within music and sound. I hope SMC will be able to both expand my knowledge of these intersections as well as offer a chance to develop within them.\n\n\n  Email\n  Website\n\n\nMagda - NTNU\n\n\n\nI’m Polish, acoustic engineer by education, travel- and radio-lover by choice. After graduating from the first degree I took a “gap year” which ended up lasting for 2,5 years. During this time I lived in Lisbon, Portugal, Ireland and Budapest, Hungary. I’ve spent many years working in the radio as both broadcaster, producer and an assistant by the side of bigger Polish radio personalities.\n\n\n  Email\n\n\nSimon - NTNU\n\n\n\nI am from Molde, Norway and have been in Trondheim for about 5 years now. I did my bachelors in Music Technology there. Prior to that i studied Art History for a year and spent a year in Kristiansand where i studied Music. I make some electronic music, and i highly enjoy sound design. I have a little bit of experience here and there from my background in music technology.\n\n\n  Email\n\n\nJarle - UiO\n\n\n\nI was born in Oslo 1976, raised in Iceland, Kenya, and Switzerland, and found an interest in sound and music at an early age. After a short but explosive career as a (failed) rock-star I settled into a career in live-sound engineering - with a firm focus on expressions of experimental music. Through the SMC-program I endeavour to develop a user friendly interface democratizing microtonal MIDI. I also like kittens.\n\n\n  Email\n\n\nOur group\n\nOur group spans across Norway, Poland and the United States with a wealth of experience between us. We are excited about the prospects of the new ideas and techniques we will be introduced to, along with the ways each of us might be able to fill in the gaps of each other’s knowledge. Our backgrounds include live audio engineering, radio broadcasting, artificial intelligence, and art history.\n\nWE ARE VERY VERY FUNNY\n\n\n",
        "url": "/people/2019/08/28/b-team.html"
      },
    
      {
        "title": "Meet SMC Group C 2019",
        "author": "\n",
        "excerpt": "We are a diverse group of people, with members originating from France, Singapore, United States and Norway. Our background stems from different fields of the music industry. Music performance, music technology, musicology, sound design, spatial audio, acousmatic composition and music recording are amongst our expertise fields. We are all eager to get started with the hard work, and hopefully, change the world as technological humanists.\n",
        "content": "\n\nGroup C\n\nWe are a diverse group of people, with members originating from France, Singapore, United States and Norway. Our background stems from different fields of the music industry. Music performance, music technology, musicology, sound design, spatial audio, acousmatic composition and music recording are amongst our expertise fields. We are all eager to get started with the hard work, and hopefully, change the world as technological humanists.\n\nBio\n\nAleksander Tidemann\n\n\n\nIm a musician and sound technologist from Kongsberg, Norway. I started off as a drummer but gradually got more interested in various music and media technologies after high school. Prior to this degree i´ve studied jazz drumming, live electronics, musicology, sonology and sound design in both Norway and Australia. Apart from university and working part time as a technical supervisor at an arts/comedy stage in Oslo, I like to play with my bands, watch russian movies, read books and climb plastic mountains at the gym.\n\n\n  Website\n  Soundcloud\n\n\nAntoine Hureau\n\n\n\nI’m a french composer, sound designer and art technology consultant based in Bergen, Norway. I used to be a choir singer when is was younger but I dropped any kind of acoustic instrumental practice to focus on live and studio electronics, putting emphasis on spatial audio and acousmatic composing, I’m very much into audio networks as well. I like to work on my own synths when I have a bit of time and I compose and work with other artists on several kind of collaborative projects (ensembles, installations, soundtracks). I also work at the university of Bergen as an AV technician in the art, music and design department.\n\n\n  Website\n  Bandcamp\n\n\nPaul Koenig\n\n\n\nBack when the earth was still cooling and mobile phones had not yet coalesced from the analog mist, I received a degree in Music Composition from the University of North Texas. After running away for a bit (i.e., several decades) from music as a career, I finally ran out of options and have made my living for the past ten years as an Arizona-based songwriter and performer, touring all over the U.S. and eating a lot of ramen, both literally and figuratively.\n\nBeing both frightened and mildly aroused by the beeps and boops of digital things, and with a philosophical interest in the concept of Technological Humanism, I enrolled in the SMC program in an attempt to move from tech voyeurism (I like to watch other people do it) to a more hands-on approach.\n\nIn the meantime, I love to manually manipulate acoustically-vibrating media and bellow my compositions into a microphone, performing as a one-man-band under the nom de guerre PK Gregory.\n\nI also love cooking, long walks on the beach, and soup. I have a wife (Erin) and two kids (River, 12; Zevon, 4) who are still unsure of what just happened but are going with the flow for the moment.\n\n\n  Website\n\n\nTom Ignatius\n\n\n\nI’m a Record Producer with specialisation in Vocal Production and Recording from Singapore. Studied in Liverpool, United Kingdom. I play a lot of instruments and with most of them at a horrible level. I have worked on music featured on Top Gear and the Grand Tour and I am obsessed with pushing the limits of sound design and the limits of recording techniques. I still work with musicians from all over the world and I’m head of sound with the company Andsoforth.\n\nThomas Anda\n\n\n\nI´m a guitarist, composer and educator who originates from the west coast of Norway. I have a broad range of interest fields, and as a consequence of that, I have a somewhat diverse academic background. I´ve studied music performance, social studies, business, musicology and songwriting &amp; production. When I´m not studying I work with music-related stuff, play music with my friends, eat panang gai and binge-watch sports documentaries.\n\n\n  Website\n  Instagram\n\n",
        "url": "/people/2019/08/28/Group-C.html"
      },
    
      {
        "title": "Meet SMC Group A 2019",
        "author": "\n",
        "excerpt": "We are a very diverse group, with backgrounds ranging from graphic design to electrical engineering. This complementary has been felt since we first met, and it helps us considering many different points of view for a same problem\n",
        "content": "\n\nWe are a very diverse group, with backgrounds ranging from graphic design to electrical engineering. This complementary has been felt since we first met, and it helps us considering many different points of view for a same problem. Our collaboration is fruitful, and this is especially because it seems like everyone is open to each other, and to any new input. We all realised that our differences were our strength. Moreover, we don’t hesitate to question ideas, discuss and review them in a very constructive way. We are glad to work together!\n\nTeam Members\n\nGaute Wardenær\n\n\n\nLive sound engineer at heart, but very eclectic in interests. Started at a young age putting crazy ideas to life by taking technology apart and trying to make it work in different ways, still considers himself as an inventor/entrepreneur more than a sound engineer. The most exciting thing is learning new things to expand the understanding of how the world works; technology, psychology, sociology, philosophy, biology…\n\ngautesinmailadresse@gmail.com\n\n\n  Website\n  Instagram\n\n\nRayam Luna\n\n\n\nRayam is a Graphic Designer with a special taste on Audiovisual and Interaction Design, focusing his works on music and its surrounding arts. He’s also a musician that started to get involved into music production and its technologies for the last few years, working as a music producer and audio engineer. Rayam composes his own sounds, mostly naturally, when they come to his mind.\n\nrayamsoeiro@gmail.com\n\n\n  SoundCloud\n  Instagram\n  YouTube\n  Facebook\n  Behance\n\n\nThibault Jaccard\n\n\n\nThibault started classical violin at the age of 6. He never stopped discovering new instruments and musical horizons since. While he was studying electrical engineering, he started playing gipsy jazz in Gipsy Tonic, and managed to do it as a living. By exploring complex harmony, he begun to design music theory user interfaces, which brought him to this very master degree.\n\nthibault.jaccard@gmail.com\n\n\n  Website\n  Facebook\n  Soundcloud\n\n\nUlrik Halmøy\n\n\n\nUlrik has always been very interested in music, initially as a listener but then he also got into organizing concerts, DJ-ing and small-time music production. His educational background is from computer science, so SMC was a way of combining his interests in the intersection of technology and arts.\n\nulrikhalmoy@gmail.com\n\n\n  Website\n  Instagram\n\n\nTeam Skills\n\nDuring this activity, only two of us were present, Rayam (RP) and Thibault (TJ).\n\nTheory Knowledge\n\n\n\nPractical Skills\n\n\n\nPersonal Characteristics\n\n\n",
        "url": "/people/2019/08/30/Group-A.html"
      },
    
      {
        "title": "An introduction to automix",
        "author": "\n",
        "excerpt": "At first glance, automix might look like your regular old expander or gate, but what makes automix special is that it does not only work on a channel to channel basis, but links all the channels in an automix group together and opens up the channel that has the strongest signal, while ducking the others.\n",
        "content": "What is automix?\n\nAutomix is an algorithm originally developed by Frank L. Clemet and Bell Labs, but popularized by its implementation in the Yamaha CL and QL digital mixers in when they released it as a product of the collaboration with legendary engineer Dan Dugan, under the name Dugan Automix. After its Widespread popularity, the algorithm has been copied by almost all major console manufacturers and comes as a standard tool, either as an insert effect or on a channel basis.\n\nIn a brief summary, you could say that automix is an automated way of doing the engineers job when trying to avoid feedback and unwanted ambience when mixing multiple microphones at the same time.\n\nTypical uses are:\n\n  Theater or musical play.\n  Panel debates.\n  Conferences.\n  A loud stage with multiple vocalists.\n\n\nUnder the hood of automix\n\nAt first glance, automix might look like your regular old expander or gate, but what makes automix special is that it does not only work on a channel to channel basis, but links all the channels in an automix group together and opens up the channel that has the strongest signal, while ducking the others. Another feature that makes automix different from a gate or expander is that it does not operate with a set threshold that is absolute, it simply prefers the channel in the group with the strongest signal, regardless of its absolute amplitude.\n\nAutomix on the Midas M32\n\nThe Midas M32 has two automix-groups - X and Y -, able to handle eight channels each. To assign a channel to a group you simply select a channel with the “select” button over the fader, press the “Home” button next to the main screen and page up or down with the arrow keys until the parameters on the bottom row of the screen says “Automix group” “Weighting” and so on.\n\n\n  \n  You are now on the correct screen\n\n\nTo assign a channel to an automix group, simply turn the encoder below the text box that says “Auto group”. In most cases you will only need to use one group, so assign the channel to group X. Repeat this process on the other channels you want to assign to the same group. The “Weighting” -option should be used if you want to prioritize one microphone over the others, for instance the teachers podium-mic over our table microphones in the classroom.\n\nTo see the automix in action, press the “Meters” button next to the screen and scroll over to the tab named “Automix”. The meters you see correspond to the gain reduction happening on the channels assigned to that automix group. Try pushing up the faders of the relevant microphones to equal level and you will see the meters on the automix dancing up and down in correlation to what microphone has the strongest signal. Try speaking into one of the microphones and you will se the meters opening that channel up while simultaneously pulling the others down. Getting used to how this looks visually will give you a good impression of what is happening inside the mixer.\n\n\n  \n  Make yourself familiar with how the meters respond to sound\n\n\nThat’s it!\nEven though it seems like magic at first glance, it’s not.\nAutomix is just a really handy tool to use when you are working with multiple live microphones, but you don’t want to have them all open at the same time.\n",
        "url": "/networked-music/2019/09/10/Automix.html"
      },
    
      {
        "title": "Classification of string instruments",
        "author": "\n",
        "excerpt": "During a 2 week intensive workshop in the course Music and Machine Learning I had to develop a machine learning system for the field of music technlogy.\n",
        "content": "During a 2-week intensive workshop in the course Music and Machine Learning I had to develop a machine learning system for the field of music technology. With no previous practical experience with machine learning (ML), this turned out to be quite a challenge. During the two weeks we got a broad overview over ML theory and practical ML in Python. Several example systems were shown during the weeks by our professor Stefano Fasciani, which would come in very handy when working on my own project.\n\nA machine learning system uses ML algorithms to build a model based on data that is fed into it, which we call “training data”. This model can then make predictions on new data without being explicitly programmed. When choosing a project, it is important to know that the problem you want to solve cannot easily be explicitly programmed. Machine learning would then be overkill. Let’s say you want to automatically separate between the sound of a bass drum and a hihat. These can very easily be separated with a simple threshold on the frequency spectrum, and using machine learning to figure this out, would clearly be overkill.\n\nMy classification project\n\nWith my project I wanted to see if a machine learning system could separate between three instruments that have a lot of spectral similarities, and which seem quite similar to the human ear. These instruments are guitar, mandolin and banjo. All are string instruments, and for an untrained ear it can sometimes be hard to separate between them. It’s at least not as easy to separate as a bass drum and a hihat.\n\nThis process of separating between input data with machine learning is called classification. In classification a machine learning algorithm predicts a class/label based on a given dataset. In the case of classification, the dataset is labelled, which makes it fall under the category of supervised learning.\n\nI can divide my system into three main processes:\n\n\n  Data preparation (light red)\n  Feature extraction (red)\n  Model training and prediction (dark red)\n\n\n\n\nSystem flow\n\n\nThe data\n\nThe data preparation part is where you would get rid of noise, silence etc. in the dataset; get rid of random variables that will mess up the classification. In my case I had 39 samples of each instrument. I chose to trim each file (total of 117) to last for 1 second from the first note onset. This was of course done with python code (with the help of a package called Librosa), or else I would have had a really boring time.\n\n\n\nAudio trimming\n\n\nFeature extraction (MFCC)\n\nThe machine learning algorithm works with numbers. Which numbers is up to me, and of course it should be numbers that tells some characteristic of the data. I chose to give my ML algorithm something called Mel frequency cepstral coefficients (MFCCs). Pretty self-explainable, right? No, absolutely not…“MFCCs are short time power spectral representation of a signal and represents psychoacoustic property of the human auditory system”. Now, when that is completely clear for everyone, why do I give the ML algorithm this? Well, it turns out to be one of the most, if not THE most popular feature when working with speech recognition, genre classification, music insformation retrival and more. One of the reasons for this is that it tells something about the timbre of the sound. Exactly what I want to separate my instruments by, so great.\n\n\n\nSpectrogram showing 39 guitar samples stacked on a row, and MFCCs on the columns\n\n\nModel training and prediction\n\nSo, when I had the “timbre” of each sample of each instrument, I just gave 80% of the data to an ML classifier called support-vector machines (SVM) as training data, and 20% as test data, tweeked some parameters to make it throw my data into a higher dimensional feature space, and then a few ms later it told me that it had classified everything with a 100% accuracy. Could not be any better than that right? Turns out it is not a problem for a machine to classify between three similar string instruments, if you give it some good data that is.\n\n\n\nSVM report\n\n",
        "url": "/machine-learning/2019/09/14/ML-Classify-instr.html"
      },
    
      {
        "title": "Triggering effects, based on playing style",
        "author": "\n",
        "excerpt": "Technology in collaboration with art could create creative solutions and achievements. In here we use machine learning in order to ease the work of player while playing an instrument.\n",
        "content": "Technology in collaboration with art could create creative solutions and achievements.\nIn here we use machine learning in order to ease the work of player while playing an instrument.\n\nIntroduction and background\nAs a guitar player, while playing live, I can feel that it could be very useful to have a system in which engages and disengages the necessary effects, based on my playing style. This lead me to creation of a system, based on machine learning, which engages and disengages reverberation based on my playing style.\nFor that the machine needs to learn how to differ between playing styles.\n\nFeature Extraction\nFor the so called matter stated above, some samples were needed. Therefore I reorded two sets of 52 samples, with each 5 seconds lenght. A group of them represented the rhythm guitar playing style and the other represented the solo playing style. Also, I have to mention that since aesthetically having the reverb on Arpeggios was more efficient as well, I mixed some arpeggios with the solos.\nHere you can hear a sample, from each group of the files:\n\n\n  Rhtyhm sample:\n\n\n  &lt;/source&gt;\n  Your browser does not support the audio element.\n\n\n\n  Solo/Arpeggio sample:\n\n\n  &lt;/source&gt;\n  Your browser does not support the audio element.\n\n\nAfter that, we have to consider the fact that, the machine needs to analyze and understand the audio files. For that there were few features extracted from the files and presented to the machine, so the machine could identify the audio files for itself.\nThe language that was used for programming the system was python and the package that was used in the script to work with audio files was LibROSA.\nThe main features extracted from the samples included RMS, Spectral Flatness and Chromagram stft (indicating the power of the playing).\nThese features were extracted and were fed into the system.\nThe files were clustered by an algorithm called PCA (principle component analysis). The reason for that, was to reduce the set of variables (features) to a small set that still contains most of the information in the original one.\nBelow you can observe the graph that projects the two group of samples, based on their features.\n\n\n\n\n\n\nClassification, training and prediction:\n\nFor the classification and learning processs, a system called Support Vector Machine was used. This is a supervised learning system which is very efficient system for classification of such data. It finds a hyperplane in an N-dimensional space (N = the number of features) that distinctly classifies the data points.\nAs it is shown below, it could classify the samples with a high accuracy.\n\n\n\n\n\n\nFuture work\nThe next step is implementing the system on a real situation. Although after implementation there is going to be a bit of latency in live situations, but in the case that more features could be added to the system, in regards to the automation of the effects, it definitely will worth going through it.\n",
        "url": "/machine-learning/2019/09/14/ML-SH.html"
      },
    
      {
        "title": "Multi-Layer Perceptron Classifier of Dry/Wet Saxophone Sound",
        "author": "\n",
        "excerpt": "The application I have decided to work on is of a machine learning model that can ultimately differentiate between a saxophone sound with effect (wet) and without effect (dry).\n",
        "content": "The application\n\nThe application I have decided to work on is of a machine learning model that can ultimately differentiate between a saxophone sound with effect (wet) and without effect (dry). After pre-processing the dataset, it was possible to extract reliable features for classification, to train and test the model, and to categorize the audio correctly by its classification category (wet/dry). A machine learning model that can classify different kinds of sound timbre is related to the interdisciplinary science of Music information retrieval (MIR), and more specifically, to instrument recognition. This type of application can also contribute to processes of track separation, music transcription and sound effect retrieval. The machine learning technique that is used in this model is supervised learning. “The algorithm builds a model of the relationship between two types of data: input data (the list of features for each example) and output data (labels or classes). The training dataset for a supervised learning problem contains examples of input-output pairs. Once the model has been trained, it can compute new outputs in response to new inputs” (Fiebrink et al., 2016). Using a Multi-layer perceptron algorithm (MLPClassifier) available from the scikit machine learning package, it was able to train an artificial neural network (ANN) and to successfully classify the two sounds (with 90-100% accuracy).\n\nThe dataset\n\nI collected 102 samples of saxophone notes provided by Philharmonia. The samples were downloaded as mp3 files and were converted to wav files for handling with the Librosa library. I made sure that all samples are around the same volume level and length (approx. 1 second). After processing the sounds by cutting out all silent parts from both the start and end of each sound file, I copied the same 102 files into a new folder labeled wet and applied two effects on each sample. The effects I have used for creating the wet samples were a simple chorus effect and an octaver that is doubling the note an octave lower from the original pitch. It is common knowledge that a small dataset for training results in a poor approximation. “An over-constrained model will underfit the small training dataset, whereas an under-constrained model will likely overfit the small training data, both resulting in poor performance” (Brownlee, 2019). I started with a much smaller dataset at the beginning of the project, where I only had 30 samples of each class, but as the project progressed, I decided to increase my dataset for achieving a higher accuracy rate.\n\nFeature extraction\n\nTo be able to differentiate between the two classes of sound at hand, I first had to decide on several features that can represent and show clear differences between dry and wet signals. After numerous trials and errors during the feature selection process, attempting to find a suitable combination between several features to extract, as well as plotting visual examples (figure 1-6), I settled on three features that fit my classification problem and produced the best results over time.\n\n\n  Melspectrogram - a visual spectrum representation of a Mel Scale (y-axis) over time (x-axis).\n  Spectral Centroid - a calculation of a spectrum which indicates where the center of mass of the spectrum is located.\n  Spectral Flatness - a measure to quantify how much noise-like a sound is, as opposed to being tone-like.\n\n\n\n\nPreparing the dataset\n\nThe extracted features are being converted from lists into NumPy arrays and labels are being attached to each class (0-dry, 1-wet) by using a function from the NumPy library. The next step in preparing the dataset is the implementation of a scale function from the scikit-learn library for standardizing all features and to avoid a single variable dominating the others just because it consists of a different range of numerical values. Once the values of all features are rescaled, I convert the dataset into a two-dimensional data structure using the Pandas DataFrame and apply labels of the three features and two classes.\nCalling the train_test_split function from scikit model selection, I was able to determine the size of my testing and training data. This has also been a point where I tested different split points to optimize the model’s results. The model’s accuracy rate was at its highest when the test size parameter was set to be between 25-35%. At this point, the dataset is ready to be tested and trained with the algorithm.\n\nThe Algorithm\n\nThe algorithm used in this ML model is provided by the scikit library and is called a Multi-Layer Perceptron (MLPClassifier). The perceptron was a machine that “could be taught to perform certain tasks using examples. This surprising invention was almost immediately followed by an equally surprising theoretical result, the perceptron convergence theorem, which states that a machine executing the perceptron algorithm can effectively produce a decision rule that is compatible with its training examples” (Minsky and Papert, 2017). The idea behind this algorithm is that a single perceptron can only solve linearly separable problems, but by linking multiple perceptrons together, more complex problems that are not linearly separable can be solved (Haykin, 1994). The single perceptron is a simplified model of a biological neuron which receives input, contain an activation function and produces an output.\nThe multi-layer perceptron model, which is a simplified model of a neural network, contains several layers of perceptrons (neurons) whereas each perceptron in a layer is connected to each of the perceptrons in the other layers. The input layer of the machine learning model presented here is of the three extracted features: Melspectrogram, Spectral Centroid and Spectral Flatness. The output layer is of the two classes (dry/wet). Any number of layers in between the input and output is considered hidden, and the whole design can be viewed as an artificial neural network. The MLPClassifier function provides the option to specify the number of hidden layers and the number of perceptrons in each of those layers. The connection between the neurons is being adjusted by weights which are responsible for the amplitude of the connection between two nodes. A bias value is also being linked with each node in the input to help control the triggering value of the perceptron’s activation function. In practice, the bias is being treated as the weights. Using a backpropagation algorithm already implemented within the MLPClassifier function, the weights and bias values are being adjusted in proportion to how much they contribute to the overall error with each training run of the test data. The process of updating the values of the weights and bias is possible due to a gradient descent optimization algorithm which calculates the partial derivatives of the cost function with respect to each parameter (weight and bias) and stores the result in a gradient. This lets the neural network not only feed-forward information but also feed the learned information backward through the network, adjust the weights and bias, and test the model again based on previous results.\nFinetuning of parameters inside the MLPClassifier function took some time and effort. I have set the maximum limit of training iteration to 2000, but usually, the model completes the training within the range of 500 to 1000 iterations. The MLPClassifier function is set to stops the training if the training loss did not improve significantly over the last ten epochs. I have mostly trained the system with the hyperbolic tangent as the activation function and found better results with lower iteration count when using the hyperbolic tangent or the linear (‘identity’) functions. The number of hidden layers and the size of each layer also played a significant role in the results but considering the size of the dataset and all parameters that can be attuned (n_mel size, test_train_split, hidden_layer_sizes, activation function), I am not able to determine which is the ‘best combination’ of parameters for 100% accuracy. I settled on three hidden layers containing five, four and three neurons.\n\nImplementation, replication, and evaluation\n\nTo be able to run the code and conduct training and testing run, one should download the data folder and the Jupiter notebook containing the code, and place both in the same folder. The first cell in the Jupiter notebook deals with importing of all required packages and libraries for running the model and setting up the environment. The second cell loads the dataset, set up the sample rate and print the number of files. In the third cell, the features are extracted from the samples and converted to NumPy arrays and Pandas data structure, labels are being attached to classes and features, and partitioning of the data set to train and test data is being done. In the fourth cell, one can adjust the MLPClassifier parameters, train the model and apply the trained model on the testing data. In the fifth cell, it is possible to plot and print the results and get a complete report of the trained model. The machine learning model is not a complicated one and can be run and provide results within a minute or two at the most. For future work, it might be interesting to compare between three or more classes of various applied effects on a dry saxophone sound or even to compare between other instruments while using the same effects setup.\n",
        "url": "/machine-learning/2019/09/16/ML-Dry-Wet-Sound-Classification.html"
      },
    
      {
        "title": "IR Reverberation Classifier using Machine Learning",
        "author": "\n",
        "excerpt": "Using Machine Learning to classify different reverb spaces (using impulse response files)\n",
        "content": "Now SMC (music, communication and technology) course is in its second year - and finally we are covering Machine Learning!\nThis is how my original expectations were, high hopes, large complex projects involving real time production of sound through learning computers, maybe even breaking to a new paradym of music technology, however…..\n\nMachine learning is HARD!\n\nFrom the first day, dreams were dashed and expectations slashed. Real time is possible, as well as sound generation - but its best to start with getting to grips with the basics.\nSo, after this initial shock, I decided to settle down on a go-to of machine learning (ML), a classification problem (you know, like is it a dog or cat picture?). When thinking of what data to use, it was advised to use short samples when working with lossless audio. In this vein I though about reverberation impulse response files - short audio clips that represent the reverb in a space, such as a room or hall. These files can be convolved with audio to create a representation a how a sound will be projected in that space.\n\nSo, I aimed on teaching a computer how to differentiate between audio files that have had different convolution reverb applied. For instance, can a ML model know a sound of a dog barking in Taj Mahal compared with dog barking in a a toilet? For this experiment I used completely random short sounds for testing, such as a car engine or a violin or a brick hitting the floor for example. This was done to make sure it was not too easy for the model.\n\nAfter much trail and tribulation a model was set up. Using three kinds of reverb as classes to be classified, and 150 audio samples as test data, it was time to extract features from the audio. In other words turn some aspect of the audio into statistical data so the computer can understand. This can be too do with the tonality or frequency spectrum - essentially you need to measure something to be able to compute it (a computer doesn’t really know what sound, or music, is!). This is one of the most important factors in preparing audio for analysis in ML algorithms.\n\nI used Mel frequency cepstral coefficients (MFCC) as the main feature. This is  a way to represent the frequency spectrum of a sound, in the Mel scale. It is very common with speech recognition. After some experimenting, this was the most effective in achieving the best results possible for the model.\n\nThen this data gathered via MFCC is put through various different machine learning algorithms - K Nearest Neighbour, Support Vector Network were the most useful. Here are the results:\n\n\n\n\n\nResults\n\nOverall the project had some positive results, which I am quite pleased with; for two weeks time limit it was touch and go that it would work at all. Support Vector Network achieved the highest, most accurate score. Now to hassle google and get some masses of data for deep learning!\n",
        "url": "/machine-learning/2019/09/16/ML-Roman.html"
      },
    
      {
        "title": "Could DSP it?",
        "author": "\n",
        "excerpt": "Is polarity the solution?\n",
        "content": "This week in the SMC Portal\n\nAn old problem…\n\nWe were having quite a bit of trouble with something called feedback! Because of the low-latency audio connections between our classrooms in Oslo and Trondheim, there are occasions where the sensitivity of the microphones we are using enter a feedback loop with the sound played through the speakers. Simply, our voices in Oslo travel to Trondheim which are then played on their speakers and some of that sound will unfortunately end up going through their microphones (when it should only be their voice) and it comes back to us as an echo. But when that echo is just loud enough, it will start a loop with ever-increasing volume until one of us sprints over to the mixing console and throws a chair at it (or we just turn down the master).\n\nEveryday we spent at least 10-15 minutes of class working on EQ-ing out the problem frequencies or turning down the gain of the microphones or even the master (on both sides). The accumulated result of all this tweaking is a sloppy and confused setup whose history is unknown because we had made all the changes on the fly.\n\nWe are music technology students, of course we can find a more effective solution right?\n\nCould DSP it?\n\nThe last two weeks were spent studying digital signal processing. One main feature of our studies was listening to the effect of the various filters we had applied to our sample sounds. To better understand the transformations at play, we creating two tracks, one with the sample and filter applied and the other with just the untampered sample whose polarity was inverted. The result of playing both at once was the effects’ transformations heard in isolation (because the inverted track would be “canceling” the original track’s waveform resulting in silence).\n\n\n\nWell, I was thinking about all of this new found knowledge one day last week and realized there might be some practical use of inverting the polarity of a signal with live sound interfaces. I made the comment to Jarle who first told me to never call polarity inversion “phase flipping” (see here) but then politely said that this might be a potential solution.\n\nWe walked over to the mixer, selected the channels we were receiving from Trondheim and inverted their polarity and - success! No echo, no feedback. All of our problems were solved and I graduated the master’s program early. But really, what is going on and why does that work?\n\nHere’s why\n\nAs with our example with isolation the effects of filters and such, reversing the polarity of the channel will actively reduce feedback when it begins to loop, as the sound that we receive from our mics in Oslo will be flipped compared to the sound we receive from Trondheim. These two sound sources, coming from their mics through our speakers and our mics to their speakers will collide “out of phase” if a feedback loop ever begins - thereby reducing the peak frequencies that were causing so many issues. What a simple solution to a complicated system.\n",
        "url": "/networked-music/2019/09/16/Could-DSP-it.html"
      },
    
      {
        "title": "MIDI drum beat generation",
        "author": "\n",
        "excerpt": "Most music production today depend strongly on technology, from the beginning of a songs creation, till the the last final tunings during mix and master. Still their is usually many human aspect involved, like singing, humans playing instruments, humans using a music making software etc..\n",
        "content": "Abstract\nMost music production today depend strongly on technology, from the beginning of a songs creation, till the the last final tunings during mix and master. Still their is usually many human aspect involved, like singing, humans playing instruments, humans using a music making software etc. The skill to make music is not something we are born with, but something we can learn to do. This learning process is something we can mimic in a computer by using machine learning, and then generate music.\nMagenta1 is a project started by the Google Brain team to demonstrate machine learning as a tool in creative processes; one of which are to make music. Drum RNN (Simon et al. 2018) is one of their models, which applies language modeling to drum track generation using an LSTM. In this report I will explore the Drum RNN model, by training the model with MIDI drum files from the “Groove MIDI dataset” (Gillick et al, 2019). When the Drum RNN model is trained it can generate drum MIDI files. I will have a look at the training of the model regarding number of neural layers, neurons in each layer and training cycles. I will also discuss the quality of the drum MIDI files generated by the model from a drummers perspective in short.\n\nAlgorithm\nThe Drum RNN model are based on a recurrent neural network, hence its name (RNN). The Drum RNN model uses a LSTM (long short-term memory) architecture. LSTM (Hochreiter and Schmidhuber, 1997) has a feedback connections which makes it able to learn how to map sequences to sequences.\nFor the Drum RNN model to learn, we need a sequenced data structure to learn from. To do this we simply train the Drum RNN model by giving it sequences of drum patterns. When the model have used these drum patterns to learn, it is then able to take the beginning of a drum pattern/sequence, and produce the rest/following sequence. Take for example a simple pattern of a single kick- and snare drum, hit after each other in a loop. If this was the training sequence, the system would have learned that the snare comes after the kick and the kick after the snare etc. Giving the trained data a kick drum as a starting point, it would then produce a snare drum sound, a kick drum sound etc… RNNs ability to make use of sequential information in the data, is what makes it suitable to learn and produce music, since music is a sequence of sound (for example MIDI notes).\nTo take advantage of the sequences that have been played previously and use them to learn what should come next, the neural network need some type of memory. This is where the LSTM architecture comes in handy. The LSTM architecture is based on memory cells and gate units. The memory sells are used to convey useful information about the current state. The gates are used to decide when to keep or override the memory sells. Which are one of the reasons this type of architecture is suitable to learn music patterns. Since it can use the information from previous sequences to predict the next note.\n\nThe data and feature extractions\n“Unlike melodies, drum tracks are polyphonic in the sense that multiple drums can be struck simultaneously. Despite this, we model a drum track as a single sequence of events by a) mapping all of the different MIDI drums onto a smaller number of drum classes, and b) representing each event as a single value representing the set of drum classes that are struck.”(Simon et al. 2018)\nIn the model you can choose to use two different configurations. Either you configure to map all drums to a single drum class, which uses a basic binary encoding of drum tracks, where the value 0 means silence and 1 means at least one drum is struck at the step. The other configuration is to map all drums to a 9-piece drum kit consisting of bass drum, snare drum, closed and open hi-hat, three toms, crash and ride cymbals. I chose to use the 9-piece kit configuration, because I wanted a full kit playing as an output.\n“Our first step will be to convert a collection of MIDI files into NoteSequences. NoteSequences are protocol buffers, which is a fast and efficient data format, and easier to work with than MIDI files.” (Simon et al. 2018). In Magenta there is a script that lets you convert your MIDI files (input) into NoteSequences (output) by running the “convert_dir_to_note_sequences” script.\n “SequenceExamples are fed into the model during training and evaluation. Each SequenceExample will contain a sequence of inputs and a sequence of labels that represent a drum track”(Simon et al. 2018). By running the “drums_rnn_create_dataset” script, we extract drum tracks from out NoteSequences and save them as SequenceExamples. These examples is which we use to train and evaluate the model.\n\nData set used to train and test the system\nAs mentioned above, the data used to train and test the system are MIDI drum files from the “Groove MIDI dataset” (Gillick et al, 2019). These MIDI drum files have been played on a Roland TD11 by a total of 10 drummer. Most of them hired professionals. The dataset consists of a total of 1150 MIDI files, including a range of genres which is labeled in the file names. I chose to use MIDI beat-files of  the rock genre (excluding the fills MIDI files), which consist of a total of 239 MIDI files. This is a genre I am familiar with when it comes to drumming and I did not want to combine genres, since more genres means more options for the model when it comes to learning. Which means the model would most likely require more cycles of training to get the same result of perplexity (if it would ever get there). The reason for this is simply that different genres often have different drum structures/patterns. I assume including different genres would have made the data set more complex. The same assumption goes for adding the drum fills.\n\nTraining and evaluation of the model\nTo see if the system is actually learning from the data and are getting better for each cycle, we can have a look at the perplexity of the model during training. The perplexity is shown in the terminal during training. This allows me to have a look at how good the training are doing for each 10th cycle. By changing the number of RNN layers and the number of cell in each layer, I can brute force to find the most optimal configuration for the model given my data set, with some inspiration from when it comes to fewer layers with more neurons in each (Hewahi, N et al. 2019). Even though the end result seam to be quit similar when I react a certain amount of cycles.\nFor example during one training of the model with two layers with 64 neurons each this was the outcome when it comes to perplexity:\nFirst cycle the model had an perplexity = 510, at the 11th cycle the perplexity = 182, at the 21th cycle the perplexity = 59, at the 31th cycle the perplexity = 28 an at the 91th cycle the perplexity = 13. If I try other configuration, it might get to a perplexity of close to 13 earlier, but then it might take more time for each cycle to run, since I either increase the amount of layers or the amount of neurons (or both).\nWhen it comes to the artistic quality this is a bit harder to evaluate. The MIDI drum files generated by the trained Drum RNN model sounds like it belongs to the rock genre, but the pattern seems to be a bit random. I expected the generated MIDI drum files to follow a more strict pattern and  go into a loop, playing the same over and over. But from what I have tested so far it seams like it does the opposite, by both changing the pattern of the beat and by randomly adding toms and crashes, like a child with a lot of talent that have learnt to play drums, but are not mature enough to stick to any particular pattern/beat.\n\nPersonal reflection\nMy original idea for this project was to record my own drum MIDI files and use these to train the Music VAE model. To then have a look at the MIDI drum file generated by the trained model and compare these to my own recorded MIDI drum files. But after many attempts of recording and train the Music VAE model with my recorded MIDI files, without any success, I had to give up. At first I had some problems with getting the Music VAE model to work. I then found Drum RNN in the process of trying to fix the problem, and decided to change to this model since it were a better fit for my project. But I still had the problem of using my recorded MIDI drum files as a dataset for training the model.\nAfter spending the first week on troubleshooting I decided to changed the project to deal with a MIDI dataset recorded for GrooVAE (Gillick et al, 2019). I also trained the model using some MIDI metal drum files, but had no time to compare the result with the “rock model”. Given more time it would have been exciting to use more data to train the model and compare rasults.\nNone of the machine learning tools that I have used are made or contributed by me. My contribution to this project was to explore the Drum RNN model by training it with MIDI drum files (rock genre) and changing the training configurations.\n\nGenerated drum beats\n\nReferences\nSimon, I., Roberts, A., Schatz, J. and Hawthrone, C. (2018). Drum RNN. [Viewed 11 September 2019]\nhttps://www.github.com/tensorflow/magenta/blob/master/magenta/scripts/README.md\n\nHochreiter, S. and Schmidhuber, J. (1997). “Long short-term memory” Neural Computation 9 (8), pp. 1735–1780.\n\nGillick, J., Roberts, A., Engel, J., Eck, D. And Banman, D. (2019) “Learning to Groove with Inverse Sequence Transformations.” International Conference on Machine Learning (ICML) [Viewed 12 September 2019]\n\nHewahi, N., AlSaigal, S. and AlJanahi, S. (2019) Generation of music pieces using machine learning: long short-term memory neural networks approach, Arab Journal of Basic and Applied Sciences, 26:1, 397-413, DOI: 10.1080/25765299.2019.1649972\n",
        "url": "/machine-learning/2019/09/16/ML-Drum-Beat-Generation.html"
      },
    
      {
        "title": "Clustering high dimensional data",
        "author": "\n",
        "excerpt": "In the project for Music and Machine Learning I was using raw audio data to see how well the K-Mean clustering technique would work for structuring and classifying an unlabelled data-set of voice recordings.\n",
        "content": "Filmstill ‘The Truemanshow’, retrieved 13.09.19, 06:26\n\nIn this project I was using raw audio data to see how well the K-Mean clustering technique would work in structuring and classifying an unlabelled data-set of voice recordings. This blog post is a reduced approach towards the course project.\n\nThe data-set “Phonation modes dataset” designed by Polina Proutskova (2012) is a collection of vocalised phonation modes for training computational models in automated detection of these phonation modes. 4 different states were sung by one female singer, breathy, neutral, flow and pressed. Out of 900 samples, pitch C4 was preprocessed into 53 unlabelled samples. Each sample is one second long and contains one vowel vocalized in each of those phonation modes. In order to enhance the audio signal (Schuller, 2013) and prepare it for feature extraction some preprocessing is required. The found set-up therefore made slicing of the original files into smaller units via the xxx function not necessary. To optimize the test/training performance of the clustering algorithms, dimensional reduction of the samples could be directly applied as length and frequent sound occurrence enabled further processing of the raw signal.\n\nPhonation modes differ in their loudness (Sunberg), while a breathy vocalisation is soft for example, pressed sound would be louder. Further on Polina Proutskova explains in Sundberg’s model loudness would directly be related to subglottal pressure, and subglottal pressure would rise from breathy to neutral to pressed. (Proutskova)\n\nData mining in music\nIt was not only helpfull for the understanding that the procedures presented in this project are to be situated within the field of data mining. It helped to critically assess and reflect upon the tools deployed. According to Tao and Lei Li, data mining is the nontrivial extraction of implicit, previously unknown, and potentially useful information from large collection of data. The mining process would usually consists of an iterative sequences of data management, data preprocessing, mining and post-processing (Tao Li and Lei Li, 2019).\n\nData &amp; Feature Computation\nIn the data mining process the computation of features is the first step. From all the different features (high dimensional data) a subset of features have to be selected and dimensional reduction applied, otherwise the clustering algorithm will have a hard time allocating the nearest neighbour of the data points. In this project only one feature was tested.\n\n\n\n Figure 1: Short-term power spectrum of a sound with the 64 mel spectogram\n\n\nDimensionality Reduction with Isomap\nThe high dimensional data, already trimmed and sliced beforehand will be now represented in a lower dimension, organized by similarity. With Isomap – isometric feature mapping - it is possible to apply a non-linear reduction method (see benalexkeen). The key is to find relationships among data points, measuring the geometric shapes formed by items in a non-linear data set. They become predictable in high dimensional data (see trnmag). For pattern recognition on real-world data the algorithm is useful to see what kind of data there is.\n\nClustering with K-Means, unsupervised learning\nAs a method of classification the data can be segmented through shared attributes. This algorithm only uses input to group and structure the data without knowing the outcomes (i.e., unlabeled data without defined categories). The data points are grouped into clusters according to their similarities. Each cluster is defined by a centroid in which the data points relate  in the squared eucledian distance (compare datascience).\nAfter running test/training the data in K-Means there are three centroids detectable around which the data is clustering:\n\n\n\nFigure 2: Data structuring by K-Mean\n\n\nThere are 4 clusters, the largest group are obviously the blue dots which in respect to the used feature, must sound quite similar. There is a smaller group of red dots and one purple and green dot each.\nIt could be possible, that the difference of the phonation modes are not so well expressed through the computed feature. More features or one different one could be applied to achieve more differentiated results. However, the results are hard to interpret since there only 53 processed samples. After applying unsupervised learning on the data set, I also noticed the remark of the researcher who created the set, in which she is not recommending using the recordings of the flow mode for recognition purposes.\n\nFinal thoughts, further application\nConsulting all data that is available might have produced interesting results. It could probe a test-run to explore weather or not the entire data-set could be suitable to train an algorithm for a real-time sonic interaction, in the context of vocal trainings for example for humans. But to achieve meaningful results with these techniques (like Recurrent Neural Networks) much more data must be deployed and storage is needed. The 900 samples available were except for pitch C4 unprocessed. For the scope of this project it would have been too time consuming to trim the original files into a format the algorithm could digest. The samples were recorded in an supervised, controlled environment using only one voice.\n\nReferences\n\n  \n    https://osf.io/pa3ha/ - Data-set on phonation modes\n  \n  \n    The Machine Learning Algorithm as Creative Musical Tool – Rebecca Fiebrink and Baptiste Caramiaux\n  \n  \n    Online, Loudness-invariant Vocal Detection in Mixed Music Signals – Bernhard Lehner, Jan Schlueter, and Gerhard Widmer\n  \n  \n    Mechanisms of Artistic Creativity in Deep Learning Neural Networks – Lonce Wyse\n  \n  \n    Music Data Mining: An Introduction – Tao Li and Lei Li\n  \n  \n    Chain of Audio Processing and Audio Data in Intelligent Audio Analysis – Björn W. Schuller\n  \n  \n    https://www.datascience.com/blog/k-means-clustering\n  \n  \n    http://benalexkeen.com/isomap-for-dimensionality-reduction-in-python/\n  \n  \n    http://www.trnmag.com/Stories/031401/Tools_cut_data_down_to_size_031401.html\n  \n\n",
        "url": "/machine-learning/2019/09/17/Clustering-high-dimensional-data.html"
      },
    
      {
        "title": "Machine Learning, it's all about the data",
        "author": "\n",
        "excerpt": "For my machine learning project, I wanted to see if I could teach my laptop to distinguish between different types of music using a large amount of data. Using metadata from a large dataset for music analysis, I tested different machine learning classifiers with supervised learning to distinguish between tracks labeled belonging to ‘Rock’ and ‘Electronic’. The project was developed using Python and libraries for data analysis and machine learning.\n",
        "content": "For my machine learning project, I wanted to see if I could teach my laptop to distinguish between different types of music using a large amount of data.\nUsing metadata from a large dataset for music analysis, I tested different machine learning classifiers with supervised learning to distinguish between tracks labeled belonging to ‘Rock’ and ‘Electronic’. The project was developed using Python and libraries for data analysis and machine learning.\nI managed to make some progress in the end, but these things take time, so it is merely a preliminary dip into what machine learning can accomplish.\n\nThe dataset\nSourcing or collecting data can be both costly and time consuming, but luckily for me and others looking for large amounts of music data, someone has already done the work for us. I was lucky enough to find a dataset that was both large and applicable for the task at hand.\nFMA: A Dataset for Music Analysis is a large dataset containing free and legal music along with metadata and extracted features.\nThe dataset is sourced from The Free Music Archive, and contains 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums.\nIn other words, a whole lot of ones and zeroes.\n\nProcessing the audio in such a huge dataset would require too much time, so I would have to make do with the preprocessed metadata designating the different features and values for each track. The audio features for each track had been extracted using Librosa in Python, resulting in 518 values per track containing information and different statistics for the various features Librosa can extract.\nIn the end, a file containing a list of all the tracks, their title, artist, and most importantly genre, was used together with a file containing all the audio features for each track. This still meant that I had 106,574*518 datapoints that needed to be organised before using it for machine learning.\n\nSelecting columns and rows\n\nMuch like in an excel table, the data I was working with was organised in a .csv file, which Pandas, a data analysis toolkit for Python, can store and manipulate in many various ways.\nBy selecting items with the genres I was looking for and picking the relevant columns, I made a new list of items that could be correlated with the features for each item. This selection numbered 18845 examples which were split with a 70/30 ratio for learning and testing. The last part was to select features and normalise the values.\n\nTeaching the models\nI tested four different machine learning algorithms with Scikit-learn to see how they performed both in accuracy and time. Each model gave an accuracy of its predictions, and using a Jupyter Notebook command I could see how long it took for the CPU to execute a given command.\n\nGaussian naïve bayes gave the worst result of the four models with an accuracy of about 66%, but it only took 24.9 ms to train the model.\nK-nearest neighbour was also quick, but it performed much better with an accuracy of about 74%.\nThe Support Vector Machine I tried was much slower to train than the other models, using about 5.44 seconds. It also performed slightly lower than some of the others with about 72% accuracy.\nThe last model I tried was a Multilayer Perceptron, which is a basic neural network. This managed to get the best performance with 75% accuracy after tweaking some of the settings, ending up with four hidden layers of six neurons each. It was however also slower to train, taking 3.77 seconds.\nBelow is a graph of the loss curve showing how the model improved its performance as it learns from the data.\n\n\n\nLoss prediction of MLP\n\n\nResults\nWas it really successful?\nProbably not. However, the results raise a few questions regarding the dataset, relevant features for genres, and techniques for separating classes better using dimensionality reduction. It’s all really about how the data is understood and presented for the machine which determines how well it’s going to perform.\n",
        "url": "/machine-learning/2019/09/19/All-about-the-data.html"
      },
    
      {
        "title": "UltraGrid",
        "author": "\n",
        "excerpt": "Exploring an alternative audio and video network transmission software in the SMC portal.\n",
        "content": "Group C\n\nThe UltraGrid software\n\nUltraGrid is a free low latency and high-quality video network transmission software. The main benefit of using UltraGrid in the portal is its ability to transmit both high quality 4k video and IP-audio between campuses from the same software. Our mission is to successfully install UltraGrid in Oslo and Trondheim so that we can together explore its features and compare it to our current setup.\n\n\n  \n    \n      Assumed pros\n      Assumed Cons\n    \n  \n  \n    \n      Audio and video transmission from same software\n      High latency\n    \n    \n      High quality audio and video\n      interconnectivity problems\n    \n    \n      Highly Flexible\n      userfriendliness\n    \n  \n\n\nUser interface\n\nThe installation and run format of UltraGrid is command line based, which means it does not have a proper GUI at the moment. However, they are currently working on implimenting a GUI which will make it much more practically applicable and user friendly in the near future.\n\n  [Figure 1]\n\nInstallation process\n\nThe installation process is surprisingly extensive and has six unique steps:\n\n\n  \n    System test - Set up a testcard, a virtual video capture card, and broadcast a basic colour information stream locally (see figure 1).\n  \n  \n    Video input - Connect UltraGrid to computers capture card and recieve video.\n  \n  \n    Video output - Directing the video input signal to the correct local output (video card).\n  \n  \n    Audio input - Connect Ultragrid to the appropriate sound card and channels.\n  \n  \n    Video and audio output - Set up local network transmission which broadcasts sound and video to an external IP address.\n  \n  \n    Recieve and broadcast audio and video - Set up bidirectional network transmission between two locations.\n  \n\n\nStep 1-4 is achieved locally prior to any cross-campus transmission, but step 5 and 6 require both campuses to be on the same page and cooperate.\n\nWeek 38\n\nOur first priority was to check the LoLa computer’s specs to see if it was compatible with Ultragrid. When this was confirmed we downloaded the software on a personal device and transferred the installation files locally to the LoLa PC. This had to be done this way because the LoLa computers cannot be connected to the internet.\n\nStep 1 went smoothly for both campuses, however we encountered problems with step 2 and 3 in Oslo when trying to make UltraGrid connect and recognize the computer’s capture card, the Blackmagic Intensity pro 4k also known as “Decklink” (see figure 2). This is the device which we will use to receive high quality HDMI video information that will be transmitted to the external location. Our plan to resolve is by rebooting or finding relevant updates for the capture card.\n\n  [Figure 2]\n\nOn the other hand, step 4 was successful in Oslo. We managed to recieve an audio stream from the local portaudio soundcard on the LoLa pc. However, we did not manage to explore receiving audio from the other soundcards, due to the complications with steps 2 and 3.\n\nTo be continued\n\nIn the following weeks there are 2 important tasks we need to focus on for the installation process to succeed:\n\n\n  Make UltraGrid communicate with all the desired computer components (Capture card, Video card, Video cameras and Sound card).\n    \n      This also involves collecting the relevant information from these components like correct addresses, channels, desired compression formats etc.\n    \n  \n  Opening several IP-ports so that transmission between campuses is possible.\n    \n      This involves contacting UiO and NTNU´s IT-departments.\n    \n  \n\n\nWith the help from other SMC students and teacher guidance, we should be able to get UltraGrid up and running during the next couple of weeks.\n",
        "url": "/networked-music/2019/09/22/UltraGrid.html"
      },
    
      {
        "title": "The importance of sound quality",
        "author": "\n",
        "excerpt": "Reflections after several lectures with less sub-optimal sound quality\n",
        "content": "During the last couple of days, we have been using Zoom to communicate with Oslo in lectures. The (very interesting) WoNoMute session on Monday required an experimental Portal setup, which was too much to handle for two Portal newbies like ourselves the following day. For some reason, we didn’t get it to work even after class, and it wasn’t until Thursday that Eigil randomly came by and solved the issue. On the positive side, it made us reflect on the importance of maintaining a high quality communication channel in order for this programme to work.\n\nOne of the main problems with Zoom communication is the reduced interactivity. Since we didn’t understand how the mixer was set up, we had to use the Logitech V-U0034 speaker and microphone as our means to communicate with Oslo. The difference in audio quality between the two setups (Zoom vs. full immersive Lola) is remarkable. We had to concentrate more to understand what the lecturer said, and to speak to Oslo required us to move closer to the microphone. Consequently, it felt more interrupting to pause the lecturer in Oslo to ask a quick question, which resulted in a much less interactive session. It felt more as we were participants in a video lecture then part of a shared collaborative space.\n\n\n\t\n\tLogitech V-U0034\n\n\nThis would probably not be a problem if we had more experience with setting up the Portal. It is likely that these issues eventually will solve themselves as we develop our console skills. However, there should perhaps be an internal requirement about reverting back to the original settings after such a session. Even though cleaning up after a long day can be very tiresome, it’s often a lot more difficult to do for someone who are not aware of what’s been done.\n",
        "url": "/networked-music/2019/09/27/ImportanceOfSoundQuality.html"
      },
    
      {
        "title": "Entrepreneurship for SMC - Group B",
        "author": "\n",
        "excerpt": "Summary of SMC4015 project from group B\n",
        "content": "SMC4015 Entrepreneurship - Group B\n\nFor this class, we were divided into cross-campus groups and presented with a\n“problem” to solve via entrepreneurial methods learned through readings, class\ndiscussions, and presentations made by real-world entrepreneurs.\n\nGroup B consisted of Magda and Ulrik in Trondheim, and Gaute and Paul in Oslo.\n\nThe problem\n\n\n  Audio tape recordings suffer magnetic distortion and material deterioration\n\n\nAfter initial discussions on what seems on the surface to be a rather bland and\nuninspiring problem to tackle (at first glance it seems like a problem for which solutions\nare obvious, and already exist), we set to work, researching independently and together,\nvarious possible areas where this problem might exist and possible entrepreneurial\nangles to exploit in coming up with a solution. Using tools we learned in class like the\nAction Log and Learning Diary to document our research, we would then get together\nas a group via Zoom to discuss strategy and brainstorm new ideas.\n\n\n  \n  Flowchart of the development process\n\n\nPutting into practice entrepreneurial models derived from Sarasvathy’s theory of\nEffectuation (Causation and Effectuation: Toward a Theoretical Shift from Economic\nInevitability to Entrepreneurial Contingency, Sarasvathy 2001) we eventually narrowed\ndown an appealing market and product that we all agreed on.\n\nSolution - the Andvarì\n\nThe Andvarì is a temperature and humidity-controlled storage unit for magnetic audio\ntape reels. It provides security, storage and peace of mind for the high-end reel-to-reel\ntape aficionado as an all-in storage and indexing system for their prized (and very\nexpensive) tape reels. Designed and manufactured from the ground up in Norway with\nno expense spared, it should appeal to the targeted luxury audio segment for whom\nquality is paramount, and cost is no barrier.\n\nNamed after a dwarf from Norse mythology who guarded a golden treasure of\ninestimable value, the Andvarì, with a perfect blend of Nordic design, high technology,\nand impeccable workmanship will fit seamlessly into any audiophile’s dream system.\n\n\n  \n  3D model of the Andvarì\n\n\nSuck it, Bang &amp; Olufsen!\n",
        "url": "/other/2019/10/07/Entrepreneurship-Group-B.html"
      },
    
      {
        "title": "Recovio: Entrepeneurship - Group C",
        "author": "\n",
        "excerpt": "Group C’s project for the SMC 4015 Entrepeneurship course. Recovio is an audio digitization and storage company that serves companies at a scale and pricepoint that best fits their needs.\n",
        "content": "We were tasked with what looked like a simple problem, audio tapes suffer physical and magnetic deterioration over time?\n\nOur Solution\n\n\nOur solution, is a company called Recovio. Our strategic angle was to provide a full package for small cultural institutions and recording studios looking for a solution to help preserve the value of their deteriorating audio tapes. We believe that these markets are untapped and our services will allow us to address these small institutions from every angle.\n\nOur five fundamental services are:\n\n  Documentation of metadata, and non-auditory reference info\n  Evaluation of the deterioration or damage to the tapes in question\n  Reconditioning of any physical deformations of the medium prior to digitization\n  Digitalization of the tape (in its best condition) and metadata logging\n  Secure physical and digital storage and access\n\n\nOne strong selling point of Recovio, was that these services would be offered at a modular and package level that would enable us to access customers at varying needs and price points.\n\nWe know that this industry of archival and digitization is vast, but we believed that providing a complete package on a small scale as we start will allow us to adapt to the changing market and remain open for opportunities along the course of our company’s development.\n\nBusiness Model Canvas\n\n\nHere is a brief overview of what we decided would be some of our value propositions, resources, and market segmenetations for our company.\n\nMarket\nOur market would be focused initially on Norwegian private cultural institutions or very small public collections as well as recording studios who may be in need of our services. Our modular service suite would allow us to approach each from a number of angles but with an emphasis of being the go to solution for the digitization and archival of their old tapes. However, we would additionally be able to work with new formats for studios that may need support hosting their files on the clouds. Based on our research, were the markets we’ve found to best to target and fit our model.\n\nThese markets are out of the range of the public sector and may be in need of the technical services that otherwise don’t exist on the private and small-scale. Our research suggested that a lot of public cultural institutions and libraries already have access to public services that would allow them to digitize at scale so we adapted our focus.\n\nFuture goals\nAgain, we believe Recovio could provide these small institutions with the full suite of services to recover, digitize and store their valuable collections while also operating at a modular level for customers who may need just a little technical support in preserving their tapes. We felt our solution would fill a vacancy left by larger public operations who would be unfit to serve our potential clients unique projects and needs.\n\nGoing forward, we would like to make our whole package seamless and hassle free from start to finish. In addition, we also think there may be potential for growth in providing a dedicated cloud platform for recording studios who want to store and send files during recording sessions.\n\nReflections\nI think we felt this mini-course was a good excercise that, while may not have been strictly related to the content of the program, helped us advance soft social skills like group collaboration and task assignment, market research, and managing realism and viability within our proposal.\n",
        "url": "/other/2019/10/08/Recovio.html"
      },
    
      {
        "title": "Portal Flowchart",
        "author": "\n",
        "excerpt": "The SMC portal has been subject to many configurations over the last couple of months. In this post, we explore how flowcharts may help us see a brighter tomorrow (we need as much light as we can get here).\n",
        "content": "Week 41 in the portal, Group C\n\nFlowcharts\n\nA rigid signal flowchart is an absolute must in any complex system. These charts help systemize, organize and streamline our interactions with the system. Ideally, a detailed flowchart will helps us save time setting up before class and enable more students to orient, use and explore the portal features. Because of our endless adjustments, 5-minute fixes, and general non-sense that drove Iggy to push a preparedness test upon us, we decided it was about time for an up-to-date flow chart to help us on the way to portal stardom. Or at least a setup that we can turn on and start class with.\n\nThe SMC portal chart\n\nIn our quest for the perfect chart, we agreed that detailed connection specifications and a good design is key. Therefore, we decided to center the flowchart around the Midas mixer and LoLa PC, since these two machines serve as the central hub of our system. While this chart is specific for the UiO setup here, there are minor differences on the Trondhiem side and perhaps they would be able to make a chart using this template.\n\nFollowing Anders Tveit’s recommendation, we used Lucidchart, an application which allows people to collaborate on building flowcharts online. After making a draft, Paul Koening printed out the chart and took it to class. This way, we could inspect and explore the portal first hand while writing down the connection specifications. This process led to further dialog and more discoveries which in turn led to editing, and finally the final chart seen below.\n\n\n\nAll the objects in the chart have inputs on the bottom and and outputs on the top. The philosophy behind the object coloring is fairly arbitrary but it tries to discriminate between more “active” machines (yellow), “passive” machines (purple), output oriented machines (green), input oriented machines (orange) and finally ports between campuses (pink).\n\nThe connection wires’ shape and color also have meaning. The shape determines what type of signal passes through the wire, and the different colors indicate some signal chain examples, for instance the red path which illustrates the route audio takes from Trondheim to our Oslo ears. All of this information can be found in the legend.\n\nLoLa and Midas chart\n\nThe routing and interplay between the LoLa PC (with RME soundcard) and Midas mixer is often a source of trouble in our portal. Therefore, we felt it necessary to take extra measures to highlight this relationship. This second flowchart (below) follows the same principles as the one above, with the exception of having object inputs and outputs on the sides as well on the top and bottom.\n\nEventually, (we hope and pray) we might have a system that begins with an overhead mic directly to LoLa or Zoom, and use a breakout cable to pass Oslo inputs through the Midas mixer when we need it. Having a well-functioning setup with as few wires, sliders, or settings to configure is the current dream for Gaute at least.\n\n\n\nSummary\n\nWe hope that these flowcharts can be used as manuals for setting up a default SMC portal rig, troubleshooting, as well as serve other educational purposes in our cross campus classroom. We also hope that our charts can help future SMC students become comfortable with the portal and inspire them to update the system in various ways.\n\nCopy our charts, both the Portal chart and the LoLa and Midas chart.\n",
        "url": "/networked-music/2019/10/10/Portal-Flowchart.html"
      },
    
      {
        "title": "The HySax - Augmented saxophone meant for musical performance ",
        "author": "\n",
        "excerpt": "an augmented saxophone meant for musical performance, enabling background layer and delay to be controlled via gestures.\n",
        "content": "Backgroud\n\nAs a saxophonist, a jazz musician and an improviser, I sense a growing need in an interactive music system to augment the harmonic and sonic possibilities of the saxophone. A lot of important work has been done in the last 20 years in the field of instruments expansion, increasing both the sound and ways of interaction with the instrument. Recent work in the fields has been presented at NIME in 2006 with the Electronically-augmented saxophone, in 2008 with the Gluisax and the utilization of open sound control (OSC), the Gest-O Sax in 2012, using gestures mapped to signal processing parameters. The most recent work has been presented in 2019 with the HypeSax - Saxophone acoustic augmentation (Flores et al.) The HypeSax holds many unique features and creative design but the one that excites the most is the built-in speaker within the saxophone bell, advancing into ‘instrumental augmentation as a fully integrated acoustic/electric hybridization with a standalone system’.\n\nThe idea behind my interactive music system, the HySax, is to create a background or an accompaniment layer that would interact with the player by allowing body gestures to control sound parameters of a synth pad and delay parameters of the saxophone sound.\n\nThe system\n\nThis interactive music system that I worked on contain three sensors:\n\n  9DoF IMU – A versatile, motion-sensing system-in-a-chip which houses a 3-axis accelerometer, 3-axis gyroscope, and 3-axis magnetometer that gives us nine degrees of freedom.\n\n\n\n\n\n  Ultrasonic Distance Sensor\n\n\n\n\n\n  Rotary Knob\n\n\n\n\n‘Some players have spare bandwidth, some do not’ (Cook, P.R.,). I had to carefully consider this when planning my system. When it comes to the saxophone, the right hand is significantly more available than the left hand while playing. One can produce notes with only the left hand, but the opposite is not very common in traditional saxophone playing.\n\n\n\nThe system utilizes both the right hand of the player, together with gestural body movements. The position of the player is being tracked by the 9DoF sensor, measuring essentially the player’s posture on three-axis. When distancing the saxophone away from the body, the system measures the distance between the two via the ultrasonic distance sensor. During troubleshooting I have realized that clothes do not reflect the ultrasound back so well, so I am improvising by wearing a Tupperware box lid.\nFor sound generation I am using the saxophone’s acoustic sound with a Korg contact mic connected to the minijack mic input on the Bela, and a Pd patch containing four sine oscillators producing a pre-determined chord. The chord starts and stops playing with a push of a button.\nThe standalone Bela unit is mounted on a saxophone sound deflector/reflector (made by JazzLab) attached to the saxophone’s bell in a way that is facing the player and visible to both the player and the audience. The Rotary knob is located on the top right corner of the unit allowing for easy access with the right hand. To the left of the rotary Knob, there is the push button. The 9DoF sensor is located on the unit as well.\n\n\n\nThe Ultrasonic distance sensor is mounted on the saxophones body, under the right thumb. Placing the distance sensor there allows for maximum distance between the saxophone and the player, allowing for a longer movement producing higher range of input readings from the sensor.\n\n\n\nPure Data Patch\n\nThe software is based on a Pure Data patch which is uploaded to the Bela Board.\n\n\n\nThe mic input is connected to a delay line. The delay feedback amount is mapped to the Rotary knob. The ultrasonic distance sensor which is scaled to except only readings ranging between 6 to 15 centimeters (estimated min/max distance from the sax) is mapped to control the delay mix. When the sensor is close to the players body, the amount of delay in the mix is minimal to none (0). As the saxophone being pushed away from the body, the delay in the mix is at its maximum (1) (one-to-one mapping). \nAs previously mentioned, with a push of a button, four sine oscillators are playing a chord.\nThe chord is being faded in and out over a one second time frame to make it easy on the ear.\nSome maneuvering had to be done to cancel the initial bang when starting the system.\nThe fusion pose readings from the 9DoF sensor are mapped to control both the amplitude of each of the oscillators (tremolo), as well as the pitch modulation (vibrato) (one-to-many mapping). Additional tuning within the scaling objects still need to be done to achieve better results.\n\nEvaluation\n\nIn a more abstract level, the attributes that are being suggested by Hunt and Kirk, of characteristic of real-time multiparametric control systems are mostly being met with my IMS:\n\n  There is no fixed ordering to the human–computer dialogue. (if you don’t consider my push button)\n  There is no single permitted set of options (e.g., choices from a menu) but rather a series of continuous controls.\n  There is an instant response to the user’s movements.\n  The control mechanism is a physical and multiparametric device which must be learned by the user until the actions become automatic.\n  Further practice develops increased control intimacy and, thus, competence of operation.\n  The human operator, once familiar with the system, is free to perform other cognitive activities while operating the system.\n\n\nEvaluating this prototype of a system is a difficult task in such an early stage, but from a view point of the performer, myself, in terms of interaction, the system does allow for further exploration. In a moment where my body posture fulfill my sonic desire, I can freeze my movement and explore more settle/minor movements. The down side of that is that I am not able to then to control the delay amount which is mapped to the distance sensor. In terms of learnability, the system is fairly easy to grasp, understand and play with, but to be able to control it to its fullest, more calibration and scaling needs to be done to.\n\nIn terms of expressivity, the system is not yet fine-tuned to produce the exact required frequency and amplitude modulation, therefore, expressivity is limited. But expressive instrument usually means ‘’an instrument that affords expression’’ meaning ‘’an instrument that enables the player to be expressive’’ (Jordà Puig, S.) and personally I feel free to be expressive with the HySax. As mentioned by Notto Thelle during our evaluation session at the workshop yesterday ‘’ There is much potential to be expressive with this system in combination with a saxophone’’. I wanted to create a background pad layer that will vibrate together with the saxophone once I find the right posture but at the moment it is too unpredictable. The one chord pad is also quite limiting and more variation in tonality, voicings, chord type and sound texture are in need to give this prototype of the HySax a real value.\n\n\n",
        "url": "/interactive-music/2019/10/12/HySax.html"
      },
    
      {
        "title": "Body Drums - A wearable drumset",
        "author": "\n",
        "excerpt": "Body Drums\nFor the course SMC4045 - Interactive Music Systems, I built a wearable drumset.\nThe wearable drumset consists of a piezo-element placed on one hand, a force-sensing resistor on the other and a accelerometer on the fot. These sensors are then used to trigger one file each. In my case I used a kick drum sound for the foot, snare drum sound for the piezo element and a hi hat sound for the FSR. Then when these sensors are triggered, the sound that are mapped to the sensor will be played. For example if I stump my foot, the kick drum sound will be played.\n",
        "content": "Body Drums\nFor the course SMC4045 - Interactive Music Systems, I built a wearable drumset.\nThe wearable drumset consists of a piezo-element placed on one hand, a force-sensing resistor on the other and a accelerometer on the fot. These sensors are then used to trigger one file each. In my case I used a kick drum sound for the foot, snare drum sound for the piezo element and a hi hat sound for the FSR. Then when these sensors are triggered, the sound that are mapped to the sensor will be played. For example if I stump my foot, the kick drum sound will be played.\n\n\n\nBody Drums\n\n\nThe setup\nThe three sensors are connected to Bela, with processing and playback of audio files in C++. The playback of audio files will be played from the analog audio output of the Bela. Below you can see a picture of the Bela and the connected sensors. On top left you see the force-sensing resistor, top right is the piezo element and bottom right are the accelerometer.\n\n\n\nBela and Breadboard\n\n",
        "url": "/interactive-music/2019/10/12/BodyDrums.html"
      },
    
      {
        "title": "Microture",
        "author": "\n",
        "excerpt": "Microture is an interactive music system, based on manipulation of the input sound (microphone sound) with small gestures..\n",
        "content": "Microture is an interactive music system, based on manipulation of the input sound (microphone sound) with small gestures. It uses Bela. Bela is an embedded computing platform for creating beautifully responsive interactive applications. Bela provides ultra-low latency, high quality audio, analog and digital I/O in a tiny self-contained package that can be easily embedded. Built upon the BeagleBone family of open-source embedded computers, Bela combines the processing power of an embedded computer with the timing precision and connectivity of a microcontroller.\n\n\n\n\n\nIntroduction\nOne of the main principles of this project is augmentation. The current research field on augmented instruments is motivated by the assumption that the combination of traditional acoustic instruments with today’s sound technology yields a high potential for the development of tomorrow’s musical instruments. Considering that we are living in tomorrow in a sense, it is a current matter.\nThat means integrating the tactile and expressive qualities of the traditional instruments with the sonic possibilities of today’s digital audio techniques creates a promising perspective for instrument design. (Lähdeoja, 2019)\nThis augmentation is based on the use of gestures in order to make and expand the sonical perspective of the instrument.\nAs Pierre Hébert says, the measure of a work of art is whether one can sense in it the presence of the artist’s body. If so, then it is a success, and if not, it’s a failure. (Ostertag, 2002)\nOf course, he is talking about a corporal presence of the artist emanating from the work. Which in here refers to our body gestures.\nIn here the body is used to express oneself and that includes all the small-movements. In other words, combination of body gestures with technology to expand the amount of expressiveness.\nHistory\nAt the beginning of the process, the goal was to create an Augmented electric guitar, by the use of gestures. But there were three phases that I went through, and in the end\nas you can observed the outcome became so different. \nFirst I decided to expand on the idea of slide guitar, considering the limited bandwidth and practicality of the guitar I though it is going to bring me more control.\nBut unfortunately it did not and that was because of a couple of reasons:\n\n  Not having sufficent skills to play slide guitar\n  Not having sufficient equipment and time to build the system\n\n\nEven with overcoming those issues the main problem was the lack of time to practice using the instrument and present a decent performance. Therefore I decided to use the microphone as the sound source, which gave me much more bandwidth and reliabality.\n\nKey features\nThe key features in this instrument are:\n\n  Capturing the sound input: Vocals or any other sounds produced by mouth or other objects (sound effects)\n  Manipulation of sound input with various effects / controllers (based on gestures), which is being down in a Pure Data patch, uploaded to Bela.\n\n\nDesign and implementation\nThe sound, after getting into the system goes through various effects and signal processing systems. Those include:\nSound manipulation process:\n\n  Delay\n  Pitch\n  Distortion\n\n\nAfter that they are being manipulated by the controllers. The controllers are the sensors that are implemented in the system.\nSensors being used in this system are:\n\n\n  Accelerometer, being worn as a ring, using its X and Y axis\n  Distance sensor\n  Potentiometer\n  Slider\n\n\n\n\n\n\n\nThe senors, transform the movements into values and those values are being modified to change the effects parameters. The mappings being done are as follows:\n\nOne to one mapping:\n\n  Right hand:\nThe use of the accelerometer sensor: X axis for the delay mix, Y axis for the delay time\n  Left hand:\nUltrasound sensor for the distortion amount, Potentiometer for the pitch shift amount\n\n\nOne to two (many?) mapping:\n\n  Left hand:\nSlider: The normal use of it controls the delay feedback and by moving it rapidly, the velocity amount is being used for engaging/ disengaging the pitch shift and distortion processing chain.\n\n\n\n\n\n\n\nAlso, Here is a picture that shows the pure data patch.\n\n\n\n\n\n\nExpressiveness, Playability, Learning\n\nExpressiveness: Expressiveness in this case could refer to intelligent recognition of gesture in order to characterize the gesture. Of course by make the appropriate mapping and more gesture-sound mapping, more expressiveness could be acheived. But it is important to mention that the performer should achieve a level of virtuosity by more practice, in order to experience that matter.\nPlayability and learning: This instrument is tangible. That is because of the use of acceleromenter. It is being extremly sensetive. But it can be addressed in further developments, by using a low pass folter for the data being captured by it. But considering the amount of effects that can manipulate the sound and enough practice, the player can lead to an acceptable degree of control regarding playing hte instrument.\n\nEvaluation\nThis is the expert user’s evaluation that I received, from Karolina Jawad.\nOverall impression: The augmentation of the voice is very engaging and interesting. There is a great amount of playfulness that is provided through the different controllers. Through the gesture controllers it was especially inviting to explore the instrument and spend time with it.\nThings that can be worked upon: audibility of what is happening through which action could be a little bit more\ndirect since there are many parameters, but it is at the same instance a positive effect of complexity.\n\nPresent issues / Further development\n\n  Robustness: There should be more time spent and more relevant equipment used in order to robust the instrument\n  Moving on from the initial idea: Considering having more time to practice and build the instrument adn also having proper equipment in touch, the initial idea could be implemented.\n\n\nFollowing there is a short video of the protoype being played:\n\n\nAcknowledgement\nI would like to thanks Stefano Fasciani for his crucial support and guidance in the process of making the instrument.\n\nReferences:\n\n  Lähdeoja, Otso. (2019). An approach to instrument augmentation: the electric guitar.\n  Ostertag, Bob. (2002). Human Bodies, Computer Music. Leonardo Music Journal. 12. 11-14. 10.1162/096112102762295070.\n  Dobrian, Christopher &amp; Koppelman, Daniel. (2006). The ‘E’ in NIME: Musical Expression with New Computer Interfaces.. 277-282.\n\n",
        "url": "/interactive-music/2019/10/13/ims-microture.html"
      },
    
      {
        "title": "LoopsOnFoam",
        "author": "\n",
        "excerpt": "During a 2-week intensive workshop in the course Interactive Music Systems I worked on the development of an instrument prototype, which I named LoopsOnFoam.\n",
        "content": "During a 2-week intensive workshop in the course Interactive Music Systems I worked on the development of an instrument prototype, which I named LoopsOnFoam. The instrument is made up of hardware sensors and Pure Data patches. The system is made for the Bela board, which runs the patches and manages IOs.\n\nI wanted a system that could loop audio input and be able to manipulate/control each loop while performing.\n\nThe looper\n\nThe looper can record and play three loops, with a dynamic microphone as audio input. The three leftmost buttons on the breadboard controls this. Press once and you start recording, press again and you stop recording and it automatically starts playing, press a third time and you stop it. Then you can overwrite the loop, following the same procedure.\n\n\n\nLooper device\n\n\nThe Foam\n\nThe foam has a piezo microphone and an ultrasonic distance sensor attached to it. You choose which loop to control by pressing the rightmost button on the breadboard. A blue LED will light up next to the button (loop) you are selecting.\n\n\n\nController\n\n\nThe foam controls parameters of a granular synthesis patch, and you can explore tapping, scratching, hitting and pressing the foam to change the sound of the selected loop. Also, the distance from your hand to the ultrasonic distance sensor will affect the sound.\n",
        "url": "/interactive-music/2019/10/13/LoopsOnFoam.html"
      },
    
      {
        "title": "Alien_Hamster_Ball",
        "author": "\n",
        "excerpt": "The Alien Hamster ball - an instrument expressed through a 3D space\n",
        "content": "The main concept for the Alien Hamster ball is an instrument expressed through a 3D space. The movement of the sphere will trigger the modulation of the tone, with a sound that is space age or “alien”. Other functions create further options and depth to the model, and allow further exploration through the timbres created.\n\nInspiration\n\nIt is directly inspired by the alien reece project i produced last year. It is a web based audio model that had oscillators mapped to the mouse movement, on both X and Y axis. I found it refreshingly intuitive to find various clashing and morphing frequencies as you move around the page. This felt very different to adjusting frequencies on a regular synthesiser, and encouraged creating abstract tones and sweeps.\nI wanted to translate this idea to a movement in a 3D space with a physical object.\n\nhttps://SMC-master.github.io/sound-programming/2019/02/11/Alien-Reese.html\n\nAlso to a lesser extent i was inspired by a group project presented in the first year exploring the use of feedback in a collaborative instrument, and wanted to combine these elements to create a wholly new one.\n\n\nThe Prototype\n&lt;/figure&gt;\n\nHere are some concepts when regarding designing a digital music instrument, that the hamsterball hopes to be.\n\n“Instant music, subtlety later” . As newcomers grow to more expert levels, the music gets more varied and interesting if they experiment with the relative pressures and tilts.\n\nThe “Smart instruments are often not smart” principle, in that the instrument doesn’t change at all, but rather trains the user to use more gentle and subtle manipulations of the sensors.\n\nAnd of course - “Everyday objects suggest amusing controllers.”\n\n\n\nI used three types of sensor - Accelerometer, microphone and 3 potentiometers.\n\n\n  \n    The accelerometer adjusts parameters that make up the main body of the instrument.\n  \n  \n    The mic adds another audio input that adds elements of feedback and unpredictability.\n  \n  \n    Potentiometers are for adjustments of other basic functions and FX, for expression and shaping.\n  \n\n\n\n\nX-Axis\n\n\nMy pure Data patch has three oscillators as the sound source - split into Osc1, Osc2 and Sub Osc. it also has a microphone as a secondary input source. To add spice and complexity to the timbre, a FM oscillator is fed into Osc 2.\n\nThe X axis is the core of the system, and blends with the y axis to create the drifting oscillators drone sound and phasing frequencies.\n\nAs the accelerometer in moved along the X axis -\n\nOsc 2 will move from 112hz to 302Hz. This is a small range, however in conjunction with the two other oscillators, FM synthesis and FX - many partials can be formed and give a full range in frequency. Also, as i was designing a bass range instrument, it makes sense to keep the frequencies low.\n\nAlso when moving the ball along the axis it is easier to control with a smaller range, and doesn’t sound like a transient or FX by tracking too quickly through the frequency spectrum - this kept the sound design more as a drone akin to cinematic tension.\nAt the same time, the sub oscillator moves along the same axis - just 3 hrz below. This results in the two oscillations phase together at all times, thickening up the sound. The sub is a sine and adds thickness rather than the more extreme saw used for Osc2.\nAs well as this, the wave shape of Osc 1 will cycle fully from a sine to a square over the range, adding further complexity. Lastly, a subtle increase in the delay amount will occur as the frequencies move higher in the spectrum. This acted as a sweetener to the harsher, thinner frequencies higher in the spectrum, and helped keeping the sound aesthetically pleasing at all angles.\n\n\nThe Failed Prototype\n&lt;/figure&gt;\nThe functionality of the ball as a concept has many possible future uses. Throwing? a game of catch? Juggling? Rolling the ball on the floor? football? Could also put a hamster inside…\n\nWith some more time, i believe i could create the prototype i aimed for, and maybe even try some of these techniques out. Unfortunately i had little time to play with the instrument, and so feedback is lacking on that part.\n\nOverall the expressiveness of the instrument felt far less when missing key functions. Aside from this i stick by the concept, and hope to improve the system in the future.\n",
        "url": "/interactive-music/2019/10/14/AlienHamsterBall.html"
      },
    
      {
        "title": "AudioBend",
        "author": "\n",
        "excerpt": "My project idea for the Interactive Music Systems was to build a glove that can manipulate sound.  It was actually inspired by seeing the “mi.mu Gloves”.  The paper on the “Data Glove” gave me ideas on the design aspect of the glove although the way it works is a bit more different than what I use in my glove. “Data glove” uses multiple flex sensors on the fingers and force sensitive sensors to contact the finger tips and an accelerometer to get data from the wrist control. In my glove I used flex sensor on index finger, 3 – axis accelerometer on my hand and a Distance Ultrasonic sensor on my palm. Attaching those stuff to the glove was a bit tricky but “ducktape” saved my life.\n",
        "content": "Project Idea\n\nMy project idea for the Interactive Music Systems was to build a glove that can manipulate sound. It was actually inspired by seeing the “mi.mu Gloves”. The paper on the “Data Glove” gave me ideas on the design aspect of the glove although the way it works is a bit more different than what I use in my glove. “Data glove” uses multiple flex sensors on the fingers and force sensitive sensors to contact the finger tips and an accelerometer to get data from the wrist control.\nIn my glove I used flex sensor on index finger, 3 – axis accelerometer on my hand and a Distance Ultrasonic sensor on my palm. Attaching those stuff to the glove was a bit tricky but “ducktape” saved my life.\n\nDesign and implementation.\n\nThe circuit made to sit top of the Bela Cape and sensors were connected to the circuit through long wires. The Bela Cape is connected to the beaglebone black wireless board which has the Operating system. The sound module is a pure data patch made by Riccardo Sraccia. Check out his work. So the work flow in the building process is to use the bela IDE on browser to edit the PD patch and test with sensors. Once everything is finalized the final the project was set to boot on start on Bela. Then it runs on its own without any connection to the PC.\n\n\n  \n    Flex Sensor\n\n    \n\n\n  \n  \n    Acceleroeter\n\n    \n\n\n  \n  \n    Ultrasonic Sensor\n    \n\n\n  \n\n\nPD sound Module.\n\nThe Pure data patch is based on granular synthesis. It has following sections. And It needs and audio sample and I used a pad sample.\n\n\n\n\n\nThe Mapping\n\nThe mapping is mostly “one to one “and single instance of “One to many “I tried to control the starting point of the sample and the length with sensors but it tends make the instruments sound crappy and not very expressive.\nAccelerometer.\nX – Acceleration controls the frequency of the granular phasor.\nY – Acceleration mapped to sine LFO frequency and the cutoff.\nZ –acceleration mapped to Sawtooth LFO\nFlex sensor - speed of the sample.\nUltrasound sensor is mapped to Resonance Frequency.\n\n\n\n\n\n\n\n\n",
        "url": "/interactive-music/2019/10/14/AudioBend.html"
      },
    
      {
        "title": "Physical computing Day 1 - Group B",
        "author": "\n",
        "excerpt": "First day of physical computing\n",
        "content": "The first day of the physical computing course was for the most part spent exploring sound creation with different hardware components. We got introduced to concepts such as hardware hacking and circuit sniffing and also got to test patching in PureData.\n\nFor the workshop part, each group was supposed to create a variety of sounds from sniffing circuits, experimenting with hardware and circuits and from soundwalking. After collecting the sounds, we put them into a Pd sampler, which would later be used for a performance in the Portal. The Pd patch can be seen below.\n\n\n\nWe decided that we would do the sound collection part separately in Trondheim and Oslo, and then meet again to play with the collected sounds over Zoom. In Trondheim, we first recorded a small hardware jam and then split it up into one-shot samples. After that, we went outside to collect a different range of sounds, and ended up including sounds of ice breaking and splashing water.\n\nThe video below shows how we passed a battery current through a loudspeaker, which makes the cone of the speaker move in and out, creating a percussive sound. We added some small items on top of the speaker to make the sound richer and more industrial.\n\n\n  \n    \n    Your browser does not support video tag.\n  \n\n\nBelow are some audio clips that we used in our sampler. The end performance didn’t turn out that well, due to some technical challenges with Pd. We will make a better performance tomorrow!\n\n\n  \nYour browser does not support the audio element.\n\n\n\n  \n  Your browser does not support the audio element.\n\n\n  \n  Your browser does not support the audio element.\n\n\n  \n  Your browser does not support the audio element.\n\n\n  \n  Your browser does not support the audio element.\n\n",
        "url": "/interactive-music/2019/10/15/Physical-Computing-Day1-Group-B.html"
      },
    
      {
        "title": "Physical Computing Day One: Victorian Synthesizer Madness! Group C Reports In From Heck",
        "author": "\n",
        "excerpt": "The first day of Physical Computing started and ended with a bit of confusion in the Portal, but that is par for the course. Once we set up the various cameras and microphones, and dealt with feedback, echo, etc, the fun began!\n",
        "content": "Physical Computing Day One: Victorian Synthesizer Madness! Group C Reports In From Heck\n\n\n\nThe first day of Physical Computing started and ended with a bit of confusion in the Portal, but that is par for the course. Once we set up the various cameras and microphones, and dealt with feedback, echo, etc, the fun began!\n\nThe goal of the Victorian Synthesizer workshop is for each group to create an electronic instrument (or instruments) using parts and techniques that would have been known to the Victorians, and then make a final performance in the Portal.\n\nOnce we understood the basic techniques, we separated into our groups and set to work.\n\nWith motley assortment of raw speakers, wires, piezo pickups, batteries, nails, coins, some lentils (really), a strip of aluminum foil, inductive pickups, etc. the first task was to make some noise… which we certainly did!\n\nThe Performance:\n\nGroup C is a bit Oslo-heavy, but Iggy in Trondheim more than made up for it by recording some amazing sounds with an inductive pickup and his cell phone . . . and then processing the resultant sound into a mad bass bed-track for the rest of our improvisations. During the performance, he also created real-time sounds with the inductive pickup. So nice to hear.\n\nThomas and Paul in Oslo created some interesting sounds with a lentil-filled raw speaker connected to a 9-volt battery via a momentary switch, and a piezo contact mic running through a battery powered amplifier.\n\nThe lentil-filled speaker was quite rhythmic when played (rhythmically) via the momentary switch, recalling a cabasa shaker being throttled by an over-excited telegraph operator. SOS! This ship is going down!\n\nThe piezo was variously rubbed and scraped across various surfaces while the hand-held amplifier was lowered in and out of a SiO-sponsored refillable coffee cup (Coffee deal!), creating a modulation to the amplified tone that was quite satisfying, like picking an aural scab off of the hairy hindquarters of Mephistopheles him/herself, running like hell, and actually getting away.\n\nMeanwhile, Aleks used the very non-Victorian computer program PureData to create a basic sampler-looper, and using a jerry-rigged small speaker as a microphone, proceeded to sample the noise we created, manipulate it in real time, then feed the sound back through one of the battery powered amps. Various echos, feedback loops, and howls that literally peeled the paint off the ceiling was the sonic result.\n\n\n\nWe played all this for like, I don’t know, 5 minutes? When the last howls of the demon we had aroused faded and we emerged once again, blinking in the glare of the bright sunlight as if awakening from a dream, our comrades greeted us with a most thunderous applause.\n\nThis was probably because it was over, we assume.\n\n\n        \n        \n        Live-Demo: SMC-Students performing in the Portal        \n        Your browser does not support the video tag.\n\n\nLive-Demo: SMC-Students performing in the Portal\n",
        "url": "/interactive-music/2019/10/15/Group-C-Physical-Computing.html"
      },
    
      {
        "title": "The B Team: To Heck and Back",
        "author": "\n",
        "excerpt": "Today we began our experiments with some lofi hardware, simple contact mics, speakers, batteries, and some crocodile cables to connect it all. We left in pieces.\n",
        "content": "The B Team makes an attempt\n\nToday we began our experiments with some lofi hardware, simple contact mics, speakers, batteries, and some crocodile cables to connect it all. This class is focused on providing us with a basic understanding of how we would be able to create sound from analog materials in a workshop based environment. Below are some of the objects we recorded with an how we managed to do it. You may click on the header links to listen to the sounds we recorded from Freesound.org!\n\nIn Trondheim\nFor all the below in Trondheim, a jack cable was plugged to the speaker, and on the other end there was a contact mic connected to an alligator cable.\n\nBottle elastic band\n\n  Contact mic was attached to the plastic bottle with a piece of tape\nSimon kept the bottle with his knees, putting the elastic band on the top and pulling it\n  The elastic band was torn with a finger\n  Depending on how strong the elastic band was pulled, the sound changed\n  Additionally, the bottle was being crushed a bit with the legs which also produced interesting side sounds\n  The sound was recorded with pocket Roland recorded put in front of the speaker\n\n\nCoin spin\n\n  The contact mic was attached to the bottom of the plastic bowl (the side which was touching the table)\n  The coin was spun at the bottom of the bowl\n  The sound was recorded with pocket Roland recorded put in front of the speaker\n\n\nTape unrolling\n\n  The contact mic was attached to the roll of duct tape\n  The tape was being unstuck rapidly\n  The sound was recorded with pocket Roland recorded put in front of the speaker\n  Problem: the mic wasn’t too stable on the roll because of its surface\n\n\nPlastic box pressure\n\n  The contact mic was attached to the soft plastic box which was laying on the table\n  The surface of the box was being touched quickly with different pressure\n  The sound was recorded with pocket Roland recorded put in front of the speaker\n\n\nPlucking an elastic band\n\n  The contact mic was attached to the roll of duct tape\n  An elastic band was put around the roll of duct tape\n  The elastic band was torn with a finger\n  Depending on how strong the elastic band was pulled, the sound changed\n  The sound was recorded with pocket Roland recorded put in front of the speaker\n  Problem: the mic wasn’t too stable on the roll because of its surface\n\n\nIn Oslo\nIn Oslo, we did not have the amplifiers to really pull out delicate sounds from the contact mic so we had to be a bit more abrasive.\nCompleting a circuit with a knife\n\n  We passed a 9V battery through a metal knife and then connected the other end to a speaker\n  We then completed the circuit by running one alligator end along a metal knife\n  The frequent contacts that happened as the rough metal end of the alligator clip rubbed against the knife handle made for a rough, scratchy and distorted sound\n\n\nRubbing against fabric\n\n  A contact mic amped through a 9V battery and rubbed against a sweater, jeans, and the seat of a chair\n  Because we weren’t able to amp the mic up enough for more delicate sounds in  response to subtle vibrations, we had to be a bit more abrasive with the mic\n\n\nAnd after a long day of technical complications…\nTowards the end of class we attempted a performance but gravely misunderstood the aim of these recordings, so we weren’t in a position to perform with our sounds with the sampler we created using Pure Data. The embedded sounds were then uploaded to Freesound, a wonderful site that allows anyone to upload their recordings license free. However, there is much to learn and even with these few tools it is obvious that so much is possible with creative applications.\n",
        "url": "/interactive-music/2019/10/15/b-team-physical-comp-day-1.html"
      },
    
      {
        "title": "Instant Music, Subtlety later",
        "author": "\n",
        "excerpt": "When drafting ideas in unknown territory one can become overwhelmed with the sheer endless options to create an IMS (interactive music system). Here a real-time processing board for voice with gesture control.\n",
        "content": "“Instant music, subtlety later,”\n(Cook, P. 2001. “Principles for Designing Computer Music Controllers,”)\n\nInspiration\nThe main principle of my IMS is based on an instrument from 1985, a bit older but not much older than me:\nThe processing of ambient sound and vocals in real-time through gesture (and granular synthesis) control is something that pioneer Michael Waisvisz virtuously performed with his ‘The Hyperinstrument’ / ‘The Hands’. It is a fundamental piece in NIME and was among other ideas quite influencing for my way of proceeding. Engineering this form of sonic interaction was adapted by artists like Pamela Z for example. Pamela Z transferred similar principles very interestingly in a complex artistic set-up. Her performance ‘Memory Trace’  deals with a real-life problem, memory loss. The memory of humans and of engineered circuitry is interesting to make use of performative. It can ideally reflect upon our temporary condition by exploring its artistic dimensions while being embed it in a meaningful context.\n\nHowever I had to accept that we are operating from another end. And mentioning Michael W and Pamela Z\nshows in my perspective this way to approach engineering. There is this state of ‘that is possible to create’ to\n‘that is possible to do with’.\n\nSystem functions\nWhen drafting ideas in unknown territory one can become overwhelmed with the sheer endless options to create an IMS (interactive music system). There were quite adventurous objects that could produce sounds I went virtually pregnant with. The follow up is usually an ongoing negotiation process between available equipment, own skills, time and research that would determines the final outcome.\n\n\n  Microphone – capturing sounds of any kind, amplified through vocals\n  Potentiometer - Delay, effect\n  Slider  - Feedback, effect\n  Distance Sensor (Ultra Sound) – granular synthesis, received from input that went through the other effects\n\n\n\n  \n  Pure Data Patch with all effects\n\n\nMapping and perception\nThe use of the sound manipulation is not only audible but visible. The mapping of the the Ultra Sound Sensor to gestures is quite linear,\nthe closer the hand gets to the sensor, the less intense the effect. On a distance of 30 to 50 centimetres the amount of granulation is most dense. In the video below it is perceptible how the hand acts as an synthesizer. The level of engagement and expressivity is certainly limited, but not exhausted after 5 min as there are still unpredictable elements within the interaction.\n\n\n        \n        \n        Exploring the instrument. While performing and before.        \n        Your browser does not support the video tag.\n        \n\n\nIn the first part of the video, the performance,  I was trying hard to get control of the processing parameters, compared to the last/first rehearsal which follows after the demo. For some reason it was way more difficult to keep the same amount of control on the single effects as in a relaxed ‘unobserved’ moment – maybe it was the demo effect as I haven’t touched upon the patch any more in the meantime.\nThe rhythmically performed gestures were helping me to get in tune with the activating the intensity of the granular synthesis and its delay.\n\nThe workshop was very fun, however in the development process of the IMS I ended up with and also while exploring the interaction with it I was wondering about how much artistic meaningfulness I could possibly create. Parameters that are important to evaluate the expressivity of an engineered product like\n\n  reliability,\n  repeatability\nare most likely not sufficient in other context. The unreliable or unpredictable can be a huge part of the aesthetic. Of course, it should not fall apart, on the one hand, the ‘success’ of a design is its technical realization. On the other hand (again), the level of technical sophistication will never replace meaningful interaction. The context probably determines the evaluation of the outcomes as well. As being someone with a past in theatre, I can hardly extract more than a nice application from it, that counts for all of the instruments. But inspires me to look further and include the potential applicability in the design process. In this interface of creation of things, it feels a little bit like a gap between ‘designing’ things and creating art. But it can help to generate an intersectional practise of technology, music and theatre for example, which is an inspiration for where one could go beyond the circuitry design.\n\n\nAcknowledgements\nI would like to thank heartfully all SMC-IMS-Trondheim peers who helped me developing the board, a big thank especially to Sepehr Haghighi! Thanks also to Stefano Fasciani.\n\nReferences\n\n\n  Dobrain, C. The ‘E’ in NIME: Musical Expression with New Computer Interfaces\n  Arfib, D. Strategies of mapping between gesture data and synthesis model parameters using perceptual spaces\n  Waisvisz, M. The Hands, a set of remote Midi-controllers\n  Benjamin, W. The Work of Art in the Age of Mechanical Reproduction\n\n",
        "url": "/interactive-music/2019/10/17/Instant-Music-Subtlety-later.html"
      },
    
      {
        "title": "The Fønvind Device",
        "author": "\n",
        "excerpt": "For my interactive music systems project, I wanted to make use of the Bela’s analog inputs and outputs to make a synthesizer capable of producing not only sound, but also analog control signals that can be used with an analog modular synthesizer. This post goes briefly through some of the features and the design of my system, and at the end there is a video demonstration of the system in use.\n",
        "content": "\n\nThe Fønvind device and a modular synthesizer\n\n\nIntroduction\nFor my interactive music systems project, I wanted to make use of the Bela’s analog inputs and outputs to make a synthesizer capable of producing not only sound, but also analog control signals that can be used with an analog modular synthesizer.\nThis post goes briefly through some of the features and the design of my system, and at the end there is a video demonstration of the system in use.\n\nSeveral other synthesizer modules in the Eurorack format have used the Bela, perhaps most notably the Pepper module.\n\nBy using sensors and other inputs to the Bela, it is possible to make an expressive synth module that enables interaction in ways not commonly used with modular synthesizers. The joystick is however something many people are familiar with, and already have the controller intimacy and muscle memory usually needed for expression with musical instruments.\n\nSystem overview\nThe system is implemented using Pure Data and prototyping sensor modules soldered to two double-sided prototyping PCBs. The system is operated by interacting with a joystick and the environmental sensors for light and temperature. It should be noted that the temperature sensor isn’t as responsive as either the LDR (Light dependant resistor) or the joystick, but it still influences parts of the underlying code.\n\nThe CV (control voltage) signals are used by patching minijack cables from the prototype to any CV input on an analog modular synthesizer. Please see the video below for more on how the device operates. (skip to 3:05 for an improvised performance).\n\n\n\nNote: Since the presentation at the end of the course, I have made some small adjustments and improvements to the system. the joystick operation has been improved slightly by lowering the input voltage from 5v to 3.3v, thus not overloading the analog inputs on the bela (which only reads up to 4.096v). Some of the digital signal processing has also been slightly modified to increase expressivity by utilising a larger range of values that the sensor mapping can offer.\n",
        "url": "/interactive-music/2019/10/18/The-Fonvind-Device.html"
      },
    
      {
        "title": "Wizard_of_Vox",
        "author": "\n",
        "excerpt": "Wizard Of Vox - Wizard Of Vox is a gesture-based speech synthesis system that can be can be “made to speak”\n",
        "content": "Wizard Of Vox is a gesture-based speech synthesis system. I have been fascinated by the flexibility of the human voice for a long time. The first two formants can be used as parameters that have a large amount of expressiveness, and can be applied to any broadband frequency sound source. Instruments can be “made to speak”. The combination of several sound sources in the human speech production system and several branches of resonant chambers in the oral and nasal cavities are also a fun challenge to model. Artificial voices are always a bit creepy, and this can be used artistically.\n\nTheory\n\nThe prototype I devised in this project is based on the source-filter theory of speech, which posits that speech outputs can be analysed as the response of a set of vocal tract filters to one or more sources of sound energy. A source in the vocal tract is any modulation of the airflow that creates audible energy. For speech, there are two main categories of sources:\n\n\n  Glottal constrictions\n  Supralangyreal constrictions\nGlottal sounds can be voiced or unvoiced. Both the voiced and unvoiced sources (unvoiced as in whispering voice) are highly modifiable by the shape of the oral and nasal cavities. All sonorants (vowels, glides, liquids and nasal consonants) are characterised by what is termed as formants. In particular, the first and second formants have a large role in how we perceive sonorants. For this system, I have relied on these two formants (hereafter called F1 and F2) to control the speech synthesis. F1 and F2 were mapped to the accelerometer axes X and Y. By tilting the accelerometer right/left and backward/forward, I could thus easily access all sonorant within a short time period (fraction of a second).\n\n\n\n\nFigure 1: The vowel quadrilateral with F1 frequencies on the Y axis and F2 frequencies on the X axis. Vowel placements are shown as coordinates on this X-Y plane.\n\n\nIn addition to the glottal sources, supralangyreal sources are turbulence noise happening higher up in the palette, tongue, teeth and lips. These are independent of the glottal sounds, and they do not have formants as such. These sounds (most of what we call consonants) are characterized by the type of friction occurring. From now on I will call these source sounds obstruants, which is the linguistic term.\n\nSynthesis\n\nAll these sounds have been modelled in pd using subtractive synthesis:\n\n\n  Source sound for voiced glottal sounds: sawtooth wave (pd object phasor~)\n  Source sound for unvoiced glottal sounds: white noise (pd object noise~)\n  Source sound for unvoiced obstruants: white noise\n  Source sound for voiced obstruants: sawtooth + white noise\n  Filtering technique for sonorants and aspiration: formant synthesis (using the vcf~ object in pd)\n  Filtering techniches for obstruants: band pass/band stop filtering\n\n\n\n\nFigure 2: Audio flow chart\n\n\nMapping\n\nAccelerometer X/Y controls the formants (F and F2)\nThe flex sensor controls pitch and amplitude at the same time (the higher the energy, the higher the pitch). But the range of the flex sensor is divided into two separate regions. For the first 40 % of the range, the source is white noise, and this is meant to model aspiration (i.e. unvoiced) before voicing actually kicks in for the final 60% of the range. This makes it possible to simulate the breathiness of the human voice as it starts vocalizing. It also makes it possible to simulate the “H” consonant properly, because the phoneme “H” is actually different depending on the subsequent vowel. The “H” in “hello” is phonetically different from the “H” in “happy”.\n\n\n\nFigure 3: The flex sensor\n\n\nFinally, there are seven different buttons triggering consonants. These are divided into three buttons for the nasal consonants “M”, “N” and “NG”, which have a voiced glottal sound source and thus modified by formants, and four buttons for the consonants “S”, “P”, “T” and “K”. For this prototype, I did not make triggers for more than these consonants, but the longer-term plan is to be able to trigger more consonants through the combination of these same buttons.\n\n\n\nFigure 4: The subpatch for glottal source sounds and their processing\n\n\nConsiderations\n\nExpressiveness: there is a much potential for expressiveness\nPlayability: it takes practice to form intelligible speech with this system, but it is possible\nVirtuosity: it is possible to show a high degree of virtuosity using this system, with enough practice\nAeshtetic aspects: it is a fun instrument to listen to, but it can cater to darker emotions (the uncanny valley effect)\nAudience perspective: this could be an engaging instrument to witness for an audience (the performer as a wizard creating speech by movement)\n\nReflections on feedback\n\nworking 100% at the time, as only three of the seven buttons were working and the flex sensor had the wrong resistor in the circuit.\nHowever, the review was overall positive. He mentioned that the system is a bit difficult to play with. I agree, and personally, I don’t consider this a weakness. Most interesting instruments are difficult to master.\nIt is important to keep in mind, however, that this prototype uses sensors that were available in the Bela kit. For future iterations, I would like to control the accelerometer using a lever where the buttons are placed on the handle. And instead of a flex sensor, I would like to use a proximity sensor with high resolution.\n\nReferences\n\nDiehl, R. L. (2008). Acoustic and auditory phonetics: the adaptive design of speech sound systems. Philosophical transactions of the Royal Society of London. Series B, Biological Sciences.\n\nGawron, J. M. (accessed 2019). Chapter 1 Phonology. From a course compendium: https://gawron.sdsu.edu/intro/course_core/lectures/phonology.htm (Links to an external site.)\n\nMori, M., MacDorman, K. F., &amp; Kageki, N. (2012). The Uncanny Valley [From the Field]. IEEE Robot. Automat. Mag., 19, 98-100.\n\nSawyer, J. (accessed 2019). The Acoustic Property of Vowels and Consonants. CSD 349: Speech and Hearing Science. Compendium text, Illinois State University. http://my.ilstu.edu/~jsawyer/consonantsvowels/consonantsvowels_print.html (Links to an external site.)\n\nSmith, J. (2008). Speech Synthesizer. Documentation of university coursework. McGill University, Montreal, Quebec, Canada. http://www.music.mcgill.ca/~jordan/coursework/mumt307/speech_synth.html\n",
        "url": "/interactive-music/2019/10/18/Wizard_of_Vox.html"
      },
    
      {
        "title": "Microphone Testing Results",
        "author": "\n",
        "excerpt": "We’ve spent a few days (in addition to the many miscellaneous hours during class) reconfiguring the Portal and testing out new hardware and how it might improve the quality of our sound.\n",
        "content": "We’ve spent a few days (in addition to the many miscellaneous hours during class) reconfiguring the Portal and testing out new hardware and how it might improve the quality of our sound. Here are a list of microphones we were able to test over the SMC 4021 class. This list will likely be updated with any new mics we test and other miscellaneous notes we find about our experiences streaming with them.\n\nMicrophone criteria\n\nThese criteria were loosely based around these qualities of a microphone and their general usability during class and for performances we may have to setup. Maintaining this balance that allows us to keep quick access to the default classroom model.\n\n\n  sensitivity of mic\n  power needed to drive\n  dynamic range\n  quality\n  noise floor - high pitch whine? on Trondheim - size and usability/practicality - sound texture (perceptual)\n  pattern control\n  position, static\n  directionality\n\n\nMD421 (in Oslo)\n\n\n    \n\n\n• needs more gain\n• less sensitive\n• no room sound\n• dry\n• distance is the same\n• generally a little better\n\n\nSM7b (in Trondheim)\n\n\n    \n\n\n• dryer\n• more low end\n• fuller spectrum\n• less tinny\n• sounds like a vocal booth\n• you would need to be in close proximity\n\t○ not as practical space wise\n• better than the MD421\n\n\nSennheiser 835 (in Oslo)\n\n\n    \n\n\n• drier\n• direct, less room\n• not as distinguishable\n• weakest\n\n\nCeiling mic (in Trondheim)\n\n• too low\n• very poor sound quality\n• unusable\n\n\nBoundary mic (in Trondheim)\n\n\n    \n\n\n• space wise, very little clutter\n• but room sound is apparent\n• table noise\n• muddy\n\n\nShotgun mic (in Trondheim)\n\n\n    \n\n\n• clear and collected\n• directional\n• can be used at quite a distance and from overhead\n• can still hear the reverb from other side\n• may be able to cover a large space (would only need two?)\n\n\nAT AE5100 (X/Y Stereo) (in Oslo)\n\n\n    \n\n\n• clear and transparent\n• less muddy\n• less room sound\n• thinner?\n• more natural?\n• visually obstrusive\n\n\nOverhead mic\n\n\n    \n\n\n• good spatialization\n• no reverb\n• little whiney (high end)\n\n",
        "url": "/networked-music/2019/10/21/many-microphones.html"
      },
    
      {
        "title": "Physical Computing: Heckathon: Group C",
        "author": "\n",
        "excerpt": "Taking our cue from the main theme of the Day 4 Hackathon of “Recycling”, Team C chose the 2017 U.S. withdrawal from the Paris Agreement on climate change mitigation as a central theme in our work.\n",
        "content": "Paris (Paris.. Paris.. )\n\nIn a June 1, 2017 televised announcement, President Donald Trump announced the withdrawal, characterizing the agreement as a deal that “aimed to hobble, disadvantage and impoverish the US”. There are of course many angles one could exploit here! However, in consideration of the limited time we were allotted and the theme of “Recycling”, we decided to narrow Trump’s speech down to a single word, “Paris”, extracting that word from the audio and manipulating it in several ways. Thus, we hoped to recycle a small piece of what many would see as a very ugly and destructive speech and turn it into something beautiful.\n\nOr, if not beautiful, at least we could defend it as a personal expression of political art.\n\n\n\nHow we done it\n\nWe combined a couple of approaches here, both low- and high-tech:\n\nIggy\n\nOur man-on-the-ground in Trondheim, Tom Ignatius, came through with sound design, extracting the desired audio clip and twaddling it until it rendered all the elements of a lush electronica track. He used a series of effects such as pitch and time manipulation to extend the word 100X using a software called paulstretch. Further on, he manipulated the audio to create three different chords using a 100seconds reverb and a vocoder to extend out the chords.\n\nFor his side of things, He used the clips to make the percussions to the track using transient designer, multiband compressors, harmonic distortion and pitchshifters.\n\nOriginal sample:\n\n\n\nPercussion:\n\n\n\n\n\n\n\nPad:\n\n\n\nChordal stabs:\n\n\n\n\n\n\n\nMeanwhile in Oslo, our favorite Norwegians Aleks and Thomas set to work on the Pure Data architecture. Working diligently in convivial Nordic silence, they completed their tasks with nary a hitch, the perfect stillness of the room only occasionally broken by the triumphant bleating of electronics, indicating that, yes, everything was working perfectly.\n\nAleks\n\nAfter the topic was decided I quickly began to search my PD-pathces for inspiration. Shortly after I began to further developing a looping and sampling patch I made day two in the workshop.\n\nThe instrument design consisted of three buttons, one rotary knob and two audio samples. The first button enabled the recording and determined the length of the loop. The other two buttons triggered audio samples which fed into the sampler. The audio consisted of two separate segments of the word “Paris”, streched and slightly pitched by my man Iggy. \n\nThe rotary knob controlled both a low-pass filter and a delay. However, the low-pass filters sole function turned out to be volume control.\nRotary value 0 = 0 hz cutoff and no delay\nRotary value 1 =  3000 hz cutoff and full delay.\n\nAfter manically trying to solve Pauls copper drumstick and metal rod riddles (more on this later in the post), we stumbled upon an effect which actually produced results. When Paul replaced the rotary knob on my board with connections to the copper and metal rods (voltage and lead) we discovered that touching the rods together boosted the input values while removing them from one another deceased the values. This effect can be seen and heard in the video below.\n\nFurthermore, choosing a very short amplitude envelope resulted in some clipping when triggering the audio samples, an effect which was used musically in the performance and generated a new iterated dimension to an otherwise “flowey” sound texture.\n\n\n\nThomas\n\nMy PD-patch was quite minimalistic. With the help of the PD-guru, Aleksander, Iggy’s manipulated samples were triggered by the push of a button. To keep it simplistic and within the recycling-spirit I “chose” to only have one button trigger the four samples. The samples were assigned to four different digital outlets and I changed between them by plugging and unplugging the cord from my button.\n\nI also had a little delay in the patch, and manipulated the mix of the delay with a potentiometer knob.\n\nI spent most of my time being frustrated that I couldn’t get anything to work, and I ideally wanted to use more knobs and buttons, but my equipment was not cooperating. \n\nPaul\n\nBeing the only ‘merican in the group, I (Paul) decided to go low-tech cowboy and attempt to recycle some garbage into variable resistors that would interface with the Bela and replace some of the switches and knobby-things on the breadboard.\n\nI started the day walking around outside picking up trash. There wasn’t much! Kudos to Norwegians for being adults and placing their søppel in the appropriate søppelbøten. I ended up scouring the break room bins for usable items, coming up with three soda cans, some candy wrappers, and a bit of foil. One thing I did find outside, though, was a small roll of corroded wire. Yee-ha! I reckoned I could turn that into a dandy variable resistor.\n\nI was wrong, or at least, only partly right (being generous here).\n\nAfter some testing, I determined that all my trash was either non-conductive, or too-conductive to make a proper resistor. Even those candy wrappers that look semi-metallic . . . aren’t.\n\nTo make a homemade variable resistor one needs a conductor (a wire, rod, etc) that is only minimally conductive along its length. Connecting an ohm meter probe to one end and then touching along its length with the other probe, a feller can see that the resistance goes up the farther out from the first probe one goes. Used in a circuit, the second probe is the “wiper” and can be run along the wire to change the resistance in the same way a potentiometer does, controlling parameters such as volume, effect level, modulation, etc.\n\nNichrome wire of the sort used to make heater elements would have been ideal, but surprisingly, I didn’t find any of that lying about. So, I decided to try making a wire-wound resistor out of the bit of found wire, wrapped around a drum stick I “recycled” from the SMC portal.\n\n\n\nTurns out, the resistance in this wire was only minimal along its length. After the corrosion was partly rubbed off, it was revealed to be what looks like 14 gauge copper wire; an excellent conductor of the type used to wire houses. Basically, I would need a couple miles of this wire wrapped a very large insulator to get the results I wanted. But, it looked cool and we were able to make some unexpected noises with it (see performance).\n\nIn the end, the day was mostly about failure for me. I say this not as a negative thing, as I have several new ideas for instruments that could be made using what I’ve learned through miserable, abject humiliation and defeat. The technologies I’ve been introduced to through this course are very exciting, and the potential is great. I hope to learn more and return, triumphant. Stand by . . .\n\nThe Performance\n\n\n\nDespite the various hitches in our giddyup over the course of the day, the performance went well. We were able to trigger Iggy’s sound files and make music. The rhythmic loop that Iggy made gave us something to nod our heads to, and the triggered pads and stabs sounded great. The variable resistor plugged into the Bela made some racket but not the originally intended racket.\n\nConclusion\n\nThis was quite a week, with many challenges. Day 4 was probably the most stressful, and all of us were wishing for a little more time to work things out before the performance. Overall, though, we felt like our concept was very good. With a little more time to work out the kinks it could be turned into a cohesive artistic statement that we could all be proud of.\n\nViva Paris!\n",
        "url": "/interactive-music/2019/10/22/GroupC-Physical-Computing-Day4.html"
      },
    
      {
        "title": "The B Team: Mini-Hackathon",
        "author": "\n",
        "excerpt": "For the SMC 4000 mini-hackathon in the physical computing module we tried to send sound at the speed of light.\n",
        "content": "Sound at the Speed of Light\n\nFor the final workshop in the Physical Computing workshop of SMC 4000, we were tasked with developing a performance that utilized the Bela device for a brief “Hackathon”. The theme of the performance this year was “Recycling”, a broad and flexible term, our group interpreted it quite liberally as you will see.\n\nOur original idea\n\nWe discussed the idea of feedback as a signal over the net, since our group consists of two of us in Trondheim and two in Oslo. This way we would be able to have some direct contact with each other, regardless of physical location, and, in theory, be able to interact through our performance. Though this list was expanded a bit after our initial ~15 minute meeting, these were the core features:\n\n\n  We have an initial trigger to increase the volume of a synthesizer on the first group member’s Bela\n  The amplitude of the sound will be sent to an led, causing it to swell brighter according to the volume of the sound (no sound → LED is dark, peak amplitude → bright LED)\n  The light from the LED is then streamed through a smartphone camera over Zoom to the second team member\n  The second member of the group will have a light sensor aimed at the screen of their smartphone that is receiving this video feed of the glowing LED\n  The light sensor will then send a signal corresponding to the intensity of the brightness of the screen and trigger the activation of the synthesizer on that member’s Bela device\n  This would propagate forward to the devices of the remaining two members until it reaches the first team member’s device\n  A delay between the light sensor sending its signal to initiate the synth would allow for each of our devices to take turns in speaking\n  We would also have parameters that would be able to tune and shift the texture of the synth\nUnder the theme of “Recycling” we thought of this experiment as recycling sound through light (over IP)\n\n\nFor some reason we thought this would be a simple and possible to finish within 3 hours.\n\nFour major components\n\n\n  An overly complicated instrument built in PureData on hosted on a Bela device\n  A blue LED\n  A light sensor\n  A smartphone to send and receive video of the LED\n\n\nOur Process\n\nWe thought that since the additive synthesis was largely complete (we were wrong), we could manage the few additional pieces we would have to configure for the transmission and reception of light in response to the instruments’ swells of sound. By the end, I think we learned that in unfamiliar territories, it is quite difficult to consider the difficulty and timeframe of a project when unsure of the details of each component from the outset.\n\nThe synthesizer was built using an additive synthesis using the harmonic series. The partials can be spread or condensed using a knob as well as the fundamental for the series. Four of harmonics were considered partials that had unique factors that were able to be manipulated. Factors that shift the frequency of these partials can be randomized with a click as well as the overall “color” of the sound - the basic principle of additive synthesis being the potential to reproduce “any” sound by manipulating the amplitude of a fundamental frequency’s partials. In short, there were three knobs, adjusting, volume, fundamental frequency, and width of partials as well as three buttons that acted to initialize, as well as randomize some features.\n\n\n\nThe light sensor configuration was being worked on in Trondheim, which would be able to scale the light being received from the phone screen into values that would be able to increase and decrease the volume of the synth. This was being worked on by Simon and Magda to fit the scaling of the sensor to the max/min brightness of the LED. Trondheim was also mirroring the setup we had built in Oslo. Coordination was a huge obstacle when it came to setting up four devices in parallel over two campuses.\n\nHere is a picture of the light sensor clamped by two alligator cables.\n\n\n\nIn Oslo, we were working on mapping the knobs and buttons to work with the Pd patch as well as figuring out how to make an LED’s brightness reflect the amplitude of the sound that we were outputting from the bela through our synth. We discovered that since LED’s only have two states (on/off) we had to instead modify the frequency of the LED’s pulses so that it was perceived as being more or less bright (see pulse width modulation).\n\nHere’s a photo of the config (minus the LED and light sensor)\n\n\n\nRunning out of time :(\n\nWe really made an attempt, that much can be said. And, in fact, most of the components made their way into completion. The LED was responding to the audio signal in a dynamic way, the knobs and buttons were configured to interact with the patch (for a brief moment) and the light sensor was scaled to received light. All that was needed was to put it all together and throw in a phone. Yet, knowing how long this process itself took, one can be sure those final tasks alone would have taken a considerable amount of time. And we were out of time.\n\nSo, we resorted to playing with the synth through PureData from our laptops and sending the audio signal from our headphone jack to a speaker locally. It was quite a nice performance in itself and we were able to listen and respond to the subtle changes in texture and pitch that each of us were making through our patch. In reflection, there was a massive gap in our sense of how long this project would take and the computation details we hadn’t had experience with yet. I think we made a great attempt and learned quite a lot about working with the Bela devices, audio routing, electrical engineering, and coding in PureData. It was overwhelming and perhaps we hit it too head on, but oh well!\n\nIn Oslo\n\n\n\nIn Trondheim\n\n\n\nAnd here is a recording from our performance in Oslo.\n\n\n  \n\n\nUpdate: And yet!\n\nWe came back, to step up to bat with one final swing at the nefarious machine we had drawn blueprints for. And we did it - from one device to the other.\n\n\n\nWhat you’re seeing here is Jarle sending the sound of a synth (through a simple volume knob), that sound being converted to light based on amplitude and then the light being sent, via Zoom, through the camera from his phone and streamed to my phone screen. Trapped between the screen and a black box is a light sensor that captures the light from my screen that then modulates the volume control on my synth (with a 6s delay). You can see the Zoom session on our laptops as well.\n\nJarle is thereby controlling the activation of my synth through light. Afterwards, Jarle and I perform through tuning various parameters within the synth like the width of the partials, the fundamental frequency, and other spontaneous shifts achieved through randomization mapped to two of the buttons.\n",
        "url": "/interactive-music/2019/10/24/b-team-sound-light.html"
      },
    
      {
        "title": "Orchestrash hackathon performance",
        "author": "\n",
        "excerpt": "The title of our project is “Orchestrash” inspired by the theme of the competition and our approach to solving it, by making individual instruments controlled by recycled materials and “recycling” sound by sampling\n",
        "content": "Description\n\nThe title of our project is “Orchestrash” inspired by the theme of the competition and our approach to solving it, by making individual instruments controlled by recycled materials and “recycling” sound by sampling.\n\nBelow is a video from our group performance. The video is edited by Rayam, and was filmed by Robin Støckert and Stefano Fasciani in Trondheim and Oslo, respectively.\n\n\n\nTechnologies\n\nBela:\n\n\n  We all used Bela as our sound source and to receive signals form the various sensors.\n\n\nSensors:\n\n\n  Distance sensor to control the lead-voice.\n  Pressure sensor attached to a plastic bottle to control the notes of the bass-synth.\n  Rotary knobs to control the LFO and lo-pass on the bass-synth.\n  Microphone placed in a glass bottle.\n  Buttons to trigger samples.\n\n\nHackathon Performance\n\n\n\n\n\n\nInstruments\n\nBelow, each of us will present their instrument.\n\nUltrasonic ‘Trasheremin’ (Rayam)\n\nThe Trasheremin is an instrument based on the touchless gesture control of the classic Theremin, but differing from it in all of the other aspects of this instrument. In this case, Rayam designed the instrument to provide also a tactile interaction to one hand, while having the non-physical control in the other hand. For that, he built two independent inputs to control unusual parameters on the sound produced. On one hand, the performer got the possibility to modulate the sound character with touch-free interaction. It was possible by using an Ultrasonic Distance Sensor, which was assigned to control the rate of one amplitude modulation applied to distinct frequencies generated by two oscillators. One oscillator was sending a pure C note and the other sending a pure E note, which last one was the affected note by the ultrasonic sensor input. Also, to control the pitch of one of the oscillators, a ball of trash paper was specially designed to give to the performer an immersive experience in the recycling meaning. To do that, a pressure sensor was inserted inside the paper ball, providing an eccentric tactile interaction. That way, the Trasheremin contributes reinforcing the concept of recycling, as it generate uncomfortable sensations when the performer is dealing with the trash. When the performer have to press the paper ball to play the instrument, it stimulates both audience and performer to think about the inconvenient volume of the trash generated in the society’s trivial daily consumption. By the other hand, literally, the performer have a contrast of a hand free of trash.\n\nSampler/looper (Ulrik)\n\n\n\nThe sampler was the main rhytmical instrument in the performance. It was a very rudimentary four voice sampler, with four corresponding recording buttons, some of which can be seen in the photo above. One of the sample slots was mapped to a pitch shifter, that I received as a sub module by Thibault, which in turn was controllable by a rotary potentiometer. The samples were recorded live through the analog input on the Bela. During the development and the performance, a contact microphone was used on different sound sources to make the recording. However, any analog sound source could have been used, as the analog input on the Bela supported any 3.5 mm jack connection.\n\nThe recording of a sample started when the corresponding button was pressed, and ended when the button was released. After a sample had been recorded, the sample automatically got looped. No time quantization was done due to the limited time frame, so it was quite difficult to get the timing right on the recordings. At the same time, this led to some interesting evolving rhythm patterns, as it was practically impossible to record the samples in perfect synchronization.\n\nResonator (Thibault)\n\n\n\nThis instrument aims to make music out of acoustic feedback. To do so, a glass bottle is used as a resonator. A microphone is placed inside it, and a speaker lies under it. Routing the microphone to the speaker in Pure Data immediately creates an annoying high pitched feedback. In order to control the pitch, I downloaded a pitch shifter patch. By mapping a rotary potentiometer to the transposition amount, I managed to find some frequencies producing an interesting sound. But the instrument was just producing a steady pitch, with very few controlling possibilities, as most frequencies would not let feedback grow, and the resulting sound was very week. The best sounding frequency was correspond to a B note, but the rest of the groupe was working in a C minor scale. By adding water into the bottle, I managed to pitch the behaviour of the bottle up to a steady C note.\n\nWhen Ulrik started to play with his looper in the same room as my resonating bottle, a very interesting artifact was revealed. As my instrument produced sound using an acoustic source, it was in fact reacting to percussive sound. This can be heard at the beginning of our performance video, when the beat starts (0:36). Moreover, when the bass started to play, the bottle was acting differently, with more resonating frequencies. This allowed me to improvise with the others, being able to generate a B sound, and even a higher weird sounding Bb pitch.\n\nBass-genie in a plastic bottle (Gaute)\n\nThe bass instrument consists of a fairly basic sawtooth wave being fed into a high-cut filter with a variable LFO. The interesting part is the input, and how this was mapped:\n\nThe input to Bela was a pressure sensor attached to a plastic bottle, underneath a tight strap. When the bottle was squeezed, the walls of the bottle would expand and push more against the strap, giving different levels to the pressure sensor. The pressure sensor sent voltage variations to Bela where the PD patch converted the voltage variations into integral numbers. The integral numbers were then fed into a “select” object, which sends out a message to the different outputs, depending on the input. The different outputs were then patched to six different sawtooth oscillators, each corresponding to a note in the chosen scale.\n\n\n\nThe design of the patch could be improved in the future by mapping buttons to transpose it to different scales, more inputs for controlling effects, more advanced oscillators etc. But all in all, I am happy with the result, it did the job and sounded pretty bad-ass.\n\nTimeline\n\nIn the start, we had a vision of how we wanted the instrumentation of our “band” to look like, but we had no idea of how we were supposed to get there. It was through a creative process of exploring the possibilities of the software, within the limitations of the given theme that we ended up with our final concept.\n\nDivision of labour\n\nAs the level of knowledge about Pure Data and Bela greatly varies within the group, we found a natural distribution of the labour as we worked on the project. Being able to use the screen sharing tool in Zoom was priceless in order to build on each others strengths and collaborate on the software.\n\nAchievements\n\nNone of us had been using Pd or Bela before, so we all felt like we had accomplished quite a lot at the end of the week, when we were able to construct four interplaying instrument prototypes in just a few hours. During the week, we experimented with different sensors. The hardest one to make use of in Pd was the ultrasonic HC-SR04 sensor, because we had to run an additional C++ program on the Bela to make it work (thanks to Stefano for giving us help to do that). Rayam ended up using this sensor in the performance, which also felt like an accomplishment in itself.\n\nChallenges\n\nOur challenges mostly consisted of getting the right inputs in Bela to map to the correct parameters in the Pd-patch, within the right values. For instance; getting the pressure sensor to accurately hit the various notes in the bass synth was a challenge due to its very untraditional input method and that it has to start and stop the different oscillators in relation to the pressure applied.\n\nWe had a major challenge with getting one of the Pd-patches to work as it should in Bela. Fore some reason the lead synth would not play the same way it did in Pd when loading it to Bela, so we were left with a somewhat sub-optimal solution on that part.\n\nPerformance/reflection\n\nThe performance went surprisingly well. The interaction between our instruments ended up better than we hoped. The resonator was reacting to the sampler, as well as the bass synth. We had both a percussive, melodic and a bass instrument, as well as some ambient drone on top.\n",
        "url": "/interactive-music/2019/10/24/Orchestrash.html"
      },
    
      {
        "title": "Spatial Trip - a Spatial Audio Composition",
        "author": "\n",
        "excerpt": "In this project, we recorded and synthesized sounds with spatial aspects which is meant to be heard over headphones or loudspeakers (8-channel setup in Oslo/Trondheim). Coming to simulate both the indoor and outdoor real and or fictional scenarios.\n",
        "content": "Introduction\n\nIn this project, we recorded and synthesized sounds with spatial aspects which is meant to be heard over headphones or loudspeakers (8-channel setup in Oslo/Trondheim). Coming to simulate both the indoor and outdoor real and or fictional scenarios.\nOur composition is made out of four scenes lasting 1 minute each.\nScene I - At home - The whole composition based on a person who suffers with schizophrenia that he hears voices inside his head sometimes. The starting scene is based on day to day activities happens in a busy morning in the kitchen.\nScene II - After the accident - Connecting between the morning scene and the accident scene is the TV rapport about an accident. This scene can be an unforgotten experience or maybe memory. We are listening from a point of view of a person that is laying on the ground, facing upwards by a highway after an accident. As the scene develops, more help is arriving connecting us to the next scene at the hospital.\nScene III - Connecting the accident scene and the space scene. This seen follows the person lying on the ground from previous scene. The person is badly injured and needs medical help. The person finds himself lying on an operating table at the hospital.\nScene IV - As a ambisonic composition, it is quite ironic to do a scene in the absolute silence of space!\nAs the subject of our piece has elements of abstract mental struggle, and a clear death scene - we felt it fitting that the last scene would be ambiguous, or other wordly - there is no clear understanding of what happens beyond death…..\n\nHave a listen to our result (Headphones required):\n\n\n   \n     \n     Your browser does not support audio tag.\n   \n\n\nComplete scene and timeline description\n\nScene 1 - At Home - Ashane Silva\n\nConcept:\nThe whole composition based on a person who suffers with schizophrenia that he hears voices inside his head sometimes. The starting scene is based on day to day activities happens in a busy morning in the kitchen.\n\nSounds:\nA basic scenario in a kitchen was recorded using a soundfield recording in a first person’s perspective. The recording includes, sounds of water pouring into a glass, opening a door, opening and closing cupboards, stirring in a cup. The recording was done in multiple takes and combined together.\n\nAdditionally, Some voice recordings were done to create the voices that the main character hears in his head. The phrases are referring to the accident scene which he starts to regret. Mainly multiple samples of normal voice and whispering voices were recorded.\n\nSoundfield mic and converting to B-Format\n\nThe raw state of the audio consists of LF, RF, LB, RB recordings and converted to B-Format(Ambix) which has W, X, Y, Z axis for rotation. The B-format source was directly routed to the decoder.\nW – a pressure signal corresponding to the output from an omnidirectional microphone\nX – the front-to-back directional information\nY – the left-to-right directional information\nZ – the up-to-down directional information\n\nExtra sounds:\nMultiple samples has been used to emphasize a busy environment.\n\nTimeline:\nDoor opens, kitchen work, people chatting\nCat sound, phone rings, crying baby.\nTurning on the radio, News,\nThe character turns towards the radio and turns back to front position.\nVoices starts to appear in his head.\n\nScene:\nThe scene starts with the person entering the kitchen and doing some basic work. He washed a cup andt trying to make some tea. And opens one of the cupboards and take some food packets. Once he entered the room the cat starts to make noise which can be heard far from behind and also moves around back left and front-right later.Then a phone starts to ring and a baby wakes up to that sound and starts crying..And also there are some  people talking in another room and also a person goes down the stairs of the other side of the wall.\n\nThe was used with automating the source from right back to left back. And a low pass filtering has been done cut recreate the perception of the object moving away.\n\nAfter the radio turned on, the scene rotates as the character face towards the radio and in few seconds the scene rotates back to previous state.\n\nCreating voices\nThis section is not meant to sound realistic but more dramatic and create more tension. After news stating about an accident the character starts to remind of a past situation. And he speaks “ Oh not again” , “ another accident” and suddenly other voices starts to appear in his head. The main voice is placed in front while other voices moving around the 360 soundfield.\n\nEffects and plugins.\n\nPlugins from the IEM plugin suite\n\nMulti-Encoder - Once instance of the multi encoder was used to place the sound samples of cat, baby crying,Phone ringing and people talking. Each sample is placed differently  in the 360 degrees soundfield by controlling the azimuth angle and automating to add some movement.\n\n\n\nLowpass Filtering - adding Low pass filtering for the samples that has to be placed a bit far in the soundfield. This more like additional reinforcement with the gain adjustment to move objects further away from the listener.\n\nRoom Encoder -  Room encoder was used to create a large space that include stairs.  The position of the person was automated from right back to left back. And the cutoff frequency of the  low pass filtering has been automated to add a better perception of distance and the size of the environment.\n\n\n\nLFO shapes - LFO shapes of Saw,Triangular, Square and Parametric were used as automation patterns of the Azimuth angle in Multi_Encoder  to create fast movements for the voices. Using LFO shapes seems to be an efficient way of spreading sound in the field evenly and changing the number of cycles with time.\n\n\n\nDual Delay -  This was used to add repetitions for voices and automation of the delay time to add a pitch warping effect to voices add the very end. The delay added more glue to the section with voices and also gave a better transition to the next scene.\n\n\n\nScene 2 - After the accident - Guy Sion\n\nConnecting between the morning scene and the accident scene is the TV rapport about an accident. This scene can be an unforgotten experience or maybe memory. We are listening from a point of view of a person that is laying on the ground, facing upwards by a highway after an accident. As the scene develops, more help is arriving connecting us to the next scene at the hospital.\n\nThe scene starts with a loud Thunder composed of two layers: mid-high and a deep dark/far thunders, together with rain sample. The dark/far thunder continues playing through the scene, setting up the scene’s spatial depth - it is raining, there is a storm in the distance.\n\nAdditional layers that start playing from the beginning of the scenes are an ambisonic recording made on the side of a highway (by ullevål stadion in Oslo).\n\n\n\nWe can hear the heavy breathing of the person, breathings that will slow down as the scene continues and more help arrives.\nA car is stopping by to assist. A door is open and we can hear a baby crying from within the car. Someone is running out of the car to approach the person on the ground, calling for his attention as he gets closer. Reverb is applied over the voice offering help to give the feeling that those words have been registered by the person on the ground who might be losing conscious.\n\nTimeline:\nThunders - 2 layers, Rain, Ambiance/Traffic - Ambisonic (original recording), Road.\nA car breaks - front right to back right\nA car door opens and closes - a baby crying being (EQ change)\nSteps (wet ground) and Voice (Ashane) approaching\nMore Steps\nAnother car door opens and closes\nPolice radio is being heard\nViolins with a long tail reverb. Intervals: 5th up (uplifting), 5th up down to 4th (suspension), 5th up and up to the major 7th (hopeful).\nAn ambulance is being heard in the background, passing by buildings.\n\nScene 3 - The Hospital - Elias Andersen\n\nConcept:\nConnecting the accident scene and the space scene. This seen follows the person lying on the ground from previous scene. The person is badly injured and needs medical help. The person finds himself lying on an operating table at the hospital.\n\nTimeline (Person lying on operating table):\nBreathing, heartbeat, heart monitoring beeping and doctor speaking while taking on rubber gloves.\nOperating (bones cracking), heart monitor beeping, heartbeat and breath starts to fade until they disappear completely.\nHeart monitor beeping stops, heart monitor alarm goes off, doctor speaking, defibrillator and heavenly sound.\nHeart monitor alarm stops (person is dead) and heavenly sound continues.\n\nScene:\nThe scene starts with a person lying on an operating table. The idea is that he wakes up/get conscious. The person is breathing heavy, hearing his own heartbeat and the heart monitor beeping. Beside him there is a doctor taking on rubber gloves and talking about his condition and that they have to operate. The doctor then starts to operate and you can hear the sound of bones cracking (his rib cage is crushed). The heartbeat and breathing fades away. The heart monitor beeping then stops, obviously. Then the heart monitor alarm goes off and the doctor acknowledge that they are losing him and then the doctor use a defibrillator. It does not work. A melancholic sound start to play, to lead the listener to perceive that the person is dying. The defibrillator is used again, but does not save him. All sounds then fades away and the only sound left is the melancholic sound.\n\nSounds:\nAll the sound samples used are taken from freesound.org, except from the doctor speaking. Most of the sounds I use are meant to sound like the sound object that I want to recreate. What I mean by that is for example the heart beat and the heart beat monitor was put up on freesound.org as heartbeat and heart beat monitor. For the defibrillator on the other hand, I use a kick drum sound as the sound of the kick the defibrillator gives to the body.\n\nThe sound of the doctor speaking is a recording of Sam talking. This recording was done in the studio in Oslo with the AKG C414 XLS microphone.\n\nEffects used:\nMulti encoder: The melancholic sound are placed in the space using the multi encoder. They are played in a stereo perspective, moving 360 degrees from 180 degrees to -180 degrees.\n\nRoom encoder:\nAll the other sounds are placed in the space with a room encoder. The reason is that the scene takes place in a fixed sized room and is there for easy to place in the room to make it sound realistic; like you are in that room. I also used the room synchronization to get the perspective that the sounds are all in the same space/room.\n\nVolume:\nVolume are used to make some of the sounds to fade out (away). At first I tried to make the same effect with placing the sounds further away from the listener’s perspective in a linear manner, but got a more realistic result with using the volume for this. Examples where the sounds are fading are the heartbeat, breathing, the heart monitor alarm and the doctor speaking. In the beginning the fade in effect are used to make it sound like you are waking up; being conscious.\n\nReverb:\nReverb are used for the same effect as volume, to make the object sound like they are fading away; making the room sound bigger. This effect are used on the doctors voice.\n\nScene 4 - Space - Sam Roman\n\nConcept\nAs a ambisonic composition, it is quite ironic to do a scene in the absolute silence of space!\nAs the subject of our piece has elements of abstract mental struggle, and a clear death scene - we felt it fitting that the last scene would be ambiguous, or other wordly - there is no clear understanding of what happens beyond death…..\n\nThis ambiguity allowed for an opportunity to be more abstract. I find it very interesting the idea of augmenting reality with 3D sound scenes - creating a “ultra- reality”. Sound that we hear is based on real world physics - we are used to the way spatial audio works in real life. So by playing on these laws, and subverting them one can create a scene that is beyond reality. It should be noted that there is a balance between realism and extrinsic sounds, and manipulating these sounds to be unreal. The audience needs have some footing in reality to appreciate and understand when it differs from it, thus understanding the scene is producing sounds in unrealistic and improbable ways.\n\nThis was inspired by the recent WoNoMute held in the portal with Nattasha Barret. In her piece it starts with a normal outdoor scene, that over time gets subverted and twisted as it progressed.\n\nI also wanted to subvert expectations - Firstly introduce the scene, highlighting sounds that suggest space scene stereotypes. However, I then wanted the scene gradually break down - physics would start to go awry with glitchy processed sounds, and the spatial information would turn extreme, with sounds that are stretched and unreal. This will ramp up to the end with a crescendo - finishing the piece. In my opinion, for this to be successful, the audience needs to be invested in the reality of the scene from before - to have a reference to how abstract it will become. In other words, if the scene is abstract all the time, there would be no contrast to make sense of being in a particular space and time.\n\nTimeline:\nScene is split into three components-\nFloating in space - reaching a ship or space station\nOpening and entering airlock and walking inside the vessel\nAugmentation of the scene - breaking down of reality\n\nSound sources\nRecording - Stairs in ZEB building.\n\nThe stairs in the ZEB building are made of stone and are four floors high, with good isolation of sounds from outside the stairwell. This space had a long reverb time, and I used as representations of a tunnel in a ship. This sample i used for a quiet ambience bed inside the ship, designed to be a similar acoustic space. The whistle I did for testing - however i liked the mood of the sound and decided to keep it in.\n\nSampled sources\nAction Sounds\n\nFootsteps - The sample represents walking down the tunnel and is in constant movement,  This is in stereo, and put up in elevation, and panned slightly to the front. I found this gave the most realistic spatial position to imitate hearing your own footsteps within a long, metal tunnel like space with large reverberation. Having the sample in stereo spreads the sound over a 90 degree angle. The sample came from hitting a shoe on a radiator!\n\nMetal - To compliment this, there are metal clunk sounds, that fill out the footsteps and other actions. This are panned differently, and give the imagery of a clunky, metallic structure that creaks as it is walked down.\n\nAirlock Open &amp; Close - There are three different samples that make up the opening and closing of the ship. These are positioned in the centre on the first sample, then pans to the left as the swoosh goes - similar to a sliding door. The next sample is the loudest - and signifies the full opening. This is positioned the opposite, starts on the left of the listener then swipes to the middle. This was to show the airlock closed behind the listener. This is where there is silence for a second - this is to signify the change of space for the character. The is a last swoosh for the opening of the main station. An impulse response of a car garage ha was used on the door - modulated quite extremely to enhance the power. It had the right “metallic” quality i was looking for.\n\nBleepy - There is also a bleepy sound placed on the right of the space, to show the input or recognition of an action that will open the door. It was placed here to counter the door action working on the left, as if the dial is on the right of the door.\n\nVoice - acknowledged. This confirms that there is a system for the door the character is accessing. In the space the speaker it is located on the right side of the hatch.\n\nBleep - In the ship as the character is walking down a tunnel. There is a bleep coming from in the tunnel, that the character walks past on the left. As he walks past, the spatial position comes from front left, left then to left back. Volume automation is added to for a mild doppler effect.\n\nPercussive glitch - This element was made through experimentation. I used E4L spatial slicer on an ambient sound source, and it produced an almost “stick brushed over radiator” sound. This sounded glitchy to me, and it worked well indicating the breaking down of reality. The timbre fits with the scene, sounding like a computer or electronics on the ship. The placement gets more and more erratic as the piece comes to a close, spinning around the listener twice at the very end.\n\nHeavy Human Breathing - This is the same sample used in the roadside scene, which adds a nice touch of continuity. This is used later to show the human reaction to such a scene, and for us as the viewer to understand his condition. It is also twisted and stretched later on, panning increases in intensity. As breathing is one of the most human sounds in the world, it was intentionally used to break convention, and let the listener know that reality is getting distorted.\n\nAmbient Sounds:\nSpace drones - 3 layered sounds at the start to signify space. 2 starts out stereo image out of the front sides, one on each side then collapses into the very centre before the door.\n\nPad - There is a tonal pad that sets the tone of the scene in the ship. The pitch is raised at the end to signify the crescendo finale.\n",
        "url": "/spatial-audio/2019/11/03/Spatial-Trip.html"
      },
    
      {
        "title": "3D Radio Theater - Lilleby",
        "author": "\n",
        "excerpt": "A 3D radio theater, produced by SMC students, Trondheim\n",
        "content": "Introduction\n\nOur project is a radio theatre that explores space through different places, intended to sound realistic and evoke spatial illusions. However, there are mystic/fantasy elements that are placed in real-world settings.\nWe mixed ‘pure’ nature sound without unwanted noise together with real-world sound environments, containing traffic sound and more.\nThe main figure in the plot has the first person perspective and is wandering from one place to another without a clear motivation rather forced through exterior circumstances.\nThe plot for the scenes was developed collaboratively, we decided beforehand what is happening. The realization of the scenes was processed individually.\n\n\n\n\n\n\nScenes\n\nTrain station\n\nThe play starts when the person is on the way to the train station and realizes that it is about to miss its connection. The ascending train is audible through the Horn and the train breaks. The person runs and stumbles on the floor, continues running while the train is about to leave again. The movements are accompanied by footsteps and breathings, but also short elements of speech, from the first person.\nFor this scene an ambisonic recording from the train station Lilleby was used and stereo/mono recordings placed around it. The footsteps for example or the breathing (dog breathing turned out to be the most realistic for the running sequence).\n\nForest\n\nThe person runs into the forest and goes for a walk. The sound of the city is replaced by a tranquil forest with birds and trees. Some of the birds fly close and around the person, and the soundscape follows and rotates to follow the birds. Suddenly, the weather changes, and thunder can be heard in the distance. The person escapes from the storm into a Cave.\n\nThe start of this scene uses field recordings done at NTNU botanical garden at Ringve, which being close to the city also had a lot of urban sounds of traffic and construction. A dog park several hundred meters away could also clearly be heard from where we recorded. The rest of the scene is made with samples from Freesound.\n\nCC attribution for sounds from FreeSound:\nAmbience_forest_austria_w4tel_thunder_light_rain.wav by magedu\nLong Birds Forest Ambience.wav by Motion_S\nBirds Chirping.wav by Motion_S\n\nCave\n\nYou hear your footsteps, water dripping around you, and a low sounding stream of water in front. The stream of water moves closer and to the left, where you should perceive that you are walking closer to the source. Wing flaps (maybe bats) are moving past your head, rocks are falling behind you, a drone bass sound gets louder accompanied by a ticking clock. The drone and clock sound spins around your head, and you are portalled to the shore.\nThe sounds are made up of layers, and sent through a convolution reverb to create the sense of a bigger cave space. The impulse response used was from a tunnel, which I thought sounded more like a cave than the cave IR’s I had available. Two people in the audience still experienced it as more of a tunnel than a cave though. Maybe I have never been in a cave.\n\nShore Scene\n\nBeing transported from the cave through the portal, the spinning, filtered sounds imply the dizziness and disorientation. Then the full range hearing comes back and the ambient sounds can be heard (Sea and Sea Guls). The intention was to use a convolution reverb, but regarding the unpleasant results from the beach impulse response, and the sounds being reverberant enough, it was not used.\nSubject’s breathing and walking are the constant sounds available. Then she walks towards the sea and suffers from a bird attack. For making the scene the sound’s gain and pannings has been automated. Subject runs into the sea and goes underwater and hears the whales. In the end, an unrealistic digital reverb is being used to imply the sense of space. The mix of reverb has been automated to imply the distance. The scene finishes with the song: Praise Blindness Eye.\n\nTake home message and Challenges\n\nWe realized that it is less time consuming to work with samples instead of capturing sound for every scene with ambisonic recordings.\nAn important feedback we got from the audience was to explore the possibilities of synthesizing early reflections.\nIn making the composition we have used various spatial audio tools with Reaper. For encoding mono sources to Ambisonics we used several IEM plugins: MultiEncoder, RoomEncoder, DirectivityShaper, AIIRADecoder and BinauralDecoder.\n\nHear the radio theatre here! (Binaural audio, headphones required)\n\n\n\n  \nYour browser does not support the audio element.\n\n\n",
        "url": "/spatial-audio/2019/11/05/Lilleby.html"
      },
    
      {
        "title": "Music and machine learning - Group A",
        "author": "\n",
        "excerpt": "Using machine learning for performance and for classifying sounds\n",
        "content": "Performing with the Wekinator\n\nWe started out with choosing an input method, and later discussed how to make the algorithm learn behaviour from the input and transform it into meaningful audio. The final system consisted of a web camera feed as its input (100 averaged pixel values) and drum machine patterns as targets. The Wekinator was used to classify colors from the web camera feed, and map those color classes to trigger pre-determined patterns in the drum machine, i.e. “blue” corresponds to pattern A and “red” corresponds to pattern B. In total there were four patterns to trigger in the drum machine, which suggested choosing four as the number of classes for the classification algorithm.\n\nPart of our plan was to send the web camera from Oslo to Trondheim, and then to train a model in Trondheim. Although we initially managed to set up an OSC stream over UiO’s VPN over the UDP protocol, something with the routing failed on the presentation day. The presentation of the model was therefore done locally, but with the same ML principles.\n\n\n \t\n\t Wekinator during training \n\n\nDue to the development context of the system, there were certain aspects of the system that were easier to deal with than it would be in a real world application. The problem of overfitting did not apply in this context, because the model would only be used for a one-time scenario. Another implication of the context was that the amount of training data needed was fairly small. After several rounds of testing, the conclusion was that only around half a minute of web camera recording (800 frames of 30 FPS) in total were needed to train a rough classifier on four different color states. Most of these seconds were used to record the background, to establish a stable base class for the classifier. If the system were to be regularized and made production ready, e.g. as a DAW plugin, a lot more effort would have had to be put into training the model with different kinds of data. Inferring the model with new data from a separate testing and validating set would be a high priority before deploying such a system in production.\n\nClassifying sounds with scikit-learn\n\nWe explored the Python library scikit-learn to learn more about how machine learning algorithms can be used for classification of different sounds.\n\nAudio Data Classifier\n\nDivided into three groups, each team got the task to classify a set of 200 audio files in four distinct classes. To implement the data classification, we used scikit-learn library. Each of the four classes was equally divided into 50 files, previously defined by the team members based on audio features that could be distinguished by humans, not requiring special expertise in audio analysis.\n\nOur group selected the following classes: modular synths, orchestral, nature environment, and birds. We had to classify the dataset that we created, and also, the dataset provided by the group B, with the classes glass, stone, wood, metal.\n\nMethods\n\nIn the beginning of this process, we had a shallow acknowledgment on how the parameters would affect the machine learning performance, so the first step was to make individual changes for each parameter that we had a previous understanding on the correlation with relevant audio features for the task.\n\nStarting the tests with the Group A dataset, we chose key parameters such as sample rate, and the number of max iterations, making notes on how each setting affected the performance of the classifier. After that, we gradually increased the complexity of the parameters changed, diving into a deeper interaction with the machine learning process. Thus, we started to adjust parameters such as activation, hidden_layer_sizes, and extract feature’s.\n\nOne important decision was to always try to use the minimum computing process power reasonable for the maximum accuracy goal, meaning that for each parameter tweaked, we considered only meaningful changes, avoiding overfeeding the model with irrelevant audio information, and consequently, confusions in the learning process.\n\nIn this way, it was noticeable that for the dataset A, increasing the sample rate to 24kHz, improved the accuracy of the results, since some audio sets, such as birds, for example, have relevant frequency information relying above 10kHz. Next, we started to interact with the machine demands. From changing the activation parameter, we could observe how effective was to increase or decrease the max_iter values. The tanh activation combined with max_iter=3300, worked well to the demand, and when increased the iterations to values above 3500, the results got less precise.\n\nArranging the neural network, we observed that the results improved when increased the number of layers and neurons from hidden_layer_sizes=(2,2) to (6,8,4), but more than that did not give me better results. After enlarging the size of the network, consequently, the machine required more iterations, becoming max_iter=8800 an optimal value.\n\nAt this stage of the process, after testing different features in parallel with the previous settings, we realized the best combination of features that fit better for each dataset.\n\nWe used the same method to achieve satisfactory results with the dataset B, but with a bit more experience with the parameters, we could use a more objective approach.\n\nResults / Discussion\n\nIn the case of the dataset A, the best feature arrangement was spectral_centroid and spectral_bandwidth, delivering better results than the default setting (zero_crossing_rate). It is probably due to the distinct spectral characteristics of the classes, having different located mass of energy in the spectrum.\n\nAs you can see in the graph below, the worst precision result came from class 4 (Birds). Probably due to the inconsistency among the duration of the audio files of this set, in addition to the background noise captured in the recordings.\n\n\n \t\n\t Dataset - group A \n\n\nFor the dataset B, the sample rate optimal value had to be much higher to deliver better results, ending up with 44.1kHz. The activation and the neural network were very similar to the ones selected to the dataset A, but the max_iter of 5500 fit better, delivering worse results if increased.\n\nHowever, the feature extraction required to dataset B was way different. The best combination of features was spectral_ bandwidth with spectral_ flatness. It makes sense, considering that spectral flatness is a tonality coefficient and combined with the spectral bandwidth, they can be very effective to distinguish sounds that have peculiar and clearly distinct tonal characteristics such as glass, stone, wood, and metal.\n\nWith dataset B, we got the best accuracy results, relying around 92% as you can see in the graph below. Also, the worst precision came from class 04 (Metal). Probably because the spectral bandwidth variety among the audio samples was large, being hard to identify a standard pattern regarding the wavelength.\n\n\n \t\n\t Dataset - group B \n\n",
        "url": "/machine-learning/2019/11/25/ML-GroupA.html"
      },
    
      {
        "title": "Group C Learns to Think about how Machines Learn to Think",
        "author": "\n",
        "excerpt": "Wherein we describe the denouement of SMC4000, Module #9: Machine Learning.\n",
        "content": "Machine Learning, a (very) brief primer\n\nOur final module in SMC4000 was in Machine Learning. ML is a sub-discipline of Artificial Intelligence.\n\nML can be described this way:\n\n“A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.”  - Tom Mitchell\n\nThere were several ways we explored this formula. The first was via a program designed by Rebecca Fiebrink called the Wekinator. More on this later, but basically, the Wek is a supervised machine learning program. It uses a polynomial regression algorithm (among others) to train itself to infer outputs from unknown data inputs, by using explicitly-mapped examples as training data.\n\nNext up in the toolkit is Scikit Learn. Scikit is a free Python machine-learning library that contains numerous classification, regression, and clustering algorithms for testing data.\n\nWith this program, we were able to implement a neural network to train a ML algorithm to classify data into four sets, or classes. Data was compiled by the group and consisted of sound files in four classes: Cats, birds, handclaps, and guns(!). Would the algorithm be able to distinguish between these sounds? As we discovered, sometimes it’s not that easy…\n\nMachine Learning with Wekinator\n\nOn the last day of the ML workshop, we had to perform with our Wekinator programs. We’ve all decided to use the same input and output to compare how different each of our programs work after we did some recordings with Wekinator. We decided as a group that all of us will use the XY cursor input into an FM synthesizer through Wekinator.\n\nOne of the observations we noticed is that despite the XY value being linear, the machine learning done would be far from it. As we were using basically a two dimensional, two axis graph, we were using the regression method. In the picture below, we see that after plotting just three initial points that the graph starts to non-linearly flatten out at the end without any recorded points.\n\n\n\nWhich leads on to our next observation of the training of the program. Our team member, Tom (Iggy), tried plotting three points with the left and right of the bottom of the Y axis changing pitch and the top right changing the filter. However the machine couldn’t logically presume that he wanted the top left hand corner of the screen to filter the lowest pitch from the initial plots.\n\nAfter plotting more points, we could use the XY plot to have linear and nonlinear curves to adjust our different parameters to allow for more expression.\n\nOverall its been a great learning experience understand how regression works with Wekinator\n\nTraining a classifier with Sci-kit learn\n\nOn day 3 of our ML workshop, we got introduced to a python library called Sci-kit learn. Our task was to collect and use a set of labeled data with a specific program in Sci-kit learn to train two classifiers with supervised techniques. The goal was to build highly accurate classifiers and reflect on why certain configurations generated these levels of accuracy.\n\nThe data sets consisted of 4 classes with a total of 200 audio files. Our first training set had examples of cats, guns, birds, and handclaps, while the other featured modular synth-shots, orchestral, nature, and bird sounds. As shown in images below, the complete training procedure resulted in two models with an average classifier accuracy of eighty percent.\n\n \n\nMuch to our surprise, it was much harder to achieve higher accuracy with the first training set then with the second. It seems that audio with similar frequency density and sonic characteristics are more demanding to classify. The findings, therefore, indicate that a diversity of labeled data can be beneficial when using these techniques and tools. This provided insight into the possible structural limitations of our program as well as knowledge about the connection between accurate machine learning classifiers and training procedures.\n",
        "url": "/machine-learning/2019/11/25/Machine-Learning-Group-C.html"
      },
    
      {
        "title": "The B Team Dives in Deep (Learning!)",
        "author": "\n",
        "excerpt": "Well, here we are, at the end of a semester where one of the most challenging courses remain - only Rebecca Fiebrink can save us now.\n",
        "content": "Gyro-synth: A Musical Instrument Built with the Wekinator\n\nFor the November 21st performance, our group trained a Machine Learning (ML) model using the Wekinator application to easily map a synthesizer with a mobile OSC controller. We used the gyroscope sensor in our phones and an FM synthesizer application that was included in the example applications provided by the Wekinator author. The gyroscope sensory sends three continuous data streams for the x, y, and z axes, and the FM synth had three input parameters for modulation, frequency, offset. The gyroscope data streams were sent directly to the IP of our PCs with the port and OSC message specified within the Wekinator program via two different OSC apps: Sensors2OSC for two of the members’ Android smartphones, and GyrOSC for the third member’s iPhone. The Wekinator, simply, is a software that makes the process of building a custom musical controller using ML easy and approachable (Fiebrink, 2010).\n\nHere’s a brief intro from it’s developer, Rebecca Fiebrink\n\n\nSetup\n\nOnce these apps were configured with each of our PC’s Wekinator applications, we specified the project parameters needed for our performance: port number, OSC message, 3 inputs, 3 outputs, output type as continuous, and model type as neural network. Afterwards, we chose unique parameters from the 3 outputs available within the Wekinator that would be mapped to the synth and would serve as our first training state. As a group, we first configured silence to the state of the phone held with the screen facing the user. The direct forward tilt was also roughly mapped to a different note on each device that would form a major chord when played together by all three devices. We then took these 5 recordings of 3-dimensional gyroscope data and trained (that’s “ML-talk”) the model using the Wekinator. Once complete, we were able to click “Run” and use our phone’s gyroscope as a controller to send both novel inputs to the Wekinator and produce novel outputs via the trained model’s interpolations.\n\nThe Wekinator then deploys a supervised learning algorithm on the labeled data we generate when recording OSC input messages while playing the FM synth. The model that we trained from this data was then able to predict a blend of these outputs even when receiving new data. The effect of this model allows us to have a continuous mapping of our synth to (virtually) any orientaton of our phones.\n\nPerformance\n\nIn our performance, we were able to achieve a level of expressivity and responsiveness that was able to be quickly achieved through limited training, and with just a few sonic anchors to navigate between. During our testing and evaluation of the configuration, we noticed that the continuity of the FM synthesizer’s parameters in response to the gyroscope was dependent on whether the labeled states of the FM synth were able to correspond between one another in gyroscopic space. The Wekinator’s ability to seamlessly organize various sounds within these states effectively allowed a fluid process: from a) idea, to b) mapping the parameters of the synth to our gyroscopic movements, to c) performance.\n\nClassifying Impact Sounds using Scikit-learn\n\nIn addition to the creation of new instruments using the Wekinator, we dug a little deeper within some light coding using the Scikit-learn toolkit. Our dataset consisted of 4 categories of short impact sounds: glass, rock, metal, and wood. We chose these categories for their unique sonic characteristics. Assessing our dataset, we checked that a random sample of these sounds could be accurately categorized by each of us, and thus, share some consistency across each category. Theoretically: employing an ML technique upon this dataset ought to learn a set of features similar to how humans may recognize these four categories apart and be able to categorize an unfamiliar sample from our dataset into one of these categories with reasonable accuracy.\n\nAn Overview\n\nWe used the Scikit-learn package to train a supervised neural network classifier using our dataset (Scikit-Learn…). Since we were using supervised learning, our audio samples were loaded and labeled via our own labels. Here we chose a sample rate at which the audio files would be resampled. We chose the scalar and vectorial features that corresponded to sonic elements within the sound samples. These features were then extracted and appended into an array along with an array for the feature’s labels. Finally, the model was trained using a 70/30 split of the dataset (very typical) with 70% of the data used for a training set and 30% as a validation set for the classifier. Evaluation was based on the number of mislabeled examples, a confusion matrix to describe the frequency of label identity within each category, as well as a metric for accuracy across all classification of the validation set. Upon evaluation, parameters like sampling rate, scalar and vectorial feature set, hidden layer count and size, epoch maximum count, and activation function (Figure 1).\n\n\nFigure 1: Some of the more common activation functions (Jadon, 2019)\n\nTesting and Configuration\n\nAfter running a few evaluations of Team B’s dataset, we decided that because a lot of the sonic energy of these impact sounds was in the high end, it would improve the model to choose a sampling rate of 44.1kHz, which was the actual sample rate for our audio samples. During our feature selection, we found it difficult to discern which features dramatically improved the predictions as we weren’t able to simply A/B test the model. However, we did note that a large amount of features lead to the detriment of the model’s predictive ability. A fewer number of features were able to provide higher accuracy in our tests. This is likely due to too many irrelevant sonic features “polluting” the extraction of salient features within the sample pool. In an attempt to A/B test all of the features, we found that the only scalar feature that did not improve the model was spectral flatness. After testing several different activation functions and hidden layer counts and sizes, our best results ended up merely being a single layer with 100 nodes using the ReLU activation function.\n\nResults?\n\n\nFigure 2: Jarle’s results for one attempt\n\nBuilding a model to categorize Team B’s dataset was largely successful, with an accuracy of 94% over a 10-fold validation (at our best, see Figure 3). Afterwards, we trained the model with another group’s dataset to compare whether our configuration would apply to another database: it did not. We found that Team B and Team C compiled very different datasets, resulting in less accurate classifiers built from the sample database. We were only able to achieve a consistent 70%, regardless it seemed, of changes to the activation function or hidden layer architecture. Team C’s sounds, it seemed from examination by ear, shared less sonic similarities, and appeared to be sourced from multiple different sample packs (shame on them!).\n\n\nFigure 3: Jackson’s results averaged over 10 iterations\n\nIn any case, through this module, we were able to learn quite a bit about the theory (and some practice) of basic machine learning. We trained an instrument using a readily made neural model to produce control dynamic synth in space and a classifier that was able to recognize categories of sound using their spectral qualities accurately. With this perspective, we did succeed in dipping our toes into the great big world of machine learning.\n\nReferences\n\nFiebrink, Rebecca, and Perry Cook. “The Wekinator: A System for Real-Time, Interactive Machine Learning in Music.” Proceedings of The Eleventh International Society for Music Information Retrieval Conference (ISMIR 2010), Jan. 2010.\n\nJadon, Shruti. “Introduction to Different Activation Functions for Deep Learning.” Medium, 6 Nov. 2019, https://medium.com/@shrutijadon10104776/survey-on-activation-functions-for-deep-learning-9689331ba092.\n\nScikit-Learn: Machine Learning in Python — Scikit-Learn 0.21.3 Documentation. https://scikit-learn.org/stable/index.html. Accessed 23 Nov. 2019.\n\nQuick Walkthrough of Wekinator. YouTube, https://www.youtube.com/watch?v=dPV-gCqy9j4. Accessed 25 Nov. 2019.\n",
        "url": "/machine-learning/2019/11/25/b-team-deep.html"
      },
    
      {
        "title": "Reflections on the Christmas concert",
        "author": "\n",
        "excerpt": "Trondheim reflects on the Christmas concert 2019\n",
        "content": "Setup\n\nTuesday the 26th was set aside for the yearly Christmas concert where high school musicians from Trondheim and Oslo play together through an SMC setup. The original plan was to rig up a portable version of the Portal at the high schools. Due to the discovery of technical problems on the portable Portal setup, together with a general lack of people, it was decided to invite the high school students over instead.\n\nThe rest of Monday was spent on staging the Portal for a concert. This work consisted of rigging up a set of new cameras, speakers, microphones and appropriate lighting. We had to figure out how to set up the Portal so that the musicians on both sides would be able to look at each other while playing without too much effort, and that the musicians also could see the audience in Trondheim and Oslo. At the end of Monday, we were done with everything except connecting the multi-channel audio setup in Lola and mixing the instruments together. The picture below shows the state of the Portal at the end of Monday.\n\n\n\nTuesday was spent on setting up a multi-channel audio connection between the two campuses, and to mix the audio. Surprisingly, there were some hiccups with the Lola connection, and we initially received a lot of clicks and pops through our Midas mixer. Initial attempts to fix the issue included changing the buffer size and verifying sample rates all over, but nothing helped significantly. After a while, we found out that the issue was on our side. After some further debugging, it became clear that the issue was related to the conversion of the signal in the RME card. There happened to be an extra RME card in the aforementioned portable Portal setup, so we swapped the two cards, which solved all our problems. Thanks to great problem solving by Daniel who saved the day.\n\nPerformance\n\nThe high school students arrived two hours before the concert, in order for us to be able to mix the instruments on both sides together. The students had agreed to play some jazz standards beforehand, which they practiced playing together during the soundcheck to get a feeling of how to play together in the Portal. Feedback from the students from Trondheim suggested that they had a fun experience playing in the Portal. They were surprised by how well it worked playing together over the Portal. The picture below was taken during the performance. Faces are anonymized due to privacy reasons. If we get consent from the students, a video will be published from the performance.\n\n\n\nReflections\n\nFor next year, time should be set aside in the the Portal course during the fall to be able to ensure that the portable setup works before the same week of the Christmas concert. Also, there was some confusion regarding the placement of the concert. As it initially collided with our exam in SMC4010, several students had already left Trondheim to work with their exam from remote.\n\nOne suggestion to avoid both problems would be to consider the Christmas concert as the ultimate goal of the first semester of the Portal course. The concert could be announced as part of the evaluation of the Portal course. In my opinion, the concert was not discussed sufficiently during the workshops and Portal meetings along the semester. The focus of the Portal workshops during the semester could be on the Christmas concert, so that all the students already were familiar with the technology beforehand.\n",
        "url": "/networked-music/2019/11/30/ChristmasConert_Trondheim.html"
      },
    
      {
        "title": "The B Team Wraps up the Portal",
        "author": "\n",
        "excerpt": "We made it out of the Portal, now what?\n",
        "content": "Reflections\n\nNavigating the SMC Portal been an incredibly interesting journey so far. The program is innovative through a number of ways, one being that the learning environment is part and parcel of the education. In fact, much of the learning experience has revolved around the Portal. There is the daily setup, troubleshooting, and improvements that often coincide with the classes that took place within the learning space. This interactive room provided a degree of responsibility and familiarity over a complex, live audio system.\n\nBut on the flip side, the Portal has also been a source of stress for students and teachers alike and has led us to make improvements over the course of the semester. Though many issues remain, I believe we have made considerable progress and have a clear vision of how we ought to improve it in the coming semesters.\n\n\n    \n    Here's a shot of this year's X-mas concert: a culmination of our technical efforts!\n\n\nLearning Outcomes\n\nTwo of the more prominent learning outcomes of the portal course has been becoming familiar with the mixing board and its capabilities as well as the attention to the character of sound during live sessions. We spent a lot of time with the M32 Midas mixer and as a result, the students of the SMC program were able to deftly handle the mapped channels, effects, and hardware when configurations needed to be changed. Because of each of our individual backgrounds, some of us were more experienced with these audio interfaces than others. This led to teaching roles within the student groups at Oslo and Trondheim that were able to assist with bonding as the semester began and fill in the gaps of our formal education. One consequence of this was that when there arrived situations that called for a comprehensive understanding of a particular system, the experienced students often took to the role instead of those who were less familiar. We also became attune to the quality of the audio that we were receiving from Trondheim and during musical performances. Most of this concerned the reduction of feedback and hum within the system but I would also argue that it made us more attentive listeners in the process.\n\n\n    \n    A lovely diagram by Jarle.\n\n\nGroupwork\n\nWhen it came to group work, the weekly setups went steadily the first few weeks and we took turns in setting up the Portal. Later in the semester, with everyone familiar with the setup process, we ended up sharing the work in the morning and the initial setup was quite quick. Many of my classmates worked together, cross group, to make improvements regarding the gain structure of the system or creating a preset that was fine tuned to reduce feedback. This work happened after class and I would say, as a class, we put in a considerable amount of work into taking care of the Portal. My group, in addition to another group as well, spent a good amount of time testing various EQing techniques, polarity inversions, and mic placements. About halfway through the semester we were fairly happy with the preset and the classes that took place in the Portal seemed to run with less interruptions as a result.\n\nLooking towards our glorious future…\n\nTowards the end of the semester, we, as a class, did reach the extent of what we were able to tune given the equipment we had available. Looking ahead, many of the problems that still exist are due to unknown factors regarding the LOLA protocol for audio and video. A number of occasions we were faced with random popping that did not seem to derive from either side but as an artifact of the bandwidth or LOLA’s processing. It might behoove us to try out the beta version of 2.0 or test UltraGrid for video streams. This way we can finally navigate away from Zoom and be fully operational without a 3rd party service. It will also be nice to update the M32 mixer so that we can route individual channels without having to do so in buses of eight.\n",
        "url": "/networked-music/2019/12/01/b-teams-thoughts.html"
      },
    
      {
        "title": "Portal ideas",
        "author": "\n",
        "excerpt": "Instead of starting up the M32 every day, recalling the correct preset, adjusting the faders, turning on the screens, turning on the speakers, opening LOLA, connecting to the other side, pulling your hair out because nothing will work… Imagine just pressing a button and it all just works.\n",
        "content": "Introduction\n\nThis is a post about the future potential of the portal, with plenty of proposals for technical solutions to solve our existing problems.\n\nThe problem\nThe portal is in dire need of a fixed setup that “just works” when you press the ON-button. The problem now is that there is way too much complexity and room for error, but not enough flexible functionality, which all in all does not make for a very streamlined experience. But fret not! There are plenty of overly-technical solutions out there, here are some of them:\n\nGear and solutions\n\n\n   \n\n\nAs you all are very familiar with, we rely on the Midas M32 for our audio processing. This is great, isn’t it? Having faders, lots of I/O, buttons for everything, a maze of menus, a decent amount of inherent latency, the worlds least intuitive snapshot-solution, lots of room for error… What? You don’t like using the M32 on a daily basis? Don’t worry, there are others freaks like you out there, and solutions to your despair: Let me introduce you to the audio processor!\n\nA processor, unlike a live sound mixer, is a device that is meant to sit in a rack and process audio with very little outside interaction. Think of a very flexible mixer which you program once, plug all your cables into and later interact with through a very simple UI.\n\n\n\n A typical audio processor \n\n\nThere are so many benefits to using a processor in situations like this, here are some:\n\nSet it and forget it.\n\nWhen you first set up a processor from scratch, it can take some time. One usually has to patch everything from the ground up, in a way that closely resembles working in Pure Data. The beauty of this is that you can get exactly the functionality you what you want from it! Once this is done, you store the settings and if you have done everything right, you hopefully won’t have to see that menu for many months.\n\n\n\nThe software used to program a processor\n\n\nUsing a control panel.\nInstead of starting up the M32 every day, recalling the correct preset, adjusting the faders, turning on the screens, turning on the speakers, opening LOLA, connecting to the other side, pulling your hair out because nothing will work… Imagine just pressing a button and it all just works.\n\nThat dream could become a reality through a simple touch panel, some clever programming and a GPIO-relay. In short; a GPIO is a way for digital devices to communicate with the outside world through analog on/off messages. This opens up a world of possibilities when it comes to automation, since you now can make your processor control a power strip powering the other equipment, trigger events on another device, maybe even talk to a Raspberry PI or a Bela?\n\n\n\nNo, it's not an ipod classic\n\n\nAEC, Dante, VOIP and several other goodies.\nAEC: When you have a closed loop situation like the one we have in the portal you very quickly run into situations where the microphones feed back into the system, have you noticed? AEC or “Acoustic Echo Cancellation” is a an algorithm that in short fixes the problem by subtracting the audio coming from the speakers, being reflected by the room and back into the microphones from the signal sent to the other side, thereby greatly reducing feedback and echo. This will require a fixed microphone setup, more on that later.\n\nDante: Audinate’s Dante is an adio over IP protocol that lets you freely route any source to any output in a network. Imagine being able to plug Natasha Barret’s computer to the network and patch up to 64 channels of audio directly to any loudspeaker in Trondheim or Oslo, without ever having to go through the M32 or LOLA. Sounds too good to be true? Right now it is, but that is because the network won’t let us to this, but more on that later.\n\nVOIP: VOIP is an ancient technology, but is still in use daily all over the world, as a testament to how reliable it is. Voice Over Internet Protocol is… Well, exactly what you think it is: a regular old telephone, remember those?  Many processors made for teleconferencing has this feature as a substitute for Skype or Zoom. It allows you to dial up another VOIP unit and communicate directly, with minimal fuzz. It is only mono and does not sound great, but will work for troubleshooting those times the portal is a bit grumpy.\n\nBetter network communication\n\nTo be able to realize the ideas listed over, we will need to do some modifications to the network between Oslo and Trondheim. The problem now is that there are a lot of hops on the network, with some sleeping switches and other challenges along the way. A possible solution to this problem could be to set up a router on each end and a dedicated VPN-tunnel between them. The VPN would trick the devices plugged in at both sides to think that they are on the same network and keep the sleeping switches awake between Oslo and Trondheim, allowing us to connect two audio over IP enabled devices together directly. This could even allow the two M32’s to communicate with each other directly throug AES50! Maybe, possibly, probably not…\n\nA possible hardware VPN-configuration \n\n PFsense is a very good open source VPN-service which can be installed on the school’s existing hardware \n\nBut why only audio, what about video?\nThe network we have should be able to support 4K video with almost no compression together with 64ch of bi-directional audio, if the network is configured correctly. Did I mention that the newest version of Dante supports video as well?\n\nMicrophones mounted in the ceiling\n\nIn order to make the AEC work and to make our lives more pleasant in general, we should mount the microphones we use to the ceiling. If we know the positions of the microphones at all times, we could make sure that they are always 180 degrees out of phase from the signal coming out of the speakers, making the possibility of feedback much less likely. This fact and the mere idea of not having a spaghetti of cables on the floor at all times is more than enough to make the case for doing this, in my opinion.\n\nOne argument against this could be that we then would place the microphones further from the source (us), picking up more of the room. This could easily be mitigated by microphones with a tighter pickup-pattern like a shotgun mic, or even the existing AKG C414 set to supercardioid.\n\nReducing room reflections\n\nMaybe it’s not very high-tech SMC, but putting up good old physical absorbents in the classroom is a very easy way to improve the audio in the portal. This might serve a double function by making the space more pleasant to look at as well; During the christmas concert, the Oslo side put up molton over some of the walls, in order to hide the cables. What a difference! Doing this was like enabling dark-mode on your phone.\n\nMolton is extremely cheap, so is Rockwool and regular floor carpets at IKEA or the local flea market. This might be the easiest way to greatly improve our day to day experience of being in the portal. Further improvements could maybe involve some plants? An SMC cat? The possibilities are almost endless.\n\n\nThe SMC cats\n\n",
        "url": "/networked-music/2019/12/02/portal-ideas.html"
      },
    
      {
        "title": "Picture Carmen",
        "author": "\n",
        "excerpt": "Promotional video for a new chamber production of the Opera ‘Carmen’\n",
        "content": "Backgroud\n\nOur task was to plan and produce a promotional video for a new chamber version of the Opera Carmen by George Bizet. Roy Henning Snyen, a classical guitarist and teacher at the department of musicology at the Aalborg University, has arranged Bizet’s opera for a small ensemble consisting of violin, two guitars and three singers. In our case, only the main singer was included.\n\nIn this chamber version of the opera, the ensemble goes straight to the core of the drama between the three main characters and stay close to the novel by Prosper Mérimée. The musicians are an integral part of the performance on stage, helping to portray the essence of Bizet’s musical universe in a setting that reflects the gypsy-life in Sevilla, Spain, approx. 1820.\n\nObjectives\n\nOur objectives were:\n\n  Producing professional video over two locations that can be used by the opera production team as a promotional video for getting funding, performance opportunities, audience, and exposure.\n  Utilizing the technologies available in the portal as well as our skills and expertise, for creating this promotional video.\n\n\nImplementation\n\nImplementation, execution and results\nWe divided the production into four main stages that include clear tasks and milestones:\n\n  First stage was testing technology and methods.\n  Second: Audio recording - producing high-quality audio that will be used in the video and that the musicians can use as playback for video filming.\n  Third part was the video shooting.\n  Post-production - Mixing and mastering of the audio and editing the video into a coherent piece of work.\n\n\nWe concentrated on three main dates as milestones:\n\n  October 17th (week 42) - Initial testing of the LOLA system\n  November 2nd-3rd (week 44) - Test recording with the musicians in the portal\n  November 8th-10th (week 45) - Audio recording and filming in the portal\n\n\nIn Oslo\n\nEspen and Guy prepared the portal to record the violinist and two guitarists. After deciding on the placement of the musicians in relation to the screen which was projecting the singer from Trondheim, we installed the video and lighting equipment and experimented with different cameras, camera angles and movements, different lighting setups, etc. The audio recording from both locations was conducted in Oslo and Guy was responsible for that. We kept a shared screen of the Ableton session on zoom for Sepehr to monitor from Trondheim as well. Espen ran the video recording, directing the musicians during the playing and interviews, filming, etc.\n\nIn Trondheim\n\n\n\n\n\nSepehr prepared the portal for the audio recording of the singer as well as the video broadcasting of the singer over zoom. He monitored the Ableton session recording from Oslo via shared screen and troubleshooting all audio issues that came up during the recording session.\n\nThe Audio Recording\n\n\n\n\n\nThe recording of the audio took place before the video filming. Since the lead singer was based in Trondheim we planned a test-recording with “Carmen” placed in Trondheim and the musicians in Oslo, using LOLA. The musicians had to be placed in a way so that they could observe each other efficiently on the screen, however, the visual latency did not allow visual cues. In regards to the space limitations of the portal in Oslo, we did not have many options available for the tracking. We could not find an ideal place for the instruments to have a stereo recording. Therefore close-miking multichannel recording was the best choice - rather than different stereo techniques that are usual in recording chamber music. Also, considering the possibility of post-production that was the best choice possible. All microphones used in the recording were AKG C414. We monitored the sound and chose the placements in regards to that.\nWe recorded three takes of each of the three songs we agreed on with the musicians.\n\n\n\n\n\nRouting\nIn Oslo - The Midas mixer was used as an interface and was connected to Guy’s laptop and the inputs were being recorded in Ableton Live.\nRegarding the sends on the Midas mixer in Oslo, there were two stereo pairs (Mixbuses).\nOne was used for downmixing the inputs into a stereo pair and getting the playback from the DAW, sending them to LOLA (Trondheim).\nThe other was being sent to the headphone amplifier in order for the musicians and the producer to be able to monitor themselves. This was to avoid the use of local PA system and to eliminate any sound bleeding from the corresponding location. The producer was in touch with Trondheim through talkback.\nTrondheim was being received on the Midas input channels and sent to the monitoring Mixbus.\n\nIn Trondheim - The routing system was the same in Trondheim. There were two stereo pairs (Mixbuses), one for monitoring and one for sending to LOLA. The singer was being sent to both and the producer was in touch with Oslo through talkback.\nAs a backup, a laptop was connected to the mixer, using it as an interface. So it could send and receive from and to the mixer for recording and playback.\nFollowing you can find a diagram of the routing:\n\n\n\n\n\nLater on, the audio files were shared over google drive and Sepehr prepared a rough mix of the chosen takes to be sent to the musicians. Once we agreed with the musicians on the specific musical segments that should be featured in the video. After receiving additional feedback from the musicians (mix, reverb, panning, etc.), the final mixing and mastering were done.\n\nVideo filming\n\nDue to time issues, some not apparent before we actually were in production, the video shooting had to be conducted as quickly as possible. Without going too much into detail, we had to turn a few things around. The room itself offers some challenges; although we let the audience know that we are in a rehearsal situation, we obviously want the video to look as good and clean as possible, but also needed to work fast.\nWe chose to film the same sequence from three different parts of the opera, from different angles, different shots, and focal length, to be sure we had enough footage to put it together, looking like a multicam production.\n\nThe light equipment in use are mainly 3 battery-driven LED mats with dimmer. Two of them also with variable color temperature. All this, together with the fact that they don’t get warm, makes them very versatile and easy to move around, something that helps a lot when in need of a fast workflow.\nThen, regarding the main shot of the musicians, we wanted a steady camera movement, adding a more dynamic design to the film and chose to use a slider, with a motion control system - also easy to mount and use. The camera, a Panasonic LUMIX GH4 is a lightweight, versatile camera, with a micro four third sensor. For these shots, we used an Olympus Zuiko 12 - 40 zoom lens.\n\nFor close-ups; we used a lens from Voightländer, with an extreme aperture and light sensitivity, that gives a very shallow depth of field, to add a kind of movie-feeling to the visual design.\nRegarding the main total shot, there were two basic challenges: First balancing the light in the room with the bright screen - obviously, we needed to make compromises. Then we have the fact that the singer on the screen is not in visual sync with the musicians, something that also leads to compromises when it comes to editing.\n\nEditing\n\n\n\n\n\nThe overall challenge, as no surprise, was synchronization - in all parts. Getting all the footage into sync - or at least to look as if it was in sync.\nYou really have to pay attention while editing this kind of material - one false move and everything moves out sync.\nA special problem occured due to the fact that the different music takes, that were used in the final mix, had small, variations in tempo. In other words, the final mix of the songs, that were to be used in the video, at some points consisted of different takes than the one the musicians originally used for playback, when recording. So, on the video timeline, the clip used for video would start in sync with the final audio mix of the song, and get out of sync as it played along.\n\nEvaluation\nConsidering our finale result, the production of a 4-5 minutes promotional video that includes high-quality audio and video, looks reasonably professional and convincing, we believe that we reached our objectives. Within our group, we kept assessing the results throughout the process by maintaining close contact between us for constructive feedback.\nAnother advantage of using this method was the division of the workload into smaller segments. Separating the production into four stages (testing, audio recording, video shooting, post-production) allowed us to maintain flexibility and helped us have better control over our progress, which lead to a satisfying outcome.\n\nVideo\nFinally, following you can watch the two different versions of the video (Norsk and English):\nNorsk version:\n\n\nEnglish version:\n\n",
        "url": "/applied-project/2019/12/11/Picture-Carmen.html"
      },
    
      {
        "title": "Tree as Speakers",
        "author": "\n",
        "excerpt": "A project in collaboration with ÅF engineering. The goal of the project was to create a non-intrusive soundscape and/or noise-masking installations in an outdoor public space by using trees as speakers, installing audio exciters on trees.\n",
        "content": "Background\n\nIn this project, we collaborated with ÅF Engineering. Martin Hallberg acted as our external supervisor and Stefano Fasciani as the internal supervisor. The goal of the project was to create a non-intrusive soundscape and/or noise-masking installations in an outdoor public space by using trees as speakers, installing audio exciters in the root system of the trees. We got information on a similar project by ÅF Engineering, an installation at the Sofiero Park in Helsingborg, Sweden. The installation was called “A fairytale walk”, and consisted of 8 trees, with one transducer on each, playing 8 fairytales. Visitors could then walk from tree to tree and put their ears to the trees to listen. This was inspired by a similar project by MIT, called “ListenTree”.\n\nWe were supposed to investigate the following questions during the project:\n\n\n  What kind of tree/wood gives the best resonance?\n  How can we mount the transducers in the root system to achieve the best sound output?\n  Do we need one or more transducers to get the best sound quality?\n  How far can sounds, emitted by a tree, reach?\n  What is the effect (qualitative and quantitative) of having several trees as sound sources close to each other?\n  What sound should a tree “play”?\n  How should the sound file be mastered so that it can be “played” out of a tree with minimal losses?\n  Can sound-emitting trees be used to hide unwanted noise in public spaces?\n  How can such systems make noisy public spaces more attractive to people (user surveys, concept design surveys)?\n\n\nProject plan\n\nOur plan was to do several field recordings, mounting sound exciters (the type of transducer we were to use) to trees in several setups, and then do analysis of the recordings. Tests would be done both in uncultivated areas and eventually in the city. We would play sinusoidal sweeps through the trees, investigating the following setups:\n\n\n  Number of exciters, on different number of trees\n  Placement of exciters (roots, trunk, w/o bark)\n  Different recording distances\n  Different trees (type, humidity, size)\n\n\nAnalysis would follow, getting the frequency response of all setups. From the frequency responses, research and SPL-meter readings we could be able to answer the questions provided.\n\nImplementation\n\nIt all started when we received equipment from ÅF engineering. Two problems arose:\n\n\n  The amplifiers required a power supply of 32V\n    \n\nsd250\n\n  \n  The exciters were meant for glass application.\n    \n\nsd1g\n\n  \n\n\nWe knew we would get this type of transducer, but we saw it as problematic to use these outdoors in the cold winter. Luckily, we were able to borrow the same type of exciters from Øyvind Brandtsegg, but with a surface mount plate, made for wood application. This made it easier for us to mount.\n\n\n\nsd1gsm\n\nWe tried to solve the issue of portable power supply in several ways:\n\n\n  Borrowing an inverter from the electronics department\n  Borrowing power from Estenstadhytta in Trondheim\n  Borrowing stationary power outlets from Trondheim municipality\n  Borrowing a car amplifier (12V) from Robin Støckert and borrowing a 12V battery from Jørgen’s brother, Fredrik Varpe\n\n\nIn the end, it was only the last point that solved our issue.\n\nBefore being able to go outside we got familiarized with the equipment and a software called Room EQ Wizard (REW).\n\nField recordings\n\nAt Tømmerholtdammen in Trondheim, a remote and quiet place, two types of trees (birch and spruce) were picked, where we also investigated different sizes.\n\n\n\nbirch\n\n\n\n\nSpruce\n\n\nTo improve transmission from the exciter to the tree, we carved out a flat surface. Since we did not have any shovels nor waterproof exciters, we decided to start by mounting the exciters on the tree trunk.\n\n\n\nSpruce\n\n\nSine sweeps, exported from REW, and a song from Spotify was played through the tree, using a phone, connected to the amplifier. The recordings were done using a directional condenser microphone, borrowed from Eigil Aandahl.\n\nWe ended up with five setups, where we did 5 sinusoidal sweeps, played a song, and measured sound pressure level with a SPL-meter, also borrowed from Eigil Aandahl. The setups are shown in the analysis.\n\nWhen we listened to the recordings of the first field trip, we could hear that the sine sweep sounded more like a square wave sweep. The gain on the amplifier was set too high, causing clipping. When we went out the second time, we turned it down, and it solved the issue.\n\nPlease take a look at this awesome video made by Shreejay, showing the field recording process:\n\n&lt;/figure&gt;\n\n\n  \n  Your browser does not support video tag.\n\nField recordings\n\n\nLastly, we recorded the background noise outside Fjordgata 1 in Trondheim City.\n\nResearch\n\nResearching wood acoustics, we found that the acoustical performance of wood is determined by density, Young’s modulus (measures stiffness of a solid), loss coefficient and hardness. These physical properties will affect the speed of sound, impedance, sound radiation coefficient, loss coefficient and the eigenfrequency of the tree. (Wegst, 2006)\n\nWe also researched auditory masking and found that the sound’s masking ability depends on its intensity and spectrum. When the intensity of a masker is raised, there is an increase in masking upwards, but very little masking at lower frequencies. This is known as upward spread. There will also be strongest masking around the immediate vicinity of the masker frequency, and lower frequencies mask a wider range than higher frequencies. (Gelfand, 2009)\n\nAnalysis and results\n\nThe initial visualization consists of so many data points that made it impossible for comparison. We experimented with different types of smoothing, and landed on 1/6 as the best-applied smoothing. Then we discussed which recordings to compare and analyse.\n\nSummary of Analysis\n\n\n  \n    Comparing small birch tree with one exciter and two exciters. The results showed that overall the number of Exciters doesn’t seem to affect the frequency response other than levels.\n\n    \n\nBlue: 2 exciters, red: 1 exciter\n\n  \n  \n    We had to investigate if the number of trees can affect the produced frequency content. From the results, we identified that using one exciter per each tree tends to produce a slight drop in 100 Hz - 380 Hz range. This might be due to microphone placement, pointing between the trees.\n\n    \n  \n  Green: 2 trees, red: 2 exciters on 1 tree\n\n  \n  \n    We analyzed the frequency responses of birch, Spruce and the large birch tree. The results were interesting and led to some ambiguity about the recording itself. According to the graph, the large birch tree is producing lower frequencies in the 100 Hz -200 Hz region. Then we had to make sure it was not due to background noise. The recording had a background noise of rain which was not possible to clean or remove during the editing process because it will drastically affect the frequency response graph.\n\n    \n    \n    Red: Big birch, blue: spruce, green: small birch\n   \n  \n  \n    We created frequency response graphs of the background noise in the recording by choosing 1-3s clips of the background noise. And also created another frequency response graph by using the half-length sinusoidal sweeps (so the sweeps will start from 500 Hz- This step was for further confirmation that the low end was not produced by any background noise). As the final result, it suggests that the larger birch tree produces more low frequency content.\n\n    \n\nGreen: Big birch, Black: small birch, red: bg-noise big birch, purple: bg-noise small birch, yellow: bg-noise spruce \n\n  \n  \n    According to the urban noise analysis we did, higher levels of low-frequency content below 100 Hz should be produced from trees to mask public noise. By listening and looking at the frequency spectrum, We realized that vehicles contribute most to the low frequencies in urban noise. We concluded that this kind of system is not suitable for noise masking but more appropriate as sound installations.Trees can be used in office spaces to play background music to break the extreme silence. Also, interactive installations can be done in public parks and children parks. As an example, playing sounds through trees which will trigger if a person gets closer to trees.\n    \n\nBlack: Trondheim city noise\n\n  \n\n\nBig thanks to all who helped us:\n\n\n  Robin Støckert\n  Stefano Fasciani\n  Øyvind Brandtsegg\n  Andreas Bergsland\n  Daniel Formo\n  Eigil Aandahl\n  Fredrik Varpe\n  Eirik Havnes\n\n\nReferences\n\n\n  \n    Gelfand, S. A. (2009). Hearing: An introduction to psychological and physiological acoustics (5. ed). New York: Informa Healthcare.\n  \n  \n    Wegst, U. G. K. (2006). Wood for sound. American Journal of Botany, 93(10), 1439–1448. https://doi.org/10.3732/ajb.93.10.1439\n  \n\n",
        "url": "/applied-project/2019/12/15/Trees-As-Speakers.html"
      },
    
      {
        "title": "Trippi BySykkel Sounds",
        "author": "\n",
        "excerpt": "Portray city bike user data into a sonified, interactive display by making use of public space and public data.’\n",
        "content": "Background\n\nIn the context of the SMC Applied Project 2, our group collaborated with the external partner Urban Sharing, a company that promotes sustainable micro mobility models through its daughter companies Oslo and Trondheim Bysykkel. The project involves the use of open data from Bysykkel users to make an interactive sound experience for public display. Intended as a web-audio application for online composition for users of Bysykkel in Oslo and Trondheim, we developed a simple prototype of sonification. As composers, music technologists and programmers we approached the task from a less data scientific perspective. From the beginning we were interested in the artistic possibilities to work with large amount of public data in a multi-disciplinary design process. The goal was to portray user data into a sonified, interactive display by making use of public space and public data.\n\nDivision of Labour\n\n\n\n\n\nWe were all working together on the development process however, we were able to recall each of our areas of expertise.\n\nInspiration\n\nFor the concept and sound design approach Karolina was researching in different musical but also theoretical spheres. The sounds chosen for the mapping were then to a large extent inspired by the artist Colleen. Her playful, dreamy Art-Pop gives a light, but not shallow musical sound experience to the Bysykkel trips. But also were the ambient sound of different seasons during the year in combination with the sounds of different day times giving inspiration. On a conceptual level Samuel Thullin has been guiding us with his visions on sound maps (Cartophony) through the project. They are resonating not only in terms of the soundmap features, but also in terms of collective empowerment. The data that has been collected on the Bysykkel users goes back to the public and can explored and comprehended in a playful way. With the soundmap as we imagined it, users are part of reshaping the idea of a non-cartographic mapping process. Micro Mobility as an interaction model for electronic music making could create Acoustic communities (Thulin, 2016) in which authors and users are co-producing their sounds. In doing so it could be a great tool to raise data awareness through an aesthetical, pleasant interactive sound experience.\n\nData Awareness - The city bike trips as a sonic interface\n\nSound maps appeared to be a good solution for combining sonification and visuals, becoming one display. The soundmap is a relatively recent invention, having emerged at the intersection of soundscape studies, acoustic ecology, and sound art practices in the late 1990s (Ceraso, 2010). Usually they are geographic collections of distinct “sonic inscriptions” (Thulin, 2016). We were using this idea when allocating the sound design to bike trip parameters.\n\nData inspired music or parametric sonification? Possible Research Questions\n\nTo what extent can the prototype play with patterns, commonalities and casualties that lie in the data? Can our model of soundmap/cartophonic model portray or even affect micro mobility?\n\n\n\n\n\nSound Design\n\nIn charge of the audio design, Sam created the primary sounds using Ableton Live.\n\nThe audio was all created in Omnisphere by spectrasonics. This tool is a very powerful synth that can easily stack both samples and synthesis in layers, often used for cinematic sound design. It seemed fitting for the task. When discussing the style and design of sound inspiration was drawn especially from Colleen, drawn to the complex but aesthetically pleasant, bright and modern sound. Three sounds were created, with three variants of acoustic tones to supplement different periods of the day. Here is a more in depth description of each:\n\n1. Morning\n\nLayer 1\n\nThe primary layer is a contemporary marimba sound, recorded from sample and included in Omnisphere’s soundbank. It has a light, bright modern feel with a clean timbre, and has a dynamic, short duration - acting as the lead of the sound\n\nnon resonant low pass filter taking out the unneeded sub frequencies around 700hrz\n\nan 1/8th note hi passed delay with modest feedback is added at 40% wet. This creates   a more ereathal sound that will last over time\n\nA tape slammer, that warms and slightly affects tone and reduces dynamics\n\nLayer 2\n\nSecond layer was originally created on a Kawai K-5000 hardware synth, is a soundscape with a ‘tube’ like sounding timbre. It is more legato in nature, and sits underneath the marimba sound at the same note, thickening the overall sound\n\nLayer 3\n\nThird layer is subtle. It is a stock ‘pop female ahh’ sound that comes in when the note has been held for 6 seconds. This was added to keep the sound evolving when the note has been held, and keep the timbre interesting.\n\nFully wet, slow chorus is added to this layer for taste\n\nData Inspired Music Model, Sunday 7am:\n\n\n  \n  Your browser does not support the audio element.\n\n\n\nData Inspired Music Model, Monday 7am\n\n\n  \n  Your browser does not support the audio element.\n\n\n\n2. Daytime\n\nLayer 1\n\nPrimary layer is a metallic sound with a music box feel with a strong attack, short decay.\n\nHas a very light reverb at 2% wet to soften slightly, with a little pre delay to avoid dampening the attack\n\nI feel this resonant, bright and short sound signifies the awareness and bright feel of the daytime, where the city is alert, positive and moving.\n\nLayer 2&amp;3\n\nIt has been layered with the same 2 layers (the ‘tube’ and ‘pop female ooh’ to thicken the sound again, but also to have some continuity with the first patch\n\nData Inspired Music Model, Sunday 4pm\n\n\n  \n  Your browser does not support the audio element.\n\n\n\n3. Night\n\nLayer 1\n\nClassic analog bell like sound, with a lower bass element.  Has two waveforms:\n\nan uneven triangle wave for the lower part of the sound, that sits legato underneath.\n\nand a sine for the higher frequencies. The sine has a short attack and decay, then with volume automation slowly rises back in. There is also a unison widening the sound that gives it its fuller texture.\n\nLayer 2&amp;3\n\nAgain, this is then blended with same 2 layers (the ‘tube’ and ‘pop female ooh’ to thicken the sound. In their regard, the tube sound fills in the lower spectrum, and the female ‘ooh’ has a very slow attack time, and comes in when the note is held, bringing more depth in the mid to upper range after long legato notes.\n\nData Inspired Music Model, Sunday 8pm\n\n\n  \n  Your browser does not support the audio element.\n\n\n\nData Inspired Music Model, Sunday 10pm\n\n\n  \n  Your browser does not support the audio element.\n\n\n\nThe Data Inspired Music Model\n\nIn charge of making one of the two sonification systems, Elias created the “Data inspired music model”. This model were made using Python and several Python packages. The system is based on mapping parameters from CSV files (open city bike data) to the sounds that were created by Sam. The Python script would start by reading the CSV file, which were an arbitrary selected file from Oslo Bysykkels open data. The file selected consisted of data from all the bike trips in Oslo from a specific month. The data consists of 13 variables from each trip. The variables used in this model was the starting time of the bike trip, it’s duration and the latitude of the bike station.\nDuring the reading of this CSV file, the start/end date and time of the bike trips were converted to timestamps. Which are numbers of seconds since 1970. This makes it easier to get a grasp of the time between one bike being picked up and the next on the timeline. This is used to make the sounds being played in whatever time we select, but with the same amount of time between the different sounds in regard to the playback speed. The time would also be used to decide what sounds to play in regard of the different sounds depending on what time of the day it is.\nThe latitude of the bike station decide what note to play. From lowest note furthest south to highest note furthest north.\nThe duration of the bike trips decide if the model are going to play a long note or a short note. If the duration is below average duration, a short note is being played. If the duration if above the average duration the long note will be played. The duration of the bike trips also decide on the playback volume. Longer trips are played louder than the shorter one.\n\nParametric Sonification Model\nEigil made another way of sonifying the data from a CSV file using Max/MSP. This model used a similar way of extracting unix timecode from the start times of each line in the CSV file and then creating a delayed trigger within Max/MSP to read and parse the next line.\nBy extracting parameters from the data such as length in meters, the average speed and duration, this can be played back almost like a score with different note events being changed by the parameters. Collecting all the necessary values and feeding them into a poly~ object makes it possible to generate multiple sounds layered on top of each other with the sounds being generated from the data. In this case, a note would have a lower pitch based on the average speed, and the pitch would rise according to the length of the trip. The duration of the sound is also mapped according to the duration of the trip, meaning if the playback is running at a factor of 1, the sound would take as long as the original trip took.\n\nReferences\n\n\n  \n    Thulinn, Samuel (2016). Sound maps matter: expanding cartophony\nhttps://www.tandfonline.com/doi/full/10.1080/14649365.2016.1266028\n  \n  \n    The future of Micromobility\nhttps://medium.com/urbansharing/what-this-years-sxsw-tells-us-about-the-future-of-micro-mobility-df2dbf521fb3\n  \n  \n    Composition With Path Musical Sonification Of Geo Referenced Data With Online Map Interface\nPark, S., Kim, S., Lee, S., Yeo, W.S., n.d. Composition With Path Musical Sonification Of Geo Referenced Data With Online Map Interface\n  \n  \n    Images from https://medium.com/@jonolave/exploring-open-data-from-oslo-city-bike-67985a101268\n  \n  \n    Sonic City: The Urban Environment as Musical Interface\nhttps://pdfs.semanticscholar.org/e3ab/f9dff14ca7c68b1ddeba113f982b512d6519.pdf\n  \n  \n    Online Map Interface for Creative and Interactive Music Making\nhttp://www.educ.dab.uts.edu.au/nime/PROCEEDINGS/papers/Demo%20N1-N20/P331_Park.pdf\n  \n\n",
        "url": "/applied-project/2019/12/15/Trippi-BySykkel-Sounds.html"
      },
    
      {
        "title": "Prototyping musical instruments",
        "author": "\n",
        "excerpt": "Prototyping musical instruments in the name of recycling - exploring Orchestrash from an HCI point of view\n",
        "content": "During the HCI course, we examined the instruments prototypes built as part of the physical computing hackathon. This blog post is a summary of our reflections of prototyping musical instruments in the name of recycling, a project we called Orchestrash. Where the previous blog post discuss the technicalities of prototyping instruments in a physical computing environment, this one will examine Orchestrash from an HCI point of view.\n\nIntroduction\n\nThe accessibility of open source platforms such as Bela and Pure Data is boosting the creation of new musical instruments. The relationship between sensor technologies and visual programming provide great possibilities in the conception of interactivity, having an expressive impact on the contemporary music production (Poupyrev et al., 2001). Creative uses of such technologies arise from the versatility in the user experience while developing with these tools. Equally important, the simplicity of these mechanisms tend to decentralise the production of devices to the artistic expression, giving more democratic perspectives to the production of musical devices.\n\nThe system\n\nThe system presented here consists of a four instruments ensemble aiming to make music out of trash. The instruments were designed to be simple interactive interfaces, with distinct roles and frequency bandwidths, and consisted of a looper/sequencer, bass, drone and a synth. To make sure everyone was playing in the same key, we constrained the three melodic instruments to play within the C minor scale.\n\nOur interpretation of the theme was to a large extent related to the notion of reusing sounds, through techniques such as feedback, sampling and looping. We decided that we also should try to use recyclable materials as sound sources and controllers as much as possible, e.g. by recording sounds from a plastic water bottle and a cleaned-out yoghurt tray. Using objects that don’t traditionally belong in music performance turned out to foster different creative approaches during prototyping and while performing. Looking back, this could very well be related to Cook’s principle that “Everyday objects suggest amusing controllers” (Cook, 2017). Moving away from traditional controllers and sounds liberated us in the creative process, and also augmented the improvisational structure of the performance.\n\n\n \n Component diagram of the drone instrument\n\n\nPerformance\n\nConsidering that the telematic system we used when performing the music (networked musical performance over LOLA) had such a high impact on the musical expression, it is fair to say that the inherent properties of the telematic system became a big part of the performance in total. This builds on the idea of “musicking” elaborated in Small (1998), where it is not only the musical performance of the artists, but all the properties of the system that play into the action-perception loop and thereby plays a part in the entire “musicking” performance.\n\n\n \n Networked music performance over LOLA\n\n\nReferences\n\n\n  Ivan Poupyrev, Michael J Lyons, Sidney Fels, et al. New interfaces for musical expression. In CHI’01 Extended Abstracts on Human Factors in Computing Systems, pages 491–492. ACM, 2001.\n  Christopher Small. Musicking. In Musicking: The Meanings of Performing and Listening. University Press of New England. p. 9. ISBN 978-0-8195-2257-3, 1998\n  Perry Cook. Principles for designing computer music controllers. In Alexander Refsum Jensenius and Michael J Lyons, editors, A NIME Reader: Fifteen Years of New Interfaces for Musical Expression, pages 1–13. Springer, 2017\n\n",
        "url": "/interactive-music/2019/12/27/Prototyping-Musical-Instruments.html"
      },
    
      {
        "title": "Staggering towards the light",
        "author": "\n",
        "excerpt": "During a hackathon in our introduction course to physical computing, we developed a prototype of a DMI. In our blog post from this project we explained how the system was built and gave a short summary of our performance. In this blog post however, we will look at the instrument from an HCI-perspective. Where the main focus will be a summary of the problem space, the research question, the methodology used and our main findings and contributions. \n",
        "content": "During a hackathon in our introduction course to physical computing, we developed a prototype of a DMI. In our previous blog post from this project we explained how the system was built and gave a short summary of our performance. In this blog post however, we will look at the instrument from an HCI-perspective. Where the main focus will be a summary of the problem space, the research question, the methodology used and our main findings and contributions. \n\nResearch question\n\nIn the spirit of all great hackathons we were handed a theme as our starting point. For this particular project the theme we were delt was «recycling».\nThe way we chose to approach this theme was to embrace the topic in every aspect of our DMI. First of all we created new sounds by recycling sound itself and second of all chose to manipulate these sounds with controllers made of recycled materials.\n\nAn important aspect we tried to take into consideration was the importance of gesture and physical correlation of movement to sound production, both for performers and for the audience. Our main goal for this project was to create a link between theme, gesture and sound.\n\nMethodology\n\nOur first approach to the task was to define the problem space by asking the questions: What are we trying to achieve, how will we achieve it and what will we create?\n\nOur first decision was to limit ourselves to just one sound source. By doing this we managed to stick to the theme in several aspects of our system. The limitation also gave us an opportunity to work on our creative thinking. By defining a set boundary we were forced to figure out how we could turn a single sound into a creative musical performance.\n\nOur second task was to find everyday objects which could function as possible controllers. By using a metal rod and a drum stick wrapped in rustic copper we were able to achieve our goal of creating music with recycled materials.\n\nAll in all we felt that this was a good solution in our quest to make a link between theme, gesture and sound.\n\nMain findings and contribution\n\nIn conclusion we succeeded in creating a functional and unique instrument with a wide range of musical sounds and controllers. However, where we perhaps didn’t manage succeed were in the terms of usability of the system. The controller made out of trash turned out to be difficult to control, unreliable and highly unstable. The one-shot sample controller (see other blog post), due to the fiddliness of the Bela, was also somewhat hard to control. At the same time the lack of control contributed with an element of surprise in our performance. As neither the performers or the audience knew what to expect.\n\nThrough this project we showed how music technologists can develop new and exiting DMI’s in a green and sustainable manner. The garbage can is full of usable materials one can use in the pursuit of musical greatness.\n\n\n \n Behind the scenes\n\n",
        "url": "/interactive-music/2019/12/30/Staggering-Towards-The-Light.html"
      },
    
      {
        "title": "Testing Latency in the Portal",
        "author": "\n",
        "excerpt": "We ‘officially’ test some the latency in the Oslo and Trondheim Portal\n",
        "content": "Internal Latency\n\nFor the internal latency tests, we used the Audacity tutorial but keeping the distance from the mics to speakers the same. The basic idea is that we opened an instance of Audacity and generated a click-track to send through the speakers which was then recorded by the mics and received into Audacity as another track. We then measured the average difference in time between these two tracks.\n\n\n    \n    Measuring latency in Audacity\n\n\nOslo\n\nIn Oslo, it was fairly easy to install Audacity on the LOLA machine and properly route the inputs and outputs and send some click-tracks through. After ten measurements, we found the latency to be around 27-28 ms. This was quite similar in Trondheim, however, the latency of the mixer and LOLA computer is something to consider here, along with the distance between the speakers and microphones mounted above (about 2m).\n\nTrondheim\n\nAs Audacity doesn’t accept Asio as audio driver, very few audio device manipulations are possible. We discovered that the audio is routed to the Analog 1/2 channel in Oslo, while in Trondheim the sound is coming from ADAT 1/2. Without Asio, it was impossible to choose the latter as audio input in Audacity. For that reason, we had to download and use Reaper in Trondheim. But the latency testing process was the same. We obtained a latency of 25 ms.\n\n\n    \n    Differences in the TotalMix\n\n\nZoom latency\nWe measured the latency in Zoom using Zoom’s built-in utility for “Meeting and Phone statistics”. We connected a client from the Portal in Oslo to a separate client in Trondheim, through Ulrik’s computer. We then measured different data, i.e. audio latency, bitrate, jitter, packet loss, etc. over time and averaged the results. As we were primarily interested in audio latency, we did some brief notes on the variance in the measurements. We also experimented by adding more clients to see how that affected the latency.\n\nThe constants that we used to measure the end to end latency:\n\n\n  \n    \n      Location\n      m\n      ms\n    \n  \n  \n    \n      Midas\n       \n      1\n    \n    \n      Distance mouth to mics Oslo\n      2\n      5.8\n    \n    \n      Distance mouth to mics Trondheim\n      1.5\n      4.4\n    \n    \n      Distance speakers to ears Oslo\n      3\n      8.7\n    \n    \n      Distance speakers to ears Trondheim\n      5.2\n      15.5\n    \n  \n\n\nResults\n\n\n  \n    \n      Zoom condition\n      Only Zoom\n      Zoom + distance (mic and speakers)\n      Zoom + distance + Midas\n    \n  \n  \n    \n      Trondheim sending (AVG)\n      35.5\n      39.9\n      40.9\n    \n    \n      Trondheim receiving (AVG)\n      38\n      51.5\n      52.5\n    \n    \n      Oslo sending (AVG)\n      25.8\n      31.8\n      32.8\n    \n    \n      Oslo receiving (AVG)\n      23.5\n      32.3\n      33.2\n    \n  \n\n\nAs can be seen in the table above, the total end to end latency ranges from 32.8 ms - 52.5 ms, depending on the local configuration.\n\nRegarding the other data measurements (jitter, packet loss, video resolution etc.), we noticed that latency varied the least. It seems like Zoom’s algorithms prioritize maintaining a constant low latency over e.g. good video resolution.\n\nOne surprising result that came up during these measurements were the acoustic latencies from the speaker/listener to microphone/loudspeakers, respectively. The distance from the speakers to the audience in Trondheim is approximately equal to half of latency of sending that same audio between the two cities. While our minds have been focused on optimizing network latency, it suddenly became apparent that there are obvious acoustic latencies that could be optimized as well. The long distance from loudspeaker to ear in Trondheim can be improved by sending the incoming Lola audio to some loudspeakers that are closer to our ears (around 1.5 meters), which would reduce the end to end latency for the audience in Trondheim by close to 12 ms, from the calculation (5.5 - 1.5)/340 * 1000 = 11.76 ms.\n",
        "url": "/networked-music/2020/01/27/testing-latency-portal.html"
      },
    
      {
        "title": "Camera Optimization in the Portal",
        "author": "\n",
        "excerpt": "On the quest for optimizing the visual aspect of the Portal\n",
        "content": "Camera testing\n\nBelow are screenshots from the testing of different cameras.\n\n\n   \n   VHD-V302\n\n\n\n   \n   Logitech\n\n\n\n   \n   Huddly\n\n",
        "url": "/networked-music/2020/02/03/Camera-Testing.html"
      },
    
      {
        "title": "The MIDI Remixer",
        "author": "\n",
        "excerpt": "This sequencer based poly FM-synthesizer invites its users to remix and play with some of Johann Sebastian Bach’s most famous preludes and fuges.\n",
        "content": "Overview\nThe MIDI Remixer is a sequencer designed to remix and play with some of Johann Sebastian Bach’s preludes and fuges. The application reads and stores MIDI-files before letting the user control which sections of the composition to sequence upon through a poly FM-synthesizer featuring external OSC-controls.\n\n\n\nSystem\nMIDI Storage\n\nAn application that allows someone to sequence different sections of a MIDI-file needs to have a system where the MIDI information is labeled, timestamped and stored for later retrieval. I decided to base my MIDI-storage device with cyclones [coll] object following MIDI execution objects such as [seq] and [midiparse]. If correctly done this would allow the user to index any part of the MIDI-file they wanted.\n\n\n   \n   Coll\n\n\nThe overall logic of the storing mechanism can be described as such; when reading a MIDI-file to the [seq], and executing it with a ”start” message, the MIDI information (note and velocity) is split, packed and indexed into the [coll] object. Ensuring careful scheduling was crucial for this storing process to work.\n\nI figured out that the best way to index the [coll] was at the very beginning of each step by using the first note-on value from the steps (MIDI sequences) starting with “index 0”, which simply functions as a blank index. This little hack properly ensured correct scheduling in all cases, as the image below shows.\n\n\n   \n   Scheduling\n\n\nHowever, this method has several limitation when working with polyphonic compositions which I will discuss further in the reflection below.\n\nTrigger Mechanism\n\nI considered several retrieval mechanisms for accessing and playing the stored MIDI for this application. My first idea was to use piezo transducers and drumsticks as triggers. However, I ended up with an internal sequencer system because it is easier to use and enables users to create different, interesting and more meaningful content (remixes) faster.\n\nThe sequencer features the following functions:\n\n\n  You can choose how many steps you want to loop. This enables the user to set the whole “track” on loop if desired. An 8-step loop is default.\n  There are eleven 8-step presets the user can choose between (highlighted in green). These represent different sections throughout the stored composition. The starting point of these presets can also be altered by clicking “new presets”. There is also a way to go back to the original presets.\n  A BPM navigator so the user can raise or lower the speed at which the notes play in succession.\n  A simple drum machine that solidifies the true techno-barouque experience.\n\n\nSynthesis\n\nThe polysynthesis of the application was inspired by a Youtube Tutorial on building simple FM-synthesis in Pure Data, as seen in the image below. I went for something very simple and melancholic due to the previously discussed purpose of the application. Therefore, I believe too much sound control and complexity in the synthesis could distract the user from the application main purpose.\n\n\n   \n   FM\n\n\nHowever, not all parameters all fixed. I implemented some OSC-control to effect something resembling a filter (modulation index) as well as the total reverb amount. The OSC-control messages come from TUIOpad, a smartphone application that sends X and Y parameters, among many, to the patch in realtime.\n\nReflections\n\nDuring the development of this project, a few things caught my attention which is worth discussing/reflecting upon. First, my MIDI storage method did not end up to be as elegant as first expected. Another approach could be to use a different indexing method that would write note on-and-off values in different indexes, making full use of the [poly] functionality. A third, and perhaps a better, way could be to have an abstraction before the [pack] object that stores and sends the index value with a note value first, before sending the same index complete with note duration (when note-off is received) later. The [coll] could then be rearranged chronologically, from first to the last index, after the writing process is complete.\n\nOn a related note, this method would still require a realtime storing process which is quite time-consuming. To achieve faster writing speeds, which is possible with the [seq] object, one can not use the [timer] object to calculate note duration as the latter example would require and what my application uses. This could be avoided if there was an object which could dump out the duration of each MIDI-notes at non-realtime speed.\n\nHowever, considering my sequencer trigger mechanism, I believe my method works just fine for the task at hand.\n\nFinally, my choice of trigger mechanism is an action that inevitably restricted and limited the boundaries of my project. In theory, anything that bangs could function as a sound trigger here. This leads me to reflect on other possible applications for my remixing sequencer. It would, for instance, be interesting to modify the application to fit an installation setting, where users interact with the MIDI via gestures/movements or manipulation of objects in a confined space, or an educational setting where the music theory and modern relevance of Bach’s music were more in focus.\n\nReferences\n\nReally Useful Plugins. (2019). PureData Tutorials - rich synthesis (6)- FM. YouTube. Online at: https://www.youtube.com/watch?v=mvtN7de6Oko\n",
        "url": "/sound-programming/2020/02/08/The-MIDI-Remixer.html"
      },
    
      {
        "title": "The Delay Harmonizer",
        "author": "\n",
        "excerpt": "This chord generator uses a variable length delay fed by a microphone input as sound source.\n",
        "content": "Overview\n\nThis project consists of a four notes chord generator, using a microphone feeding a variable-length delay. The length of the delay is determined by the desired MIDI pitch, and four delays are used to generate the four notes. The sound source (microphone) is then used to choose the timbre of the resulting sound. Indeed, even if the fundamental frequency and so the pitch is independent of the sound source, the harmonics are determined by which sound is fed into the microphone. The same sound source is used to feed all four delays, so the whole chord has a common timbre.\n\nSystem\nChord selection\n\nTo choose the chord(s), two methods are possible. The first is using the chord selector, which consist of three selectors, the key (arranged in fifths, Db = 0, Ab = 1, …, F# = 11), the type (major = 0 and melodic minor = 1) and the degree (I = 0, II = 1, …, VII = 6). The latter defines which chord within the chosen scale will be sent. For example, let’s consider the C major key. The degree I will be the C major quadriad (Cmaj7), the degree II will be Dm7, then III = Em7, IV = Fmaj7, V = G7, VI = Am7 and VII = Bm7b5. Those three selectors can be found in the middle left part of the patch shown in the image below. One limitation of this system is that sixth, ninth, eleventh and thirteenth chords cannot be accessed, as well as diminished seventh, sus, and any other types. But as every key is available, there are still a lot of chord progression possibilities. Those three selectors can either be accessed in the patch or using a Launchpad Pro, for which the pads are mapped to show colors corresponding to the PD selectors. The latter is the best solution, as it does not require the use of a mouse, and is therefore way more playable.\n\n\n \n Pure Data patch (UI) of the Delay Harmonizer\n\n\nThe second chord selection method is by programming a sequence, shown in the bottom part of the patch image. The chord sequencer is eight steps long (but could easily be extended), and consists of three voices (key, type, and degree). For sake of clarity, the degree numbers are offset by 1, in order to have the correspondence I = 1, II = 2, etc. The tempo can also be determined, by changing the metro object’s creation argument (inside the subpatch content). Once the sequencer messages are changed, one has to click the set sequence button in order to update the values in the sequencer. The logic for the chord selection is in the chordselector subpatch.\n\nOther elements\n\nEach delay voice has a limiter, in order to avoid saturation. Thes are then mixed together, and the user is given the possibility to change the volume of each voice independently (middle right of the patch). The total mix of then sent to a flanger effect (frequency and depth can be modulated), and finally a VCF (frequency and Q can be modulated).\n\nThe other accessible parameters are the minimum MIDI note, which basically corresponds to the octave at which the chords are played (the root note can be up to 2 octaves lower than the rest, in order to have a bass sound), and the delay gain, whose modification leads to changing reverb time of chords. Both these paramters are modified on the UI patch, as shown on the patch picture. A gain of one gives infinite droning chord. A gain of 0.9 lets the user play chords rhythmically.\nThe main use case of this patch is to harmonize a percussive track, like beatboxing of just claps. Other good sounds can be obtained too. For instance, with my cheap PC microphone, I obtained a very interesting sound by moving a cup around on the table. It’s also great to tap on the microphone or to click your tongue. A demonstration of the resulting sound generated by these excitation can be heard in the following recording.\n\n\n  \n  \n\n",
        "url": "/sound-programming/2020/02/09/The-Delay-Harmonizer.html"
      },
    
      {
        "title": "Multi voice mobile sampler",
        "author": "\n",
        "excerpt": "A mobile tool to dabble with small audio recordings wherever you encounter them\n",
        "content": "Idea\n\nI set out wanting to create an intuitive and playable looping interface, based on live recording of samples through analog input. My main motivation behind this project was to learn Pure Data (Pd), as well as to create an interface that affords to dabble with spontaneous audio recordings in a way that is fun and interactable to a non-musician. As a person who is still in the early process of learning about music production myself, I really enjoy interfaces that let you explore manipulation of sounds. Also inspired by granular synthesis, I wanted the user of my interface to be able to explore the little details of the recorded sound material.\n\nSetup\n\nThe final project can be found on my GitHub page. To run it, open the patch main.pd in the folder grains/mobmuplat/, which should contain all the relevant externals. Alternatively, you can download the zip file grains/mobmuplat/mobmuplat.zip to your phone and open it in MobMuPlat to get the full mobile experience. A third option is to use the Pd patch on a desktop and then open the grains.mmp file in the MobMuPlat Editor to simulate the mobile experience. Anyhow, all the functionality is available through GUI in the standalone patch.\n\nOverview of the interface\n\nThe result consists of a four voice sampler. Each voice can be either 1, 2 or 4 seconds long. The sample slots are visualized in a horizontal green radio bar, with the selection indicating the current sample slot. All the effects except the mix and the total gain affect the currently selected sample slot (note that it can be tricky to perceive this due to the effect not being reset when a different slot is selected). Below are figures showing the Pd interface and the mobile interface, respectively.\n\n\n \n Pure Data interface\n\n\n\n \n MobMuPlat interface\n\n\nImplementation\n\nThe actual sampling is done by writing from and reading to arrays via tabread4~ and tabwrite~, and controlling the index with a phasor~ object. I initialize one instance of the sampler per voice. I added functionality to divide the sample in up to 32 parts, and to offset the playhead by a chosen percentage of the total sample length. One interesting way of using this feature is to “scan” through the sample with a slider for interesting grains, for instance by looping the sample at 1/16 of the sample length. I also added an LFO to modulate the playhead position. If the LFO is set to anything greater than zero, it will modulate the playhead position at the rate of the chosen frequency. In order for the playhead index to stay within the array length, I multiplied the unipolar LFO value with half of the current sample rate times the chosen offset percentage. In other words, an offset of 100% with an LFO rate of 0.5 and a sample rate of 44100 oscillates the playhead position between 0 and 44100 every 2 seconds. Upping the LFO rate even more, this creates interesting artifacts that remind of the sound of scratching a vinyl record. This actually makes a lot of sense, as it is essentially oscillating between playing nothing and playing the full sample.\n\nAn important part of the application was to able to control four voices with the same set of GUI objects, i.e. not have four sliders per effect per voice. The way I implemented this was through the routing abstraction showed in the figure below. When a new value comes in in the left inlet, it gets routed and forwarded to the outlet that corresponds to the correct sample slot. The trigger object was initially connected to the prepend object to trigger a routing when a new slot was selected. However, I figured out that it made more sense interaction-wise to only let new values trigger the routing. In order to have more than four voices, I could simply have extended the amount of slots and added some outlets in the router abstraction.\n\n\n \n Routing\n\n\nReflection\n\nLooking back, I’m quite satisfied with the interface that I made, considering the time spent and my initial skill level in Pd. I tested out my interface on the bus after the presentation, and it is indeed quite fascinating to create soundscapes by manipulating the sounds that are in your immediate presence. It is not an interface I would believe to have very high value in a music production setting, but I believe the interface affords what I wanted it to do. However, there is always room for improvement.\n\nFor future projects, I want to dive deeper into the actual signal processing part of Pd. Working with audio signals is very interesting, and I’m for sure going to spend more time looking into it in the future. My next step will likely be to learn more about sound at even smaller chunks of samples, and to manipulate grains of sounds to create rich textures.\n",
        "url": "/sound-programming/2020/02/10/Sampler.html"
      },
    
      {
        "title": "Practice-Toolbox-for-students-of-music",
        "author": "\n",
        "excerpt": "In our audio programming course we were tasked to make a PD-patch without any restrictions on what it should be. I wanted to make something useful I could incorporate in my daily practice routine, and also distribute to some of my guitar students.\n",
        "content": "Overview\n\nIn our audio programming course we were tasked to make a PD-patch without any restrictions on what it should be. I wanted to make something useful I could incorporate in my daily practice routine, and also distribute to some of my guitar students. I set out to make a practice toolbox wich would cover several different topics, but where the main focus is on rhythm, time-feel and developing your visualisation of the guitar-neck. I had no previous experience in using PD, so the learning curve during these two weeks was quite steep. Luckily we had great access to help from both Øystein in Trondheim and George in Oslo and I managed to incorporate most of my ideas into the patch. In addition to the patch I also made a MobMuPlat-integration so anyone who choose to use the patch can interact with it on their mobile phone.\n\nThe system\n\nMy main patch consists of several different modules which all serves a dif-\nferent purpose. The modules labeled “rhythm generator” was my\frst addition to the\npatch. It consists of a metronome which cycles through 4 bars in 4/4, with an\naccented click on the\ffirst beat of each bar. I used the counter-object from Cy-\nclone and also made a makeshift spigot-object. It is just a sub-patch consisting\nof several spigots to clean up the already messy brute-force infested patch.\nIn my formula generator I got good help from Georgios to fi\fgure out\nhow to generate a set number of random numbers where no number is repeated\ntwice. Luckily for me I could just use the urn-object from cyclone and the uzi-\nobject to complete this task. I group the numbers into a list, and sort them to\nmake it look nicer. For further understanding of this modules purpose I would\nrecommend the book An Improvisers OS - by Wayne Krantz. I also used the\nsame technique to generate the random patterns in the h-radios in the module\nbelow.\n\n\n \n Formula generator\n\n\n\n \n Rhythm Generator\n\n\n\n \n MobMuPlat interface\n\n\nThe main metronome is what I spent most of my time working on. The\nmetronome mechanics was learned from Rafael Hernandez on Youtube, with the\nratelogic for the subdivisions. The rest I fi\fgured out through trial and error.\nThe cycling through the subdivisions was the hardest part. I only managed to\nget it working when the time signature is set to 4, due to some weird behaviour\nin the counter object that I didn’t understand.\n\n\n \n Metronome\n\n\nMy looper/recorder I also picked up from a YouTube-video. I experienced\nsome problems with this particular module. The\ffirst time it starts, it has to\ngo through the whole length of bars before the recording starts. And after that\nyou get a count-in for three beats. 4 or 2 would of course be better, but it does\nits job. The goal here was to make a tool for practicing time-feel and sense of\nform. By including a click in the abstraction you will easily hear if you’re on\nor of the grid. This works of course only if you let the sound out through your\nspeakers.\n\n\n \n Looper\n\n\nAs a fi\fnal touch I routed everything into objects in MobMuPlat and\nmade a decent looking GUI for myself and my students to interact with.\n\nReflection\n\nAs a\ffinal conclusion I think that most of what I wanted to achieve with this\nproject was accomplished. As a newcomer to the world of Pure Data the learning\nexperience was quite good. I spent a lot of time on debugging and planning.\nAnd many sleepless nights to make this work and look nice. This patch should\nwork as a great resource for lots of hours of practice and has the potential of\nbeing used in several different ways.\n",
        "url": "/sound-programming/2020/02/10/Practice-Toolbox-for-guitar.html"
      },
    
      {
        "title": "Strumming through space and OSC",
        "author": "\n",
        "excerpt": "A gesture-driven guitar built in Puredata and utilizing OSC\n",
        "content": "OSC Guitar\n\nMy second project for the audio programming course was to accurately simulate a guitar strum in Puredata using the sensor information sent from a mobile device in motion. I wanted its interaction to be reflective of the direction, speed, and acoustic uniqueness of a real guitar strum. By uniqueness I mean the qualities of a strum that are not necessarily intended by the musician, like the intensity of each string strike and the delay between each individual string as the fingers (or pick) slide across the strings. These additions make both the sound and the experience of strumming quite realistic and, as implemented in this patch, benefit the realism of the virtual strum. However, a number of unforeseen difficulties made the feat of a seamless gesture-to-sound production quite a challenge. As I will discuss, one of the greatest barriers to responsiveness was network latency and the inability for native-local interfacing within both Puredata and sensor data on my mobile device.\n\n\n    \n    Frontpage of Pd patch\n\n\nStrings and Things\n\nI began the project with a detailed search for an accurate model of a string. While I could have built a simple string, the focus of my work was integrating the string model into a framework from which it could serve as a guitar. The string was built using a digital waveguide model by Edgar J. Berdahl and Julius O. Smith. Their model is quite good, both to the ear and to the standards of Stanford’s Department of Music.\n\n\n    \n    Waveguide model of a string\n\n\nHowever, the model was not designed in a way to be used in polyphony. To correct their design, I renamed each array, send, and receive to be unique for each instance of the waveguide model using a prefix of “$0-”. This allowed each object to be instantiated with a unique identity and as a result, enabled the messages passed by each of the six strings to exist without overwriting potential shared arrays or variables. I also reformatted some of their code to better fit my purposes like adding inlets and outlets that would communicate with the main patch and the six strings in concert. I also cleaned and reorganized their layouts to make more functional sense.\n\n\n    \n    The guitar model sub-patch\n\n\nOnce the strings were in place, I assigned the fundamental frequency of each string of four chords to four messages and made a simple metronome to send a chord pattern to play the chords (for a non-interactive demo). To create a delay between each string I used the “pipe” object prior to reaching the waveguide model. The delays’ right-hand argument allows for an input number, whose sign (positive of negative) determines whether the string delays will cascade downwards or upwards. The expression also considers randomness by calling six variables that were assigned six random numbers included in the sub-patch “rnd-strings”.\n\n\n    \n    Sub-patch for the randomizer\n\n\nThese random variables are created on every “strum-bang”, a global variable used to send a bang when there is a strum (either from the metro or OSC). The random variables in the delay increase or decrease the total delay time (and is then factored by the acceleration of the strum). After the audio signals are generated, they have their amplitude reduced by the same random factors used before. There is then some light panning that pans the mono signals of the top two and bottom two strings. This is finally sent to the dac.\n\nThe Notorious OSC\n\n    \n    Sensors2OSC\n\n\nThe last step was into employ a method to send OSC messages from my smartphone’s accelerometer. I used the Android app Sensors2OSC to send OSC messages from the gravity and linear acceleration sensors in the phone. The “mrpeach” external library facilitated receiving these messages into the patch. I chose the gravity sensor over the accelerometer sensor because the tilt values that correspond to rotating the top of the phone downwards and upwards were distinctly positive and negative as the top the crossed over the y-axis (horizon).\n\n\n    \n    Receiving and manipulating OSC data from the phone\n\n\nTwo mechanisms were created to trigger a bang from a strum using the phone. The first opens or closs a gate depending on whether or not there is a sufficient change in the tilt of the top of the phone from the gravity sensor. The second mechanism determines if the acceleration of the strum is enough to trigger a bang. In its design, the second impulse generator can only be activated if there is enough of a properly shaped strum gesture. In addition to the thresholds preventing any rapid activation, I also created a refractory period that prevents a bang from being sent until the previous strum has completed from calculating strum duration. The impulse that is sent when a strum makes it through the thresholds is a positive value from the linear acceleration. The sign of this number is then changed depending on whether the strum was down or up (using the sign of the gravity sensor’s change). This number determines the speed and direction of the strum. If all thresholds were tuned perfectly and there was zero latency (my next point), this would allow rapid strumming of the virtual guitar.\n\nComplications\n\nUnfortunately, one of the first issues I recognized was the latency that appeared from communicating over the local network. The latency was also inconsistent as well as the message rate that was being received (not to mention messages dropped). There were some moments where the speed and response of the OSC messages was incredibly fast and at other times inoperably slow. Because of this, a good portion of my time was spent finding threshold values and averages that catered to the message rate and latency. There is also the issue of this configuration being device dependent. Different smartphones will likely have different sensor hardware, sensor names, ranges, rates, etc. so this patch is certain to need some configuring in each case. The initial goal of this project was to build the app in Pd and afterwards “simply and easily” port it over to the MobMuPlat application for Android and iOS. However, this app does not appear to read the necessary sensor data natively and there do not appear to be alternatives in hosting Pd patches on Android. For the time being, this patch will have to be hosted on a remote computer and receive OSC messages from a device connected to the same network.\n\n\n    \n    Video demo of the OSC functionality - credit: Thomas Anda\n\n\nThe audio within the video was recorded internally (oddly the pops only appeared after video editing - my computer was struggling with that fabulous transition).\n\nAll in all, this project was a great introduction to Puredata and definitely taught us the general principles of the language as well as some of the challenges you only encounter in hour 11 of debugging. While making this a standalone app would require a bit more investigation into MobMuPlat or some other Pd host-able interface, I think my goals were met. And when the network connection is good and the thresholds are polished, it’s quite surprising how responsive a network connection can be.\n\nMy project’s code can be found here.\n\nWorks Cited\n\nIglesia, Daniel. Monkeyswarm/MobMuPlat. 2013. 2020. GitHub, https://github.com/monkeyswarm/MobMuPlat.\n\nMobMuPlat - Mobile Music Platform. http://danieliglesia.com/mobmuplat/. Accessed 7 Feb. 2020.\n\nPlucked String Digital Waveguide Model. https://ccrma.stanford.edu/realsimple/waveguideintro/. Accessed 7 Feb. 2020.\n\nSensorApps/Sensors2OSC. 2014. SensorApps, 2020. GitHub, https://github.com/SensorApps/Sensors2OSC.\n\nSensors2OSC - Sensors2. https://sensors2.org/osc/. Accessed 7 Feb. 2020.\n",
        "url": "/sound-programming/2020/02/10/osc-guitar.html"
      },
    
      {
        "title": "Sonification of plants through Pure Data",
        "author": "\n",
        "excerpt": "I am not sure if I am going crazy or if I am actually interacting with plants, but here me out here\n",
        "content": "Introduction\nFor this project I wanted to do something I had heard was possible, but never had experienced for myself. The consept has often been used as an explanation for “plant intelligence” and proned as i am to thinking in those ways, I am not totally convinced. Yet…\n\nI wrote a journal throughout the project and have chosen to publish the entire thing here as a way of capturing the essence of the creative process. Feel free to skip to the bottom where I have written a more comprehensive summary of the project, including the performance.\n\nJournal\n\nOk, my goal now is to make this thing sound a bit more “humane”, maybe to the point that it could be used as a legit instrument.\n\nWhat I really miss from the original idea is some sort of percussion and melody.\n\nThat’s why I decided to implement a step sequencer. I found some inspiration on the internet and made a very simple one from a metro, a selector and a lot of sliders to control the pitch of the oscillator.\n\nI don’t know what happened just now, but it suddenly struck me that I wanted to do something with binaural audio. It should be very simple, just having two oscillators (one for each ear) and pitching one up a couple of hertz. (this was a total digression, but the patch is still in the folder).\n\nDay two of actually working on this!\n\nThe step sequencer I added suddenly got a purpose: I want it to make random melodies from different scales, according to the input value!\n\nAaaaaand creativity struck!\n\nI had the idea that I wanted to make some sort of melody, but I didn’t know what it should be. I figured that I might use five different pre-determined melodies for the five different states of the amplitude on the input, but somehow the idea of making this pre-determined didn’t seem right.\n\nThen I figured that I would make it random. Not just a little random, but very random. Like a “random” object feeding into another “random” object kind of random! But still, it would need to reflect some sort of “mood” in the output of the thing. What if I restricted the randomness to scales? This way, state one could be major, state two could be phrygian, state three could be minor, and so on and so forth.\n\nWOW!\n\nThe weirdest thing just happened.\n\nI am not sure if I am going crazy or if I am actually interacting with plants, but here me out here:\n\nI soldered the pins of an XLR-plug to three leads, connected to alligator clips. The XLR was then connected to the input of the sound card in order to capture the differing conductivity in the material connected to the leads.\n\nI touched the ends of the clips together, and sure enough, the signal spiked.\n\nSince I have a large plant right by my desk anyways, I immediately connected the clips to this plant to find out once and for all if they really do have intelligence. Expecting to be disappointed, I calibrated the input so that the baseline current floated around 0 and 1 on the output and held my breath as I started interacting with the plant.\n\nTo my great surprise and delight, the signal started fluctuating once I touched the plant and the more volatile the interaction, the greater the fluctuation in the signal. WHAT?!\n\nOk, there has to be a good explanation for this.\n\nOf course! The noise I get on the signal comes from the cables I attached to the plant hitting each other and therefore creating “mechanical” noise! I tried to mess around with the cables, shaking them; nothing… No major fluctuations in the signal.\n\nAha! I was the one who interfered with the signal with the conductivity of my skin, that must be it! So I put on a pair of plastic gloves and repeated the process. I almost did not believe that the fluctuations in the voltage still occurred at the input!\n\nMaybe the fact that I rattle the plant made the change, so that the teeth on the clips grinded on the stem of the plant and thereby caused the reaction? That had to be it. I therefore preceded to pinch my green friend while still wearing the rubber gloves, making sure not to rattle the leaves or cause any form of movement in the leaves. Still, there was a reaction! The graph in the array and the sound produced corresponded perfectly with how hard I pinched, almost like a pressure sensor.\n\nI am sure there has to be a good explanation for this, but right now I am too ecstatic from the results to have any sort of critical opinion on this.\n\nAllright.\n\nAfter the initial excitement of achieving interspecies communication, I proceeded to do two things:\n\n1: Clean up the patch from something resembling a drunken bird’s nest to something a lot neater. It is truly amazing what one can do with some “send” and “receive” boxes and nudging objects around to be symmetrical.\n\n2: Making the thing sound nice.\n\nThe original patch used a very basic sawtooth oscillator as the sound-generator for the instrument, but since I decided that I wanted only one voice, this started to sound a bit thin by its own. A fuller sound was achieved by adding more oscillators and multiplying them slightly, to spread them out in the frequency spectrum.\n\nI wouldn’t call the sound it makes now “beautiful” but I deem it a lot better than when I started.\n\nThe performance\n\nTo be honest I was very nervous before presenting for the class, since all the other presentations before me (all the other presentations before me) involved patches that were so much more complex and sophisticated than mine. I want blame the lack of time I had to prepare for this due to the seeming conspiration made by the rest of the universe to sabotage my work in PD up to this point, but it was probably a bit of my planning as well.\n\nDespite my nervousness, I thought the performance went pretty well.\n\nThis might be flattering myself, but I think that me not focusing too much on making the patch any more complicated or overly technical than absolutely necessary was of great help to the creative process and facilitated “out of the box” thinking.\n\nI guess that the act of bringing a live plant as as the interface for my instrument to class and showing that the patch really worked helped a lot as emotional support during the performance.\n\n\n&lt;/img&gt;\nPhoto credit to the great Thomas Anda\n\n\nReflections on the process of learning Pure Data\n\nAlthough I had experience in PD from earlier projects done here at SMC and some knowledge of MAX MSP from my bachelor’s degree, the last two weeks has been a quantum leap when it comes to learning the software.\n\nI quickly adopted the process of figuring out where I wanted to go with the idea and then making a patch for it, not the other way around. Having creativity drive innovation has been my preferred way of thinking ever since I in 4th grade refused to learn to do division on paper until the teacher presented me with a real life situations where I had to and this was no exception.\n\nI feel like so many examples on the internet of the use of Pure Data are done only for the sake of making a patch in Pure Data. In my opinion, the reason for making a patch in PD and not using some product that already exists on the internet should be because the solution to what you want to do does not already exist. The patches presented by my classmates today really reflect the same idea by almost exclusively being original and novel ideas.\n\n\nA video of me demonstrating the patch\n\nAbout the patch\n\nThe patch is made entirely out of PD Vanilla -objects.\n\nEvent the reverbs are plain vanilla, so do I need to credit them? If so I will credit the entire Pure Data community.\n",
        "url": "/sound-programming/2020/02/11/plant-interface.html"
      },
    
      {
        "title": "Making Noises With |noise~|",
        "author": "\n",
        "excerpt": "Wherein I attempt to either program paradise or just make bacon-frying noises, which could be the same thing, actually\n",
        "content": "Nice Weather\n\nThe project I undertook was to design several soundscapes derived from nature and implement them in a mobile application. The sounds I attempted to model were Ocean Waves, Wind, Rain, and Thunder. I also included a simple filtered-noise generator for my wife, used to help mask external noise while sleeping.\n\n\n  \n  How It Works, More Or Less\n\n\nOcean Waves\n\nThe ocean wave generator uses three nearly identical “engines” panned left, right and center. In each engine, white noise is modulated by a low-frequency sine wave to produce the basic slowly-swelling undulation of an ocean wave. For additional character and variation, I piped this through a low-pass filter with a cutoff frequency varying between 30 and 400 hz, and then automated this value using metro and random objects to create an element of unpredictability to the sound.\n\nThe last element in the ocean waves generator is a little filtered white noise cut off at 218hz, to provide some background rumble. I had noticed that the randomly-generated waves occasionally would conspire to align their cycles and result in a split second of near-silence, as if the sea had suddenly evaporated. The background noise keeps this apocalypse a reasonable arms-length away, and also smooths out the sound in general. Pleasant!\n\nThe panning of the three main elements does a pretty good job of giving the impression of waves crashing to the left, right and center as you stand on the beach. A little imagination helps.\n\nWind\n\nAs it turns out, the sound of wind (at least, a certain kind of wind) is not altogether unlike ocean waves in its basic implementation. The patch takes the same form, using a low-frequency sine wave to modify noise to produce a slow undulation in sound.\n\nThis time, however, I used a very narrow bandpass filter to give the resultant sound a little pitch, then automated the cutoff frequency to randomly range between 300-600hz. The automation is set a bit faster than the waves, causing the sound to rise and fall like the wind whistling through a high mountain pass in an old Western, say the 1969 classic “Paint Your Wagon”, which just so happens to feature Harve Presnell singing “Mariah”, a song about the wind.\n\nIncidentally, also featured in the film is a young Clint Eastwood, who sings(!) “I Talk To The Trees”, which might be of some interest to my colleague Gaute Wardenær and his attempt to communicate with his houseplants.\n\nRain\n\nThis is where things started to get interesting. Rain is pretty complex and, needless to say, can take a great many different forms. Since a full-on downpour is pretty close to full-spectrum noise, I tried to reproduce something slightly more subtle, a pretty good soaking shower that creates a variety of distinct sounds as millions of droplets strike and then run off the roof of the porch, drip off the leaves of the trees, and soak the load of clean laundry you forgot to bring in before it started raining.\n\nThe rain engine has five elements. The first is just filtered background noise to keep things ambient. Another element consists of noise modulated by noise filtered at a low frequency and then bandpass filtered at 400hz to create a rough, muffled series of low-frequency spikes that remind me of rain on a well-insulated roof (if you’re inside). Intrepid observers will also note this part of the patch’s similarity to the Thunder generator, coming up next.\n\nThe next three elements create the main body of the sound, and get quite interesting in their implementation. Once again I used noise modulated by a sine wave as in the prior examples, only this time the signals interact at huge, city-block-leveling amplitudes. I experimented around quite a bit with the numbers, always being sure to view the waveform on the array before piping it into my ears. What one gets is gigantic amplitude spikes that are a bazillion times (I’m using scientific terms here) greater than the mean. Greatly reduced in amplitude and filtered, one gets either a series of random pops like individual raindrops falling on a old Samsonite suitcase, or, depending on the relative hugeness of the numbers, a more even sizzling sound that could be the bulk of the rain coming down or bacon frying, depending on whether or not you’ve had breakfast yet.\n\nThunder\n\nAnd now the good bit - a little distant thunder to go with the nice, soaking rain shower. Using the same principle as the low-frequency rumble generator used in the Rain patch, combined with a couple of line objects to create a slowly fading boom, and the sound is pretty close to distant thunder already. I put two of these elements side by side, filtering the second at 100hz and delaying its bang by 4000ms to create the “aftershock” of a good crack of thunder as it echoes off the mountains several kilometers away.\n\nThere is a lot that could be done with this patch to introduce some randomizing elements, but for now it’s one kind of thunder.\n\nFiltered Noise For My Wife To Sleep To\n\nI probably don’t need to explain this one, but it’s included for its simple appeal: heavily filtered, low-frequency noise to sooth you to sleep. Zzzzzzz.\n\nConclusion: The Application\n\nAll of these patches were combined into an app using MobMuPlat! Now I can take the weather with me everywhere I go. It’s especially fun, and perverse, to listen to the rain on headphones, while walking home in the rain. Who does that? I do, apparently.\n\n\n   \n   Mesmerising GUI\n\n\nFor all of these patches, I am deeply indebted to a series of online tutorials by Obiwannabe, who I believe to actually be Andy J Farnell, author of an excellent book on Pd sound synthesis (Farnell, 2010) . I also did a bit of reading in Tony Hillerson’s book, Programming Sound With Pure Data (Hillerson, 2014).\n\nReferences\n\nFarnell, Andy. Designing Sound. Cambridge, Mass: MIT Press, 2010.\n\nHillerson, Tony. Programming Sound with Pure Data: Make Your Apps Come Alive with Dynamic Audio. Pragmatic Programmers. Dallas, TX: Pragmatic Bookshelf, 2014.\n",
        "url": "/sound-programming/2020/02/11/Making-Noises.html"
      },
    
      {
        "title": "[ pd Loop Station ]",
        "author": "\n",
        "excerpt": "This is an attempt to create a Loop Station with features that I wish I had in such a pedal / software.\n",
        "content": "[ Overview ]\n\nWhen you switch from being the user of a product, to become the one who is building the product that you need yourself, things change completely. This is an attempt to create a Loop Station with features that I wish I had in such a pedal / software.\n\nWith four independent channels routed to integrated FX busses, such as pitch_shift and reverb, this Loop Station aims to attend several needs of artists that use loopers to jam in live performances or as tool in the creative process of composing.\n\n\n   \n\n\n[ Features~ ]\n\nThe Loop Station is designer to be used for live performances, but also, as a tool that can be powerful to help with the creative process composing and arranging songs. So, in addition to the usual features mentioned above, I decided to risk my sleeping time to implement a drum beat trigger based on the sounds generated by the users using any surface, from the mouth like a beatbox or using one’s own body as a percurssive surface.\n\n\n  Inputs: Line In / Microphone / Midi\n  4 tracks (independent)\n  Unlimited overdub cycles for each track\n  Record / Save each track in unique files\n  Record / Save Master Mix\n  2 Fx BUS: Octave / Reverb (Pre Loop / Post Loop)\n  Built-in Drum Machine with 2 modes (Body Drum / Pad Beat)\n\n\nWhen implementing the features mentioned above, I started by the most important first: the loop record and playback functions. To create the overdub, a bang feeds a counter which goes through selections and bang triggers that pass the signal in the right order to calculate the time/size of each sample, record, stop recording and play it in a loop. This way is possible to start a recording tapping one key once, and if you tap again it stops interrupts the audio input that is sending to [tabwrite~ ]. Then the [timer ] is activated, so we can extract the exact duration of the sample recorded and send it to the array receiving the loops. A [line~ 10] was used to open/interrupt the signal without pops and clicks on the sound output.\n\n\n   \n\n\n[ Drum Machine ]\n\nPlay drums using your body or any other surface as the beat generator! When implementing the Drum Machine, my intention was to provide a creative way to generate beats in a Live Loop Performance, creating an expressive and spontaneous experience within the process of composing and improvising layers in a song. To make it happen, Pure Data has to detect an acoustic audio input distinguish it by frequency region and attack, and by these features, bang the assigned beat samples for each type of audio received.\n\nWhen designing this function, I did several tests using the objects [bonk~ ] and [fiddle~ ], to see which one would be more accurate detecting and reading the audio input features and unpacking it into distinct data. This process is very useful to trigger sounds based on specific aspects of the acoustic sound produced. After testing different sound sources and initial arguments for each one of the objects mentioned, [fiddle~ ] proved to be more accurate to the task. It detects attacks and the amplitudes very well, and send the domain frequency and its partials unpacked in numbers. Which is very useful to trigger different beat samples based on the frequency range and attack of the specifc sounds produced by hi-hat, snare, and bass drum.\n\nIt works cool, but it is still very unstable and needs a silent and controlled environment to avoid random frequency and attack detection. I am still developing this function, refining the arguments, specially the amplitude and attack, to filter the audio information in a way that makes it more controllable. “Audio is wild”, as mentioned during the course. To get around this issue, while I am developing this function, I provided two modes for the Drum Machine function, one is the described above Body Drum, and the other is the Beat Pad, in which I assigned the beat samples to different midi notes, in a more common, but stable approach.\n\n[ Conclusion (\n\nThe process of designing such functionalities opened my mind to a huge amount of new information regarding audio programming signal flow, digital audio processing and effectively made me understand many of the issues and limitations encountered within my experience using equipment, hardware, and software when performing or producing music. To design a Loop Station with all the desired functionalities, is still a challenge, especially when it comes to playback latency and delay, tempo accuracy, time manipulation, artifacts, and synchronization. All in all, it is a valuable way to learn audio programming.\n\nTusen takk!\n\n[ Prototype working: (Demo)nstration ]\n\n\n\n\n\n",
        "url": "/sound-programming/2020/02/13/The-Loop-Station.html"
      },
    
      {
        "title": "The Immersive Portal",
        "author": "\n",
        "excerpt": "The SMC portal is rigged for ambisonics to fit a virtual classroom in a classroom\n",
        "content": "Bringing Ambisonics to the Portal\n\nInitial tests on a simple binaural setup between the Trondheim and Oslo portals gave some insights into what rendered “effective” sonic placement, and also what: not so much. Using the IEM Software Suite, we were able to create an ambisonic environment over a two classroom setup. There was a bit of difficultly regarding the previous complexity of our configuration involving the Lola system.\n\n\n    \n    IEM Room Encoder\n\n\nHere are some screenshots for those who are interested of what it would look like in the Reaper DAW.\n\n    \n        \n            \n            Reaper setup\n        \n    \n    \n        \n            \n            Reaper routing\n        \n    \n\n\nResults\n\nWe found that the experience of listening to our classmates on the other side of Norway was enriching and offered a massive improvement of intelligibility. We were able map each of microphones that we sent into a virtual space (and receive in the same way) and as a result hear the input from each of the microphones as a point in a virtual room (complete with reverb). Our virtual room had the same dimensions as the classrooms we were currently in. Thus, we hoped to put our classmates in Trondheim into the Oslo classroom and vice versa.\n\nHaving room-dimensions set correctly - and a virtual reverb emulation providing spatial reflections entailed some confusion. After some massaging of equalization, as well as a factoring for the different refractional qualities of the walls, floor, and ceiling - a more “natural” rendition of the room gave a wholly more natural experience of our space in our headphones.\n",
        "url": "/networked-music/2020/02/24/immersive-portal.html"
      },
    
      {
        "title": "Sonifying The Coronavirus Pandemic",
        "author": "\n",
        "excerpt": "Finding a voice for difficult data\n",
        "content": "Introduction\n\nFor this sonification project, our team chose to map the contemporaneous spread of the Coronavirus from China to the rest of the world. This is obviously a phenomenon that is ongoing, so being able to update the sonification as new data came in was an important consideration.\n\nAfter initial discussions, we developed a game plan to create our sonification using widely-available data from John Hopkins, CDC, etc. on the following parameters of the pandemic:\n\n\n  Confirmed cases\n  Total Recovered\n  Total Fatalities\n\n\nOur initial idea was to create a worldwide “orchestra” of sounds by superimposing the layout of a traditional symphony orchestra over a map of the world. By rotating the symphonic layout 90 degrees counter-clockwise, we centered the strings over China with the rest of the orchestra fanning out in a westward direction (see fig.1). We quickly realized that it would be better to assign instruments by general region rather than country, because with over 80 countries reporting at the time of this writing, we would not only quickly run out of instruments, but also run the risk of creating sonic soup out of the data.\n\nFigure 1 shows our initial thinking on how to approach the instrumentation by region.\n\n\n    \n    Fig 1 Initial Instrument Assignments By Region\n\n\nThis mapping made sense to us, as the strings make up the traditional core of the symphony orchestra, and situating them over the “epicenter” of the pandemic puts the bulk of the instruments in the same area where the bulk of the data is coming from. At the same time, our hope was that the remaining instruments, representing areas with much less data, would still be distinct in the overall soundscape.\n\nOur plan was to sonify the data in the following way:\n\n  Daily-updated confirmed cases data for each region would be assigned to pitch data and sent to individual instruments in a DAW. As the number of cases reached a certain threshold, a new note would be triggered, and the sound would become more complex.\n  Recovery data would be represented by a percussion instrument, such as crotales, which would sound a single note each time the data crossed a pre-set threshold.\n  Fatalities would be represented in the same way as recoveries, using a lower- pitched percussion instrument such as the tympani to place it in a different sound spectrum to keep it distinct.\n\n\nAs we will describe later, we decided to drop the second and third points in this plan, focusing only on the confirmed-cases data. However, we did add one additional data stream we had not noticed initially, the daily rate of increase in the numbers. After observing a few large spikes in these numbers, we decided that this was important data that could not be ignored.\n\nImplementation\n\nPython programming for real-time data delivery was done by Simon in Trondheim. The program is designed to grab the needed data from the web and send it to Pd as OSC messages. Although we did not wind up using the Python program in our final demonstration on Friday (06.22.20), as the Oslo contingent had set up Pd to read data from a static csv file, this work will not be wasted, as we will need this to further implement our sonification in a dynamic webpage (see: Future Work, below.)\n\nRayam, Paul, and Dongho on the Oslo side collaborated on implementing the sonification in PureData and Logic. The first task was to separate the data by region, and assign an instrument or set of instruments to each. We created eight regions, with each region’s center point represented by it’s latitude and longitude. See fig. 2.\nEvery reporting country’s coordinates are compared with these set numbers to determine which region they belonged in. Using the Pythagorean theorem (a2+b2=c2) we triangulated the location in comparison to the predetermined set points, and then sent each data string to one of eight channels assigned to each one of the selected regions.\n\n\n    \n    Fig. 2 Final Orchestral Layout\n\n\nEach incoming data string consists of Country, Lat, Long, Day #1, Day #2, etc. Each day’s number represents the increase in either confirmed cases, recovered cases, or deaths. Using the (zl slice) object in Pd, we separate the map coordinates from the daily numbers. Each data string is then separated by region, using the method described above. The daily numbers are used to trigger midi notes, a new note being added every time the daily numbers exceed a certain threshold, and sent to the DAW.\n\nAfter working with the “confirmed cases” data for a while, we became aware of an important parameter in the numbers that we had initially ignored. The day-to-day cases occasionally registered huge spikes in the numbers that seemed quite significant. We wished to devise a way to represent this in the sonification. For this reason, we decided to focus on only the sonification of the confirmed-case data for this stage of the project, in consideration of our time constraints.\n\nIn order to make sense of the spikes in the numbers, we created a sub-patch in Pd that compares each day’s number with the previous day, subtracting the prior day’s number from the current. This number is converted to midi note information and sent to DAW, triggering an arpeggio that becomes more complex as the numbers grow in value. Hence, larger daily spikes in daily case numbers are reflected in the soundscape as arpeggios covering a wider note range.\n\nFigure 2 shows the assignment of instruments by region. Instrument one represents the daily increase in confirmed cases, while instrument two represents the rate of increase between consecutive days.\n\nImplementation in Logic was reasonably straightforward, though involved quite a bit of routing work and setting proper ranges for each instrument, setting up the arpeggiator, etc. We tried to stay within “appropriate” ranges for each instrument, although properly constraining each instrument with its idiomatic bounds would take more time than we had for the project.\n\nSonification\n\nThere was a huge amount of data to work with, and we needed to narrow our focus on just a few data points in order to be able to create a sonification that was faithful to the data without creating an aurally confusing soundscape. Therefore, we chose to work with only the confirmed cases dataset, from which we also derived the daily rate of increase. Extracting the data, making sense of it in Pd, then configuring the DAW took quite a bit of time, with much thanks to Georgio, Jarle and others for their invaluable input.\n\nEach day’s worth of data is allotted 3000ms of sonic space to unfold. Each region’s data is delayed slightly from the prior region to allow the data sonic space to be better understood by the listener. See fig. 3.\n\n\n    \n    Fig. 3 Delay Onset By Region\n\n\nIn the final mix, care was taken to level each track so that all instruments could be heard. Binaural panning was used to give each region (and its instruments) its own position in the sonic landscape, though this only apparent when listening through headphones.\n\nResult\n\n\n\n\n\nFuture Work\n\nThere is still much that could be done to fully implement this data.\n\n\n  \n    Implement more data. The recovery and fatality data could be added to the overall sonification, with some care taken toward not overloading the listeners’ ears with too much information. One possible way to convey meaning with the recovery and fatality data is to consider them as “resolved” cases, and at set threshold levels in the resolved cases, remove a commensurate amount of confirmed cases from the sonification. In this way, the composition would eventually come to an end as the coronavirus is either contained and defeated.\n  \n  \n    Instrumentation. We feel that the data is mostly well-represented by our instrument choices, but with careful listening and experimentation, it’s quite possible that we could improve the clarity of the sonification by changing instruments, adjusting range parameters, etc.\n  \n  \n    Dynamic webpage. A very worthy addition to this project would be to create a dynamic webpage that incorporates a time-elapsed heat map to provide a visual reference for the unfolding sonification, as well as tools for the viewer/listener to rewind, change speed, and play the data by region. This would involve the creation of an online orchestra using a sample library, as we would not be able to use Logic. Also we would need to implement the ongoing updates pulled from the GitHub database that Simon created tools for in Trondheim.\n  \n\n\nConclusion\n\nAlthough there is a lot more work that could be done to fully implement all of our ideas, we are encouraged by our results and will continue to work towards a full implementation of this sonification.\n",
        "url": "/sonification/2020/03/09/Sonifying-Corona.html"
      },
    
      {
        "title": "Weather Music",
        "author": "\n",
        "excerpt": "Experiencing weather is a multi-sensory and complex experience. The objective of our sonification project was to sonify the weather through the use of online video streams.\n",
        "content": "Introduction\n\nWeather is a multi-sensory and multi-dimensional experience. With this, we mean that weather can be described both quantitatively, as temperature, down-fall, brightness, etc., and qualitatively such as “a beefy sunset” or “a crappy sunset”. With the use of the available tools to our team, our group set out to translate these experiences to sound! Our objective was to sonify the weather based on online webcam live-feeds in space through spatialized audio.\n\nRoles:\n\n  Aleksander - Python Implementer/ Programmer.\n  Jarle - Puredata Implementer/ Programmer (Sound Engine)\n  Gaute - Puredata Implementer/ Programmer (Spatial Panner)\n  Tom - Make fancy slides, documentation, and contribution of ideas\n\n\nSofware Dependencies:\n\n  Python 3 with PIL, numpy, opencv2, color-science, pythonosc, argparse, datetime, time and pytz.\n  PureData Vanilla with Cyclone and Else.\n\n\nImage extraction\n\nThe frame extraction is based on pythons PIL and color-science libraries in particular and reads consecutive frames from your computer screen. The program extracts weather information by collecting color temperature values based on RGB-averages from nine equally large sections on your screen, as seen in figure 1. Additionally, we extracted the color temperature average from the entire image, grayscale histogram information (if the entire image is darker or brighter on average), and time values from a specified timezone for more accuracy. This data gave us a good indication of what the weather was like and what the image/camera feed looked like.\n\n\n   \n   Figure 1 - Screen segmentation\n\n\nHowever, we decided to scale down our data intake by focusing on four squares, as seen in figure 2, only extracting color temperature averages. This enabled us to focus on the color sonification and calibration more clearly given the limited project time-frame.\n\n\n   \n   Figure 2 - Four corners\n\n\nSonification\n\nThe sound engine was built in PureData and features note generation based on a Markov-chain and non-equal temperament scaling. The information received from python is re-formatted in PureData for different usage. For instance, by calculating the difference between frames we were able to distinguish rising and falling color values. The information then triggers pairs of PureData instruments, combinations of harps, Rhodes, chimes, and marimbas. Finally, the music is then run through a reverb before being sent to the speakers. The color temperature average of the whole screen decides which instrument combination is used based on their aesthetic qualities. The individual color temperature values from each square are then used to position the music in space, as seen in figure 3. The higher the color-temperature is in a given square, the louder the volume is in that particular speaker.\n\n\n   \n   Figure 3 - Spatial concept\n\n\nThe audio example beneath is a demo of our sound engine playing through a 24-hour cycle in roughly 2 minutes. Notice how the different times of day affect the combination of instruments.\n\n\n   \n     \n     Your browser does not support audio tag.\n   \n\n\nPractically, this entire setup was achieved using two machines as figure 4 shows. One machine extracts its screen (webcam or image feed) and sends that information to PureData via UDP. From there, we send this data via [netsend -u] to another machine that runs the actual sonification. We decided to design the system like this because we all felt that the idea of representing an image through spatial sound was very interesting, hoping that a user could close their eyes and perceive whether the sun was rising or setting through the location and texture of the music in the room.\n\n\n   \n   Figure 4 - Signal chain\n\n\nFuture Development\n\nIf our project time-frame had been greater, we would have done more testing and calibration before expanding our space into 8 squares with an equal amount of speakers. This would allow for possible expansion of the sonification of different scenes or places at the same time. However, the current product is useful in its own right, however, the said expansion would have been a great addition to create more possibilities for more use.\n\nA future use-case for our technology could be to sonify the output of a live camera at a site. Imagine a vantage point where one would usually go to watch the sunset and sunrise and get a sonification of the experience as well as watching it. The ideal scenario would be to have the panning of the voices done in an Ambisonics encoder so that it could be played back on any number of loudspeakers and in a three-dimensional sound-field as shown in figure 5.\n\n\n   \n   Figure 5 - Expanded spatial concept\n\n\nSummary\n\nIn this weather sonification endeavor, we succeeded in extracting necessary data from video feeds through Python and use that information to produce pleasant spatialized compositions in PureData based on the weather. Although our scope ended up being limited by both time and resources, our project shows that weather sonification through color extraction is very much possible and that spatializing audio to coordinates on a screen is an interesting arena for further exploration. This kind of sonification could be used for installations purposes, like sonifying climate change, or in social applications like aiding the visually impaired by sonifying their surroundings.\n",
        "url": "/sonification/2020/03/09/weather-music.html"
      },
    
      {
        "title": "Soniweb: An Experiment in Web Traffic Sonification and Ambisonics",
        "author": "\n",
        "excerpt": "For the course SMC4046 Sonification and Sound Design, our group was tasked to create a system that collects and sonifies data in real time. For this project, we went even further(!) and spatialized the network data that passes through a computer.\n",
        "content": "Introduction\n\nFor the course SMC4046 Sonification and Sound Design, our group was tasked to create a system that collects data in real-time and to sonify this incoming data. Our initial plan was to filter network traffic exclusively from the web browser and then analyze the HTTP response headers for information regarding the resources that are loaded when one visits a webpage. From there, we would send that data over OSC to Pure Data, a visual audio programming language and generate sound from it.\n\nHowever, we found (after extensive testing with various programs) that HTTPS webpages encrypt the view of the pages’ resources (the s stands for the SSL protocol) - this should have been more obvious at the beginning. This prevents a 3rd party application like Wireshark from spying in on the network activity and reading the data that is transferred. While this is a wonderful standard for the sake of security, it prevented us from going in that direction as Wireshark is unable to decrypt SSL layers. Another application, Fiddler, an alternative to Wireshark, can achieve this, however, its implementation has poor integration with Python and even less on macOS. So our vision broadened to include all possible network activity that is transmitted on a computer and we went with Wireshark as a tried and true staple of the data-sniffing community.\n\nImplementation\n\nWe decided early on that it would be quite interesting if we were able to sonify the locations of the data packets that were being sent and received when one uses the computer. From a research-oriented position, the sonification and spatialization of web data might provide the computer user with insights into:\n\n\n  Where the data you see, while casually browsing the web or using a web-dependent application, actually originates from (digital-physical correspondence &amp; global network monopolies)\n  How one’s device is constantly sending and receiving information, even in a seemingly inactive state (the inextricable internet) and in contrast…\n  How some webpages and applications take much more aggressive approaches in making contact with your device (web tracking &amp; privacy overreach)\n\n\nThese guided our development and led us to tailor the data manipulation towards a sonified experience that could encompass these ideas. This led us to look for databases that would provide a massive list of IPs with associated geodata (country, city and IP address). We ended up with one of Geoip2’s free databases, which appeared to be the most comprehensive, free collection of IP-to-coordinate entries available for this task. With their accompanying python package, we were able to import the database quite easily and look up the source and destination IP addresses of every packet that passed through our Wireshark server (tshark) that would be hosted in Python.\n\nJackson and Thomas were tasked with the development of the network sniffing script in Python. This involved working with a Wireshark wrapper for Python called pyshark which allows us to host a Wireshark instance and read and filter packet data completely within Python.\n\n# Set up capture and filter by host IP and packet size\ncapture = pyshark.LiveCapture(interface=\"en0\",\n                              bpf_filter=\"host \"+host_ip+\n                              \"&amp;&amp; length &gt; 60\")\n\nThis snippet of code constructed the capture stream and specified the network we were listening to (the Wi-Fi), which IP addresses we wanted to filter into our stream (our machine’s host IP), and what size we wanted the packets to be (this was to reduce the torrents of data that may be less relevant to our network activity).\n\nThere wasn’t too much documentation on pyshark’s full capabilities, so we spent a good portion of time getting familiar with the package as well as Wireshark, which the package relied upon. Once we had exhausted the aforementioned possibility of getting metadata from webpages, we began to program a simple packet to OSC script that would send OSC messages to Pure Data(Pd) when a packet was sent or received into the computer. We would have a Pd patch running simultaneously to the Python script to listen to our OSC messages with the port specified. Here’s the code that does this below\n\n# Set up OSC server\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--ip\", default=\"127.0.0.1\", help=\"The ip of the OSC server\")\nparser.add_argument(\"--port\", type=int, default=8888, help=\"The port the OSC server is listening on\")\nargs = parser.parse_args()\nclient = udp_client.SimpleUDPClient(args.ip, args.port)\n\n\nFurther development included generating longitude and latitude from the IP addresses and transforming them into ranges that we could eventually use in the Pd patch. We ended up splitting longitudinal values across 8 positions and 4 positions for latitude. This was a compromise due to the computational expense involved using 32 channels from which we would play multiple samples. Because we filtered in packets that either contained our host IP (the IP of the computer running the script) as the source IP or destination IP, we were able to send a single latitude and longitude pair along with a message giving the direction (sent or received). The protocol of the packet (TCP, UDP, TLS, DNS, other) and its length (or size) were also sent as OSC messages. Here are some of those details if the packet was sent from the host IP. It was also possible to retrieve the city and country as well.\n\n# Sent\nif packet.ip.src == host_ip:\n    dst_resp = reader.city(packet.ip.dst)\n    client.send_message(\"/dest_name\", dst_resp.country.name)\n    client.send_message(\"/direction\", 0)\n    client.send_message(\"/ip_lat\",\n                        round((dst_resp.location.latitude+90)/60))\n                        # 0 to 3 (4 degrees of differentiation)\n    client.send_message(\"/ip_long\",\n                        round((dst_resp.location.longitude+180)/51.43))\n                        # 0 to 7 (8 degrees of differentiation)\n\n    print(\"Sent to \"+str(dst_resp.country.name))\n\n\nSonification\n\nAll the while, Thibault developed the Pd-patch simultaneous to our work in Python. He began with a sample handler that would play five different samples that corresponded to the protocol type, amplitude. We initially attempted ambisonics production through Reaper as a DAW but found a wonderful (and actively developed updated) library for Pd called vstplugin~ where we loaded the IEM plugin suite for ambisonics. The library worked quite well for our purposes and we were able to load each of our 32 channel’s azimuth and elevations as an initialization process. Each of the “voice” sub-patches contain five “samplehandler” sub-patches to play any of the potential five sounds given the protocol message. Within these sub-patches is one “singlesample” sub-patch that plays the sample with “tabread4~”. However, this sub-patch is cloned for times for the possibility of a single channel playing a particular sample up to four times simultaneously. This was built in due to the high frequency of packet data that we receive during the python script’s playback.\n\n\n   \n   The main Pd patch\n\n\nSummary\n\nWhen running the script and patch simultaneously, the network activity that your current machine is relaying is sonified and located in a 5th order ambisonics environment. The various bells ping around the listener as if they were sitting in the center of the Earth listening to the machines around them speak to one another. While we achieved our set goals, there are a few unanswered questions as to whether the packets that are being received are actually too quick to receive and process in real-time and may become backlogged over the course of a session. This may either have to do with limitations imposed by Pd, Python, Wireshark or the CPU power afforded by the user’s computer. In a recent update, converting one of the “if” statements to a dpf filter within pyshark’s instantiation actually made the incoming packets quite responsive to web browsing activity. However, it may be possible to mitigate these issues and enhance the actual sonification by generating the instruments within Pd rather than using sample files. This project has great potential for an extension into a live installation, one, perhaps, that asks the audience to connect their devices to a network hosted in the space so that they could listen to the network activity of their collective devices.\n\n\n  \nA demo of Soniweb (headphones required!)\n\n\nWorks Cited\n\n“Fiddler - Free Web Debugging Proxy - Telerik.” Telerik.com, https://www.telerik.com/fiddler. Accessed 19 Apr. 2020.\n\nGeoIP2 Downloadable Databases « MaxMind Developer Site. https://dev.maxmind.com/geoip/geoip2/downloadable/. Accessed 19 Apr. 2020.\n\nGreen, Dor. KimiNewt/Pyshark. 2013. 2020. GitHub, https://github.com/KimiNewt/pyshark.\n\nPure Data — Pd Community Site. https://puredata.info/. Accessed 19 Apr. 2020.\n\nREAPER - Audio Production Without Limits. https://www.reaper.fm/. Accessed 19 Apr. 2020.\n\nRessi, Christof. Spacechild1/Vstplugin. 2019. 2020. GitHub, https://github.com/Spacechild1/vstplugin.\n\nRudrich, Daniel. IEM Plug-in Suite. plugins.iem.at, https://plugins.iem.at/. Accessed 19 Apr. 2020.\n\nWireshark · Go Deep. https://www.wireshark.org/. Accessed 19 Apr. 2020.\n",
        "url": "/sonification/2020/03/09/Soniweb.html"
      },
    
      {
        "title": "SMC vs. Corona",
        "author": "\n",
        "excerpt": "In light of the recent microbial world war, we have taken matters into our own hands by sharing audio programming expertise through small introductory courses on Zoom.\n",
        "content": "If we have learned anything over these past seven months, its how to deal with education through telecommunication and online platforms. So, in light of the recent microbial world war, we decided to take matters into our own hands by sharing audio programming expertise through small introductory courses on Zoom. Our first “lecture series” is conducted by Thibault Jaccard on the mastery of C++; the true Lingua Franca (or Newspeak) of audio programming. The first class was held today, 13th of march from 10 - 13 am, and we are looking forward to more in the coming weeks!\n\n\n\n\n\nFor our next installment, we have the privilege of enjoying a “series” on the power and grace of programming in Python, conducted by our very own Jackson Goode. Who knows what will follow, there is certainly no shortage of relevant topics and knowledgeable people in our class. This tragedy seems to be the beginning of a totally new SMC chapter, a unique opportunity to further explore our field in new ways.\n\nOur hearts out to all who have been affected by the virus and those (many many) less fortunate than us.\n\nTo be continued…\n",
        "url": "/other/2020/03/13/mct-vs-corona.html"
      },
    
      {
        "title": "Testing out Jacktrip",
        "author": "\n",
        "excerpt": "As we have begun settling into the COVID tech cocoon of isolation, we test out a technology that might be able to fulfull our dreams of real-time audio communication.\n",
        "content": "Introduction\n\nIn our latest assignment, we testing the efficacy of Jacktrip, a utility developed at CCRMA at Stanford for low-latency audio transmission between multiple clients. Jacktrip employs the Jack protocol, a high definition API for audio communication between applications. Jacktrip appears to be one of the few viable tools that, in theory, works on every operating system (Windows, macOS, Linux) which makes it a potentially worthwhile investment from the angle of usability and access alone. Could this software, built by leading researchers behind the Ivory walls of Stanford, be the skeleton key to our audio dreams. Let’s find out!\n\nInstallation and setup\n\nIt took us some time to set up Jacktrip. One of the reasons for that was due to the somewhat scattered documentation. At time of writing, Jacktrip version 1.2 has not been released as a standalone package, and needs to be built from the source code from here.\n\nThere are three main steps involved, which we will go through here.\n\n1 - Downloading Jack\n\nJack is the underlying audio driver that Jacktrip is built on top of. There are maintained version of Jack. We will use Jack2. You should be able to just download it from the official webpage for all popular operating systems. However, we weren’t, so we’ll guide you through our alternative path.\n\nMacOS\n\nWe found out that the easiest way to install Jack on MacOS is by using the package manager Homebrew. Homebrew is a highly recommended tool for anyone developing anything on a Mac, so it’s likely going to be useful for other things as well. With Homebrew installed, you can install Jack by typing the following into your terminal:\n\nbrew install jack\n\n\nYou can confirm that you have installed Jack by typing\njackd --version\n\nFor ease of use, you must also install a graphical Jack client called qjack. You can find other mirrors for this install in Ben Loveridge’s guide.\n\nOnce you have jack and qjack installed, open qjack, go to “Setup” and set /usr/local/bin/jackd (the location you installed the jack executable to) as the server path (see image below). Note that this is also where you can change other settings such as sample rate, audio interface, number of channels etc.\n\n\n\nTo confirm that Jack runs properly, press “Start”. It should look something like this:\n\n\n\nNow, by pressing “Connect”, there should be a patchin window showing sends and receives. If you connect as in the image below, you are sending your input to your output, so you should be able to hear yourself.\n\n\n\n2 -  Dowloading Jacktrip\n\nThe GitHub repository contains the code for Jacktrip version 1.2. If you are comfortable with building the project from the source code, you can proceed by cloning and then the repository.\n\nOtherwise, we advise you to just download version 1.1 from one of these release pages:\n\nMacOS\n\nhttps://ccrma.stanford.edu/software/jacktrip/osx/JackTrip.pkg\n\nWindows\n\nhttps://ccrma.stanford.edu/~cc/153resources/win10/jacktripInstaller/\n\nVerify that you have installed jacktrip by typing the following in your terminal.\n\njacktrip --version\n\n\nIf you have version 1.1 (or 1.2 if you built it yourself), proceed to the next step.\n\n3 - Configuring Jacktrip\n\nIn the most basic sense, Jacktrip could be as easy as:\n\n\n  one musician setting up a server with “-s”\n    jacktrip -s # defaults to port 4464\n    \n  \n  another musician connecting with “-c”\n    jacktrip -c &lt;IP_ADDRESS&gt;\n    \n  \n\n\nIf there is a successful connection, you can navigate to the “Connect” tab in qjack to route the incoming audio wherever you want.\n\n\n\nHowever, making Jacktrip work for us involved quite a few technical steps, such as forwarding ports on our home router and testing network connections with iperf. For a detailed walthrough, we advice you to follow Ben Loveridge’s guide (alternatively available here). His guide is quite extensive and features a number of FAQ’s for any trouble one might run into during installation or optimization.\n\nTroubleshooting\n\nBen Loveridge’s guide is the resource we recommend for troubleshooting most things with Jacktrip. For API documentation of Jacktrip you can read up on the documentation. However, the software documentation is old and is therefore likely to be outdated. Working with older software on newer operating systems, such as updates like macOS Catalina 10.15 that break functionality with a large number of apps, is always a challenge.\n\nTesting and experience\nAfter some initial struggles with setting everything up in the right way, we were able to test a connection from one client to the server.\n\nAfter we managed to tune the correct settings (buffer size etc.) to get rid of artifacts, the audio quality was very good.\n\nIn order to test the latency, we played a steady drum track from the client, to which Thomas (the server) played guitar on top. The audio that Thomas sent back to the client was close to half a beat off, so we quickly concluded that the latency was too high to be able to play in synchronization with each other. However, the network specs on the client side was lower than what Ben Loveridge recommended in his guide, so it may be possible to do this if all peers have a sufficient internet connection.\n\nWe didn’t manage to connect the third peer (due to poor support for Jack on Windows), but as Jacktrip is peer-to-peer this would likely have required an even faster internet connection. In addition, this configuration requires an ethernet cable, which is a small but very important requirement for this technology. Because audio information must be sent in a continuous stream (for the audio itself to be coherent), a wireless connection is not an option as it allows packets to be lost compared to ethernet, a lossless connection.\n\nConcluding thoughts\nHere our some of out thoughts concerning Jacktrip:\n\n  Definitely not plug and play\n  The installation needs to be simplified extensively\nThere were several people mentioning ways this could be done in this thread: https://groups.google.com/forum/m/#!topic/jacktrip-users/KrhojQxYy_w\n  Once everything was set up, it was fairly easy to understand how things were connected. However, even the patchbay GUI could be better designed\n  One needs an ethernet connection and decent internet connection\n  Fairly easy to use once the connection is made. JackTrip shows up as an audio device that you send/receive to, so it’s quite easy to configure for your own use.\n\n\nJacktrip might be the most robust project for large networked performances, but it is quite reliant on technical competence and a good internet connection from every party involved. It is also unfortunate that such an essential tools hasn’t been seriously updated/maintained for over a year, as it seems quite well referenced online.\n\nResources\n\n\n  The official GitHub repository: https://github.com/jacktrip/jacktrip. Although there exist several versions of the Jacktrip repository, this seems to be the most recently maintained at the time of writing. Chris Chafe, the main author of Jacktrip, announced this in the Google Forum mentioned below\n  The official GitLab repository: https://cm-gitlab.stanford.edu/cc/jacktrip\n  Ben Loveridge. Networked Music performance: A step-by-step guide to real-time  musical collaboration over the internet: https://docs.google.com/document/d/18pbu2xQRv521NKvHuYHjIVXRcLFqcDsqYnfKixyuyGg\n  Google Forums for the Jacktrip user group: https://groups.google.com/forum/m/#!forum/jacktrip-users\n  Jacktrip manual: https://sites.google.com/site/jacktripdocumentation/home\n\n",
        "url": "/networked-music/2020/03/31/jacktrip.html"
      },
    
      {
        "title": "Smokey and the Bandwidth",
        "author": "\n",
        "excerpt": "Hijacking Old Tech for New Uses\n",
        "content": "CB Radio for Real-Time Monitoring?\n\nHere in the SMC program, we deal a lot with internet communication technologies. In the years B.C. (before COVID-19), we would\nmeet four or five days a week in the Portal and chat, collaborate, and conduct classes between Oslo and Trondheim with some of\nthe best technology available. Leveraging the cutting edge and trying to push the boundaries of LOLA, TICO, Dante, and other\nlow-latency protocols was our daily task.\n\nNow, the Portal sits empty. Hopefully, someone remembered to turn off the lights. All SMC courses and meetings now takes place\nvia Zoom, pretty much like the rest of the university’s activities - like the rest of universities and businesses worldwide.\n\nIt’s kind of a step backwards for us, technology-wise. But I think it’s been a good exercise in the practical application of\n“everyday” tech, and really valuable for me personally. In the last few weeks, I think I’ve learned more about my own laptop, it’s capabilities and limitations, than I learned in the entire seven years since I bought it.\n\nAfter all, most people don’t really have access to the level of tech we became accustomed to using in the Portal. What do we\nhave to teach others about Music Communication Technology when the average person lacks a Portal? The real question becomes,\nwhat can we do with what we have?\n\nI don’t know if it’s just the effects of sitting around the house all day, carefully cultivating my quarantine beard and\npondering the future, or something that could actually be of use to somebody - but I have this weird idea to help local\nmusicians be able to collaborate in real-time, without having to leave their own quarantine bunkers. Yes, it involves CB radios.\n\nWhat’s the big deal?\n\nThe problem space here is the same as it always has been: latency. Even for musicians that live right down the street from\neach other, the cumulative round-trip latency incurred as a musical signal from Musician A is A/D converted, sent over WiFi,\nbounced from node to node all around town, then D/A converted and piped into the ears of Musician B, who reacts with a\ncollaborative musical signal which then travels back to Musician A along the same lines can be too much to be useful,\nparticularly when network traffic is high. And right now, it’s pretty much always high.\n\nIn the Portal, we didn’t have this problem. With our dedicated network, round-trip latency Oslo-Trondheim was consistently\naround 30-40ms, making real-time musical collaboration possible. At home, using Zoom or other similar technologies, latency\nhas fluctuated widely and unpredictably, veering drunkenly between 30-300 or more milliseconds. It’s enough to make me want to\ntake up drinking myself, except that I can’t afford booze now.\n\nBut I digress.\n\nThe idea works like this: Each musician has a CB receiver for each member of their ensemble, plus one\ntransmitter for themselves. Each musician transmits their signal on a separate dedicated channel, which is received in turn by\neach other musician.  Therefore, if the ensemble has four players, each player would have four CB receiver/transmitters, three to receive the signals from their bandmates, and one to transmit their own. Each player would then be able to mix their own custom monitor mix by routing each of the four signals into a simple desktop mixer.\n\nThe fun doesn’t have to stop there, of course. The players could also send a separate signal (not the CB audio) in high-\nquality via the internet, without concern for latency, since they would be playing and reacting to the audio they were\nreceiving through their CBs. These signals (audio as well as video, if desired) could be received, aligned, and streamed as a\nreal-time performance via the internet, with very good results. The end observer wouldn’t have to be aware of the novel\nmonitoring system used, all they would know is that the musicians are keeping surprisingly good time, considering their\ngeographical separation.\n\nNow, since none of that made any sense, I made a diagram. See Figure 1.\n\n\n    \n    Fig 1. How It Works\n\n\nAnyway, that’s how it works in my dream. Could it work in real life? 10-6, good buddy. Gonna need some green stamps to put the\nhammer down on that one.\n",
        "url": "/networked-music/2020/04/04/CB-Radio-Monitoring.html"
      },
    
      {
        "title": "Exploring SoundJack",
        "author": "\n",
        "excerpt": "SoundJack is a p2p browser-based low-latency telematic communications system.\n",
        "content": "SoundJack can be described as an ongoing scientific project, created and maintained by Alexander Cârot. On their webpage, SoundJack describes their services as being a browser-based low-latency communications system that provides quality and latency relevant control to the user. In out eyes, this is a pretty spot-on description. The browser UI-implementation, p2p system configuration, up to 8-channel audio support, video, and general customizability makes SoundJack a powerful and easy software to use.\n\n\n\n\n  Compatibility: macOS, Win\n\n\nHow to Use\n\n\n  First, create an account on SoundJacks webpage.\n  Download the small core application (SJC) which must run on your device before using SoundJack in the browser.\n  Run the application on your device and navigate to the Stage Tab in the SoundJack browser.\n  Configure your various audio and video settings.\n  Press the green arrow key next to the user(s) you want to connect to.\n\n\nImportant: If you are using OS Mojave, or later, you might need a slightly different installation process. The steps to be taken are well documented and can be found here.\n\nDocumentation\n\nSpeaking of documentation, the SoundJack software is especially well documented considering its ongoing scientific venture, which also might open up possibilities for us to customize and tailor the software to our needs.\n\nAditionally, we experience the signifigance of the applications integrated chat where you can easily troubleshoot and connect with fellow users. with everyone logged onto SoundJack. This is great for doing solo system testing and other debugging.\n\nThe facts\n\nTo measure the roundtrip latency, we ran six tests. The first three were between Switzerland and Norway while the other three were between Switzerland and Germany. For the CH-NO tests we used the computer in Norway as a mirror, output the sound received using a virtual audio bus (SoundFlower). For Germany, SoundJack offers a mirror user we could use. For both locations, we ran the test three times, with different buffer sizes.\n\nOn the testing computer (Switzerland), we also used virtual audio busses to route audio from and to a DAW (Reaper) in order to measure exactly the delay. The complete routing is the following: Mic -&gt; DAW -&gt; vb -&gt; SJ -&gt; mirror vb -&gt; SJ -&gt; vb -&gt; DAW (vb = virtual bus, SJ = SoundJack). We obtained the following results:\n\n\n  \n    \n      Location\n      Audio buffer\n      Network buffer\n      RTL [ms]\n    \n  \n  \n    \n      CH - NO\n      512\n      512\n      265 ms\n    \n    \n       \n      64\n       \n      207 ms\n    \n    \n       \n       \n      128\n      106 ms\n    \n    \n      CH - DE\n      512\n      512\n      111 ms\n    \n    \n       \n      64\n       \n      105 ms\n    \n    \n       \n       \n      128\n      61 ms\n    \n  \n\n\nWe can conclude that the latency becomes small enough to let people play together when they use very low audio quality settings and are not too far away. SoundJack can therefore be useful but not in every situation.\n\nThe Cons\n\nAlthough SoundJack has considerable potential as a online commincations platform, they still have some obstacles to overcome in able to consider themselves as a reliable and stable candidate in fulfilling the publics’ telematic needs.\n\nThroughout our experimentation, we experienced numerous crashes and highly unstable audio quality despite utilizing various audio and video settings. In light of this, we contacted Cârot himself who explained that they are constantly improving and experimenting with the software, which will inevitably result in unreliable user-experiences, as of yet.\n\nAdditionally, having a p2p system configuration will put extra demands on users’ network connectivity, more so than with other applications like zoom or skype which are tailored to providing a reliable and stable service to a broader number of people. In this sense, it seems natural to consider SoundJack as being tailored for more serious and advanced users, specifically for those who prioritize latency and customizability over general stability.\n\nConclusion\n\nSumming up we can say that SoundJack is a very exciting contribution to the telematic software industry that provides greater freedom to its users via allowing for more network, audio, and video customization than your usual communications application. However, this can give the impression that the system is reserved for more experienced user-groups.\n\nBut despite being tailored for more semi-advanced users, SoundJack is surprisingly easy to use and has an overwhelming amount of documentation which continues to grow. We believe that SoundJack would be an interesting addition to our portal setup as well as a worthy project to follow up on and maybe collaborate with in the near future.\n",
        "url": "/networked-music/2020/04/05/SoundJack.html"
      },
    
      {
        "title": "Zoom and other streaming services for jamming",
        "author": "\n",
        "excerpt": "One thing you can do is to enable the option of ‘preserve original audio’ in Zoom.\n",
        "content": "Zoom as a tool for jamming online\n\nShould you use Zoom as a tool for jamming together online while in quarantine?\n\nShort answer: Don’t.\n\nLong answer: Keep reading.\n\nUsing services like Zoom, Skype, Whereby, FaceTime etc. might seem like a good idea in the first place, but after looking into it, I have concluded that they are far from optimal for this use and there are functioning, easy to use services out there that does this job a million times better.\n\nLatency\n\nFirst there is the issue of latency; how much latency does these systems have inherently? Yes. Yes is as accurate of an answer as any number, in this case. Why? Because the latency can jump around so wildly that you can’t really put a single number on it.\n\nIf you have a stable connection and the traffic on their servers are low you can achieve a fairly decent number of between 40 and 80ms. While being in the higher range of what is acceptable for playing music together, it is doable. The problem is that this number can jump up to as high as 200ms when the traffic on their servers are high or you have a bad connection. 200ms is truly un-acceptable if you want to play anything other than palestrina.\n\nProcessing\n\nAll of these services has a lot of internal processing to make the experience as good as possible for the participants in the conversation. These algorithms work wonders in making sure you are heard when you speak and muted when other people speak. The echo-cancellation is essential in order to not have constant feedback and the compressors and EQ make you sound a lot more intelligible, even with a bad microphone. In short, we would be lost without this processing if attempt to  conduct meetings online. But we are not conducting meetings online, we are playing rock ‘n roll, and this is where the problems start to arise.\n\nOne thing you can do is to enable the option og “preserve original audio” in Zoom. This bypasses the processing done by the software on your computer, but you are still left to the mercy of the algorithms when it comes to the auto-mixing that happens in the cloud.\n\nI surfed the waves of the inter-webs with the compass set towards a software that would make it easy for us to connect people together online, while still providing:\n\n1: Low latency.\n\n2: Ease of use.\n\n3: Some sort of ability to mix your monitoring.\n\n4: Low price-point.\n\nEnter Jamulus\n\nAfter sifting through DIY hack-ish forums, I struck gold in a software Called Jamulus.\n\nDeveloped by a guy with a funny accent and a love for old Dream Theater merch, Jamulus is open source and very well documented.\n\n\n\nIt works by installing a client on your computer and connecting to an external server, were all the clients that are connected to that server can hear share audio. The latency is very low (some reports say below 15ms) and equally important; you are able to mix all the sources to your liking, for a good monitoring experience.\n\nThe software is very easy to install and use, is lightweight enough to be installed on a Raspberry PI and sound totally decent. What is not to love about this?\n\nI did not have the time to set up a server here in Oslo and try this out with others, so I only ran tests by myself through a server in the Netherlands, but were able to achieve a stable sub-60ms roundtrip latency from in to out in my soundcard. I am impressed.\n\nMothers and cakes\n\nMy mother has this thing going with her group of friends; they are a bunch of 60-ish year women who love to drink wine, chat and occasionally sing together, healthy behavior for close to retired ladies to do.\n\nShe called me up earlier this week and asked me for advise on how they could sing the birthday song for one of them who had just turned 60. I was thrilled! I told her that I was working on a project specifically about this and started ranting about all the possibilities software like Jamulus opened for this kind of interaction. My mother’s reaction was a bit more chilled and she quickly asked “do we have to install anything?” I replied: “well, yes. You will have to install this one software, which has a tutorial on YouTube and click on a setting or two, but it is really easy and I can guide you through the process” - “yeah, well, some of us are not that technically minded”.\n\nAnd that comment hit the nail straight on the head.\n\nThe fact that you have to install a software that is not on the app store and connect to a server puts a lot of people of, since they immediately picture some sort of hacking-montage from an 80’s movie.\n\n\n\n\n\nWhat they ended up doing was using Facetime on their phones/macs. When I asked how it worked she reported “we ended up singing one line each, since whenever we tried to sing in unison, all our voices would be cut out. Also, I was the only one not able to get my microphone to work, so I ended up just miming and waving while the others watched in silence”.\n\nI think that quote sums up the user-experience in a nutshell.\n\nFor technically literate people like you and I, installing software and adjusting a few settings is a breeze, but for the vast majority of the population the word “server” is like saying Voldemort in the Harry Potter universe, everyone becomes afraid. Who wants to be the brave little toaster who explains auto-mix, echo-cancellation and network latency to a crowd of impatient mothers? Not me.\n\nThe restriction of five people in a room\n\nTogether with a couple of other companies and individuals, EKKO has started a streaming-studio to handle the increased demand for online live performances. The reason being that all live performances in it´s conventional form has been banned due to the Corona situation. We thought we had it all covered when we had a venue, cameras, sound equipment and crew in place, but it turned out to be harder than expected to get bands that are more than five people to perform together, due to the “five people in a room together” max-limit.\n\nOur proposed solution has now been to split the stage in two, separating for instance the drummer, bassist and keyboardist in one room from the vocals, guitar and tuba in the other. Since all our equipment runs on sound over ethernet, we can easily connect the two rooms together for bi-directional sound. In this case, Zoom is a perfect solution for having visual communication between the rooms as well. Because of it´s ease of use and the fact that it can be installed on almost any device, Zoom seems to be perfect for this, as a side-car to a hard-wired audio over IP network that provides audio communication with under 2ms of latency.\n\nThe only problem with doing this is that by separating musicians who are used to playing together in a room, they will not have the same “feeling” as playing together on the same stage. Zoom can provide a decent video-feed to be used here, but the auditory experience will be very different. I have therefore proposed that we will need a substantial amount of power for the monitoring, having a decent concert PA as monitors, to play back drums, bass and other powerful instruments as well as providing spatial separation of the instruments.\n\nConclusion\n\nThe properties that makes services like Zoom, Skype, Facetime and others such great products are also their biggest pitfalls. In order to make their services user friendly and sexy, the companies that makes them has to hide most of the techy, nerdy -stuff that lets you tweak them to fit other use cases. When Steve Jobs wanted to make the user-experience of the operating system “insanely great” he did so by removing almost all the customization options, leaving the user with an experience that might be restrictive, but one they will actually use. Apple fans has taken a lot of shit from techies that prefers Linux bash over macOS, but you can’t argue with the fact that Apple became the highest valued company in the world while you still have to actively seek out the terminal if you want to get freaky with it.\n\nI can imagine a product that really delivers the streamlined user experience that is required to be appealing to the public and still allows for playing music together online with decent quality, but alas, the search for such a product will have to continue.\n\nWill the situation caused by quarantine now result in a push towards innovation in this field? Certainly.\n\nWill we see a product that delivers what I now have described in the near future?\n\nDoubtfully, but I am hopeful.\n",
        "url": "/networked-music/2020/04/06/Zoom-Jamulus.html"
      },
    
      {
        "title": "Soniweb: Epilogue",
        "author": "\n",
        "excerpt": "An update to the Soniweb project featuring dynamic sonification!\n",
        "content": "A Reworking of Soniweb\n\nThe Soniweb project was hurriedly completed within the span of about a week. However, in rushing through its implementation, there were a number of unique opportunities that would have been possible with the data we were receiving. Notably, the direction of the packets (whether they were being sent or received) was not considered in the process of our sonification. This information would be quite interesting to have conveyed through sound and provide interesting knowledge about when and how often is your computer uploading information to some other server.\n\nOne of the issues I have with sonification is the reasoning (or lack thereof) behind how aspects of data will be sonified. I feel this is a essential point of interpretation artistically that is missed by a lot of sonification projects in the pursuit of making them sonically “beautiful”. Too often the decisions that are being made are not necessarily drawing out acoustic features that parallel the relationships within the data. To this end, I wanted to make three adjustments to the Soniweb project. The first would replace sample playback to synthesized sound at the heart of the sonification, second would focus on shaping the synthesized sound in an extended range of parameters like envelope and pitch based on the features of the incoming packets, and third would be a general optimization of the python script and Pd patch to prevent stuttering or audio glitches during normal use.\n\n\n    \n    A little restructured and a much better reverb implementation\n\n\nFrom samples to FM synthesis\n\nOur first version of Soniweb used pre-fab samples for each of the different protocols (UDP, DNS, TCP, etc.), meaning there was a considerable amount of processing in simply reading these samples into memory to play in parallel. In the previous version, there was noticeable pops and stutters as a result of web browser usage (page loads), Wireshark’s backend listening to packets, and Pd producing an ambisonic environment. So, one major reason in replacing samples with a short generated FM synth would be the benefit it might have on CPU usage and memory within Pure Data.\n\nSonification of direction\n\nIt’s not totally obvious how one goes about representing motions like approach or retreat. However, one nice point of reference is the Doppler effect, the reason firetrucks seem to have a higher pitch when driving towards the listener and a lower pitch once they have passed. This might be the best feature representative of acoustic motion along with increasing/decreasing volume.\n\n\n    \n    Audio from Wikipedia\n\n\nWith this information I planned to give the sense of a sound arriving and departing by creating envelopes that had a short attack/long decay (departing) and long attack/short decay (arriving), each coupled with the proper pitch transformations one would observe with the doppler effect (departing - pitch falling, arriving - pitch rising). However, due to the massive number of packets that are received at once, anything longer than half a second for a single sound would quickly become overwhelming. Thus, a lot of tinkering with the timing and duration of each of these transformations was needed in order to make their different both intelligible and musical.\n\n\n    \n    The new patch for one of the voices\n\n\nFiltering\n\nBecause there would be potentially dozen of packets coming within a single second, I decided to restrict this stream by 1) limiting the packet size and the frequency that packets could be registered in the python script and 2) shortening the max duration of the sound each packet will make to 150ms in Pd. These changes came after a number of other tests to see if limiting the packets by duplicate IP’s (no packet can be sent from the same IP in succession) and by duplicate packet size. Both of these alternatives seemed like strange modifications to the data stream and were workarounds for my desired outcome - fewer packets at once.\n\nSo, after checking the very scarce documentation of pyshark, I found there was a way to grab the time that a packet was received and restricted the interval between packets manually (5ms). I also made sure that sequential packets of the same size (such as when loading an image) have a subsequently quieter sound.\n\n# Set up capture and filter by host IP and packet size\ncapture = pyshark.LiveCapture(\n    interface=\"en0\",\n    only_summaries=True,\n    bpf_filter=\"ip and host \"+host_ip+\" and length &gt; \"+pkt_len)\n\nfor packet in capture:\n        # Restrict flow of packets\n        pkt_time = float(packet.time)   \n        delta = pkt_time - prior_time\n        if delta &gt; .05:\n            prior_time = pkt_time\n\n            # To OSC\n            try:\n                # For reappearing packets of same size - reduce gain\n                if prior_len == packet.length:\n                    client.send_message(\"/packet_len\", int(packet.length)/10)\n                else:\n                    client.send_message(\"/packet_len\", int(packet.length))\n                prior_len = packet.length\n\n\nThere was also a helpful flag within pyshark I had overlooked. The “only_summaries” flag when setting up the LiveCapture significantly reduces the information each packet carries when it enters python. Including it likely made the entire script more efficient as the information I needed from each packet still existed within just the summaries.\n\nResults\n\nInterestingly, the replacement of sound samples with synthesized sound did not initially reduce the CPU usage of the script on the computer I was testing with (a 2013 Macbook Pro). This may simply be a result of how intensive the setup for testing this program but also that I had made quite extensive changes to the structure of the program as well. Both the script, Pure Data, and a web browser (or web intensive application) are needed to run to “feed” Pure Data packets of data. With all of these applications running, Pure Data does struggle a bit and some pops and stutters when webpages are loading, though I have noticed that this behavior with audio interruptions is common to other applications like Zoom when there is intense web traffic happening at once.\n\nAfter reworking the filtering system and tinkering a bit with the quality of the reverb I added, I think I ended up with something quite nice. Here’s a short video, please wear headphones to experience the full effect. The code and patches will be posted soon.\n\n\n    \n    A demo of the updated project (headphones required!)\n\n",
        "url": "/sonification/2020/04/13/soniweb2.html"
      },
    
      {
        "title": "On communication in distributed environments",
        "author": "\n",
        "excerpt": "Photo by Dr. Andrea Glang-Tossing. At ISE Europe, a trade fair in Amsterdam ( 11.02.-14.02. 2020) I was presenting together with SALTO the SMC course at the AVIXA Higher Education Conference. Researchers and students were invited to highlight emerging innovative methods that enhance learning and teaching experiences through AV technologies. The ISE Europe is the world’s biggest pro AV event for integrated systems, with 80000 visitors and 1400 vendors, spread over a dozens of halls. For the conference I was specifically asked to contrast the overall technically curated program with social aspects from a student perspective. A retrospective from current conditions.\n",
        "content": "\n    \n    The presenters and the host Gill Ferrell\n\n\nAt ISE Europe, a trade fair in Amsterdam ( 11.02.-14.02. 2020) I was presenting together with SALTO\nthe SMC course at the AVIXA Higher Education Conference. Researchers and students were\ninvited to highlight emerging innovative methods that enhance learning and teaching\nexperiences in distributed environments through audiovisual (AV) technologies. The ISE Europe is the world’s biggest pro AV event for integrated systems, with 80000 visitors and 1400\nvendors, spread over a dozens of halls. For the conference I was specifically asked to contrast the overall technically curated program with\nsocial aspects from a student perspective. A retrospective from current conditions.\n\nThe benefits and challenges of learning and teaching in distributed environments like SMC’s cross campus study collaboration are\ncurrently very much at display. Communication is always important, but in the light of the current situation,\nour dependency on technology that connects us is even more exposed. I always had the feeling that we at SMC were in so many ways researching\nourselves in a microcosmos set-up that at some point would turn into a mega sized social laboratory. Among many other questions, we have\nasked ourselves by what audiovisual means we can increase mediated presence and the feeling of social connectedness online.\nIn mid February most of us had no clue that a globally applied lockdown could force the entire higher education system in our promoted study\nformat and these questions became large scale issues in times of physical distancing. For the interested potential applicants out there -\nwith the difference that SMC still has co-located students in each city’s Portal, a physical existent space that unites virtually both\ncampuses to one class room. But even that seems now like a relict from another life. When everything we consider essential for a diverse and\nhuman practise (gathering in the same room for musiking, theater…) is currently on ice, the necessity to improve AV solutions that enable\nclose to face to face situations is invaluable to keep societies’ sanity. At least our children will grow up in a totally different world\nand most likely will not have to bother with the same questions and suffer under transition periods and accustoming. So we have to do a\ngood job now and we need especially more women for it!\n\nLanguage in AV\n\nIn my presentation ‘Technical AV Roles and the Glass Ceiling’ I could highlight some of the challenges that can occur while studying and\nworking in AV environments like music technology. While we are a diverse group in terms of nationalities and age variation, being currently\nthe only active female student poses questions to this circumstance. There are indeed many reasons for the imbalance of gender presence in\ntech-heavy study programs and industries. Some of them are well known, other less. They can be very obvious, or less noticeable.\nI was focussing on how biases can reveal themselves in the language of music technology. At the same time exemplifying how language can\nbecome when actively applied, a mean for inclusion again. Underlying my statements by reversing ideas of functionality of certain devices\nwhich are used for music technological practises seemed to help to see the tendencies. Like stating the Midas Mixer would be a hyper emotional instrument for example.\n\nOf course it was important to mention there can occur more obvious (often subtle applied) means of exclusion as well. Hostile language or\neven harassment is independent from the distribution of gender an ongoing practise. However, the means are getting better\nto articulate and if necessary to report them. It must be said that just because an environment is male dominated, it does not mean this is\nautomatically going to happen. The women I interviewed for WoNoMute for example did not share these experiences.\n\nThe Glass Ceiling\n\nNonetheless aforementioned factors most likely result in what is called the Glass Ceiling,\nwhich becomes even more “visible” under current conditions, when many women fall back into traditional roles at home.\nThe Glass Ceiling is a metaphor which describes the invisible barriers that prevent women or minorities from “elevated professional success”\nnamely holding high job positions in the private but also in the public sector.\nThe necessity to find strategies for women’s exit from the current situation into an active participation not only in the so\ncalled system relevant occupations is getting obvious, as they are so much effected by it. Or maybe the term must be adjusted, their role\nin caring for, designing and coordinating technologies that are valuable for societies must become stronger, if not guiding.\n\nFinally, the expo itself was quite overwhelming. It took all the days to visit the halls and to get a gripe of the materialised dimensions\nof electronic systems integration and IT for AV. For the future of the Portal it is definitely the venue to gather ideas to push new\ndevelopments. I had also the chance to visit an inspiring talk at the AVIXA diversity council and learned about large professional\nnetworks that mentor and support young women in AV. As this is my last semester at SMC, I hope the conversation and activities on diversity\nand inclusion will continue and expand.\n\nAcknowledgements\n\nThanks a lot to AVIXA and SALTO for enabling me to follow up Gill Ferrel invitation, who gave me the chance to share my perspectives.\nThanks to you all and I hope the collaboration will continue at SMC on these matters.\n\nReferences\n\n\n  How AV technology enriches the user experience in Higher Education, by Andrea Glang-Tossing / Sennheiser\n  Higher Education Conference at ISE 2020, Gill Ferrell\n\n",
        "url": "/networked-music/2020/04/15/On-communication-in-distributed-environments.html"
      },
    
      {
        "title": "NINJAM, TPF and Audiomovers",
        "author": "\n",
        "excerpt": "During these last few weeks of “quarantine” during the COVID-19 outbreak, we have tested out several TCP/UDP audio transmission software’s from home to check for latency and user-friendliness. Our group consisting of Simon, Iggy, and Jarle, were tasked with looking into NINJAM and TPF.\n",
        "content": "During these last few weeks of “quarantine” during the COVID-19 outbreak, we have tested out several TCP/UDP audio transmission software’s from home to check for latency and user-friendliness. Our group consisting of Simon, Iggy, and Jarle, were tasked with looking into NINJAM and TPF.\n\nNINJAM\nNINJAM is an open-source software to send audio over the internet with the TCP protocol. It uses OGG Vorbis audio compression and beatmatching, relatively increasing latency for everyone to give a level latency for jamming. Its main prerequisite is the Reaper DAW and a server. Out of the box, there are several public servers available that are open to anyone. If, however, you need a private connection, you have to set up a server yourself, which is not necessarily a straightforward task. The server requires adequate bandwidth but has no firewall or NAT issues. NINJAM has clients available for Mac OS X, Linux, and Windows.\nIn all honesty, the server setup is complicated, and the results are subpar to other similar software that is easier to set up. Tentatively, our group gives this solution a thumbs down.\n\nTPF\nTPF is a swiss software made for low latency telematic performances. It uses Pure Data and Jacktrip protocol. TPF consists of two PD patches, a server PD patch, and a client patch contained in an application through a .dmg. It requires a computer running the server PD patch with a public IP address and Jacktrip. The (macOS) JackTrip installation alone requires workings in Terminal, including sudo- commands that might seem cryptical to the inexperienced user. Though this will seem mundane to someone well versed in networking IP, that is something that does not apply to most of the musicking public, ourselves included. \nThe initial installation then (and solely just for JackTrip) seems an obstacle fit to dissuade any user from engaging in any immediately intuitive musical network-collaboration.\n\nTo run the client, Pure Data is required along with three additional externals, *iemnet, *osc and *slip.\n\nConclusion\nWith none of these solutions coming close to being plug and play, TPF and Ninjam would be impractical for the majority of musicians attempting to use either form of telematic software.\n\nAudiomovers\nAn alternative suggested by Iggy was to use Audiomovers Listento, which essentially creates a plug and play solution to online musicking, which requires simple routing concepts and a DAW that accepts VST, AU, and AAX plugins. The latency created by Listento can be very minimal and workable. The only setback of Listento is the requirement of bandwidth if higher bit depth formats are used, such as PCM audio.\n\nOur experience with Audiomovers was very positive. You install the VST’s (a send and a receive), login, and connect. It’s as simple as it gets, plug, and play in a pleasing UI in a DAW of your choosing. It gives you the option to choose between different qualities that you can switch in real-time and on both ends. We tried recording in serial with excellent results. By sending through the regular beat keeping hierarchy of drums to bass to guitar and recording on the endpoint. This will keep everything in perfect time with the downside of introducing more chances of dropouts. Playing in parallel, however, has its apparent limits inherently in terms of latency. What sold us on this software is the usability, quality, and multipurpose of the software. If you are willing to cash out for it, you’ll have a great way of sending audio over the internet. We give it a big thumbs up.\n\n",
        "url": "/networked-music/2020/04/17/NINJAM.html"
      },
    
      {
        "title": "MotionComposer",
        "author": "\n",
        "excerpt": "MotionComposer is a motion capture device that lets people make music with gestures. This is the presentation of our applied project, where we worked on building a new instrument for this device.\n",
        "content": "Introduction\n\nThe MotionComposer is a device that lets users use motion to make music. It has been designed with disabled people in mind, but some use cases are also oriented towards dance performances. The company is based in Chemnitz, Germany, and the device is almost ready for production. Our task was to use the existing motion tracking system and map the data to create music in a new way. We decided to build a synth engine, using MIDI files or an external MIDI controller to trigger notes, and movement to shape the sound.\n\nThe Device\n\n\n    \n    MotionComposer 3.0\n\n\nThe hardware device, which we have not contributed to build at all, consists of an embedded computer running Linux, two HD cameras and a wirelessly connected android tablet for the user interface. On the latter, the user can switch between different playing modes. Our goal was to develop one of them.\n\nThe MotionComposer analyses the video data coming from the cameras and detects the type of movements according to a “movement alphabet”. Depending on the motion input, a corresponding OSC message is sent to the Pure Data patches of the current mode. The MC has also an audio interface and can send sound to powerful speakers.\n\nSynth Engine\n\nWe developed two distinct synth engines, and the user can switch between them using some gestures. The first engine is an additive synth, the second one is a physical modeled string. Either one of them is assigned to an ADSR envelope with customizable parameters, and then the sound is routed into various effects.\n\nAdditive Synthesis\n\nOur additive synth consists of a fundamental frequency (f0) sinusoidal oscillator and 16 harmonic sine tones. Each harmonic has a different frequency (2f0, 3f0, …). The amplitude of each generated sine tone can be controlled independently. We decided to use two mathematical expressions for two different harmonic mappings. The first one works like a bandpass filter, with x as the central frequency and y as the Q:\n\n\n    \n\n\nn represents the harmonic number, in the interval [0,15]. Here is an interactive applet to see the expression in action:\n\n\n\n\nThe second equation is a linear function, which slope and intercept change according to a single parameter x:\n\n\n    \n\n\nAgain, an applet to understand its behaviour:\n\n\n\n\nString Physical Modeling\n\nThe idea to bring a new voice to our synth engine, came from the need to achieve clear changes on the sound output, as the partner expressed. With the string physical modeling synthesizer, it was possible to change smoothly between two synth voices, the additive synthesis created previously and the string synth. To do that, it was implemented with a mixer that inverts the amount of signal coming through one synth proportionally inverted to the other synth, in response to the movements of the arms of the user.\n\nEffects\n\nThe modulations and parameters were chosen with the intent to provide a sense of agency, an important concept for therapeutic purposes. Sense of agency proved to be effective as a support to the recovery of patients with physical disabilities. Challenges had arisen regarding how to design the parameters, scales, and limits that would supply interesting musical expression while preserving the sense of agency.\n\nSense of agency requires immediately obvious transformations on the sound, but gesture can vary significantly, from subtle movements to abrupt actions, which can be tricky to scale the numbers coming from the motion sensors.\n\nAfter testing some different modulation effects, we stayed with Tremolo, Flanger, Reverb, Spectral Delay and a Low Pass Filter. Exploring parameters such as Depth and Rate to react to the movements of the arms, we figured out that Tremolo works well with both additive synth and string synth, as the amplitude modulation fits well for the sawtooth waves [phasor~] from the string synth and the sine waves [osc~] from the additive synth. Other effects provide interesting results too, but at some frequency ranges, or with inappropriate monitors, they can make the sound muddy on the low frequencies, or sometimes not show clear changes on the sound output.\n\nThe parameters of the effects implementend had to be adjusted and re-scaled several times to achieve a proper response according to the data coming from the sensors.\n\nMapping\n\nWhile researching movement-to-sound literature, we were inspired by the idea of making a virtual model and using it as a point of departure. This virtual model would be centered around specific body poses (a combination of various movement tracking data points) and correlate specific sound environments to said poses, as seen in the image below. Then, we would be able to reproduce the desired soundscapes by doing certain predetermined poses.\n\n\n   \n   Skogstad,Nymoen, de Quay, &amp; Refsum, 2012, p.3\n\n\nThis culminated in two Pure Data abstractions that could imitate 2 specific motion gestures performed over time, as seen in the image below. Practically, this meant that we could move one slider (representing time) and output independent data streams of both arms (horizontally and vertically), body position, head movement, and general height, correlating to the position of the given limbs at a given time.\n\n\n   \n   Two gesture examples\n\n\nUsing this mapping scheme to experiment with the synthesizer enabled us to make certain key development decisions early on, like limiting synthesis control parameters. We could then start collaborating with our external partner with a beta version of our sound engine that was already calibrated, to some degree, to deal with movement as its control parameters.\n\nWe chose to work with vertical arms because these movements have a wide dynamic range and have the possibility of functioning like a coordinate system. It, therefore, seemed like an appropriate starting point for exploring how we could interpolate between, or “move through”, different soundscapes generated by our sound engine.\n\nOur workflow from there was highly shaped by our digital means of communication. We would first send our partner a collection of mapping schemes for testing. These mapping schemes involved 4 different interpretations of how the horizontal arm movements could control the parameters of the synthesizer. Our partner would then test all interpretations in one setting and provide valuable feedback for us in return. This workflow enabled us to effectively explore a multitude of options in a limited amount of time.\n\nRouting\n\nThe Motion Composer consists of 3 modules; the tracking module(camera), the control module(brain), and the musical environments. These all make up a bidirectional communications system which is maintained and controlled by the hardware`s integrated Linux machine.\n\n\n   \n   Movement alphabet\n\n\nFor a musical environment to get movement data from the tracking module, and subsequently to produce sound from that data, it has to send and receive various OSC-messages back and forth between the control and tracking module using the systems designated syntax, as seen in the image above. Luckily, a multitude of Pure Data packages allows for quick and reliable OSC-routing so we were able to integrate this in a relatively short period of time.\n\nThe GUI\n\nIn the finishing phase, we invested in designing a GUI. It is a beta-prototype with minimalist aesthetics, made for improving the user experience and to create a better perspective of our product for the stakeholders.\n\n\n    \n    MoShape GUI\n\n\nVideo Demo\n\nWe made a video showing the MoShape in action, in the phase of tests with Robert, the partner, and also showing the Pd patch working internally.\n\n\n\n\n\n\nConclusion\n\nThis project has been a great opportunity for us to learn more about audio programming in Pure Data, but also learning a lot about OSC communication and motion capture. We are satisfied with the result, but we will continue working on it in hope of seing our instrument implemented in the commercialised product. The relation with our ecternal partner is very good, and we are proud of the current state of our synth. This first professional experience has been instructive for us all, and gives us a better idea of what working in the music technology indutry looks like. We can’t wait for the next applied project!\n\nReferences\n\nSkogstad, S. A., Nymoen, K., de Quay, Y., &amp; Refsum, A. (2012). \tDeveloping the dance jockey system for musical interaction with the xsens MVN suit.NIME\n",
        "url": "/applied-project/2020/05/02/MotionComposer.html"
      },
    
      {
        "title": "Soundscapes for Dream Nest",
        "author": "\n",
        "excerpt": "For the spring applied project we created music for a hardware specific device to relax colicy babies in a collaboration with our external partner, Dream Nest. Our final product is a six track EP, engineered to put your baby to sleep (we hope!)\n",
        "content": "The Dream Nest is a therapeutic product that uses low-frequency sound waves to sooth infants that suffer from various colic or stress related problems, in particular those associated with pre-term birth. The “nest” acts as a crib where sound therapy hopes to ease the tension of these babies. In addition to the low-frequency sound stimulation, our external partner wanted music elements.\n\nWhat music is best for babies?\n\nThe task of our group was to design and implement complementary soundscapes that could be used in conjunction with these low-frequency vibrations. The soundscape could be a combination of musical and “environmental” sounds that seek to replicate to an extent the acoustics of the womb environment. We dove into the research to find inspiration for the kinds of sounds and techniques that might provide a baby with the most soothing listening experience.\n\nOur major questions we aimed to answer were:\n\n\n  What can research tell us about the therapeutic effect of music on newborns?\n  How can sound serve to relax and comfort babies?\n  Which kinds of sounds would effectively calm infants and stimulate cognitive development in infants (are these two compatible)?\n  How should these sounds be presented in a way that is natural?\n  What elements of music could be potentially distressing for infants?\n\n\nSome insights from research\n\nRecent research supports the benefit of music listening in the cognitive and emotional development of pre-term infants (Standley 2002; Lejeune, et al 2019; Sanchez 2019). The types of music that seem to best calm infants has been studied as well (Keith et al 2009; Haslbeck 2012; Loewy et al 2013).\n\nIn addition, the sound of the maternal heartbeat and voice has also been found to positively affect auditory plasticity in the brains of premature infants (Webb, et al 2015).\n\nOther research into the acoustics of the womb environment indicates that the maternal abdomen is an effective filter of high- and upper-mid-frequency sounds (Parga, et al 2018). This study also describes the intra-uterine audio environment to be dominated by sounds from the mother’s respiratory, cardio-vascular, and digestive systems.\n\nListening to recordings taken from inside the stomach were also quite illuminating - it’s a loud place in there. It might freak the parents out if we simply replicated the womb and it was important for the bonding experience between the parent and infant that the music be something palatable for both.\n\nThe music\n\nAll these elements are filtered in the high and upper-mid frequency ranges to attempt to properly represent the actual intra-uterine environment.\n\n\n    \n    An example of the EQ that we applied globally to the mix\n\n\nWe created a total of six original musical pieces of a duration exceeding 10 minutes each, with instrumentation including acoustic guitar, piano, bass and synthesized pads. In addition, each composition was edited to include a shorter, loop-able segment, making a total of 12 musical pieces.\n\nSamples of these musical works can be listened to here:\n\n\n   \n     \n     Your browser does not support audio tag.\n   \n   God Natt - Paul\n\n\n\n   \n     \n     Your browser does not support audio tag.\n   \n   So Ro - Jackson\n\n\n\n   \n     \n     Your browser does not support audio tag.\n   \n   Sov Godt - Iggy\n\n\nOur sound engine\n\nThough we decided to produce the Dream Nest EP as our major deliverable, one of our original ideas a generative sound engine was also something we wanted to work on. In reference to the above scientific literature, our proposed “sound engine” for the Dream Nest incorporates the following elements:\n\n\n  Original music composed specifically for Dream Nest\n  The mother’s recorded heartbeat\n  The mother’s voice (reading a story, singing, etc.)\n  Various synthesized “womb” sounds\n\n\nIn order to create a working model of our sound engine, we implemented a demonstration app in MobMuPlat, using Pure Data as our programming language. The app combines our original music, synthesized womb sounds, and recordings of the maternal heartbeat and voice of a real, actual mother (Paul’s wife, Erin).\n\n\n    \n    A shot of the Pure Data patch for our demo app\n\n\nRecording the mother’s voice is easy to do with a smartphone or laptop, but the heartbeat is a little more difficult. Ideally, a stethoscope microphone would be used for best results. Build-ing an effective stethoscope microphone is not too difficult or expensive, and perhaps one or two could be kept on hand to loan out to new Dream Nest users. In the references we have included an instructional on the construction of a simple electronic stethoscope microphone (Bhaskar 2012).\n\nThe various sound sources are combined in a simple, mixer-like interface allowing the end user to select or deselect elements and set the individual volumes of each to optimize playback on various types of audio equipment.\n\nA short demonstration of the app is here:\n\n\n  \n    \n    Your browser does not support video tag.\n  \n  Video of the MobMuPlat interface\n\n\nFuture work\n\nTesting our creations on actual infant-subjects is unfortunately beyond the scope of this project, so much work remains to be done. We based our approach to this problem on the scientific research in the field, but we can’t be sure at this point which elements of our project work and which don’t. The unexpected imposition of COVID-19 quarantine definitely delayed some testing that would have taken place on the part of our industry partner, who no longer was able to continue Dream Nest testing at the local hospital’s neonatal unit.\n\nHaving said that, we think it’s a good start. In the design of the eventual Dream Nest app that will pair with the Dream Nest itself and allow parents/caregivers to control and personalize the experience for their little ones, we recommend the following:\n\n\n  A multi-channel mixer interface for the sound engine\n  An expandable library of musical composition and loops to choose from\n  The ability for each mother to record her own voice and heartbeat to be mixed into the over-all soundscape\n  A selection of synthesized or sampled “womb sounds” to complete the sound environment\n\n\nFrom the app, the parent/caregiver can then stream the sound to either the Dream Nest itself (if the final design allows for it) or an external bluetooth or wifi enabled speaker system. Because of the importance of the low-frequency spectrum in authentically replicating the womb environment, we recommend a good quality speaker with excellent low-frequency reproduction.\n\nThe Sonos line of WiFi speakers are already found in many households, and would be a good choice, but there are many others that would be suitable.\n\nWorks Cited\n\n\nBhaskar, A. (2012). A simple electronic stethoscope for recording and playback of heart sounds. Advances in Physiology Education, 36(4), 360–362. https://doi.org/10.1152/advan.00073.2012\n\n\n\nHaslbeck, F. B. (2012). Music therapy for premature infants and their parents: An integrative review. Nordic Journal of Music Therapy, 21(3), 203–226. https://doi.org/10.1080/08098131.2011.648653\n\n\n\nKeith, D. R., Russell, K., &amp; Weaver, B. S. (2009). The Effects of Music Listening on Inconsolable Crying in Premature Infants. Journal of Music Therapy, 46(3), 191–203. https://doi.org/10.1093/jmt/46.3.191\n\n\n\nLejeune, F., Lordier, L., Pittet, M. P., Schoenhals, L., Grandjean, D., Hüppi, P. S., Filippa, M., &amp; Borradori Tolsa, C. (2019). Effects of an Early Postnatal Music Intervention on Cognitive and Emotional Development in Preterm Children at 12 and 24 Months: Preliminary Findings. Frontiers in Psychology, 10. https://doi.org/10.3389/fpsyg.2019.00494\n\n\n\nLoewy, J., Stewart, K., Dassler, A.-M., Telsey, A., &amp; Homel, P. (2013). The Effects of Music Therapy on Vital Signs, Feeding, and Sleep in Premature Infants. Pediatrics, 131(5), 902–918. https://doi.org/10.1542/peds.2012-1367\n\n\n\nParga, J. J., Daland, R., Kesavan, K., Macey, P. M., Zeltzer, L., &amp; Harper, R. M. (2018). A description of externally recorded womb sounds in human subjects during gestation. PLOS ONE, 13(5), e0197045. https://doi.org/10.1371/journal.pone.0197045\n\n\n\nRecording audio from a stethoscope · adnbr. (n.d.). Retrieved 3 May 2020, from https://www.adnbr.co.uk/articles/recording-audio-from-a-stethoscope\n\n\n\nSanchez, K., &amp; Morgan, A. T. (2019). Music therapy for neurodevelopment in hospitalised infants. Acta Paediatrica, 108(5), 784–786. https://doi.org/10.1111/apa.14745\n\n\n\nStandley, J. M. (2002). A meta-analysis of the efficacy of music therapy for premature infants. Journal of Pediatric Nursing, 17(2), 107–113. https://doi.org/10.1053/jpdn.2002.124128\n\n\n\nWebb, A. R., Heller, H. T., Benson, C. B., &amp; Lahav, A. (2015). Mother’s voice and heartbeat sounds elicit auditory plasticity in the human brain before full gestation. Proceedings of the National Academy of Sciences of the United States of America, 112(10), 3152–3157. https://doi.org/10.1073/pnas.1414924112\n\n",
        "url": "/applied-project/2020/05/03/dream-nest.html"
      },
    
      {
        "title": "NTNU Oceans",
        "author": "\n",
        "excerpt": "An immersive installation on mercury pollution in the ocean. The aim of this project is to conceptualize and implement an installation in which visitors can interactively “see” and “hear” the status of oceans and seas worldwide.\n",
        "content": "\n\nTable of Contents\n\n\n  Problem\n  Proposal\n    \n      Inspiration\n      Deployment of the installation\n      Lights and color theory\n      Creating movement in the lights\n      Implementing color palettes\n      Sound\n      Max For Live\n      Tracking\n    \n  \n  Evaluation of the proposed solution\n  References\n\n\n\n\n\nProblem\n\n\n  The aim of this project is to conceptualize and implement an installation in which visitors can interactively “see” and “hear” the status of oceans and seas worldwide.\n\n\nProposal\n\nThrough conversations with professor Murat Van Ardelan, we have learned about the increasing dangers of the mercury reserves in the permafrost in Siberia and parts of Canada. Tons of mercury could potentially be released if the global temperatures rise and the ice in the ground melts. The consequences of these mercury reserves ending up in our food chain could become a natural disaster for animal life and human beings. Articles with extensive information on the topic: [1], [2], [3].\n\nOur proposed solution is an interactve installation that aims to bring attention to the topic of mercury pollution through sonification and visualisation. The visitors of the installation can control different parameters in a dataset by moving physical objects and observe how these changes affect the auditory and visual output.\n\nInspiration\n\nThe idea behind the use of light in our installation is inspired by the installation artists Olafur Eliasson and James Turrell; using strong lighting with vibrant colors to transform the essence of a space.\n\n\n\n\n  Turrell’s installation at Ekeberg, Oslo.\n\n\nThe installation is meant to evoke great feelings without saying anything explicitly itself, letting the interpretation and emotions come from within each participant, thereby making it more genuine and relatable.\n\nDeployment of the installation\n\nThe installation will be a structure with white fabric on all sides and lamps and speakers on the outside, enabling us to change the color of the walls and project sound through the walls, without revealing the equipment outside.\n\nThe frame\n\nWe have decided on using 2x2” wooden beams, but stage truss or aluminum pipes will be a good option, if you have the budget.\n\n\n\nThe fabric\n\nAfter experimenting with different fabrics, we landed a quite thick, synthetic voile. This fabric is cheap, available in large quantities and is both sound and light -transparent enough to let the sound from the loudspeakers to come though and diffuse the light from the lamps on the walls.\n\nThe loudspeakers\n\nOur music is made to be played back on a multi-speaker system of at least four speakers, but more could be added for greater effect. The most basic setup would consist of four speakers and one subwoofer placed in the corner, outside the box.\n\n\n\nSmall Genelec studio monitors seem to have become the industry standard due to their compact size, high performance and convenience of being able to mount on a regular mic-stand, but buying them in large quantities can be prohibitively expensive.\n\nThe computer\n\nA key point is setting it up with a good boot-script that automatically boots the computer with the correct software and settings, but also installing a remote desktop solution for support when we cannot be there in person.\n\nThe lights\n\nAs long as they fulfill these requirements, most modern LED-lights with DMX512 or ArtNet -control should work fine:\n\n\n  Has to have DMX 512 control with at least RGB-mix (UV would be great for added effect).\n  Has to be able to run for extended periods of time without failure.\n  Has to have pretty decent output with a wide angle or is cheap enough to buy many.\n\n\nThe projector\n\nIf you wish to deploy this installation with a graphic display, a projector is needed. From our testing and experience, a short-throw projector capable of outputting at least 3000 ansi-lumen is preferred. A high-end LED projector would be optimal due to both low heat, low noise and a long life.\n\nLights and color theory\n\nWe initially intended to use the built-in DMX control in either Ableton Live or Qlab, but deemed them too restrictive. In order to make the experience exciting and beautiful. The by far most common platform in the professional industry is the MA lighting “Grand MA” -platform, they have a free alternative called MA onPC.Together with a USB to DMX-node this would be sufficient.\n\nCreating movement in the lights\n\nWe wanted to create a dynamic experience. By controlling each light individually, we are able to create both movement and color palettes. By selecting all the lights and applying a sinusoidal dimming chase, the lights will dim up and down as if they were points on a sine wave.\n\n\n\n\n  Imagine each fixture being a point on the sine wave.\n\n\nTriggering the different states of the lighting installation is done by assigning cues in the lighting software containing the color palettes and chases to be initiated by MIDI messages from the Max MSP patch.\n\nImplementing color palettes\n\nIn order to really take advantage of color in communicating emotions, you generally end up with a much greater impact using color palettes than single colors. The emotional connotations from movies and pop-art are so engrained in our culture that almost everyone has a relation to them:\n\nWhat does these palettes make you feel?\n\nExample one\n\n\n\nExample two\n\n\n\nNotice that even though they use very similar tones, one is very harmonious and the other is very dissonant. This is accomplished by having the colors intentionally match or not match each other according to the classic color theory developed by Goethe and Newton.\n\n\n\nExample one\n\n\n\n\n\nNice and happy.\n\nExample two\n\n\n\n\n\nNot so much.\n\nThere are endless examples of this on the internet and you can easily make your own with the color picker tool in almost any image editor (Gimp and Adobe XD are good, free ones).\n\nColor Palettes From Famous Movies Show How Colors Set The Mood Of A Film\n\nThese 50+ movie color palettes show how to effectively use color in film - DIY Photography\n\nSound\n\nThe integration between the Max-patch and Ableton Live is done by accessing Ableton Live’s API. The Live Object Model (LOM) is a road map which illustrates the path to accessing different parameters in Ableton Live. By using the built-in Live-objects in Max for Live these parameters can very easily be accessed and mapped to different aspects of the main patch.\n\n\n    \n    M4L - API\n\n\nThe Ableton project takes advantage of Ableton’s session view. Every scene in the project represents a year in time. The scenes are triggered by interaction with the max-patch. Whenever a year change triggers an event in the max patch a scene above or below the current scene is triggered, depending on whether the year slider has been moved forward or back in time. The global quantization in Ableton is also being controlled by the Max for Live patch. The global quantization controls when the scene changes will happen. To make the scene changes happen smoothly the global quantization is set to two bars for the first 11 scenes and 8 bars for the remaining.\n\nIn the Ableton project, we have created a separate track where all the samples for the sonification are located. These samples get triggered separately from the rest of the project and there is no quantization on these samples, so the playback is instantaneous. This was a deliberate choice to accentuate the interaction between the user and the auditory feedback.\n\n\n        \n         \n\n\nMax for Live\n\nThe [OCEANS] Max for Live device is designed as a framework tool, helping make the sometimes complicated process of data-sonification and interaction a lot more straightforward.\n\n\n   \n   M4l device\n\n\nInput data-sets are primarily indexed by the time of recording, then sub-indexed by location of observation - collecting the time-indexed data as parent-groups, and distributing nine location-indexed sub-sets to a 3x3 grid of [x, y]-coordinates (see the figure above). Four fixed variables are then monitored by three color-coded objects as they move around within the [x, y]-space.\n\nIf the patch registers an object as being on a node-point, the output will accurately reflect the location-indexed data mapped to that node. When moving between the nodes, however, a bi-linear interpolation makes sure the numbers glide fluidly between their fixed value-set states. Moving between the time-indexed parent-groups, set all nine nodes time-traveling in uniform motion from one temporal representation to another (see the figure below).\n\n\n   \n   Interpolation: Space/Time\n\n\nEvents can be mapped to trigger when specific objects visit specific location-nodes at specific time-indexes. A directly editable cell-matrix - its rows and columns reflecting time - space - and color of object - makes composing event-sets quick and intuitive.\n\nTogether with the movement of the object-monitored values, these events can be sent a) internally within Ableton Live - by Max for Live devices (oceansLIVEAPI.amxd) global [sends] and [receives], or b) externally to other hardware or software - by MIDI-note, or control-messages.\n\nImplemented in the patch is a TUIO tracking of Reactable-markers as well as a bi-directional communication with the OceanSC TouchOSC layout. With the possibility to import data-files by drag-and-drop - and mapping values to be monitored through direct editing - the [OCEANS] Max for Live device makes a thorough effort to have exploring sonification be as accessible as possible.\n\nTracking\n\nWe ended up using fiducial markers through the reacTIVision framework for tracking objects. We strongly considered using Pozyx, as it’s supposedly more rigid and accurate. However, Covid-19 made it difficult to access any other equipment than what we had in our homes.\n\nThe reacTIVision framework consists of a computer vision engine (a desktop application), and a client application in Max to receive OSC messages from the engine.\n\nThe computer vision engine is programmed to recognize a pre-defined set of fiducial markers, each with a unique ID. For our purpose, we would select three of these markers and attached a printed copy of them to three boxes, as shown in the figure below, which we in turn would place in the installation room.\n\n\n    \n    Tracker object\n\n\nTo be able to use the reacTIVision framework in stable way, we have made a wrapper around it to control the outcome of the tracking in a way that makes more sense for our installation. Two of these extensions are demonstrated visually. The one-to-many control was implemented to support a varying number of visitors in the installation. Once one of the trackers is detected as idle, another one controls its parameter representation. Exactly how this implementation is done can be seen in the source code.\n\n\n    \n    Smoothing of tracking input which helps make the tracking more true to actual movement\n\n\n\n    \n    One-to-many control. The blue object controls the parameters of the yellow one\n\n\nEvaluation of the proposed solution\n\nAs a consequence of the ongoing lockdown, the development of this installation has been hampered. The lack of a proper testing environment has resulted in a somewhat theoretical framework for an installation, even though we have tried to make sure that the parts fit together. As an example, testing the tracking within the proper installation environment\n\nWe have also yet to sit down with Murat Van Ardelan for a round data analysis, and to incorporate his analysis of the data in the final aesthetic shaping of our installation. We recognize this as one of the most important aspects that we would need to do before deploying the installation. As of now, the data that we use to parametrise our installation is based on a dataset that we have interpreted freely.\n\nReferences\n\n\n  Schuster, Paul F., et al. Permafrost stores a globally significant amount of mercury. Geophysical Research Letters 45.3 (2018): 1463-1471. URL\n  Soerensen, A. et al. (2016). A mass budget for mercury and methylmercury in the Arctic Ocean. Global Biogeochemical Cycles, 30(4), 560-575. URL\n  Mu, C., Schuster, P. F., Abbott, B. W., Kang, S., Guo, J., Sun, S., Wu, Q., &amp; Zhang, T. (2020). Permafrost degradation enhances the risk of mercury release on Qinghai-Tibetan Plateau. Science of the Total Environment, 708, 135127. URL\n\n",
        "url": "/applied-project/2020/05/03/oceans.html"
      },
    
      {
        "title": "Sonification of Standstill Recordings ",
        "author": "\n",
        "excerpt": "The goal of this thesis was to develop and experiment with a set of sonification tools to explore participant data from standstill competitions. Using data from the 2012 Norwegian Championship of Standstill, three sonification models were developed using the Max/MSP programming environment. The first section of the thesis introduces sonification as a method for data exploration and discusses different sonification strategies. Momentary Displacement of the position was derived from the position data and parameter mapping methods were used to map the data features with sound parameters. The displacement of position in the XY plane or the position changes along the Z-Axis can be mapped either to white-noise or to a sine tone. The data variables control the amplitude and a filter cut-off frequency of the white noise or the amplitude and frequency of the sine tone. Moreover, using sound spatialization together with sonification was explored by mapping position coordinates to spatial parameters of a sine tone. A “falling” effect of the standing posture was identified through the sonification. Also audible were the participants’ breathing patterns and postural adjustments. All in all, the implemented sonification methods can be effectively used to get an overview of the standstill dataset.\n",
        "content": "Introduction\n\nMusic and body motion are strongly interconnected. As a drummer, I always feel strongly connected to the music with my motion in a performance and I am fascinated by how a genre or the type of music influences the body movements. During the studies in Music, Communication and Technology study program, I had the opportunity to actively participate in the Norwegian Championship for Standstill competition in 2019 and grasp the methods of the whole process starting from setting up the Motion Capture system to choosing the final winner of the competition. The MoCap system outputs a rich continuous data stream with a larger number of data points and my motivation behind the thesis was to combine this data stream with sound to hear the participants’ movements. Moreover, this implementation can be used as an instrument for a “Standstill performance” and opens a new door for a sonic interaction space with human micromotion.\n\nResearch Questions\n\nThe thesis is based on One main research question and two other sub-questions. From the main research question I wanted to broadly addresses on exploring the connection between the music and involuntary motions of the body.\n\nHow can sonification be used as a method to explore music-related micro-motion?\n\nDuring the Norwegian Championship of Standstill competition, the participants are in a forced condition to not move. And based on past studies, there is statistical evidence that music stimuli have an impact on standing still. Apart from using a visualization method to analyze the micromotion, sonification can be used to listen to the data and find out any audible patterns. The main objective of this question is to find out what kind of difference can be noticed in the motion during the music stimuli is played and whether the sonification can reveal any information that was not visible in statistical studies.\n\n\n  What kind of motion patterns are audible from the standstill competition data?\n  How can spatial audio be used in the sonification of human standstill?\n\n\nBuilding the application for sonification\n\nThe sonification is applied for data from the 2012 standstill competition data. Initial idea was to build a prototype using the 2012 standstill data and use it for the exploring rest of the database including yearly data for the competition. According to Jensenius et al. (2017), around 100 participants were joined the study, and the final data set consists of 91 participants. The sessions were held in groups for about 5-17 participants at a time.\nFor making the sonification, I decided to use the Max/MSP environment. Due to the steeper learning curve, lack of coding experience, and limited time frame, using a programming language such as “supercollider” or “Python” felt like an unrealistic goal for this thesis. Max/MSP provides a great GUI based programming environment that has a much faster learning curve and also a large community of users which is helpful.\n\nData Set\n\nFrom the recorded data, two data sets were available to use for the sonification. The first data set consists of all the x, y, z position data for each participant, which is 273 columns and 35601 rows of data.\n\n\n   \n\n\nThe second data set is based on the demographic data of each participant and consists of quantitative and qualitative data. I like to mention the data variables that consist of the participant demographic data set. Which are the group each participant belongs( A, B, C, D, E, F, G, H, P), Participant number, Age, Sex, Height, Music listening hours per week, Music performing/production hours per week, Dance hours per week, Exercise hours per week. And some measurements were based on a Likert-Scale: Tiresome experience of the session (1 to 5), Experienced any motion (1 to 5), Experienced any motion during the music segment (1 to 5). Two other variables indicate if the participant had their eyes open/close or had locked knees/or not during the experiment.\n\nSonification Strategies\n\nI used parameter mapping sonification with three different approaches which are,\n\n\n  Sonification of individual participant data.\n  Sonification of group average position data.\n  Using spatial sound with individual position data.\n\n\nIn each option, the first half of the data (3 min) represents the participants standing with silence and the second half (3 min) with the music stimuli. In that way, one of the aims was to experiment if the sonification can reveal information on how the music stimuli affect the motion during standing still. Another aspect was to explore how keeping the eyes open/closed or having locked knees can affect the motion. According to Jensenius et al. (2017), there was no statistical evidence that these factors affected the micro-motions. Minimum and maximum position values of each Axis x, y, z was calculated for each participant in Excel and these values were used for scaling the parameter values during the mapping process.In the standstill experiment, data were recorded at a rate of 100hz and each session lasts for 6 minutes. Listening to the sonification faster than real-time can provide better insights on the patterns that occur in the data set and the first step was to implement a strategy to read the data from the CSV file and have the option to change the data reading speed.\n\nDisplacement of Position\n\nInstead of directly mapping the x, y, position data, a new variable is defined which is the displacement of the position. However, I’m not dividing the displacement of the position by the time factor to calculate the rate of change of the QoM since the rate of the displacement also depends on the chosen sample rate in the patch.\n\nFigure 3.3 shows a part of the patch which calculates the change of the position (displacement). First, the displacement of position is calculated for each axis of data. In each moment, the previous position value is subtracted from the current position value and the absolute value is calculated. By using this value, the displacement can be derived for each plane (XY, YZ, XZ) by pairing the sums of individual position displacements for each x, y and z Axes. According to the results of the study by Jensenius et al. (2017), most motion is happening in the XY plane, and in the sonification, primarily the displacement of the XY plane is considered.\n\n\n   \n\n\n1 . Sonification of Individual Participant Data\n\nAs presented in Figure 3.4, displacement position values in the XY plane or position values of the Z-Axis can be selected for the sonification. For mapping in the noise section, the total displacement of position in the XY plane or the position values from the Z-Axis can be mapped to the amplitude of the noise and to the cut-off frequency.\n\n\n\n  \n  Your browser does not support video tag.\n\nVideo demo:Sonification of Individual Participant Data\n\n\n2. Sonification of Group Average Position Data\n\nFigure 3.4 is an extract from the max patch that calculates average displacement values for each x, y, z Axes for two participant groups. However, this patch is only compatible with the 2012 standstill data and since the average values depend on the number of participants in the group, further customizations are necessary to use it with other standstill competition data sets. In the mapping, a similar approach to the individual participant mapping has being followed. The average position values are used to calculate the average displacement position values in the XY plane and mapped to control the noise amplitude and cut-off frequency or the Sine tone frequency and amplitude. Also, the average Z-axis values can be used to control the parameters of the noise section or the sine tone.\n\n\n\n  \n  Your browser does not support video tag.\n\nVideo demo:Sonification of Individual Participant Data\n\n\n3. Using Spatial Sound with Individual Position Data.\n\nThe third approach of the sonification is to apply spatialization for the position data. The position values of x, y, z Axes represent a location in the three-dimensional space and these values are used to “sonify” the motion using spatial attributes of a sound. The spatialization approach is developed by using the ICTS10 ambisonics module for Max/MSP. It allows to simply input cartesian coordinates (X, Y, Z) or spherical coordinates (Azimuth, Elevation, and Distance) and render the sound output for a speaker system or headphones. In this patch, the position data is only controlling spatial parameters of a sine tone.\n\n\n\n  \n  Your browser does not support video tag.\n\nVideo demo:Using Spatial Sound with Individual Position Data\n\n\nResults\n\nMapping a white noise with data variables.\n\nFirst, I would mention the results for the mapping of white noise with data variables. The mapping of white noise to displacement position variable with a 100 Hz sample rate of data playback, produced a sound of rising and falling of the frequency filtering and the amplitude. These fluctuations were present for all three participants. These up and down movements of the sound can be distinguishable in three levels. First, a darker sound can be noticed with less volume which stays much stable with fewer fluctuations. These low fluctuations of the noise can probably explain by the stability of the standing still. Secondly, a much sharper rising and falling of the noise is noticeable for 5-15 seconds periods of time. The periodicity of these sounds is not sharply consistent. These sounds are shorter rises that last for about 2 seconds but clearly stands out from the much stable darker noise. These rising and falling of the noise can be related to the breathing patterns of the participants. Statistical evidence was found in previous standstill studies regarding similar periodic movements (Jensenius &amp; Bjerkestrand, 2011). Thirdly, sudden quick bursts of the noise can be noticed, and they occur less frequently and not in any noticeable cycle. This type of sound may be occurring due to postural adjustments of the participants. As described in a study by Jensenius and Bjerkestrand (2011), some “spikes” that appear every 2-3 minutes were noticed in the analysis and these were assumed as postural re-adjustments.\n\n\n   \n     \n     Your browser does not support audio tag.\n   \n   Audio: displacement of position of Participant 1 mapped to white noise in 100Hz data playback rate \n\n\nMapping Z- Axis values\n\nMoreover, an interesting phenomenon was noticed in the mapping of Z-Axis values. By listening to the mapping of Z-axis values with the sine tone, a gradual dropping of the sine tone frequency was noticed for all three participants. It can be noticed that the sudden fluctuations of the sound are very low compared to the fluctuations noticed in the mapping of displacement position values. Which could potentially result because of the less up and down movements of the head when standing still (Jensenius, 2017). The gradual drop of the sine frequency may probably be indicating a gradual fall of the position for standing over a longer period. As mentioned in a previous study of a standstill experiment, “This could be an indication that we ‘sink’ a little when standing still over time, but the numbers are too small to point to a strong conclusion” (Jensenius et al., 2014). And a sudden rise of the frequency can be noticed after certain frequency drops. This can be possibly due to the reason of participants re-adjust their postures by trying to straighten their back (Jensenius, 2017). Also, this falling of the sine frequency is strongly noticeable towards the very end of the session, and this might be indicating a falling of the posture due to fatigue for standing for a longer period. Moreover, two of the three participants (participant 1 and participant 10) indicate a dropping in the frequency of the sine tone after halfway through the session and keep a lower frequency until the end of the session. This might be indicating that the participants tried to relax the body to maintain the stillness when the music starts and resulted in a sudden falling of the height. However, this must be tested with a larger number of participants before having a strong confirmation.\n\nLet’s listen to an audio sample.\n\n\n   \n     \n     Your browser does not support audio tag.\n   \n   Audio: Z-Axis values of Participant 1 mapped to Sine tone in 800Hz data playback rate \n\n\nApplying spatial audio\n\nFinally, mapping the position values for spatialization produced an interesting moving pattern in the ambisonic sound field. But any regularity or periodicity of the patterns was not noticed. However, details on the elevation of the sound are not accurately perceived from the headphones due to the lack of a binaural rendering option in the ICTS module.\n\nConclusions\n\nNow I would like to address my main research question and the sub-questions, based on the development and the results of the sonification process. First, I would like to consider the two sub-questions.\n\n\n  What kind of motion patterns are audible in standstill competition data?\n\n\nBy considering the results of the individual participant data sonification, the most noticeable finding is to be able to listen to the falling and re-adjustments of the posture of participants. As a previous study of standstill mentions, more strong evidence was needed to conclude the downward movement in the Z-Axis data (Jensenius et al., 2014). However, by listening to the sonification, this effect of falling of the posture is clearly audible and provides strong evidence that there is a noticeable gradual downward movement and sudden rises of the position marker along the Z-axis. Furthermore, the mapping of the noise with the position displacement variable provides supportive evidence that the breathing patterns and postural adjustments are audible from the sonification. However, a stronger investigation is needed to confirm the periodicity of such events at “Meso” and “Macro” temporal levels. But the sonification indicates that the micro-motions are in a reasonably stable state and have consistent motion.\n\nHowever, by listening to the sonification for the sections with music and without music, it is difficult to notice any comparable differences between the movement patterns. By listening to the sonification, it is difficult to notice if the participants’ motion shows any synchronization to the music stimuli or not. But it was possible to notice a falling of the height (falling of the posture) when the music stimuli start, and towards the very end of the session. Moreover, by listening to the sonification, any recognizable effects on the micro-motion according to different postural choices such as keeping eyes open/close or keeping the knees locked/not locked was not noticeable.\n\n\n  How can spatial audio be used in sonification of standstill?\n\n\nThe possibilities of using spatial audio with the sonification of standstill are not fully examined in this thesis. By maintaining proportionality between the x, y, z position values in the spatial mapping, tends to produce a less noticed spatial movement of the sound due to the very small range of motion (micro-motion). However, another approach would be to expand the micro-motion by mapping the range of each x, y, z axes values into a larger range. But the patterns that audible in the ambisonics sound stream are not explainable with strong evidence and doesn’t explain how these motions of the sound can be related to the micro-motion. Further, the development of the model is necessary to effectively draw any strong conclusions of the spatialization of the sonification.\n\nFinally, I would like to reflect upon the main question of the thesis:\n\nHow can sonification be used as a method to explore music-related micro-motion?\n\nAccording to the results of the analysis of sound samples, a conclusion can be made that through sonification it is possible to identify patterns and events to a certain extent, which were not clearly visible in the visualizations of the standstill data. The introduced sonification modules, however, provides less opportunity to actively compare between sections. As an example, comparing a data section with music stimuli and without music stimuli was a difficult task since due to the high irregularity of the sound patterns. This kind of audio comparison demands a high skill of actively memorizing and comparing a certain characteristic of the patterns. One way to avoid this kind of complexity is to play different data streams simultaneously with each stream assigned with a unique distinguishable sound. As an example, mapping the position data of two participants into two frequency ranges of a sine tone and listening to both audio streams simultaneously. However, when developing such a sonification model, the psychoacoustic phenomena of “masking” should be potentially considered. The mapping of the position displacement values with the noise seems to produce a more effective and pleasing sound compared to the sine tone. A fluctuation of the filtered noise was mostly sounding closely related to the sound of breathing and felt more natural. And the sine tone seems to be an appropriate mapping for the values of the Z-axis since the gradual dropping of the frequency was more appealing to represent a downward movement of the position. The use of white noise seems to be aesthetically appealing. Using pleasant and natural sounds tend to improve the efficiency of a sonification model(Susini et al., 2012) However, these aesthetically pleasant sounds can also have the potential of distracting the listener and hide the important information from the sonification (Vickers &amp; Hogg, 2013). Having a perfect balance between these factors is a challenge when designing a parameter mapping sonification for data exploration. Another aspect to consider in the mapping is the perceptual correlation between sound parameters. As an example, frequency and gain both contribute to the loudness parameter where higher frequencies tend to be perceived as loud or bright. As Grond and Berger (2011) point out, an efficient mapping for loudness can be achieved by applying a proper frequency- dependent amplitude compensation technique .\n\nAs final thoughts, the sonification models developed in the thesis are mostly appropriate for navigating through the standstill data set and gain a quick overview of the data patterns. To gain the full potential of the data features, more options should be developed for data management. These sonification methods can be utilized as guides along with the visualization techniques to explore the Standstill competition data. Even though the sonification can convey useful information from the data features, a successful evaluation of the sonification process heavily depends on the skill of listening. It is necessary to develop a “skill” to actively listen to the data set and identify potential information. As Hermann and Ritter (1999) emphasize, training is necessary to interpret the sonification correctly, and thus after a longer period of training it is possible to develop expertise on identifying subtle changes and patterns.\n\nReferences\n\nBarrett, N. (2016). Interactive spatial sonification of multidimensional data for composition and auditory display. Computer Music Journal, 40(2), 47-69.\n\nBrazil, E., &amp; Fernström, M. (2011a). Auditory icons. In The sonification handbook.\n\nBrazil, E., &amp; Fernström, M. (2011b). Navigation of data. In The sonification handbook.\n\nBrewster, S., &amp; Murray, R. (2000). Presenting dynamic information on mobile computers. Personal Technologies, 4(4), 209-212.\n\nBurger, B., Thompson, M., Luck, G., Saarikallio, S., &amp; Toiviainen, P. (2013). Influences of Rhythm- and Timbre-Related Musical Features on Characteristics of Music-Induced Movement. Frontiers in Psychology, 4(183). doi:10.3389/fpsyg.2013.00183\n\nCandey, R. M., Schertenleib, A. M., &amp; Diaz Merced, W. (2006). Xsonify sonification tool for space physics.\n\nde Campo, A., Frauenberger, C., &amp; Höldrich, R. (2004). Designing a generalized sonification environment.\n\nDombois, F., &amp; Eckel, G. (2011). Audification. In The sonification handbook.\n\nFranklin, K. M., &amp; Roberts, J. C. (2003). Pie chart sonification. Paper presented at the Proceedings on Seventh International Conference on Information Visualization, 2003. IV 2003.\n\nGonzalez-Sanchez, V. E., Zelechowska, A., &amp; Jensenius, A. R. (2018). Correspondences between music and involuntary human micromotion during standstill. Frontiers in Psychology, 9, 1382.\n\nGrond, F., &amp; Berger, J. (2011). Parameter mapping sonification. In The sonification handbook.\n\nGrond, F., Bovermann, T., &amp; Hermann, T. (2011). A supercollider class for vowel synthesis and its use for sonification.\n\nHermann, T. (2008). Taxonomy and definitions for sonification and auditory display.\n\nHermann, T., Hunt, A., &amp; Neuhoff, J. G. (2011). The sonification handbook. Berlin: Logos Verlag.\n\nHermann, T., &amp; Ritter, H. (1999). Listen to your data: Model-based sonification for data analysis. Advances in intelligent computing and multimedia systems.\n\nHermann, T., &amp; Ritter, H. (2005). Model-based sonification revisited—authors’ comments on Hermann and Ritter, ICAD 2002. ACM Transactions on Applied Perception (TAP), 2(4), 559-563.\n\nHunt, A., &amp; Pauletto, S. (2006). The sonification of emg data. Paper presented at the International Conference on Auditory Display (ICAD).\n\nJanata, P., &amp; Childs, E. (2004). Marketbuzz: Sonification of real-time financial dataa.\n\nJanata, P., Tomic, S. T., &amp; Haberman, J. M. (2012). Sensorimotor Coupling in Music and the Psychology of the Groove. Journal of Experimental Psychology: General, 141(1), 54-75. doi:10.1037/a0024208\n\nJensenius, A. R. (2015). Microinteraction in music/dance performance.\n\nJensenius, A. R. (2017). Exploring music-related micromotion. In Body, Sound and Space in Music and Beyond: Multimodal Explorations (pp. 29-48): Routledge.\n\nJensenius, A. R., &amp; Bjerkestrand, K. A. V. (2011). Exploring micromovements with motion capture and sonification. Paper presented at the International Conference on Arts and Technology.\n\nJensenius, A. R., Bjerkestrand, K. A. V., &amp; Johnson, V. (2014). How still is still? exploring human standstill for artistic applications. International Journal of Arts and Technology 2, 7(2-3), 207-222.\n\nJensenius, A. R., Zelechowska, A., &amp; Gonzalez Sanchez, V. E. (2017). The musical influence on people’s micromotion when standing still in groups. Paper presented at the Proceedings of the SMC Conferences.\n\nKaper, H. G., Wiebel, E., &amp; Tipei, S. (1999). Data sonification and sound visualization. Computing in science &amp; engineering, 1(4), 48-58.\n\nKramer, G. (1991). Audification-The Use of Sound to Display Multivariate Data. Proceedings of ICMC1991, 214-221.\n\nKramer, G., Walker, B., Bonebright, T., Cook, P., Flowers, J. H., Miner, N., &amp; Neuhoff, J. (2010). Sonification report: Status of the field and research agenda.\n\nMcGookin, D., &amp; Brewster, S. (2011). Earcons. In The sonification handbook.\n\nMynatt, E. D. (1994). Designing with auditory icons: how well do we identify auditory cues? Paper presented at the Conference companion on Human factors in computing systems.\n\nNasir, T., &amp; Roberts, J. C. (2007). Sonification of spatial data.\n\nPolli, A. (2004). Atmospherics/weather works: A multi-channel storm sonification project.\n\nRoss, J. M., Warlaumont, A. S., Abney, D. H., Rigoli, L. M., &amp; Balasubramaniam, R. (2016). Influence of musical groove on postural sway. Journal of Experimental Psychology: Human Perception and Performance, 42(3), 308.\n\nSusini, P., Misdariis, N., Lemaitre, G., &amp; Houix, O. (2012). Naturalness influences the perceived usability and pleasantness of an interface’s sonic feedback. Journal on Multimodal User Interfaces, 5(3-4), 175-186.\n\nVickers, P., &amp; Hogg, B. (2013). Sonification Abstraite/Sonification Concr\\ete: An’Aesthetic Perspective Space’for Classifying Auditory Displays in the Ars Musica Domain. arXiv preprint arXiv:1311.5426.\n\nWalker, B. N., &amp; Cothran, J. T. (2003). Sonification Sandbox: A graphical toolkit for auditory graphs.\n\nWalker, B. N., &amp; Nees, M. A. (2011). Theory of sonification. The sonification handbook, 9-39.\n\nWorrall, D. (2011). An Introduction to Data Sonification. In: Oxford University Press.\n\nWorrall, D., Bylstra, M., Barrass, S., &amp; Dean, R. (2007). SoniPy: The design of an extendable software framework for sonification research and auditory display. Paper presented at the Proc. ICAD.\n\nWorrall, D., Tan, D., &amp; Vanderdonckt, J. (2019). Sonification Design: From Data to Intelligible Soundfields. Cham: Cham: Springer International Publishing.\n\nZhao, H., Plaisant, C., Shneiderman, B., &amp; Duraiswami, R. (2004). Sonification of Geo-Referenced Data for Auditory Information Seeking: Design Principle and Pilot Study. Paper presented at the ICAD.\n",
        "url": "/masters-thesis/2020/05/13/Sonification-of-Standstill.html"
      },
    
      {
        "title": "Sound Painting",
        "author": "\n",
        "excerpt": "An application that tracks, visualizes and sonifies the motion of colors.\n",
        "content": "In this project, I have explored how sonification through motion tracking can make daily activities more interesting and multi-modal. I have done this by creating a sound painting application that tracks and visualizes colors in a video and produces sounds based on the motion of those colors. My goal was just to create a visually and sonically precise color tracking system, in MaxMSP, that was intuitive to use and easy to understand.\n\n\n\n  \n  Your browser does not support video tag.\n\nVideo Demo\n\n\nWhen I starting working on this I immediately realized that certain key aspects were essential for the project to succeed. The application would have to:\n\n\n  Track some motion precisely.\n  Visualize the motion tracking in a meaningful way.\n  Extract tracking coordinates in realtime.\n  Use tracking coordinates as basis for sonification.\n\n\nTracking\n\nI experimented with three different methods of motion tracking. The first method revolved around creating a bounding box around specific RGB-values before calculating the center coordinates of that box. Practically however, this was not successfull because my webcamera feed was constantly adjusting due to various lighting conditions which in turn generated quite unpredictable RGB-input.\n\n\n   \n   Tracking method 1 - Findbounds\n\n\nThe second method featured motion tracking through generating a motion image, the difference between two consecutive frames, and calculating the center of mass (centroid) from this image. Additionally, to enhance the visualization of the tracking, I implemented a feature that paints several colored circles on the display based on the centroid coordinates using the jit.lcd object.\n\nThis method yielded better visual results but still lacked a good sense of agency similar to the first method. This was because the centroid was calculated from the sum total of motion on the screen.\n\n\n   \n   Tracking method 2 - Centroid of Motion\n\n\nIt thus became clear that a combination of color tracking and centroid calculation would be the way to go. This meant that I had to devise a better color tracking module.\n\nI decided to use a set of filtering approaches to isolate specific colors in the video-stream and further processes it so that it satisfied the input requirements of the centroid calculation. Additionally, I added a controllable noise threshold parameter to allow for some calibration of the module to accommodate various contexts. Then, I implemented it together with the visualization module previously constructed and made the system respond to both blue and red colors.\n\n\n   \n   Tracking method 3 - Centroid of Color\n\n\nSonification\n\nFor the sonification I ended up using a model where the motion controls the central frequency and the number of active audio channels in the mix. I chose these parameters because of their independence of one another which I believe could enhance sonic proprioception of an object moving around in a two-dimensional space.\n\n\n   \n   Sonification diagram\n\n\nThe frequency of the sound is controlled by movement along the y-axis. When the user moves an object along the x-axis, more audio channels are audible and the frequency of the previous channel is held in place. The number of audio channels can also be adjusted which consequently divides the x-dimension of the screen into greater of fewer equally big sections. The idea is that this would generate a path of sound, like a pencil sketching lines on paper.\n\nTo further enhance this effect I used panning to spatialize the sound based on the number of audio channels. The panning values are also automatically rescaled from left to right according to the number of channels selected.\n\n\n   \n   Panning diagram\n\n\nSumming up\n\nAfter several rounds of testing I was generally surprised by how well the color tracking worked, both by itself and in conjunction with the centroid calculations for the visualization and sonification. However, there was still some jitter when subjecting the system to various lighting conditions and different locations. In these scenarios, the noise threshold parameter comes in handy and was usually able to calibrate the system to a working standard.\n\nAnother interesting discovery was the musical use of audio channels. Although I originally expected the sonification to be somewhat uninteresting, the dynamic use of audio channels uniquely colored the sound. However, when painting with multiple colors on a high number of channels, the sounds quickly gets cluttered and noisy.\n\nFuture Development\n\nA future development if this project would involve extracting more motion parameters to create more complex sonifications and visualizations. For instance, it would be interesting to extract the acceleration of the movement and the intensity of the colors to enhance the sense of agency and user control. Additionally, by having a standardized size for the tracking objects and extracting the size of those objects in realtime, it would be worth exploring whether it was possible to represent a virtual 3D space through binaural panning or other spatial audio techniques.\n",
        "url": "/motion-capture/2020/05/14/sound-painting.html"
      },
    
      {
        "title": "I scream, you scream, we all scream for livestream",
        "author": "\n",
        "excerpt": "Some cameras won’t allow you to film for more than 30 minutes, don’t use those.\n",
        "content": "Intro\nBuilding on the experience derived from having concerts in a very controlled environment, such as the portal and the experience we have from photography and live-shows, we have done several streaming concerts to help artists reach their audience during lockdown. Covid19 presented society with a host of challenges, and continuing to have concerts is one of them. We have put together a quick guide for those who want to put together their own streaming-concerts and developed routines and techniques that can be used for concerts in the portal, when the campus opens up.\n\nAfter doing some research, we came to the conclusion that OBS Studio is the best solution if you want to do streaming. OBS is open source, very well maintained and has an intuitive, modern and feature packed interface.\n\nLighting\nHave good lighting on stands. 3-point lighting with soft-boxes are preferred. The LED lights are the most reasonable type of lights for this task. When utilizing lamp bulbs, they are very hot and if you don’t have a proper dimmer it is overpowering, being even uncomfortable to the artist in the scene. If you don’t have proper material, like a diffuser for example, you are gonna deal with harsh lighting, which is not aesthetically appealing and it doesn’t look professional at all. By the other hand, when it comes to LED lights, in most models you have much more control of the intensity and other features of the light. Even in the cheapest models, you can find temperature control, intensity (dimmering) and some of them come with a soft-box for diffusing the lights. The LED lights are also much lighter and portable, requiring cheaper stands and less effort to transport it and set it up.\n\n\n\n\n\nWe have used 2 spot lights, but they were too big and too heavy for the task. No need to overkill with lighting and melt the artists during the performance. On the other hand, they were very useful, especially because we had a dimmer to control the power of the lights. They were well used when the face and other details of the subject in the scene were a bit in the shadow.\n\nAudio and Video processing\nOBS provides support for utilizing audio and video processing, you can use native plugins or 3rd party plugins of your choice. To find these options you have to right click on the video device listed, to find the option Filters on the menu that pops up. We have used it to pop up the musicians and give contrast to the scene when we were using a webcam, that despite having quality, had a bit of a washed picture.\n\nBasic video Processing\nYou can start it by colour correction. To do that, you have to add (click on the + icon) a video plugin, chose the Color Correction and you are gonna see a few basic options such as:\n\nGamma\nIt is a very sensitive parameter, small changes make a big difference. Don’t overdo it. Gamma makes the bright areas of the image brighter, while the dark areas will be darker when increased. It is similar to Contrast, but affects more the middle range or grays of the image.\n\nContrast\nIn case your picture is a bit “washed”, and you need to make the subject and objects of the scenario stand out, increase contrast. Bear in mind that increasing it too much can make certain regions of the image too dark, or too bright, and also can lead you to have a picture that feels amateur.\n\nBrightness\nSimilar to Gamma, it is better used for small corrections. If your video is dark, slowly increase it, but don’t try to fix a lighting problem that should be done by a correct light placement and natural environment lighting. Sometimes the subject can be too exposed too, in this case you should decrease subtly the value of this parameter.\n\nBasic audio Processing\nWhen mixing for live broadcast, you got to think differently than if you were mixing for a concert venue.\n\n\n\nWaves LV1 and Yamaha HS8\n\n\nCompressor\nUse the compressors the same way as you would in any other mix (try to be tasteful), but keep in mind that the audience you are mixing for will probably not hear your mix in an optimal environment. Your mix will be played through anything from earbuds to a large HIFI, so try to master your tracks accordingly. Most importantly, set a limiter before the signal reaches OBS. Think that you should aim at reducing the peaks by 3 to 6db and keep a steady signal at -8 to 12 and you will make sure that your work is heard in all situations.\n\nMonitoring\nWhen doing a stream, good monitoring is key.\nThe most difficult thing is to separate yourself from the acoustic output of the band, since this will make it very hard to have a correct perception of what the remote audience will hear.\n\nMicrophones\nStreaming a concert through a camera is a very different thing than a live concert. Since cameras often zoom close to the face of the people singing or the instrument being played and the audience does not have the distractions they do at a concert, you have to keep in mind that the microphones can become obstructive in the image. Try as best as you can to select discrete or visually pleasing microphones and it will look a lot better than using regular stage-mics.\n\nA USB mic can greatly reduce the complexity of a setup. You can find good condenser USB microphones for a good price, and it takes out the need of a sound card, since the USB microphones are plug-and-play, meaning you just need to plug it directly to your computer’s USB port.\n",
        "url": "/networked-music/2020/05/18/streaming.html"
      },
    
      {
        "title": "Breathing through Max",
        "author": "\n",
        "excerpt": "For the COVID-19 version of motion-capture, I developed a system to track your rate of breath and sonify it through Max. It emphasized the tenants of biofeedback and hopes to serve as a responsive system for stress relief.\n",
        "content": "Rationale\n\nAnalyzing motion from smartphone sensors or laptop cameras is a both a challenging scenario and exciting opportunity. This project transforms the live acceleration data recorded from an iPhone resting on the torso of a person laying down into a dynamic sonic atmosphere that may enhance the monitoring one’s breath through auditory cues. One potential use of this system would be as a meditation or relaxation assistant. The sound environment aims to be soothing and ought to coax one into a deeper and more rhythmic breathing cycle. There is ample empirical data to suggest that paced respiration can reduce anxiety and other measures of perceived stress. Following common themes of this literature, biofeedback is a topic of importance in recognizing the effects of this system on a user.\n\nThe inhales and exhales are identified as peaks and troughs within the accelerometer data and as a result, an average breathing rate can be obtained that serves as a marker of how consistently rhythmic the individual’s breathing rate is. This indicator is used to grow and shrink the sonic environment as one’s breathing and body begin to settle. On the surface, the information gained from the chest’s rise and fall appear to be an easy to manipulate stream. However, most of the time spent on the project involved developing a reliable and flexible model for processing a very noisy stream of data due to the micro-movements that were being recorded. Various parameters of the system were then modulated by other much more noisy sensor data as well.\n\nMethodology\n\nThe first task before embarking on this project was finding a suitable app that would be a reliable recorder and streaming platform for micro-motion data. Sadly, there are not many apps that provide a data logging functionality on iOS and even fewer that provide a way to record data from the sensors and only one that appears to meet the requirements I had for such an app,\n\n\n  Log data at a high frequency (&gt; 50hz)\n  Record multiple sensors at once\n  Export this data in CSV format and\n  Send this data over a networked connection\n\n\nMost of these apps transmit their sensor data over a UDP connection using the OSC protocol. This type of communication is well supported by many DAWs and platforms like Max and Pure Data. However, the app that was used, SensorLog unfortunately, did not send data with the OSC protocol. Instead, it sends the data over UDP as it would when writing a CSV file that was not compatible with Max’s [udpreceive] object.\n\nTo work around this, I wrote a python script that acted as a node to receive, format the stream into proper OSC messages, and finally, send the data locally to a specified port within Max. The script operated by receiving a row of data, splitting the list of data points into an array, and then sending each index of the array as a pair with a specified OSC message. I used the messages “/x”, “/y”, and “/z” as shorthand for the x, y, z, acceleration data. Once this was passed into Max, the data was able to be handled the same way as any other OSC message.\n\n\n    \n    Sample of the code used to transport the messages into Max\n\n\nAfter some initial testing of various sensors and positions during recording, I found that the acceleration value for the y-axis was the most reliable source of information for tracking respiratory motion. Because of the placement of the phone, none of the other sensors render any useful information. The only other sensor that appeared to follow the same information was the gyroscope, but the sensor upon inspection looked like a transformation of the acceleration data. In the first trial recording set of breathing, I was able to find that this sensor’s y-axis extracted recognizable oscillations of the rise and fall of my stomach. However, because the movements were quite small, the noise of the sensor clouded what was, in reality, a smooth envelope of motion.\n\nReading the CSV data into Max was achieved by loading each line of the CSV as a text file from which I could use a [metro] object to iterate through the [coll] dictionary that stored the sensor data (I stole this bit of Max data). From Max, I could visualize the data stream easily using a [multislider] window. I then set out to smooth this data through a variety of techniques. I found that a reliable method of smoothing the noisy data was to create a buffer of the last x number of samples (with [zl.stream]) and then output the mean ([mean]) of that sliding window. The larger the number of samples, the smoother the data, yet this stunts some of the local dynamics within the stream and creates latency. After exploring further I found a 3rd party object [dot.denoise.sliding] that includes better logic for excluding outliers within streams.\n\n\n    \n    Before and after de-noising the stream\n\n\nThis returns to an observation I noticed when looking at sensor data. I noticed that one of the gyroscope sensors also was repeating in a rhythmic pattern, but much faster and pronounced than breathing should be. I realized that these were pronounced fulgurations of my heartbeat and that these impulses were actually affecting the accelerometer sensor data I was analyzing from the y-axis.\n\n\n    \n    Notice the fast paced impulses from the gyroscope sensor\n\n\nOnce the oscillations were smoothed, the next step was identifying local minimums and maximums of each breath. Again, the dot library had a nifty object to find local max/mins by checking for changes in sign (+/-) from a previous data point to the next. This would have worked nicely if the data rose and fell with continuity. But due to the noise, there were a number of places near the peak or trough of the oscillations where even a smoothed data point might change the sign of the differences between the current and previous point.\n\nI tried number of techniques to suppress the improper max/mins that would appear such as forcing a minimum delay to check for a max after finding a min or max, as well as attempting to doubly embed local max/min detection. The most successful and flexible method by far was setting a minimum distance between the breaths that were taken. This is a reliable method as a single breath could vary in speed and the expansion of the torso would be the same.\n\nI further enhanced this solution by making the distance dynamically shifted by an average of the distance between the last 5 max and mins.\n\n\n    \n    Gating the frequent max/mins\n\n\nFinally, the last piece of data that is inferred from identifying the local max/mins of each breath is the average time between each inhale and exhale. This is calculated as a metric of respiratory rate consistency which is the difference between the last period between a max/min and the average of these last 10 periods. This value is then scaled and used to attenuate a sound clip of wind chimes that enter when the consistency value is low. Thus, the wind chimes should emerge as a result of a restful cycle of breath and provide positive feedback to the user. In addition to the wind chime samples, there is a three-oscillator drone instrument was driven by the noisy data from the x and z-axis accelerometer streams. This drone adds a subtle resonance to the bells and provides some ambient feedback of the motion of the breath. This is achieved by scaling the smoothed acceleration data from the y-axis and modulating the amplitude.\n\nResults and Conclusions\n\nAfter debugging the system for quite some time, I may not be able to provide the best perspective on how useful it is for its intended purpose. Even still, the system may allow a user to pay attention to their biorhythms and less to other distracting thoughts that might interrupt a session. Most of the time spent building the system was in an effort to fine-tune the analysis of these data streams in real-time so many of the acoustic elements were secondary to this objective. The streams were processed flexibly and reliably for the dynamic stream and I’m quite happy with the way the data is handled throughout the system – even in extreme cases such as erratic motion where the amplitude of all sounds is reduced to silence. From this perspective, I find the system quite usable but I am not entirely pleased with the sound of the instruments and sonic environments.\n\nThere are still some unanswered questions I have about more technical aspects of the system. The first being the latency contributed form formatting UDP messages in a Python node and then passing it on to Max. I was fine with this compromise considering the app’s considerable logging speed (100Hz) which enabled much of the noise to be averaged. Another question is one of compatibility: If I were to use another sensor app that was able to directly send OSC data, perhaps at a lower logging rate, would this dramatically affect the system’s ability to identify peaks and troughs? For future work, the system’s reception logic should allow general purpose OSC enabled apps to connect to the system in addition to SensorLog for better compatibility.\n\nYou can find the code here along with a setup.\n\n\n    \n    \n    Your browser does not support video tag.\n    \n    A demo of the system using pre-recorded data\n\n",
        "url": "/motion-capture/2020/05/18/breathing-through-max.html"
      },
    
      {
        "title": "a t-SNE adventure",
        "author": "\n",
        "excerpt": "A system for interactive exploration of sound clusters with phone sensors\n",
        "content": "Idea\n\nThe goal of this project was to develop a system for interactive exploration of sound clusters. More specifically, I wanted to extract features from sounds, cluster them together in a virtual 3D space with t-SNE [1][2], and make an interactive environment for exploring these sounds.\n\nThe idea was initially sketched out with an optical tracking system in mind,i.e. Optitrack. Due to the circumstances of Covid-19, the available technologiesfor tracking motion were either mobile phone sensors or web camera by usingcomputer vision algorithms to extract motion data. As one of the main conceptsof the project was to make use of a 3D space, the choice quickly landed onphone sensors, since they are readily available to extract data from in threedimensions.\n\nResult\n\nThe video below is a quick demonstration of the end result.\n\n\n\n\nThe Max patch controls the visual and auditory elements. It communicates with a Python script that receives sensor values from the phone as OSC, and then processes the data.\n\nSource code: https://github.com/ulrikah/tsne-adventure\n\nMethod\n\nThe initial plan for the interactive environment was to create a 1:1 room-scale virtual mirror of an actual room, e.g. a living room. By using a phone to trigger samples through measurements of the jerk (derivative of acceleration), the user would then be able to move around in the room and explore the sounds. With an optical tracking system, there would be no need for a phone to do this. Instead, one could use a suitable physical object, e.g. a drumstick, attach a reflective marker to it, and then use the jerk of that marker to determine a hit.\n\nExtracting position from the phone sensors (by double integration of the accelerometer values) turned out to be more difficult than expected. The complications were mainly related to noisy accelerometer values and corresponding drifting. After a couple of failed attempts to try to obtain a smooth experience by going through resources provided by the course professor, I decided to use the gyroscope in the phone to control the position instead. The way I ended up doing this was by mapping the pitch, yaw and roll to the velocity vector of a virtual controller object, inspired by ball-in-a-maze puzzles. In the figure below, the orange object represents this controller object. The grey spheres are samples.\n\n\n    \n    Virtual model with the controller in the middle\n\n\nTo trigger the samples, I used the jerk value from the accelerometer sensor. Jerk is the rate of change of acceleration, and can be derived in both x, y and z axis. By performing an abrupt movement that causes a jerk value over a certain threshold in any of the axis, the user triggers the sample to which the controller (see the orange blob in the figure) is closest to. Intuitively, this is similar to a typical percussive instrument, where the musician hits a surface to make sound. This method of triggering samples was inspired by last year’s motion tracking project. The jerk threshold was set by trial and error.\n\nTo determine which sample to trigger, I picked the sample with the lowest euclidian distance to the controller object.\n\nTo avoid that successive jerk values triggers the same sample too frequently, I also added a cooldown of 200 ms between each sample trigger.\n\n\n   \n   Jerk movement\n\n\nReflections and future work\n\nThe most apparent problem for interacting with the system is the way the position is currently being controlled by the orientation of the phone. This would naturally be solved by replacing the control method with an optical motion tracking system.\n\nBy only using the euclidian distance to determine which sample to be triggered, I realised that I often found myself triggering the same samples over and over again. To be able to better explore a sound cluster, I would look into adding some randomness into the sample selection. One way of doing that could be to do some weighted random selection of the closes N samples. It is also possible that this issue would resolve itself by using the optical tracking system, as it allows for finer control of position.\n\nIn terms of interaction and user feedback, I would like to improve the visual aspects of the patch as well. In particular, coloring the currently playing audio samples should be a top priority.\n\nDue to the multifaceted nature of the project, the auditory output of the system was not the main priority. I would like to experiment a lot more with different ways of manipulating the sound. Further work on the project should also focus on finding a better way of triggering samples. The current solution is rendering the audio buffers at runtime, which is not ideal at scale. Additional gain normalization, filters and envelopes should also be implemented.\n\nClustering\n\nTo cluster the files, I used an implementation of the t-distributed Stochastic Neighbor Embedding algorithm (t-SNE) from scikit-learn. t-SNE’s strength is visualising high-dimensional data [1]. For this project, I extracted features based on root mean square value of the audio signal, mel-frequency cepstrum, spectral contrast and spectral centroid.\n\nThe end result, as seen in figure of the virtual model, does not contain many distinct clusters, which was a bit disappointing. Perhaps it was due to the relatively sparse feature extractors. Even though I experimented with a range of different perplexity values to get the best result (see figure below), I would like to try other methods of clustering in the future. One possibility that I thought of at the very end of the project was to choose a set of exactly three features, and to map the samples according to the mean values of those features, each representing an axis in the 3D space. In that way, the sample would be distributed linearly on the axes by some known feature extraction method. This could potentially make it more understandable for the user why the samples are placed where they are. Future work could also include looking into ways of letting the user change the feature extractors themselves.\n\n\n    \n    Perplexity values [5, 10, 20]\n\n\nReferences\n\n\n  Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research, (Nov):2579–2605, 2008\n  Wattenberg, M., Viégas, F., &amp; Johnson, I. (2016). How to use t-SNE effectively. Distill, 1(10), e2. https://distill.pub/2016/misread-tsne/\n\n",
        "url": "/motion-capture/2020/05/20/tsne-adventure.html"
      },
    
      {
        "title": "The Notion of Dialogue in the Interactive Dance",
        "author": "\n",
        "excerpt": "The constituent elements of interactive dance are human and computer, which in a human-computer interaction, create a feedback loop, and present the work of art. Considering that matter, each of the opponents in this interaction has their part and space and there is an aesthetic relationship ongoing, defining the quality and amount of each opponent’s part and space. In this thesis, this ongoing matter is referred to as the notion of dialogue. To create this sense, the key element that will be discussed is surprise. In order to do that, following a certain design strategy, a practical system will be designed and executed to fortitude the logical argument that is presented in this research. In that performance, by the creative use of the body, space, time, popular art forms (i.e., Hip-Hop music and dance), and with the focus on the subject of sea-level rise, the research argument will be put in practice and further on evaluated. After the evaluation process, in conclusion - despite its limitation - it may be concluded that the use of surprise, will allow the computer to have an active role and possess a significant part in the interaction and convey a sense of dialogue in it.\n",
        "content": "Introduction, background and the main objective\nJust as other art forms, dance has been significantly influenced by the development of digital technology over the last three decades (Mullis, 2013). Electronic or digital interactive systems have been experimented within the dance domain (Bevilacqua, et al., 2011). ‘Full-body interaction with electronic or digital media has undergone experimentation for more than fifty years both in arts and science communities’ (Alaoui, et al., 2013, p. 358).\n\nAs a sound artist/engineer, I have previously participated in the creation of an interactive dance performance that involved a dancer and two sound artists/engineers, interacting with each other through a feedback loop. The dancer was the initiator of the performance and the sounds created by her movements were received by the sound artists. They manipulated the sounds and played them back to the dancer, which provided her the sonic material to dance to. In this interaction, the interesting point was that the dancer found it hard to define her active role/part in this interaction. In other words, it was challenging for her to recognize if she had more influence on the sound artists - by the creation of the sounds - or she was more under the influence of the sound artists. This presents the effort of each opponent in the interaction, to possess their part in it.\n\nConsidering Mullis’s argument in his article, Dance, Interactive Technology, and the Device Paradigm, I am referring to the non-human part of the interaction as the device (Mullis, 2013). Since the participants in the interaction are the dancer and the device, it requires the device, as the non-human part of the interaction, to take certain measures to be able to convey a sense of an active opponent for the dancer. That means in this common interaction, the device may become able to possess its part in the interaction, in a dynamic manner. In other words, in the interactive dance, the device and dancer are the participants and possess their part in the interaction. I refer to this ongoing struggle as dialogue.\n\nResearch Questions\nThe research question here is: ‘How and to what degree can an interactive dance performance with a Hip-Hop dancer using absolute orientation sensors and auditory input, convey a human-computer interactive dialogue through surprise and movement-sound mappings?’\n\nTheoretical framework\nThe logic here is to create the sense of dialogue between the participants in the interactive dance system (i.e., the dancer and the device), a sense of surprise needs to be created by the device since surprise disrupts the dominance of the dancer in the interaction and allows the device to take its part in it. According to Huran, human beings are wired for expectation (Huron, 2006). Therefore, by disrupting this expectation we can achieve a sense of surprise in the interaction and by disrupting that we can disrupt the possibility of the dancer to be in full control of the device in the interaction, and let the device e possess its parts in it. In order to do that I utilized to strategies, randomness and dependent mapping system.\nRandomness refers to random acts from the device, they are not necessarily being affected by the dancer. Dependent mapping system refers to different responses, to the same movement inputs from the dancer, based on different temporal and spatial circumstances that the dancer is executing those movements. These strategies were used in practical work to evaluate the mentioned argument. The design strategy included an initial phase that was handled by the researcher alone, and then the iteration phase, which included Abdullah Ghazanfar, as the dancer/co-designer in the procedure. The iterative design happened in four iterations. However, the presence of Abdullah was not consistent because of the Coronavirus pandemic regulations, and the second iteration was done considering the feedback received by an evaluator.\n\nImplementation and design\nIn order to fortitude the theoretical argument presented, regarding the research objective, a system was designed and implemented. This interactive system focused on Hip-Hop dance, and its subject was about the issue of sea-level rise. The device’s output was an audio-visual one. Regarding the inputs of the device, it captured the dancer’s movements by two sensors (BITaline R-iOT sensors), tied to the dancer’s wrists. Also, it captured impulsive sounds (i.e., clapping and foot-banging) by a contact microphone and a condenser one, in order to trigger events on and off. The data received by those sensors were used to some extent, to control the device’s audio/visual output. The device’s output consisted of a video projection and speakers. I used MAX/MSP as the programming environment considering its efficiency for working with both audio and video materials and also its suitable and convenient user interface.\n\nInitial design\nIn this phase, the device was mostly responsive to the dancer. Although it was designer, so it could be a platform that could be developed further, by the help of co-designer. Regarding the visuals, a video, representing the matter of the sea-level rise was projected on the walls. It was being flipped randomly on the walls. The visuals included footage of water surface, half below and a half above the water, representing the sea-level rise. The rising started from one side of the projection and continued till it reached to the other end (filled out the projection).\n\nRegarding the audio outputs of the device, it consisted of a lifeline and also Piano and Drum synthesization. The synthesization was done via sonification of the dancer movement’s data (accelerometer data). Also this data was used for implementing effects (delay) on the created drum rhythm. Therefore, he had control over the whole process (via the sensors). By Lifeline I am referring to a sound or combination of several sounds, with any possible nature (mostly ambient) that is present during the entire time, in a performance. This lifeline was controlled by the device and was linked to the video output. It consisted of an underwater sound and a ambient sea sound that was filtered out, in sync with the water level rise in the visuals. The water level rise in the visuals was tracked by color-tracking and was linked to a high pass filter which filtered out higher frequency areas of the ambient sea sound spectrum, by rising the sea level, making the sound darker and less bright as it rose. Also, an underwater sound was played back, which its level had a direct relationship with the high pass filter on the ambient sea sound and as the filtering amount rose, the level of underwater sound rose as well. In other words, with raising the water level, more ambient sea sound was filtered and the level of underwater sound was raised and it became more audible and dominant. This procedure’s sample and samples of the mentioned significations are presented below:\n\n\n  \n    \n    Your browser does not support audio tag.\n  \n  Transition between above the surface and underwater sound\n\n\n\n  \n    \n    Your browser does not support audio tag.\n  \n  Piano sonification\n\n\n\n  \n    \n    Your browser does not support audio tag.\n  \n  Drum sonification\n\n\nAlso, here is an audio example, presenting the audio output of the initial phase:\n\n\n  \n    \n    Your browser does not support audio tag.\n  \n  Initial design audio output\n\n\nUnfortunately, there is no video recorded from the initial phase.\n\nFirst iteration\nMoving on to the iteration phase, I shall start by elaborating on the first iteration. Suggested by the co-designer, the Drum sonification was replaces by played back beats (randomly chosen, in selection and playback time) for the dancer to dance to them, and the delay effect was replaced with a reverb effect which affected the beats. Also, the dancer could turn events on and off by using the acceleration values, accelerating above a certain threshold. Also, the functionality of impulsive sounds was changed and the use of Euler angles data, received from the sensors, was added in this iteration as well. The mappings in this iteration are as follows:\n\n\n  Clapping:\n    \n      Starting the beats playback and Piano instrument\n      Changing the beat manually\n      Piano engagement (on and off)\n    \n  \n  Right-hand movements:\n    \n      Roll: Piano pitch\n      Yaw: Piano velocity (up to zero)\n      Acceleration (X-axis): Piano engagement (triggering on and off)\n    \n  \n  Left-hand movements:\n    \n      Roll: reverb and video delay mix\n      Acceleration (X-axis): reverb engagement and video delay (on and off) / reverb limitation (dependent mapping system)\n    \n  \n  Foot-banging:\n    \n      1st bang: Starting the visuals and lifeline playback\n      2nd bang: Stops the system sound completely\n    \n  \n\n\nAlso, the following video presents the functionality of the system in the this phase\n\n\n  \n    \n    Your browser does not support video tag.\n  \n  First iteration video\n\n\nSecond iteration\nAfter evaluating the system with one user, there were some changes made in the design. Therefore, in the second iteration, the synthesization process was removed and a lecture playback system was implemented in the patch. It included several lectures regarding the subject of the performance, being randomly chosen for playback. Also, the reverb affected the lectures, instead of the beats. Following the implemented changes in the second iteration phase can be observed:\n\n\n  \n    \n    Your browser does not support video tag.\n  \n  Second iteration video\n\n\nAlso here are the mappings for this iteration, which remain consistent during the next ones as well:\n\n\n  Clapping:\n    \n      Starting the beats and lectures playback\n      Changing the beat manually\n      Lectures engagement (on and off)\n    \n  \n  Right-hand movements:\n    \n      Roll: Lectures speed\n      Yaw: Lectures level\n      Acceleration (X-axis): Lectures engagement (triggering on and off) / distortion amount (adding up during time)\n    \n  \n  Left-hand movements:\n    \n      Roll: reverb and video delay mix\n      Acceleration (X-axis): reverb engagement and video delay (on and off) / reverb limitation (dependent mapping system)\n    \n  \n  Foot-banging:\n    \n      1st bang: Starting the visuals and lifeline playback\n      2nd bang: Stops the system sound completely\n    \n  \n\n\nThird iteration\nThe third iteration was done by the help of Abdullah and he raised a point regarding the engagement of the dancer. He mentioned that: ‘in order to become surprised by the changes in the device’s behavior, first you have to be engaged with it’. Thus, regarding that matter, he suggested increasing the number of beats and choosing various beats, from various Hip-Hop sub-genres to make the device’s output more comprehensive and appealing for a wider variety of Hip-Hop dancers. Also, he mentioned that it may increase the chance of the dancer to become surprised by hearing a contrast between the beats the device is playing back to him/her. Following the video recording from Abdullah, trying to interact with the system in this iteration phase, can be found. However, due to a technical problem (i.e., computer overheating), I had to turn off the video pixelation process:\n\n\n  \n    \n    Your browser does not support video tag.\n  \n  Third iteration video\n\n\nFourth (final) iteration\nFourth (final) iteration:\nIn the final iteration, the number of beats was increased to five, and the lectures to six. That provided a wider verity in the device output, for the dancer to engage with it, and Abdullah admitted that matter. It could be observed that implementing a wider variety of beats in the patch, increased the chance of him becoming engaged with the device and also, made it more possible for him to become surprised, by hearing a contrast between the randomly played back beats. He also tried to interact with the visuals and lectures and adjust his movement regarding them. Following, the video recording of this session is provided below. Again, due to computer overheating, I had to turn off the video pixelation process.\n\n\n  \n    \n    Your browser does not support video tag.\n  \n  Fourth (final) iteration video\n\n\nEvaluation of the practical work\nThis cannot be denied that the more feedback the system receives, the more promising the evaluation results would be and the process of future development may be more prospective. But, considering the Coronavirus pandemic, this did not become possible. Although there was access to one evaluator and the feedback received from her (which could be positively biased, based on her, being a relative to the researcher), plus having dialogues with the co-designer and observing his state during the iteration process, in the interaction with the device, a rough evaluation became possible. Based on those, on a limited scale, it can be stated that by using a balanced amount of randomness and increase of the dependent mapping system, the device can become able to affect the dancer’s perception of its behavior and create a sense of surprise, and followed by that, a sense of dialogue in the interaction, for the dancer. However, this evaluation is certainly not optimal based on the aforementioned reasons.\n\nFuture possibilities:\nOne of the main suggestions regarding the future possible works is the use of a multi-channel playback system and randomizing the sound output, between various speakers. This possibility will expand the ability of the device to act towards the dancer and surprise him and reduce his possession of parts, to create a sense of dialogue in the aesthetic relationship between them. The other suggestion is to enhance the dependent mapping system, both in-depth and expansion.\n\nConclusion\nIn this thesis, the research objective was to define how and to what degree can an interactive dance performance with a Hip-Hop dancer, using absolute orientation sensors and auditory input, convey a human-computer interactive dialogue through surprise and movement-sound mappings. The answer to that question, a theoretical framework was defined, and based on those, a design strategy was conducted to create execute the mentioned proposal. This design strategy was put into practice, by the help of a dancer/co-designer, in order to be tested and fortitude the theoretical argument. However, the practical work needed to be used by various dancers, so they could evaluate the prominence of the presented argument in practice. But, considering the Coronavirus pandemic, this did not become possible. Although there was access to one evaluator and the feedback received from her (which could be positively biased, based on her, being a relative to the researcher), plus having dialogues with the designer and observing his state during the iteration process, in the interaction with the device, a rough evaluation became possible.\n\nBased on those, on a limited scale, it can be stated that by using a balanced amount of randomness and increase of the dependent mapping system, the device can become able to affect the dancer’s perception of its behavior and create a sense of surprise, and followed by that, a sense of dialogue in the interaction, for the dancer. However, this evaluation is certainly not optimal based on the aforementioned reasons.\n\nReferences\nAlaoui, S. F., Bevilacqua, F., Pascual, B. B. &amp; Jacquemin, C., 2013. Dance interaction with physical model visuals based on movement qualities. International Journal of Arts and Technology, 6(4), pp. 357 - 387.\n\nBevilacqua, F., Schnell, N. &amp; Fdili Alaoui, S., 2011. Gesture Capture: Paradigms in Interactive Music/ Dance Systems. In: G. Klein &amp; S. Noeth, eds. Emerging Bodies: The Performance of Worldmaking in Dance and Choreography. Hamburg: Department of Human Movement Science / University of Hamborg, p. 183–194.\n\nHuron, D., 2006. Sweet Anticipation: Music and the Psychology of Expectation. Cambridge, London: The MIT Press.\n\nMullis, E., 2013. Dance, Interactive Technology, and the Device Paradigm. Dance Research Journal, 45(3), pp. 111-124.\n",
        "url": "/masters-thesis/2020/06/10/SHthesis.html"
      },
    
      {
        "title": "Multimedia Slideshow Maker",
        "author": "\n",
        "excerpt": "During my master’s thesis, I have designed and developed a tech platform where a mobile application creates slideshows from multimedia content uploaded in a web application titled “Multimedia Slideshow Maker” (MSM). The project is carried out for an external partner, Alight AS, for a project called Alight. Alight is a mobile tech platform aiding caregivers in sending personalised video sessions to patients with dementia. This thesis aims to determine to what degree MSM can be used independently by a caregiver, without instructions from others or prior experience in video editing.\n",
        "content": "Abstract\n\nDuring my master’s thesis, I have designed and developed a tech platform where a mobile application creates slideshows from multimedia content uploaded in a web application titled “Multimedia Slideshow Maker” (MSM). The project is carried out for an external partner, Alight AS, for a project called Alight. Alight is a mobile tech platform aiding caregivers in sending personalised video sessions to patients with dementia. The thesis aimed to determine to what degree MSM can be used independently by a caregiver, without instructions from others or prior experience in video editing.\n\nThe MSM was designed and developed in iterations based on feedback from experienced evaluators and tested with end-users. The system has been compared to similar applications, where both strengths and limitations of the MSM were discovered. Participants were to a large extent able to use MSM without instructions from others, where MSM enabled them to create a multimedia slideshow without requiring experience in video editing. The sample size, however, might not account for all differences in abilities among caregivers.\n\nBackground\n\nThe number of people in Norway and the rest of the world with dementia is increasing every year. Dementia is an incurable disease and does not only affect people with dementia but also their caregivers, families and the entire society (World Health Organization, 2019). Music and reminiscence therapy has been used to improve the quality of life for people with dementia and their caregivers (Istvandity, 2017). As the number of people with dementia is rising, we need to be able to provide care in the patients’ own homes and be able to care for more people at once.\n\nA company called Alight AS is working on a project called Alight. Alight is currently a mobile tech platform for healthcare workers at nursing homes and the home healthcare service. The platform is connecting the healthcare worker to a patient, aiding and sending personalised video sessions to patients with mild to moderate dementia. These so-called “personalised video sessions” are developed with the intention of providing music and reminiscence therapy in a mobile solution, where the structure and content of a video session consist of:\n\n\n  Personalised image slideshow with personalised music\n  Video of the healthcare worker (exercise, live playing, breathing exercise etc.), or other personalised videos.\n\n\nIn the current state of Alight, the video sessions are manually made by employees at Alight. Moving forward, Alight’s vision is to let any caretaker use the tech platform independently to care for people with mild to moderate dementia. A caretaker can be a therapist, home nurse, family, friend, anyone with close relations to a person with dementia.\n\nThe purpose of the thesis\n\nTo help with Alight’s vision, this project has involved the creation of a system that creates multimedia slideshows (video sessions) from content uploaded by a caregiver in MSM. A caregiver can be just about anyone; old, young, tech-savvy, inexperienced in using web apps, with potentially no experience in video editing. This meant that the system had to be designed for a high degree of usability, making the system itself do most of the editing. The aim was then to determine to what degree the MSM can be used independently by a caregiver, without instructions from others or prior experience in video editing.\n\nMusic and reminiscence theory\n\n“Music therapy is recognized as an established health profession that uses music to facilitate therapeutic processes. Even without a professional music therapist, many patients and clinicians listen to or play music to manage stress, anxiety, and pain in clinical settings.” (Kemper &amp; Danhauer, 2005, p. 283).\n\nTone Sæther Kvamme (2008) argues that music therapy is a vital necessity for people with dementia. that music can give people with dementia access to feelings, help them express themselves, awake memories, strengthen their identity, and also give a sense of achievement and affiliation. Even Ruud (2008) argues that the experience of music and how it affects us depends on our musical background, influence, our choice of music, and the situation we are in while experiencing the music.\n\n“Reminiscence therapy involves the discussion of memories and past experiences with other people using tangible prompts such as photographs or music to evoke memories and stimulate conversation” (Woods et al., 2018, p. 1).\n\nA systematic review by Istvandity (2017) found positive effects in combining music and reminiscence therapy, with effects on the mental well-being of people with dementia, especially stress, anxiety and depression. The study could not determine what is the successful delivery of the combination of music and reminiscence therapy.\n\nThe research shows the benefits of using both music and reminiscence therapy to improve the quality of life for patients with dementia, but there is less knowledge of how these therapies should be delivered to be successful. It would be interesting to investigate the effects of music and reminiscence therapy delivered by any caregiver through a mobile app, but that is beyond the scope of this project.\n\nThe system\n\nThe system consists of a web app (MSM) and a mobile app. These platforms are connected through a backend service called Firebase. The intention is that a caregiver will upload content (images, music and video) in the MSM and then send it to an account belonging to the person with dementia. The person with dementia can then sign in to the mobile app and view the multimedia slideshow that is created in real-time in the app. Currently, the system connects the content that is uploaded, to the same account, letting the caregiver preview the multimedia slideshow when signing in to the app. Letting the caregiver send the content to another account will be implemented when it potentially will be integrated with the platform of Alight.\n\n\n\nSystem overview\n\n\nThe MSM\n\nThe MSM was developed with JavaScript, using a library called React and Firebase as a backend service. React was used to build the user interface, making functionalities for uploading content (music, images and video), and for letting the user rearrange the images to their choosing. Firebase was used as a service for authentication (login), database (metadata) and storage (multimedia content).\n\n\n\nTechnical overview of MSM\n\n\nThe mobile app\n\nThe mobile app was made to both create and present a multimedia slideshow in real-time. The reason for making the mobile app do this was the intention to include a music streaming service at a later stage. When a caregiver has to choose music to be consumed by another person it can quickly become a problem with sharing what may often be copyrighted material. Using a music streaming service can deal with this problem.\n\nThe mobile app was developed using a cross-platform development tool called Flutter. Cross-platform means that I could write code that works for both Android and IOS, making the development both easier and faster. Flutter is quite a new tool, with the first stable version released in December 2018. As with many new products, bugs can be encountered, and of course, I did. A day of work could go by, trying to solve a bug. With the help of Even Brenna, an experienced developer at Alight AS, and a strong community of developers at Stack Overflow, it was possible to fix and proceed.\n\nThe functionality of making a multimedia slideshow was developed with the help of three awesome libraries:\n\n\n  Carousel slider - For making a slideshow of images\n  Just-audio - For playing audio\n  Chewie - For playing a video\n\n\nThe programming language of Flutter is called Dart. This was used to handle fetching of data from Firebase, to add images to cache memory, and to schedule all events, making it look something like this:\n\n\n\n    \n\nExample of multimedia slideshow\n\n\nDesigning the user interface for a high degree of usability\n\n“Usability is most often defined as the ease of use and acceptability of a system for a particular class of users carrying out specific tasks in a specific environment” (Holzinger, 2005, p. 71).\n\nThe biggest part of the project was to develop a user interface with a high degree of usability. This was achieved through the use of two methods. The first method used is called Heuristic evaluation (HE), a method from what Holzinger (2005) describes as User Inspection Methods. HE involves usability specialists judging the system, providing feedback to whether the system follows established usability principles or not. I used some popular usability principles called Nielsen’s Usability Heuristics.\n\nI had a total of four design iterations where I received feedback from people with experience in user interaction, constantly improving the MSM based on the feedback received.\n\nIt went from being a single-page app, looking like this:\n\n\n\nFirst iteration\n\n\nTo a multistep form, looking like this:\n\n\n\nCurrent design\n\n\nThe image above shows the step where a user can add and rearrange images by dragging and dropping them in the white container box. The checkmarks above the box are there to illustrate progression in the form. In the previous steps, a title has been added to the multimedia slideshow, and a song has been uploaded. The final step is to add a video, and then submit all the content to Firebase. The content is then connected to the account, where it can be retrieved from Firebase in the mobile app, and a multimedia slideshow is created.\n\nUser tests and results\n\nThe second method used was to test with potential caregivers (end-users). User tests were carried out with one-on-one video conferencing sessions with a total of eight participants, using a service called Whereby. Each participant was guided to download a beta release of the mobile app, and to download multimedia content (music, video and images) which they could use during testing of MSM.\n\nDuring the user test, the participant was instructed to register and sign in to MSM and to create a multimedia slideshow by following the steps instructed by the application itself. While the participant was using the application, I used two methods: Thinking aloud and field observation. Thinking aloud means that the participant verbalises his/her thoughts while going through the process. Field observation involved the participant sharing the screen, making it possible for me to see every action the participant did and if problems or errors occurred. After the test session, I performed an interview and got feedback on the participant’s thoughts about MSM’s usability and the problems that had appeared.\n\nThe tests showed several usability problems, which affects Holzinger’s (2005) five usability characteristics; Learnability, efficiency, memorability, low error rate and satisfaction. No critical errors appeared, but there were some unhandled events with missing or insufficient feedback to the participant. There was one element that stood out as a confusing and non-intuitive, and that was the plus/cross-icon at the home page, which you can see at the bottom of the page in the image below.\n\n\n\nConfusing element\n\n\nThis is the icon that a user had to press to start making a multimedia slideshow. One user had to be told to press it to get started, and three other users asked if they should press it to proceed. The problem with the element is that it does not say “click here to make a multimedia slideshow”. There should be a label, and it should be made to look more “clickable”. There is a problem with “perceived affordance”, a term coined by Norman (2004), based on the original term “affordance” by J.J. Gibson. One participant did not perceive that an action was possible at all, and it was not clear to the other three participants what action it afforded. It can be argued that the three participants who asked if they should press the element would have done so on their own without me being present as an observer, and would have learned its action. Still, it should not be necessary for participants to learn the action’s outcome, and therefore it should be improved.\n\nSeveral other usability problems surfaced, like image previews being too small for some users, an image appearing in a large format while dragging in Firefox, placeholder text being a bit confusing, and partly unclear illustration of progression. Images being too small is quite a serious problem, where one participant was observed to move closer to the screen to view them. As MSM is meant to be used by any caregiver, the application needs to account for older people with reduced eyesight. This has to be improved in the future version.\n\nMost problems affected the overall satisfaction of use. This does not mean that it should not be addressed, it very much should. Most participants went through the application and completed all necessary steps on their own. Even with its usability problems, the participants reported that MSM was easy to use, easy to learn and that it was a quick process.\n\nComparison with similar applications\n\nThere already exit web-based applications for making multimedia slideshows. Three commercial applications that I can compare MSM to, are:\n\n\n  \n    Magisto; markets itself as an AI (artificial intelligence) based video creation and editing platform.\n  \n  \n    Adobe Spark Video (ASV); markets itself as a video maker that lets the user create videos without design skills.\n  \n  \n    Animoto; markets itself as an easy to use, quick to learn, video slideshow maker.\n  \n\n\nAll of these share a common goal: To make a video maker that is easy to use. they all edit and generate a video file from the content you provide as a user. They provide a selection of editing styles (transitions, filters and effects), but the number of editing possibilities varies across the applications. Magisto leaves most of the editing to an AI-based Emotion Sense Technology. ASV includes options such as video trimming, reordering of media, in-browser microphone recordings and more. Animoto has the most editing capabilities, including every feature present in ASV and more.\n\nWhat separates MSM most from these applications is that MSM is made for a single purpose. MSM follows the predefined structure, defined by Alight, where it starts with displaying images while the song is playing, and when the song is finished, the video is displayed (as you can see in the video further up in the blog). Since the videos that are added can be a greeting from a caregiver, musical performances, explanations of exercises and so on, it was important to stop the music while the video is shown. In all the aforementioned applications, the music will play over all the images and videos that are added, and you have no option to change this.\n\nMagisto is the application that is closest to MSM, where most of the editing is left to the system itself. MSM and Magisto are the only two applications where the system will decide the display time of images. In ASV and Animoto, the user has to manually set the display time on every image.\n\nMany of the editing capabilities found in these applications will be interesting to add to MSM, and may very well be added in a future version, but still, it is very important to keep MSM as easy to use for a user without experience in video editing.\n\nComparison with a similar application with the same goal and purpose\n\nAn app proposed by Imtiaz et al. (2018) shows similarities in both system and purpose. The application is described as “a mobile multimedia reminiscence therapy application to reduce behavioural and psychological symptoms in persons with Alzheimer’s”. Alzheimer’s disease is the most common cause of dementia.\n\nThe biggest difference from MSM is that they propose a single mobile application for both the caregiver and the person suffering from dementia, where the caregiver uploads multimedia content in the app on the device, and the patient watches the created multimedia slideshow on the same device. In my opinion, the part of the application that involves the presentation of the multimedia slideshow should be easily shared between caregivers. Let’s say a health worker puts together a multimedia show for a patient living at home. If one caregiver could send the multimedia show over the Internet to another account, any caregiver; friend, family, a health worker, can view the presentation together with the person with dementia.\n\nImtiaz et al. (2018) focus more on assessing the effects of individualised music therapy and reminiscence therapy delivered through a mobile app, and not on assessing the usability of the tool for a caregiver. The research has not gotten to the point where they have been able to test this in a clinical setting, so sadly I cannot say what therapeutic effect this application has on people with dementia.\n\nConclusion and future work\n\nParticipants were to a large extent able to use MSM without instructions from others, where MSM enabled them to create a multimedia slideshow without requiring experience in video editing. The sample size, however, might not account for all differences in abilities among caregivers. The MSM has both strengths and limitations compared to similar applications. MSM does close to all editing itself and is tailored to the structure defined by Alight. Still, it can be improved in some areas, giving the users more freedom with added features, and improving the design to account for the usability problems that were encountered.\n\nI would like to investigate the effect of music and reminiscence therapy on people with dementia, delivered through a mobile app from any caregiver, not only those that are educated as a music or reminiscence therapist. When the mobile app is designed and developed for a person with dementia, this can be assessed in clinical trials. I think it is important to assess both the therapeutic effectiveness and usability by using both qualitative and quantitative methods.\n\nIf you want to read a more detailed description of the project, you will at some point be able to find the thesis at NTNU Open. Search for Multimedia Slideshow Maker.\n\nReferences\n\nHolzinger, A. (2005). Usability engineering methods for software developers. Communications of the ACM, 48(1), 71–74. https://doi.org/10.1145/1039539.1039541\n\nImtiaz, D., Khan, A., &amp; Seelye, A. (2018). A Mobile Multimedia Reminiscence Therapy Application to Reduce Behavioral and Psychological Symptoms in Persons with Alzheimer’s. Journal of Healthcare Engineering, 2018, 1–9. https://doi.org/10.1155/2018/1536316\n\nIstvandity, L. (2017). Combining music and reminiscence therapy interventions for wellbeing in elderly populations: A systematic review. Complementary Therapies in Clinical Practice, 28, 18–25. https://doi.org/10.1016/j.ctcp.2017.03.003\n\nKemper, K. J., &amp; Danhauer, S. C. (2005). Music as Therapy. Southern Medical Journal, 98(3), 282–288. https://doi.org/10.1097/01.SMJ.0000154773.11986.39\n\nKvamme, T. S. (2008). Musikk for demensrammede – en livsnødvendighet? In Perspektiver på musikk og helse: 30 år med norsk musikkterapi (pp. 487–497). Norges musikkhøgskole.\n\nNorman, D. A. (2004). Affordances and Design. 5.\n\nRuud, E. (2008). Et humanistisk perspektiv på norsk musikkterapi. In Perspektiver på musikk og helse: 30 år med norsk musikkterapi (pp. 5–28). Norges musikkhøgskole.\n\nWoods, B., O’Philbin, L., Farrell, E. M., Spector, A. E., &amp; Orrell, M. (2018). Reminiscence therapy for dementia. Cochrane Database of Systematic Reviews. https://doi.org/10.1002/14651858.CD001120.pub3\n\nWorld Health Organization. (2019). Dementia. https://www.who.int/news-room/fact-sheets/detail/dementia\n",
        "url": "/masters-thesis/2020/06/13/MSM-Varpe.html"
      },
    
      {
        "title": "Motivato",
        "author": "\n",
        "excerpt": "Motivato: A standalone music selection system for seamless technology-mediated audience participation.\n",
        "content": "Abstract\n\nThis thesis describes the design and implementation of a standalone music selection system designed for technology-mediated audience participation: Motivato.\nThe creation of Motivato is part of a case study to address design issues emerging with the use of technology-mediated audience participation.\nFeedback and observation from existing literature in the field of technology-mediated audience participation are used to detect the emerging issues.\nDesign issues found are further used as guidelines in the design decisions of Motivatio.\nFor instance, previous works find that spectators do not tend to use objects during concerts, and therefore states that we need to consider the role of tangible interfaces in the interaction design for technology-mediated audience participation.\nFurthermore, previous works also indicate that participants prefer seamless interaction to tangible interaction in technology-mediated audience participation.\nConsequently, a seamless interface for interaction has been implemented in Motivatio. Results demonstrate several design issues emerging from the use of technology-mediated audience participation, emphasizing two key issues: clear feedback and the number of users.\n\n\n\n\n",
        "url": "/masters-thesis/2020/06/15/Motivatio-Elias_Sukken_Andersen.html"
      },
    
      {
        "title": "Harmonic interaction for monophonic instruments through musical phrase to scale recognition",
        "author": "\n",
        "excerpt": "Introducing a novel approach for the augmentation of acoustic instrument by providing musicians playing monophonic instruments the ability produce and control the harmonic outcome of their performance. This approach is integrated in an interactive music system that tracks the notes played by the instrument, analyzes an improvised melodic phrase, and identifies the harmonic environment in which the phrase is played. This information is then used as the input of a sound generating module which generate harmonic textures in accordance with the identified scale.\n",
        "content": "Abstract\n\nThis thesis introduces a novel approach for the augmentation of acoustic instruments by providing musicians playing monophonic instruments the ability to produce and control the harmonic outcome of their performance. This approach is integrated into an interactive music system that tracks the notes played by the instrument, analyzes an improvised melodic phrase, and identifies the harmonic environment in which the phrase is played. This information is then used as the input of a sound generating module which generates harmonic textures in accordance with the identified scale. At the heart of the system is an algorithm designed to identify a scale from the played musical phrase. The computation relies on established music theory and is based on musical parameters retrieved from the performed melodic phrase. A database of audio recordings comprised of improvised phrases played by several saxophonists is used to test the algorithm. The results of the evaluation process indicate that the algorithm is reliable, and it can consistently recognize the scale of an improvised melody conveyed by a live musician. This discovery led to the exploration of the affordance to influence accompanying harmony by a monophonic line and integrating the phrase-to-scale match algorithm within an interactive system for music-making. By interacting and playing with the system using a repurposed controller mounted on the saxophone, performance strategies and practical ways are offered to play, modify, and further develop the system.\n\nIntroduction\n\nIn my thesis I present a novel approach for controlling harmony, sometimes referred to as the “vertical” aspect of music, with its “horizontal” aspect, the melodic line. The suggested approach includes a software application, an algorithm, and a method for repurposing a video game controller, to track the pitch of a saxophone, capture and analyze an improvised musical phrase, determine the scale of the phrase, and use that output for music generation in an interactive manner.\n\nRecent advances in the field of music production, and the technology available for anyone who wishes to produce music by electronic means, have enabled a wave of artists to develop a personal sound, produce their own music, invent instruments or write code to serve their artistic needs and aesthetics. Musicians are now able to invent and customize music applications to further their artistic research and remain original and inventive. Machine learning algorithms allow the musician of today to interact and play with artificial intelligence models with a great deal of communication and musical expression. The use of technology, in combination with traditional acoustic musical instruments in a wide range of musical genres, is becoming mainstream. Technological developments like the loop-pedal, the harmonizer effect, various instrument augmentation projects, and the invention of the electronic wind instrument controller, have helped the saxophone to maintain its place as one of the most popular instruments across musical genres. These advancements help it to remain at the forefront of bridging technology with acoustic musical instruments.\n\nInstrumentalists in general, but more specifically, saxophonists, face several limitations when it comes to using technology when playing, whether it is performing a concert, recording in a studio, or practicing at home. The saxophone, for example, cannot be muted in a considerable way without affecting the timbre quality or the overall experience of playing the instrument, unlike the electric guitar or the trumpet.  A key challenge for saxophonists employing technology in a performance setting is the operation of additional devices, controllers, or interfaces while playing. They experience limited spare bandwidth since playing the saxophone requires using both hands, almost all fingers and the mouth. The solution for this is usually using foot-pedals or using the hands during musical pauses.\n\nFrom a personal perspective, playing the saxophone for over three decades, and being involved with performing improvised music for the past 20 years, I found myself in search of ways that will allow for harmonic control. The saxophone is a monophonic wind instrument capable of producing only one note at a time (disregarding advance Multiphonics techniques), and since note sustain and tone quality are determined by the length of the air stream and the physical combination of the instrument and player, this can be seen as limiting at times. I found that playing or controlling harmony by using additional devices can be quite tricky when considering both the limited physical bandwidth and the attention required. My motivation when approaching this thesis was to develop a tool for woodwind players that will enable monophonic instruments to control or play harmony and, at the same time, that would ‘feel natural,’ be intuitive and promote creativity.\n\nResearch question and objectives\n\nMotivated by the concept of controlling the harmonic output of an interactive music system with the melodic output of an improvised line, as well as identifying the lack of, and thus the need for  a system that grants this type of interactivity, my research question becomes: Can a system and an algorithm be developed to successfully identify the scale of a musical phrase for collective music generation? By reflecting upon this question, my objectives were realized accordingly:\n\n  Developing and evaluating an algorithm that will successfully identify the scale of an improvised musical phrase played by a monophonic wind instrument.\n  Creating a database containing improvised phrases by several saxophone players in different keys and scales for evaluating the algorithm.\n  Integrating the scale recognition algorithm in a real-time interactive music system available to users and developers as free and open-source software.\n  Developing, exploring, and presenting a practical way to play and interact with the system.\n\n\nContribution and system overview\n\nThe thesis contribution primarily stands on the development of the Phrase to Scale Match (PSM) Algorithm. This algorithm analyzes a monophonic musical phrase of any length and outputs an estimated scale name that matches the input phrase from a dataset of 21 common scales. The algorithm calculates a matching scale based on several variables like the number of note repetitions, note duration, and other changeable weight-increasing factors for characteristic scale notes and note recurrences. Furthermore, a comprehensive system is presented for handling audio input, detecting pitch, analyzing a musical phrase, and appropriating a retro game controller to be used as a control interface together with two sound generating modules.\nIn the proposed system, the data flows through the following submodules:\n\n  Analog to digital conversion of the microphone signal, which captures the musician’s improvised phrase\n  Pitch tracking module\n  Buffer capturing the performed notes, including controls allowing the musician to start and terminate the capturing process\n  Scale recognition module, based on the method presented in this thesis\n  Musical applications using the output of the recognition module (drone and arpeggiator in the current version)\nThe proposed system and the scale recognition algorithm presented in this thesis offers a way to track the notes of a musical phrase, analyze it, calculate the tonality and scale, and output the result to several applications for music generation. The users can manipulate parameters of the system via a dedicated controller that is attached to the saxophone. The user interface (Figure 1) provides visual feedback of the audio input, detected notes, matched scale, bass note and pressed buttons of the controller.\n\n\n\n   \n   The user interface\n\n\nThe system was developed in Max/MSP/Jitter environment (commonly referred to as Max), a visual programming language for music and multimedia by software company Cycling ’74. Max was chosen due to its flexibility for creating interactive music systems, the capability to generate audio and the ability to integrate other programming languages within its environment. The PSM algorithm part of the system is written in the JavaScript programming language since the procedural operations required to identify the scale are too difficult to implement using Max objects by themselves.\n\n\n   \n   Modules and processes of the system\n\n\nThe Phrase to Scale Match (PSM) algorithm\n\nThe proposed algorithm takes an improvised monophonic musical phrase and matches it to a musical scale from a scale-dataset of 21 scales. For the algorithm to successfully identify the scale to which the improvised phrase belongs, two conditions must be met. First, the first note of the musical phrase must begin with the bass note (1st degree) of the scale. Second, the improvised phrase must be based on one scale only.\n\n\n   \n   The three parts of the PSM algorithm\n\n\nThe PSM algorithm is comprised of 3 parts:\n\n  Aligning the input notes, so that they are all in the same octave, and are relative to the first degree.\n  Generating a histogram of the aligned notes played in the phrase, where each note is assigned a weight indicating its importance and impact.\n  Comparing the histogram against a scale-dataset, calculating a rank for each scale, and selecting the one presenting the highest-ranked score.\n\n\nGame controller adaptation\n\nA retro-like Super Nintendo Entertainment System (SNES) controller is used as the user-interface of the system, similar to the one in Figure 18  . The Max hi object allows for obtaining data from human interface devices. The Nintendo USB gamepad features 12 momentary push buttons, and was chosen because its shape fits well on top of the low B and Bb key-guard of the saxophone, as visible in Figure 19, and also because it allows for easy access with the right-hand which is employed less than the left-hand when playing the sax. The adoption of the SNES controller is inspired by one of the principles of Perry R. Cook on redesigning computer music controllers: ”funny is often much better than serious.” A Nintendo controller illustration has been added to the GUI of the system together with an overlaying buttons layer to provide visual feedback when pressing the buttons.\n\n\n   \n   Nintendo controller attached to an alto saxophone\n\n\nMusic Generation Modules\n\nThe JavaScript object outputs a scale name based on the analysis done by the PSM algorithm. This information can now be used in various ways for interactive music generation. At the moment, two music generating modules have been developed to give an example to the kind of opportunities the system offers.\n\nDrone Module\n\nIn this module, the 1st, 3rd, 5th and 7th degrees of the output scale are extracted to form what is known as a “seventh chord”. The notes of the chord are converted from pitch-class notes to midi integers based on the key of the phrase, and then frequencies. The four frequencies, representing the four notes, are played by four sinusoidal oscillators with randomized amplitude in different time points and lengths. This creates a drone effect where the chord is continuously playing while offering a changing harmonic texture. A Rotary knob allows for controlling the gain output of the Drone module. The player can continue improvising on the scale until deciding to feed the algorithm with a new harmonic environment by pressing the Capture On/Off  button again.\n\nArpeggiator Module\n\nThe Arpeggio module is like the Drone module, but this case, all seven degrees of the output scale are converted to frequencies and are randomly played over three octaves with bell sounds. The notes are panned back and forth to play between the left and right channels (ping-pong effect); two rotary knobs allow for controlling the speed of the arpeggiator and the volume. The speed knobs of the arpeggiator and the option to set the step size of the speed buttons (by increments of 10, ranging between 10-100) are mapped to the controller’s D-Pad in this way: up – faster, down – slower, right – decrease step size, left – increase step size.\n\nA combination of the Drone and the Arpeggiator module gives a very subtle harmonic textural background for a performer to play and improvise over. This can also serve as a practicing tool to exercise scales and improvisation. By creating a randomized melody, the bell-like notes can also serve as a melodic inspiration for the improviser, though this interaction only goes one way. More examples of additional interaction opportunities this system can offer are discussed under the future work section.\n\nEvaluation\n\n105 phrases were tested against the algorithm, or 21 phrases (in 21 scales) times five players. The entire database was pushed into the algorithm a total of eight times with different sets of factor (fi,  fr) values. The results are plotted in a graph (Figure 23), comparing the success rate of the eight tests. In test 1, the fi and fr values were set to 1.0, meaning no weight increase was applied. The success rate in test 1 is 84%. Tests 6 and 7 represent the highest success rate (89%) when tested with fi =1.1 and fr=1.1/1.15. The result of test 1 tells us the algorithm itself, without giving any increased weight to indicator notes or repetitive notes, already have a high success rate (an average of 17.6 phrases recognized correctly out of 21 per player). When reviewing the results in Figure 23, we can also establish that increasing the success rate can be achieved by setting very small fi and fr values. In test 5, where the increased factor values were higher in comparison to the other tests (fi =1.25 and fr=1.2), the success rate showed poor results (78%).\n\n\n   \n   Comparing the success rate between eight tests\n\n\nIt is important to discuss several crucial points when coming to evaluate the algorithm with the current database. First, we should keep in mind that improvised musical phrases can be seen (or heard) in a very subjective way, where one player can consider a phrase in one scale while another player can consider the same phrase in a different scale. The perception of phrases and their scales can differ substantially from one player to another. The PSM algorithm is, in a way, a deterministic algorithm, attending a subjective or an individual problem. Second, tuning (of pitch) affects the algorithm’s prediction. Several of the recorded phrases begin with a bent note or contain untuned notes throughout the phrase. A bent note at the beginning of a phrase means that the bass note of the phrase is being identified incorrectly by the pitch tracker, consequently swaying the phrase prediction. Phrases containing untuned notes will always be recognized falsely, and changing the weight factors will not affect their prediction outcome. As an example, in Table 8, player 1 holds a success rate of 95% in all tests, where 20 out of 21 phrases were recognized correctly. When inspecting the results, we can see that the Mixolydian scale was the one scale mistakenly recognized in all tests of player 1. This might suggest that there is a problem with the phrase itself and not necessarily with the algorithm. These kinds of database discrepancies make it difficult to evaluate the PSM algorithm and essentially mean that a larger and more accurate database is required for evaluation (See “future work” for additional reflections regarding a database).\n\nPlaying with the system\n\n\n\nA way to evaluate the system can also be by playing with it and develop musical approaches to advance its potential. This kind of evaluation comes from personal reflections after I was able to experiment with the system. I would also recommend the reader to see the attached demonstration video and listen to the musical examples in it.\n\nBy playing with the system in my rehearsal room, I have noticed that sound is bleeding back from the speakers into the microphone, creating false detection of notes and, eventually, scale outcome. My solution was to use two microphones, one, a condenser mic with low gain for pitch tracking, and the second, a dynamic mic for the acoustic sound of the saxophone. This solution significantly reduced the false detection of pitch and scales. I was standing directly in front of the speakers, which also contributed to the bleeding; therefore, for live performance, I would recommend standing behind the speakers and using headphones for monitoring. In any case, it is essential to be able to clearly hear the auditory output of the system to enhance creativity.\n\nJust like with any musical instrument, device, or controller, mastering requires mastery. Getting used to the controller requires time and practice, but after several hours of playing, my fingers were able to find the buttons instantaneously and without much searching and looking. Another concept that was a bit more difficult to grasp and one that will require more rehearsing is the concept of controlling the harmony by just playing the saxophone. This affordance is new to me, and it is one I have never experienced before. When talking about musical expression with new human-computer interfaces, Dobrian and Koppelman, (2006) state that to reach a level of sophistication achieved by major artists in other specialties like jazz or classical music, it is necessary to encourage further “dedicated participation by virtuosi” to utilize existing virtuosity and develop new virtuosity. A skillful musician with great technical skill and harmonic knowledge, and one that holds an advanced level of virtuosity, will undoubtedly be able to play with the system and benefit from this new affordance of controlling harmonic textures by playing melodic phrases.\n\nFrom an artistic standpoint, it is exciting and inspiring to play with the system. The long chordal Drone sounds, in combination with the rhythmic notes played by the arpeggiator, provide a subtle and comforting harmonic environment that allows for relaxed and intuitive improvisation. The system is responsive enough that I was able to communicate a scale with both short and long phrases. I was able to identify both correct and false scale matches of the system by looking at the UI on the computer screen and by hearing the sonic output of the system. This situation of staring at the computer screen is not ideal and is addressed in the future work section. While improvising, there were several instances in which the system recognized a different scale than what I communicated or intended to. This can happen due to noise, sound bleeding, or simply wrong playing on my part. I view those instances as a musical challenge and an opportunity to change the harmonic direction of the piece, just as if I would when playing with a live musician. I experimented playing with the Drone and Arpeggiator, both together and separately, and was able to achieve diverse musical textures that provoked different kinds of playing on my side.\n\nSome performance strategies that I have developed by playing with the system over this short period are:\n\n  Less is more – build the performance by adding and interacting with each sound generating module at a time.\n  Keep in mind that it is possible to start a piece by playing the saxophone first and letting the sound modules join after, or by letting the sound modules play first and joining them after.\n  It is sometimes hard to remember that the system can detect a scale with only a few notes. There is no need to play a lot to convey a scale, and certainly no need to play all the notes in scale order.\n  By setting different speeds, the arpeggiator module provides three kinds of textural material: temporal, melodic, and harmonic. It is easier to play and interact with slower tempos, while rapid tempos can be perceived as harmony to the human ear. Experiment and employ all possibilities.\n  In the current implementation, the system recognizes the scale of a phrase in reference to the first note of the phrase (the bass note). This forces the player to always start the phrase with the bass note, and it is a condition that can be somewhat limiting. A way to evade this is by starting to play a phrase from any note the player wishes to, and only press the capture button before playing the intended bass note of the scale. In this way, the listener will hear a phrase starting on one note, but the system will start capturing the phrase from a different note.\n\n\nConclusion\n\nTo conclude, the work presented in this thesis successfully addresses my original research question. The algorithm is able to recognize the scale of a musical phrase with high accuracy, and the system offers the musician the ability to influence the harmonic output by merely playing an improvised phrase. The degree of randomization applied to the sound generating modules creates the impression of several entities collectively generating music and sharing the creative control, in almost the same manner as a group of live musicians would do. The outcome of this thesis provides musicians playing monophonic instruments a novel way to communicate harmony to a machine, in such a way that other algorithms and applications can use this information to contribute to the musician-driven sonic creative process.\n\nThe implementation of the system is available for users and developers as free and open-source software. I invite you all to check it out here.\n",
        "url": "/masters-thesis/2020/06/23/harmonic-interaction-Guy_Sion.html"
      },
    
      {
        "title": "Gatekeepers by design? Gender HCI for Audio and Music Hardware",
        "author": "\n",
        "excerpt": "This dissertation looks into investigating the design of hardware for audio and music which is commonly associated with the term ‘music technology’ under the aspect of Gender-HCI, studies on science and technology as well as design research.\n",
        "content": "Abstract\nHardware for audio and music is subject to inscribed social processes and can bring\nthem to appearance through visual cues and language. This dissertation investigates\nhow established hardware for audio and music can communicate issues related to\ngender. In particular, it looks into (1) how language of live interfaces in music can\ninform about whether and how gender shapes musical tools; and (2) to what extent\ncan gender bias in the design of musical interfaces be detected through visual\ncues. With a mixed methods approach, this thesis aims to create a richer picture on\npotential gender identities in hardware for music. Two studies are presented: an\ninterview analysis with expert women from music technology and a quantitative\nstudy on gender reception of audio and music hardware. The findings and results\nsuggest that gender perception for established hardware in audio and music exists.\nTo follow up, design recommendations are proposed on how to approach the development\nof interfaces under the notion of pluralism. This implicates to involve\npeople with different backgrounds in musical hardware and DMI design processes,\nwith implications for academia and industry in order to make musical hardware\nmore accessible.\n\nProblem Space\nWhether academic or industrial, the music technology field is known as a field\nthat needs more gender diversity (Frid, 2019; Gadir, 2017; Xambó, 2018). People\nfrom various cultural and economic backgrounds, ethnicities, gender identities and\ndiverse abilities are little represented (Frid, 2019) when designing audio and music\nhardware. The question is to what extent are these circumstances reflected in\nthe language of musical live interfaces, the visual semantics of music technological\nartefacts and in its interaction design (ID)?\n\nIn a narrow sense, a hardware can be, according to Magnusson and Mendieta\n(2007), a computer, a soundcard, controllers and sensors. Although there has been\na vivid development in instrument design with electric components in the last two\ndecades (Bongers, 2000), the look of the interfaces that are available to the mass\nmarket usually have very similar shapes, dominated by edgy forms and knob type\ncontrollers (Jensenius and Voldsund, 2012). Susann Vihma adheres in “On Design\nSemiotics” (Vihma, 2010) that entire cultures can be recognised on the basis of its\nproduct environment, as humans are capable of constructing meaning through the\nform of artefacts. Different academic disciplines have ascertained that artefacts communicate\nnon-verbalized human values (Berg and Lie, 1995; MacKenzie and Wajcman,\n1999; Vihma, 2010).\n\nResearch Questions\nThis research aims to contribute to a critical reflection of current technological\npractises in the field of digital music instrument design. Informed by my previous\nresearch (Jawad and Xambó, 2020), the focus of this investigation is to enquire the\nfollowing main research question:\nTo what extent established hardware for audio and music can communicate\nissues related to gender?\nThis main research question is tackled by the following two research sub-questions:\n\n  How can language of live interfaces in music inform about whether and how\ngender shapes musical tools?\n  To what extent can gender bias in the design of musical interfaces be detected\nthrough visual cues?\n\n\nBackground\nGendered Artefacts in Music Technology?\nAccording to Lucas, Ortiz, and Schroeder (2019), in commercial DMI production the product designers would often encapsulate the goals, behaviours and abilities of a broad\ntarget user group into a fictional archetype known as persona when designing DMIs.\nThis approach implicates that commercial forces bypass large parts of the population. MacKenzie and Wajcman (1999) wrote that technologies can be designed, consciously\nor unconsciously, to open certain social options and close others. Particular design features\nup to entire technologies can be and act politically (MacKenzie and Wajcman, 1999).\nTherefore, music technology and musical instruments, like any other technology, do\nas well act as cultural and symbolic artefacts that absorb political content around\naccess to, respectively, physical ability, gender, socio-economic status, class and cultural hierarchies (Morreale et al., 2020; Zeiner-Henriksen, 2014).\n\nGender HCI – Inspiring Approaches\nBarth (2012) considered the current interface trends as predominantly biased towards male users as they have been considered the ’default’ gender in computing for decades. Gender HCI (Cassell, 2002) or feminist HCI (Bardzell, 2010) established in the last decade under the influence of HCI, design research (Demirbilek and Sener, 2003), STS, gender studies and psychology. This approach is a tool to deconstruct how gender identities shape the design and use of technological items (Bardzell, 2010). \nFor example examined Livingstone (1992) the ratings of participants that\nwere asked to assign a symbolic gender to domestic technologies in brown goods\n(e.g. TV components, stereos and PCs) and in white goods (e.g. kitchen and laundry\nappliances). Brown goods were be male biased while white goods were female biased.  According to Rode (2011), Fiesler, Morrison, and Bruckman (2016), and\nLight (1999), technologies that are femininely gendered, however, gradually lose\ntheir status as technology. Another inspiration for the survey design was given by\nDemirbilek and Sener (2003), who theoretically investigated how “meaning” could\nbe designed into a product in order to “communicate” with the user at an emotional\nlevel. Finally the research that has been undertaken around gendered software design by Vorvoreanu et al. (2019) showed that:\n\n“(…) software cannot be made “better” by having a “pink” version and a “blue”\nversion (…) to improve software’s usability across genders, software needs inclusivity across the cognitive diversity that arises not only among different genders,\nbut also within them.” (Vorvoreanu et al., 2019, p.11)\n\nMixed Methods\nIn particular, the two research sub-questions are addressed with a mixed methods approach (Lazar, 2017) of combining qualitative and quantitative research methods. According to Adams, Lunt, and Cairns (2008), using mixed methods is helpful for understanding how technology is subjectively and collectively experienced and perceived by different user groups. Beyond that, mixed methods can provide a balanced reflection of an issue. It will give a more differentiated account on the objectives as mixed methods serve both to gain\ndeep insights from experts in one field and to contrast their impressions with a wide range of experiences The participants of both the interviews and the online survey\nshare overall similar backgrounds and interests.\n\nMethod 1 – Interview Analysis\n\nThe first research sub-question has been addressed by means of analysing the interviews with women experts in the field of musical interface design, music technology research and artistic practise. Through initial exploration, different thematic patterns emerged that could potentially be explored. However, the most profound thematic cluster emerged when analysing statements in relation to music technology and engineering. We could analyse that the language of music technology, especially the term ‘music technology’ in academia carries ideas of activities that are stereotypically gendered.\n\nMethod 2 – Online Survey\n\nThis study investigates the gender perception of hardware in music and audio. Hundred and\neleven participants took part in an online survey with questions on colour, shape\nand wording of 9 exemplary artefacts of music hardware. In this study, the results\nof the responses are introduced and discussed. The gender assignments and intensities of the attributes for the respective instruments were briefly outlined. Three\ninstruments were chosen for in-depth analysis with the aim at representing a ‘male’\ninstrument, a ‘neutral’ instrument and a ‘neutral - female’ instrument. It has been\nshown that there is a tendency to perceive most of the items as neutral and/or\nmale as to observe in Figure 1.\n\n\n\nAssesment of items, all groups\n\n\nIn the gender assignments there were differences occurring among the gender groups, but they were of minor degrees.\n\nDiscussion &amp; Final Remarks\nUniversal, Pluralistic or Neutral interfaces?\nAs noted in Oost (2003), many objects and artefacts designed for “everyone” without\na specific user group in mind are based, often unconsciously, on a one-sided user\nimage. This could possibly be reflected in its semantics. Colour, shape, form and\ntexture of the designed objects are sent as messages that are part of our language\nstructures that deal with meaning, as Demirbilek and Sener (2003) explain. These\nattributes would communicate with users and can therefore never be contextually\nneutral. Vorvoreanu et al. (2019) stress, while leaning on Bardzell (2010), that attention to individual differences within genders can be emphasised by the notion of\npluralism rather than universality. Approaching the design of interfaces could support individual differences\nand non-binary notions of gender identification, which can embrace also other underrepresented groups.\n\nReferences\nBarth, Derrick Ryan (2012). “Designing the Gender-Neutral User Experience”.Worcester, MA, USA:Worcester Polytechnic Institute.\n\nBerg, Anne-Jorunn and Merete Lie (1995). “Feminism and Constructivism: Do Artifacts Have Gender?” In: Science, Technology, &amp; Human Values 20.3, pp. 332–351.\n\nBongers, Bert (2000). Physical Interfaces in the Electronic Arts. URL: https://bertbon.home.xs4all.nl/downloads/IRCAM.pdf.\n\nFrid, Emma (2019). “Diverse Sounds: Enabling Inclusive Sonic Interaction”. PhD thesis. Stockholm, Sweden: KTH Royal Institute of Technology\n\nGadir, Tami (2017). “Forty-Seven DJs, Four Women: Meritocracy, Talent, and Postfeminist Politics”. In: Dancecult 9.1.\n\nJawad, Karolina and Anna Xambó (2020). “How to Talk of Music Technology: An Interview Analysis Study of Live Interfaces for Music Performance among Expert Women”. In: Proceedings of the International Conference on Live Interfaces, pp. 41–47.\n\nJensenius, Alexander Refsum and Arve Voldsund (2012). “The Music Ball Project: Concept, Design, Development, Performance”. In: Proceedings of the International Conference on New Interfaces for Musical Expression.\n\nLazar, Jonathan (2017). Research Methods in Human Computer Interaction. 2nd edition. Cambridge, MA: Elsevier.\n\nLight, Jennifer S. (1999). “When ComputersWereWomen”. In: Technology and Culture 40.3, pp. 455–483.\n\nLivingstone, Sonia (1992). “The Meaning of Domestic Technologies”. In: Consuming Technologies: Media and Information in Domestic Spaces. Ed. by Eric Hirsch and Roger Silverstone. London, UK: Routledge, pp. 113–130.\n\nLucas, Alex Michael, Miguel Ortiz, and Franziska Schroeder (2019). “Bespoke Design for Inclusive Music: The Challenges of Evaluation”. In: Proceedings of the International Conference on New Interfaces for Musical Expression. Porto Alegre, Brazil: UFRGS, pp. 105–109.\n\nMacKenzie, Donald A. and Judy Wajcman, eds. (1999). The Social Shaping of Technology. 2nd edition. Buckingham, UK: Open University Press.\n\nVihma, Susann (2010). “On Design Semiotics”. In: MEI Objects &amp; Communication (30–31), pp. 197–208.\n\nVorvoreanu, Mihaela et al. (2019). “From Gender Biases to Gender-Inclusive Design: An Empirical Investigation”. In: Proceedings of the 2019 CHI Conference on Human\nFactors in Computing Systems - CHI ’19. Glasgow, Scotland, UK:ACMPress, pp. 1–14.\n\nXambó, Anna (2018). “Who Are theWomen Authors in NIME?—Improving Gender Balance in NIME Research”. In: Proceedings of the International Conference on New Interfaces for Musical Expression. Blacksburg, Virginia, USA, pp. 174–177.\n\nZeiner-Henriksen, Hans T. (2014). “Old Instruments, New Agendas: The Chemical Brothers and the ARP 2600”. In: Dancecult 6.1, pp. 26–40.\n",
        "url": "/masters-thesis/2020/07/04/GenderHCI-For-Music.html"
      },
    
      {
        "title": "The design and evaluation of the Gyroshuffle",
        "author": "\n",
        "excerpt": "The aim of this study is to develop and evaluate the Gyroshuffle, a real time rhythmic instrument played with body movement. It is theorised that moving to the rhythm, whilst controlling the rhythm is possible with the Gyroshuffle, blurring the lines between dancing and producing music in real time.\n",
        "content": "The design and evaluation of the Gyroshuffle\nA real-time quantised rhythmic instrument\n\n\n\nMyo armbands in hands\n\n\nThe aim of this study is to develop and evaluate the Gyroshuffle, a real time rhythmic instrument played with body movement. The instrument produces a quantised rhythmic drum loop that can be played by the user, utilising velocity based IMU sensors as an input. The hypothesis is that the more movement made, the more rhythmically complex the drum loop will become. The mapping and sound engine reflect this concept, resulting in an intuitive, physical world inspired relationship between gesture and sound. Rhythm, its characteristics, meter and its beauty are all discussed in relation to creating an IMS, as well as evaluating what makes a successful IMS in a prototyping environment. The system is created with Myo armbands, MyoMapper, Max and Ableton Live, and evaluated by users with a musical background compared against a traditional controller based IMS. It was found that the Gyroshuffle was successful for short term changes in rhythm, and is preferred for performance, however it performs less well for longer term compositional changes. Moving to the rhythm, whilst controlling the rhythm is possible with the Gyroshuffle, blurring the lines between dancing and producing music in real time.\n\nGyroshuffle mapping diagram &amp; demonstration\n\n\n\nGestures of the Gyroshuffle\n\n\n\n\nFull mapping of the Gyroshuffle\n\n\n\n",
        "url": "/masters-thesis/2020/07/31/Gyroshuffle.html"
      },
    
      {
        "title": "Meet SMC Group A 2020",
        "author": "\n",
        "excerpt": "Alena wanted to call us KitKat but we all decided against it.\n",
        "content": "\n\nMeet SMC Group A 2020\n\nTeam Overview\n\nFun Version\n\nL: Alena wanted to call us KitKat but we all decided against it. This brought us closer together as a team.\n\nA: Moreover, Leigh made fun of my grammar, while I complimented his smile. After this, he decided to call the team Yas Queen, because he promised to agree on everything I’ll say.\n\nC: IDK I envy you guys because you have lovely cats and I don’t have any pets :( We had a cow before, but not anymore.\n\nA: That’s so sad to hear… holy cow…\n\nC: it’s literally holy in India.\n\nL: Sounds like a delicious pet to have. Alena has low Iron in her blood and needs to eat red meat, but cows are holy in India, so we have an internal conflict we will have to overcome.\n\nA: After some struggles, we decided that I’ll just look for some other red meat to eat…\n\nL: I suggest elg, it’s quite popular in Norway. (and not holy in india as far as I know).\n\nA: If you ever saw an elg in India, it was most certainly holy.\n\nC: I am a vegetarian, guys, so it doesn’t matter what you guys eat.\n\nA: Here goes the hope that we’ll have pepperoni pizza together.\n\nC: I will have just the base of the pizza.\n\nL: It’s all about the bass.\n\nA: ALL.\n\nBoring Version\n\nWe are the shortest group in the SMC yet, having a diversity of exactly three cultures: an Aussie, an Indian &amp; our Queen, a Romanian, with expertise ranging from Music Production, Cognitive Science, Sound Design, Linguistics to Game Development &amp; a little bit of visual arts. It’s safe to say we all have a weird sense of humor and are introverted extroverts. Even though we just met as our destinies wanted, we all have a common goal which is to SYNTHesize our conformities and challenge our anomalies to create exciting musical innovations\n\nAll about Alena Clim (pronounced Queen)\n\n\n\nIf we were to use only two words to describe Alena, they would be beautiful artist. She has an old soul in a young body and her interests range from music, visual arts, psychology and philosophy to neuroscience, linguistics, technology and hard sciences. After studying Cognitive Science and Artificial Intelligence, she’s ready to combine this with music and learn more about music production, and music cognition and work toward her long-term goal of discovering effective art/musical therapies for people with mental diseases. In her spare time she likes creating artworks, playing piano and dancing both latin and swing.\n\n\n  Instagram\n\n\nAll about Abhishek Choubey (pronounced Chou-Bay)\n\n\n\nAbhishek calls himself a music nerd and music hunter, always on the lookout for good music (recommendations more than welcome). Just like everyone at the SMC, he also has music in his heart. He loves chill electronic beats with lush atmospheres, which is also evident in the productions which he releases under his Artist alias, “Neerr”. He has a wide gamut of interests which he will try converging in the SMC program. Having done his undergrad in Electronics &amp; Instrumentation Engineering he started living music full time.  He has also been DJing here &amp; there and loves it. Heavily obsessed with reverb and how it changes the audio he tries to take listeners on a spatial experience with his music. His favorite artists are Flume,Odesza, Shallou &amp; Two Lanes. What are yours? Send him an email.\n\n\n  Spotify\n  Soundcloud\n  Instagram\n  E-mail\n\n\nAll about Leigh Murray (pronounced Lee)\n\n\n\nLeigh comes from an academic background of games and web development. He began playing piano at the age of 3, trumpet at 6 and guitar at 9.  When covid hit in 2020 he began to get back into music production and decided to look for courses to develop his skills in this area. The SMC course at UiO was the perfect opportunity to bring together this background of games development, music and interest in psychology/neuroscience.\n\nTeam Skills\n\n\n\nTeam Reflections\n\nDuring these first two weeks, we did a lot of fun and interesting activities, while discovering new opportunities and challenges, as it’s the first 100% digital course we are attending. We are a small team but fun, nicely balanced from a skills perspective and very diverse, all being from different countries. The short presentations and the quiz helped understand our various cultural and academic backgrounds and minimize the unconscious biases that might have been present in our class. Something to consider might be to have each class member do similar presentations about themselves in addition to their hometowns as it would help bring the whole class further together as we still don’t know our other classmates very well.\n\nAt the team level, we are still trying to find the best ways to collaborate on tasks.  This far, we utilised google docs so that we can all contribute simultaneously - it is great for gathering ideas but can result in a less coherent final product and a feeling of “stepping on each other’s toes”. It was useful to have these introductory tasks, as we realised we should consider alternative platforms for future and more complex projects.\n\nIt was interesting to see that technology can make things less managed - everyone has equal responsibility within a flat team structure in contrast to a hierarchy with a team leader. In our previous experience, team projects would have each member write their own contribution and then a single member would be responsible for collating them. This is something we may consider for future tasks as it doesn’t rely on everyone being available at the same time (important with different time zones/work schedules) and allows team members to work individually in the way that best suits themselves. Combining our abilities and learning from each other is something we are looking forward to.\n\nIt was a bit disheartening to hear from the graduating students that they faced a lot of difficulties in the course despite being on campus together, and how glad they were that they didn’t have to overcome these challenges in addition to the “covid” related issues we are facing this semester. However, we are sure it will be worth the wait to meet physically - and through the Portal (hopefully in the second semester 🤞 !)\n\n",
        "url": "/people/2020/08/21/Group-A-Yas-Queen.html"
      },
    
      {
        "title": "B team, or not to Beam?",
        "author": "\n",
        "excerpt": "In August 2020, a multi-headed beast of eclectic skills was born. From the South American Andes, north through the North American Rockies, and east to the beautiful fjords of the Norwegian coastline, the pieces came together and the beast emerged.\n",
        "content": "B team, or not to Beam?\n\nIn August 2020, a multi-headed beast of eclectic skills was born. From the South American Andes, north through the North American Rockies, and east to the beautiful fjords of the Norwegian coastline, the pieces came together and the beast emerged. Together we represent three time zones on opposite sides of our beloved earth, so as we like to put it - wherever you are in the world, you can always reach the B-team.\n\nTeam B is passionate about nerdy tech, questionable noises, instrument modification, audio-visual synchronization, signal chain design, craft beers, and coffee.\n\n\n\nThe Team\n\nPedro Lucas\n\nI am from Manta, Ecuador. I obtained an undergraduate degree in Computer Science. Since I was a student, I have been involved in projects related to multimedia technologies. I worked in a game development team as a programmer with an emphasis in audio. Also, I am an empirical musician interested in the fusion among audio, music, and algorithms. My experience and my educational background have helped me to implement solutions and prototypes related with artificial intelligence, augmented reality, and sound synthesis.\n\nIn the SMC programme I want to explore about innovative instruments, music generation from a human perspective, and experience a multidisciplinary environment as well as learn from mentors and classmates, and share my knowledge with them.\n\n\n  Website\n\n\n\n   \n\n\nWillie Mandeville\n\nMusic has been a part of my life for as long as I can remember, starting with the conflicting influences of classical piano lessons, the cacophony of my dad’s bluegrass band throughout my childhood, and the even louder cacophony of my teenage metal/jazz/whatever bands. Despite this unholy mix of influences, I completed my degree in composition and had a brief and unproductive fling with graduate studies in musicology. My interests during this time shifted to string pedagogy, research in renaissance music, and the loud desecration of my amplified renaissance lute in bluegrass and jazz bands.\n\nToday I play the lute, guitars, and guitars tuned like lutes (“luitars”). In the interest of having enough money to eat, I also work as a web content management systems (CMS) contractor. I see technology as a key tool for the democratization of music education access and improved sustainability in the greater music industry.\n\n\n  Website\n\n\n\n   \n\n\nAnders Salomon Lidal\n\nI’m a left-handed guy who told his music teacher he didn’t have to rearrange the strings, because «I’m never gonna play the guitar anyway». Since then I’ve played right-handed guitar, bass, synth and made soundscapes in bands for the last 30 years. I never cared for the technique (maybe I would be better at it if I played leftys), only for making sounds, so I developed interests for gear for soundmaking, like instruments, FX-pedals, computers, samplers and test equipment.\nI’m teaching music and arts and crafts, and I always have enough lefties for any of my classes.\n\nBesides playing in bands, I also compose. My compositions are mainly sound-textures growing out of the oblivion, sometimes slowly moving towards a kind of song, before disintegrating back into oblivion.\n\nI have a cat, a wife, and we all like to sail.\n\n\n   \n\n\nCheck out this great band I’m playing in, Oslo Oscillator. You’ll find it on your favorite streaming service.\n\nHenrik Haraldsen Sveen\n\nI’m Henrik. I have a dog. I produce music for artists and I programme synthesizers and effects that I use in my work. Really love audio rate modulation and distortion and things like that. Always on the lookout for developing new ways to DSP and generate sounds. I’m also fascinated with how we interact with music technology - how a simple twist of a knob can give you great feedback and inspire you to work. Also invested in how the visual design will affect use or whether you like to work with a tool or not. Somewhat less is more. My academic background is a bachelor’s degree in Music Technology at NTNU with courses in Design Thinking, innovation and economy. Been working with music ever since. Looking forward to new inspiration and exciting projects at SMC.\n\n\n  hmu @henriktheartist\n  Pridactiøns\n\n\n\n\n\nTEAM REFLECTION, TWO WEEKS IN\n\nTwo weeks into the program, the B-team still feels like we’re in the introduction phase and learning what this program is all about. We’re still trying to get to know one another through the zooming meetings, and trying to find out where our place is; where do our competencies fit in?\nWe’re also trying to get a feeling of our fellow students in the other teams, and found the two-minutes presentation both fun and meaningful. Hopefully everyone can bring some food when you come to Norway, which we really look forward to; the coming-to-Norway part. Because it is a bit challenging to always meet on zoom. Nice to see everyone there, but real face-to-face always beats the digital meetings. The difference in time zones is also a challenge, because it makes it a little harder to find time for group-meetings.\nIt seems like the SMC-group of 2020-2022 has a very widespread joint competency, and the B-team probably speaks for everyone when saying that we look forward to exploring together with you all.\n\n\n",
        "url": "/people/2020/08/21/B-Team.html"
      },
    
      {
        "title": "Meet SMC Group C 2020",
        "author": "\n",
        "excerpt": "Joni, Lindsay, Wenbo &amp; Stephen are the inspiringly titled SMC Group C. Hailing from Hong Kong, India, China and Wales, they have backgrounds in everything from UX Design to audio engineering, music performance and printing multi-coloured giraffes on kids t-shirts.\n",
        "content": "\n\nIt’s SMC Group C!\n\nJoni, Lindsay, Wenbo &amp; Stephen are the inspiringly titled SMC Group C. Hailing from Hong Kong, India, China and Wales, they have backgrounds in everything from UX Design to audio engineering, music performance and printing multi-coloured giraffes on kids t-shirts. The plan for the next couple of years is to combine their diverse set of skills and knowledge in new, interesting and (hopefully) musical ways. SMC Group C are confident that this will involve copious amounts of cool new tech and making lots of strange beepy noises.\n\nJoni\n\n\n\nJoni is a user experience designer, photographer and a classical flautist with a strong passion for finding solutions to wicked problems, such as finding ways to enhance the learning experience for cognitively ‘not-so-fluent’ people. She believes everyone is capable to learn and to create wonderful things that we only need to find different methods and styles that suit us. She lived in 8 countries and had background in music performance and design research.\n\nJoining the SMC programme, she wants to explore the possibilities and limitations of how music or sound can create impact through the screens. Her research areas include HCI, Interface, User-Centred Design, Universal Design, Inclusive Design, Cognitive Psychology - Attention and Memory, and Neuro-Musicology.\n\n\n  Instagram\n  E-mail\n\n\nLindsay\n\n\n\nLindsay Charles is a Multi-Instrumentalist and an Electrical &amp; Electronics Engineer from Hyderabad, India. Piano was his first instruments then moving into drums, guitars, bass and as curiosity grew, he studied the workings and crafting of instruments, their circuitry, technology and what not, compelling him to build his own MIDI Controller and other hardware attachments to the guitars.\nFrom an artistic perspective, Lindsay composes background scores and soundscapes for documentaries and short films as a freelancer.\nHe loves playing bass in particularly funk, rock and jazz bands.\nHe’s looking forward to acquire knowledge and practical experience to build his career through the SMC Program.\nHe also loves long mountain hikes. ;)\n\n\n  Soundcloud\n  Instagram\n  E-mail\n\n\nWenbo\n\n\n\nWenbo Yi is a classical pianist and audio engineer from China. He has studied classical music in the Conservatory since age five and has won the first prize in the national piano competition. He got his bachelor’s degree in Recording Arts and studied at the audio engineering master program in the Communication University of China.\n\nHe viscerally believes that music is an exceptional bridge among multi-cultures and sincerely hopes to be your friend.\n\n\n  Instagram\n  E-mail\n\n\nStephen\n\n\n\nStephen is a programmer and musician from Wales in the UK. He plays bass (both the guitar and the giant violin shaped one), and loves to spend evenings tucked away in his studio getting lost in a haze of swirling, synthesised dreaminess. He is a business owner, and has spent over half his life living in other peoples countries. He also loves writing stories about underwater animals for his seven year old son, and has spent several years teaching SCUBA diving in south east Asia.\n\nThe SMC programme looks like it’s going to be a gigantic playground for Stephen - there is so much of interest that he isn’t quite sure of where to start. But it will probably involve building things, exploring new ways of performing live electronic music, and absorbing all that he can about communication, learning and pedagogy, as teaching is ultimately what he wants to do with his life.\n\nOur Group Competencies\n\n\n\nTeam Reflection and Impression from the first two weeks\n\nIt’s been a fun and intense first couple of weeks on the course. It’s also been quite different - the online only nature of this semester is taking some time to adjust to. We have found it can be difficult to connect with others over zoom - building relationships with the other students is hard, our classmates still feel like strangers. Within the team it has worked well though, it has been good to get to know each other.\nThe style of teaching is also very new for us, and not something we have experienced before, being used to the more traditional approach of lectures and working alone. This way is much better!\n\nOne thing we have found is tiredness can be an issue - the big time differences mean that classes happen later in the evening for some of us, and if we have been working all day, to then maintain attention for class can be a challenge.\n\nOn the other hand, the online nature of the course makes it feel pretty focused and convenient.\nThe course has been intense at times, but in a good way. There has been a lot of new things to learn, and it’s been great to have the opportunity to learn from everyone else - the diversity of the class means we have so much we can learn from each other. It’s been a really positive start to the course.\n",
        "url": "/people/2020/08/21/group-c.html"
      },
    
      {
        "title": "Audio and Networking in the Portal - Presentations",
        "author": "\n",
        "excerpt": "The class of 2021 recently presented broadly on networking and audio within the context of the Portal. Presentations are included in this blog post as pdfs.\n",
        "content": "Audio in the Portal\n\n\n    \n    Our presentation on audio\n\n\nA Brief on Network Audio\n\n\n    \n    Our presentation on networking\n\n",
        "url": "/networked-music/2020/08/31/audio-networking-presentations.html"
      },
    
      {
        "title": "Jamulus, or jamu-less?",
        "author": "\n",
        "excerpt": "Playing music together is not at all only about hearing, but also about the visual. Today the SMC students of 2020 experienced this in a “low latency jam session” in Jamulus.\n",
        "content": "Jamulus, or jamu-less?\n\nPlaying music together is not at all only about hearing, but also about the visual. Today the SMC students of 2020 experienced this in a “low latency jam session” in Jamulus. Short version of what Jamulus is: an online play-together program, that lets you join sessions with other musicians. You need to install Jamulus to your computer, and ideally use a cabled connection to the internet.\n\nYou can either join an existing server, or set up your own server if you like a more private jam-session. We all connected to a closed server located at NTNU in Trondheim, and total delay (latency) for those of us who are located in Norway was between 20 and 45 ms, which is decent, but also enough to make rhythmic and percussive music a little hard to play.\n\n\n\nB-jam\nIn the B-team three of us are in Norway, and one in Ecuador, meaning that we needed to figure out what kind of music we thought would work. We decided to go for an easy chord progression, based on some droning tones in a patch from a Moog Voyager. The rest of the setup was an electric guitar, electric guitar tuned like a lute (a lute-caster!), and an old pump-organ. We used a drum-loop to hold it all together, and it kind of worked. Our music was very textural, and consisted more of soundscapes than melodies.\nThe reason it kind of worked for us was primarily because the chords we used worked well with the same basic set of notes. It would be interesting to try it again with something more structural. Something with notes starting and ending at specific times. Maybe the outcome would be different then. It’s easier to get something to sound good when it doesn’t matter what chords and notes are being played where.\n\nSound\nThe quality of the sound is not recordable, and there were some clicks and pops, but it is possible to play together with ok quality, if you have ok bandwidth and you are not too far away from each other (and the server) geographically.\nWe were experiencing a phonographic effect caused by sound quality. Reducing the quality makes it less “meaningful” to play together, as some of the fun is to hear the details in what your co-musicians are playing.\nThe real challenge was to place yourself and your co-musicians in the mix. To keep the latency low, most people will use mono sound, and then the mixer doesn’t have options for panning when playing mono, making it feel a little like a wall of sound.\n\nAudiovisual\nThe experience also made clear that playing together is not only about hearing – but also about seeing – each other. When jamming with other people, especially people you haven’t played with before and don’t know musically, seeing is essential. It might be another experience if we had tried a standard song everyone knew really well, but without being able to communicate visually while playing (we tried to use Zoom for this simultaneously and found it more confusing than no visuals at all), Jamulus and other services like it will remain good ideas that haven’t yet been successfully executed.\n\nJamulus Epilogue - Can’t Forget About Human Error\n\nOne week later…\n\nIn a follow-up Jamulus session, we had hoped to experiment with more rhythmically dynamic music. It had been helpful to lean on an ambient approach in our first Jamulus session, but we hoped to learn more about the specifics of our latency problems by pairing off in duos or trios with the fourth participant listening for latency details that are harder to hear when playing.\n\nHowever, much to our surprise, Jamulus appeared during our second session to be performing much worse than it had previously. Latency was less consistent in its length, and one group member reported that the value for ‘Overall Delay’ on his end was ping-ponging back and forth between 20ms and over 200ms. It seemed that Jamulus was just not having a good day.\n\nBut wait! Right when we were about to blame Jamulus for everything, we realized the root of the problem was actually the dreaded human error. While attempting to use Jamulus the group was simultaneously uploading over .6 GB of audio files to a shared dropbox. Oops. Obviously we had found a very, very effective way to ensure that our Jamulus experience would suffer.\n\nThe moral of this story is nuanced. Is Jamulus perfect? No, issues of timing and visual experience that we detailed after our first session are valid problems that limited the satisfaction we found in using Jamulus. However, our second session proved just how important it is to optimize the conditions in which one is using Jamulus. The more that the human user can help Jamulus succeed by minimizing any other computing demands, the better Jamulus will be.\n",
        "url": "/networked-music/2020/09/03/jamulus-or-jamu-less.html"
      },
    
      {
        "title": "Jamulus: Can y-- hear -e now?",
        "author": "\n",
        "excerpt": "During the second session of the Physical-Virtual Communication and Music course from 2020, we had our first experience with telematic music performance. It was not the greatest jam we ever had, but we learned from it.\n",
        "content": "Jamulus: Can y– hear -e now?\n\nDuring the second session of the Physical-Virtual Communication and Music course from 2020, we had our first experience with telematic music performance. It was not the greatest jam we ever had, but we learned from it.\n\nThe prerequisites\n\nThis class required a wired network connection (preferably an Ethernet cable), microphone(s), audio interfaces, and musical instruments. We were supposed to test the Jamulus software during class, in a server set up in Trondheim. This software allows musicians to have real-time jam sessions over the internet by collecting audio data from each individual source, mixing the data, and then sending the mix back to each person. During a short meeting on Zoom, our team tried to play the audio of the song “Over the Rainbow” by Israel Kamakawiwoʻole in 1990. After 15 minutes of trying to combine an electric guitar (Leigh), an ukulele (Choubey) and a classical guitar (Alena), we had a decent version of the song and were ready to try to play our technological masterpiece over information highway, the internet.\n\nWhat actually happened\n\nThe teacher had asked everyone in the class to join the Jamulus server at the same time but the server was not able to handle that many connections, so some of our group members were unable to join. Instead, we all returned to Zoom and entered the Jamulus session team by team.\n\nOnce all the team members managed to join the session, there was significant latency between us (Leigh is in Norway, Alena is in Romania and Abhishek is in India). Jamulus is unable to overcome the physical limitations of latency introduced by such a huge distance (6500 km) so this was one issue - although Leigh had a latency of only 11 ms, Alena had 156 ms and Choubey had 235 ms. The primary issues, however, were that there was not a consistent stream of audio from Alena’s computer and, although Leigh’s audio did work, he had to restart his machine after each session as Jamulus would not release the audio device for other software (primarily Zoom) to use it again.  Since we were the first team in our class to use the system and we were struggling to get it working, we stopped trying to make it work and left the session to let another team have a go at it.\n\n\n\nWe conducted a trace-route to see how international network traffic reached Trondheim (where the server set up by Anders was) and noticed that it is all routed through Oslo. To slightly reduce the distance all our network traffic needed to travel, we set up our own Jamulus server using NREC which is based in Oslo. When testing this server we deduced that a wired connection for Alena was necessary to have a consistent audio connection but latency was still a significant issue. We never got to a point where we actually were able to try playing music together.\n\nAfterthoughts\n\nAfter the disappointing session, we started seeing the experience from the perspective of the audience while other teams tried testing Jamulus on the (Trondheim) Jamulus server. Listening to them jam along with each other was also not so smooth of an experience, but it was definitely better than what we experienced. Due to the latency and delay, it was hard to follow their jam as they played along with a metronome that one of the team members had on. We were watching them via Zoom, and listening to them play through Jamulus. With this experience it is evident that jamming together is not just about playing musical instruments, but rather about the physical emotion and certain cues that musicians exchange make analog jamming the fun that it is.\n\nGiven the simplicity of setting up a Jamulus server (it took us around 30 minutes) and there are only 3 groups in our class we would recommend having a Jamulus server set up for each group in the future so that we can all practice simultaneously.\n\nSecond experience: another chance?\n\nIn our second attempt to make music together using Jamulus, we had hoped that we would atleast get connected and be able to play some music, unlike our previous disheartening experience, and our expectations were satisfied to a point. We were all able to connect to Jamulus (albeit Choubey and Leigh’s devices would crash if both Zoom and Jamulus would be connected to audio) and we could listen to each other, but did we make music together? Not so much.\n\nLast time we tried to synchronize an electric guitar, an ukulele and a classical guitar, but this time we decided to only use an acoustic guitar and ukulele together with a shaker that would keep the rhythm. This time around, Alena also had a wired connection, and her latency was reduced to 63 ms. However, when trying to play together, using a metronome for reference, we only managed to be in sync two by two: if two of us appeared to be playing at the same time, the third would be totally off the rhythm.\n\n\n\nTrying to solve the latency problem, we jammed at a very low tempo, around 40bpm. Playing at such low tempo was impractical, from a musical point of view, but it improved our coordination … slightly. With the metronome as our reference we all kept in mind our delay and managed to play in sync for a few seconds. However, the musical output sounded robotic and even nostalgic, as it felt like we were just learning to play the instruments rather than jamming together.\n\nIn conclusion, our second session of using Jamulus came with some improvement. On the technological front, we were able to connect to Jamulus and hear each other play! On  the musical front, we still could not play music together in the wide sense. We played our instruments in sync and created an (interesting) musical output; however, it couldn’t be called jamming by any traditional means.\n\nBased on our experience, Jamulus is an acceptable platform for online jamming - with some restrictions. If the latency between musicians is higher than 20-30 ms and/or there is a substantial physical distance between them, only two people can play together and be in sync. If the latency for all participants is lower than that (or they are all in the same area), then it is most likely possible to have more people jam together. It is very good practice to have wired internet connection (both due to its increased stability and bandwidth), and it’s good to combine different types of instruments.\n",
        "url": "/networked-music/2020/09/06/jamulus-team-a.html"
      },
    
      {
        "title": "Jamulus test session.",
        "author": "\n",
        "excerpt": "Can a physical metronome keep us in time? Experiences from the jamulus test session in the Physical-Virtual Communication and Music course.\n",
        "content": "Jamulus I : Exciting and Awkward\n\nThe team C’s experiences from the first test session of a real time jamable technology for musicians - ‘Jamulus’ in the Physical-Virtual Communication and Music course.\n\nWhat we planned to do\n\nWe were asked to pick a song or music to jam on, before we entered the jamulus server in teams.\nThe team having diverse music tastes and backgrounds(Joni and Wenbo are classical musicians, Stephen and Lindsay are bass players.) we decided to jam on the song ‘I can see clearly now’, not knowing that jamulus had huge latencies because of our physical distances, the song choice wasn’t a good one but nevertheless, the team practised their parts and were ready to jam on the jamulus.\n\nWhat actually happened\n\nEveryone was rather, being really excited when we entered the server and could hear each other, especially after months of looking forward to being able to experience making music together online through different jamming servers.\n\nDue to the geographical factor, our very first experience to make music online through Jamulus was not optimistic. We felt awkward because we couldn’t really communicate with each other face-to-face. We weren’t able to feel each others’ presences because of the latency and poor internet connections.\nStephen was on the upright bass, Joni on Flute, Lindsay on classical guitar and most important of all, Wenbo used his metronome to keep us in time.\n\nIn reflections of what we tried out during the jam, we all agreed that we should start with more ambient music and more toward improvisation. It is because playing rhythmical-focused pieces didn’t work out with our conditions - internet, latency, time differences.\n\nNevertheless, we enjoyed listening to Team B, we found they did a fantastic job. The reason behind could also be because of the locations, most of the members were in the same region and they also chose the right ambient music to jam on.\nFinally, the experience of our first jamulus experience was awkward and rather frustrating.\n\n\n\nJamulus II : Interesting and Comic\n\n\nA week later…\n\nOur second attempt to jam in the Jamulus. This time, from our learnings from the past session, most of us chucked away our own musical instruments, Stephan’s bass, Wenbo’s piano and Joni’s flute. Instead we tried to use VST’s on iPads routing it directly into the jamulus, digital piano as synthesisers and experimented with another song that doesn’t require time precision but focuses on improvisation.\n\nPointers from our last experience were the latency and the lack of face-to-face communication making it difficult for rhythmic music. This time, we visually communicated with each other on Zoom, while listening to each other in Jamulus. Lindsay improvised a really simple but wonderful foundation on the acoustic guitar for us to add musical textures on it. Stephen took the ambient and blissful soundscapes. Joni tried her flute later on and it didn’t work out because of the poor audio input, as well as output. It seems like, musical instruments in the woodwinds or brass family need a dedicated microphone input because of the latency. Secondly, it is because of the loudness of the instruments itself that covers the audio input, finally, it is because of the geographical factor. Guitar, on the other hand, works pretty fine. Wenbo was the star of the show because his mic wasn’t working at first and then when he finally fixed it, his voice suddenly sounded like a girl, surprising all of us, we had a big laugh about it because he couldn’t fix the problem for the whole session. We believe the problem occurred due to the delay and the difference in sampling rate.\n\nFinally, our reflections on the second attempt in Jamulus was slightly better because of the way we tried to approach music-making. Nevertheless, we think it would be the best to play together in the same room.\n",
        "url": "/networked-music/2020/09/07/jamulus-team-c.html"
      },
    
      {
        "title": "Exploring Music Preference Recognition Using Spotify's Web API",
        "author": "\n",
        "excerpt": "A proposed ML model that predicts the degree to which I will enjoy specific Spotify tracks based on previous preference ratings.\n",
        "content": "During these last two intense weeks of machine learning, I ventured to design a system that sought to recognize individual preferences in music using only the Spotify environment and API as resources. The model tries to predict the degree to which the author will enjoy specific tracks by Johann Sebastian Bach based on a subjective rating given to every example in the dataset. The dataset consisted of 100 Johann Sebastian Bach tracks collected from Spotify playlists. This unfortunate size of this dataset was due to the unexpected amount of time it took to gather music I liked (who knew?!). Anyway, the particular use of Bach’s works was not coincidental as it presented an opportunity to collect a relatively consistent body of examples in one particular genre (baroque) that I was familiar with.\n\nThe project was first and foremost aimed at exploring how a relatively new and accessible online resource of high-level musical data could be used for machine learning purposes but also to examine whether machine learning in this sense can be used as creative tools to gain new interesting knowledge about our personal music preferences and/or biases.\n\n\n   \n   \n\n\nData Collection\nCollecting audio feature-data from Spotify was quite manageable due to their well-documented API. I decided to collect a variety of features for experimentation, including a few high-level features native to Spotify’s ecosystem;  duration, energy, tempo, time signature, loudness, pitches, valence, danceability, timbres, key and mode.\n\n\n  Pitches\n    \n      The pitch feature returns a normalized vector per segment (a consistent subdivision of a track based on a consistent amount of sound) with 12 values representing the degree of occurrence for each note. I collected every pitch vector of a given track and averaged corresponding indexes. What remained was one list per track with the average occurrence of each note in the track.\n    \n  \n  Timbre\n    \n      Similar to pitches, the timbre feature returns a vector per segment with 12 elements. The specifics on what this timbre feature is based remains ambiguous, but it refers to the quality of the notes in the segment. The API also presses that these timbre values are best compared to each other. The gathering process here was the as with the pitch features.\n    \n  \n  Danceability\n    \n      This measure refers to how suitable a track is for dancing based on a combination of tempo, rhythm, stability, beat strength etc. Returns a scalar value per track.\n    \n  \n  Valence\n    \n      A normalized scalar of the “positiveness” conveyed by a track. The higher this value, the more happy, cheerful and euphoric the track is. The lower the value, the more sad, depressing and angry the track is.\n    \n  \n\n\nAn interesting observation was that these high-level Spotify features did not immediately correlate with other suspected feature data. For instance, valence did not correlate particularly well with the mode of the tracks, as seen in the image below, something I anticipated.\n\n\n   \n   Valence (happiness) in relation to the average mode of all tracks in the dataset. The lower the mode value, the more minor keys occur in a specific track.\n\n\nDas Model\n\nSince the goal of my project was to train a system to recognize my taste in music, a personal rating from 1-10 was given to each track representing target values for the supervised MLP (neural network) regressor algorithm intend. Additionally, I used a repeated K-fold for training and validation because I feared that more strict dependencies on particular percentages of samples in each target class, like what stratified repeated K-fold ensures, could be problematic due to my shortage of training data.\n\nBoth principal component (PCA) and linear discriminate analysis (LDA) were considered as dimensionality reduction techniques for the dataset. By comparing the variance of different sets of features to the total feature variance, I could find an approximation of how many reduction components I would need:\n#Here we find the minimal amount of DR-components needed \nto keep 99% of the total feature variance.\n\nfeat_variance = np.var(features, axis=0).sum()\nfor i in range(features.shape[1]):\n        temp = np.var(features[:,0:i+1], axis=0).sum()\n        percentage = temp/feat_variance\n        if percentage &gt; 0.99:\n            print(\"componenets needed: \", i+1)\n            print(\"reached: \", percentage, \"%\")\n            break\n\n\nEven though the grid-search scores revealed a significant advantage of using LDA over PCA, I had to take into consideration that my system could be defined as a kind of hybrid between a classifier and a regressor. This is evident by examining the scalar nature and range of my target values. Therefore, the consequence of using LDA might be that it interprets my task as classification rather than regression, an interpretation that might compromise the validity of my algorithm. But even though PCA might have been more suited, its detrimental scores led me to use LDA instead and rather keep in mind the impact this might have on the model.\n\nLDA cross-validation grid-search results:\nbest params: {'activation': 'logistic', 'hidden_layer_sizes': (2, 2), 'max_iter': 20000}\nassociated best score: 0.749\n\nLastly, the suggestion of using a logistic sigmoid activation function seemed logical considering that it’s often used to predict normalized probabilities which is exactly what my model would do.\n\nResults\n\nThe general results were adequate and expected. However, with an R2 score of 0.75 on my limited dataset, I suspected signs of overfitting. In light of this, I conducted some tests by training the model on testing and training data before comparing the respective R2 and mean square errors. The larger the difference was between the comparisons, the more symptomatic my system would be of overfitting.\n\n\n  \n    \n      Original Training Data\n       \n       \n    \n  \n  \n    \n      Metric\n      Scores\n      Difference between training and test data predictions\n    \n  \n  \n    \n      R2\n      0.75\n      4-5%\n    \n    \n      MSE\n      -0.9\n      18-20%\n    \n  \n\n\nAs seen in the first table above, the first comparisons were not as bad as anticipated but still grounds for suspicion. To investigate further, I decided to artificially create more training data to test whether extending the size of the traing data could increase the performances and limit the metric differences.\n\n\n  \n    \n      Original Training Data + 1/4 Artificially Created Training Data\n       \n       \n    \n  \n  \n    \n      Metric\n      Scores\n      Difference between training and test data predictions\n    \n  \n  \n    \n      R2\n      0.8\n      3%\n    \n    \n      MSE\n      -0.7\n      13-15%\n    \n  \n\n\nAt best, I only managed to decrease the differences by approximately 2-3\\% when adding 1/4 of artificial training data. The lack of performance variation from extending the dataset in this way can indicate sub-optimal algorithm parameters, dataset quality and/or dataset size. Most likely due to the latter..\n\nConcluding Remarks\nEvaluating performance metrics of a machine learning model depends on the system in question. I set out to prove that using Spotify data to train a machine learning model was feasible, and I believe this project has proved this concept.\n\nThe obvious dataset limitations were largely attributed to an inefficient process of gathering and labeling data. I believe that the system could have been better if the theme of the project was adjusted. For instance, comparisons of baroque composers or different musical genres could have facilitated a more effective data collection process. Alternatively, a more specialized focus, for instance on only piano sonatas or preludes, could also have yielded better results despite having a shortage of data.\n\nFor future work, it would be especially interesting to further explore and evaluate the high-level Spotify audio descriptors (danceability, valence etc..) through machine learning algorithms like this one.\n",
        "url": "/machine-learning/2020/09/18/aleksati-music-recognition.html"
      },
    
      {
        "title": "Classification of guitar playing techniques",
        "author": "\n",
        "excerpt": "An attempt at making a model which can classify 6 different playing techniques on the guitar\n",
        "content": "Classification of guitar playing techniques\nIn this blog post, I will describe a ML-project where I have attempted to classify different types of playing techniques performed on an electric guitar. The model is based on the use of a support vector machine with an RBF-kernel and linear discriminant analysis (LDA) as a dimensionality reduction-algorithm. The algorithm has been tuned with a grid-search and gone through 10-fold cross-validation. Where the best results average somewhere between 67% and 70% accuracy with the complete dataset used.\n\nWhy do you want to spend your valueable time training a model to distinguish between a bend and a slide?\n\nGreat question! The guitar is a complicated instrument. First of all it has a tuning system which really doesn´t make any sense. You also have the ability to play the same note in different positions on the neck. An E4 can be played on five different strings if you have a guitar with less than 24 frets. So, if you want to transcribe a guitar solo you need to figure out what notes are being played, on which strings they are being played, if the note is played with a pick or with a finger, is it a bended note, pre-bend, vibrato etc etc. So, why am I rambling on about this? Well, being able to classify different techniques is a building block in the art of automatic guitar transcription. If this sounds interesting you can have a look at this or this. Since I am in no way an expert at machine learning I put my focus on only one of the aspects of automatic guitar transcription.\n\nCollecting data\n\nFor me to be able to perform this task I needed a bunch of audio clips. Luckily for me some people in Taiwan had compiled a nice dataset consisting of 6580 unique audio clips demonstrating 7 different playing techniques. If you want to have a look at this data I don´t recommend going to this website and try downloading it. The site is not being maintained and the download took me aprox. 12 hours. So just contact me instead and I will help you out. The dataset is divided into 7 main folders where different guitar sounds are used in order to emulate a “real-world” situation. And make the task of classifying stuff a bit more challenging.\n\nThe algorithm\n\nAs I hinted at in my intro the classifier I used for this project was a Support Vector Machine algorithm(SVM). SVMs gives you great flexibility, computational efficiency, capacity to handle high dimensional data. By conducting a grid search I did some fine tuning of the hyper-parameters and came to the conclusion that it performed okay with these parameters:\n\nsvm = SVC(C = 1.4, decision_function_shape = 'ovo', gamma = 'scale', kernel = 'rbf')\n\n\nThe kernel is the Gaussian RBF kernel with gamma set to scale. The gamma-parameter makes the bell-shaped curve of the RBF narrower and when it is set to scale the gamma is calculated depending on the size of the features. OvO stands for one-versus-one strategy. In the case of this particular dataset with 6 classes, the OvO decision function end up training 15 binary classifiers on the training set.\n\nThe features I choose to extract from the audio files were: Melspectrogram, MFCC, spectral centroid and spectral roll-off. The melspectrogram and MFCC were the most efficient features to extract. And both of these are for good reasons popular features when it comes to timbral tasks like this in speech recognition, music genre classification and other MIR-stuff.\n\nConclusion\n\nIn this 2-week intensive workshop I have gained a little insight into the world of machine learning. Although I didn´t manage to build a perfect model I got to experience the confusing and difficult task of classifying audio. If I were to continue on this project I would first and foremost spend more time understanding the basic concepts of meachine learning, dive a bit more into how the librosa library actually works and try to compare different approaches to this specific task. It would also by interesting to look at other components in the transcription process. And over time try to build a complete functioning transcribing system.\n",
        "url": "/machine-learning/2020/09/20/thomas-ML.html"
      },
    
      {
        "title": "[ Music Mood Classifrustration ]",
        "author": "\n",
        "excerpt": "This is an attempt to create a Music Mood Classifier with feature extractions from Librosa.\n",
        "content": "[ Overview ]\n\nThis is a first experimentation with music mood classification by getting familiarity with basic Machine Learning algorithms and Librosa. The conceptualization of this project came from a personal motivation of connecting music technology to music therapy. A first step into the direction of creating an application for machine recognition of music emotion and classification. This is also a starting point to a future project which attempts to be an application\nable to identify the user’s mood and predict variations, suggesting music accordingly to the actual mental state\nand particular profile of that user.\n\nMusic and emotion are concepts deeply connected to human experience. Hence music is known for its capability\nto change the mood of the listener and drive feelings. Thereby, Machine Learning turns to be an efficient tool\nfor recognizing human emotions in music, which can be useful when it comes to the selection and suggestion of\nmusic for specific user’s needs amongst the enormous amount of music data digitally available.\n\nHere I will describe a frustrated attempt of using Machine Learning for the music emotion recognition, exploring\ntechniques, tools and experiments within a combination of audio features to reach the best performance possible due the size of my dataset and available time to work on it.\n\n[ Emotion ]\n\nThe challenges concerning the recognition of music emotion are peculiarly complex since the definition of emotion is not unanimous regarding categories and models to be used. The main question, in this case, is how to conceptualize emotion and its taxonomy.\n\nA common procedure to MER is training a classifier to identify emotions in a few classes related to primary human feelings as happy, angry, sad, and relaxed. Frequently, supervised learning algorithms such as Support Vector Machines, are adopted to improve the machine acknowledgment of the correlation among music features (timbre, rhythm, and harmony) and emotion.\n\nYang and Chen indicate three approaches for MER. The categorical approach uses supervised learning to train a classifier with predicted labels to categorize emotions into discrete classes. The dimensional approach, which is a regression model trained to identify numerical emotion values in an emotion space, based on dimensions such as valence x arousal. The third approach is a music emotion variation detection (MEVD), which dynamically predicts emotion in sections of a song, making it possible to trace the emotion of a song within a continuous variation in time.\n\nThe perception of emotion on music is typically related to a combination of acoustic attributes. Tempo, pitch, loudness, and timbre can be linked to arousal, while mode and harmony are usually related to valence. Emotion is subjective to individual experiences and it can be an issue for the machine to interpret it adequately. With this in mind, I selected four types of sentiments labeled as: Melancholic, Calm, Uplifting, and Aggressive.\n\nSee the 2D Valence-Arousal emotion space (Russell, 1980):\n\n\n    \n    2D Valence-Arousal emotion space (Russell, 1980)\n\n\nThis categorization makes a 4-plane mood categorization, in a 2 dimensional valence-arousal graph. At the\nhorizontal dimensional we have Valence divided in negative to the left and positive to the right, and Arousal\non the vertical dimensional, divided by low on the bottom and high on the top.\n\nIn this way:\n\n\n  Melancholic = Low Arousal / Negative Valence\n  Calm = Low Aurosal / Positive Valence\n  Uplifting = High Aurosal / Positive Valence\n  Agressive = High Aurosal / Negative Valence\n\n\n[ Data Set ]\n\nThe data set used for training and classification was the FMA: A Dataset For Music Analysis, from Github.\n\n\n  Data set: GitHub FMA\n\n\nThe FMA has 917 GiB of Creative Commons-licensed audio from 106,574 tracks of 16,341 artists and 14,854 albums. It provides full-length and high-quality audio, pre-computed features, together with track and user metadata, tags, and free-form text such as biographies.\n\nFor this project I used small fraction of this data set, which is listed at the download page as “fma-small.zip”:\n\n\n  fma_small.zip\n\n\nIt contains 8,000 tracks of 30 seconds. From this file, 400 tracks were selected, being 100 tracks addressed for\neach of the four categories. The collection of songs were tagged with the type of emotion that they convey. The pre-computed features were not used to this project, since one of the goals was to compare different combinations\nof feature extraction to find the best accuracy specific for the task.\n\n[ Features ]\n\nThe perception of emotion on music is typically related to a combination of acoustic attributes. Tempo,\npitch, loudness, and timbre can be linked to arousal, while mode and harmony are usually related to valence.\n\n432 features were extracted from the audio files. The feature extraction methods were selected based on the characteristics of each pre-determined category. For instance, “Calm” and “Melancholic” can be very similar in tempo and energy, but differentiate on harmonic and melodic content.\n\nThe same happens to “Uplifting” and “Aggressive”, where they have similar overall energy, but different tempo, tonal content, timbre, and dynamic.\n\nDuring the evaluation of the impact that each feature extraction caused on the system efficacy, it was possible to observe that some features did not add much on the accuracy score but contributed to the consistency of the results between several training with distinct data split and random state numbers (repeated k-Fold Cross Validation).\n\n\n  Chroma Features\n\n\nMusic Information Retrieval: Chroma\n\nWikipedia: Chroma Feature\n\nChroma-based features are efficient for analyzing music whose pitches can be meaningful for the categorization. They extract harmonic and melodic attributes of audio, being consistent to changes in timbre and instrumentation, and having a close correlation to harmony.\n\n\n  Chroma Constant-Q\n\n\nChroma\n\nCalculation of a constant Q\n\nA chroma vector indicates how much energy of each pitch class is present in the signal. The constant Q transform\ncan be used for automatic recognition of musical keys based on accumulated chroma content.\nAs it correlates with harmony attributes, chroma was utilized to distinguish between Melancholic and Calm\nsongs.\n\n\n  Tonnetz\n\n\nWikipedia: Tonnetz\n\nDetecting harmonic change in musical audio\n\nExperiments show that Tonnetz can successfully detect harmonic changes such as chord boundaries in musical audio.\n\nThis way, Tonnetz was utilized to help distinguish between audio samples with rhythmical similarity and similar energy also with special focus on separating the categories Melancholic and Calm.\n\n\n  Tempogram\n\n\nMusic Information Retrieval: Tempo Estimation\n\nMusic Structural Tempogram\n\nCyclic Tempogram\n\nTempogram indicates the prevalence of certain tempi at each moment in time, developed to characterize tempo variation and local pulse in the audio signal.\n\nThis feature was chosen to identify clear differenced between high tempo songs and soft songs, which can be useful for classifying correctly Aggressive and Uplifting songs against Melancholic and Calm samples.\n\n\n  Spectral Contrast\n\n\nSpectral Contrast\n\nFor most music, the strong spectral peaks roughly correspond with harmonic components; while non-harmonic\ncomponents, or noises, often appear at spectral valleys. Precious studies show that spectral Contrast provide a\ngood discrimination in music type classification, performing better than MFCC for the task.\nThe Aggressive songs of this data generally show a lot of power on the high frequencies and noisy audio.\nFor that reason, Spectral Contrast was included in the features combination.\n\n\n  Zero Crossing Rate\n\n\nWikipedia: Zero Crossing Rate\n\nAutomatic Mood Tracking\n\nThe zero-crossing rate is the rate at which the signal changes from positive to zero to negative or from negative to zero to positive, which turns to be useful to classify percussive sounds. Also, it can be used as a simple indicator of noisiness. As most of the songs at the Aggressive category are under heavy metal genre or have similar characteristics when it comes to noisy content, distortion and fast percussive sounds, this feature was useful to discriminate this category against the others, having higher zero crossing values than calm songs.\n\n\n  RMS\n\n\nRoot Mean Square\n\nRMS has as result a value that makes an effective representation of the power of the signal. Also interesting to use for the same reason as the feature mentioned above, including a more clear classification of Uplifting songs.\n\n[ Implementation ]\n\nRight in the beginning, on the feature extraction step, it was important to observe that some of the features extracted needed to have the values in time preserved, which means that features such as Tempogram get useless when the mean is calculated, due to its strong correlation with the dynamic changes in time. Features such as RMS had the calculated mean.\n\nThis way, the data frame regarding the features created a table of 432 columns of features, excluding the labels and names. Also for this reason, the labels of the features couldn’t be indicated in the columns of the table, resulting in numbers indicating each column instead.\n\nAfter scaling the features and merging everything into one data segment, the table was reconfigured to present only numbers. For the training step a repeated k-Fold Cross-Validation for Model Evaluation was used with 10 folds and a 70/30 split, having 280 examples for the training set and 120 for the testing set. The accuracy average was 0.41, varying between around 0.35 and 0.45 between the repeated training.\n\n[ Evaluation ]\n\nAt first the system was trained and tested with a smaller data set, which was 40 songs per class, and it was showing better accuracy scores, but also when the random state was changed, it could vary from high scores to very low scores, showing inconsistent results each time that the model was trained and tested again.\n\nAfter increasing the data set, the accuracy results were more consistent, not varying as much, but surprisingly, the classification of Melancholic and Calm became much worse than before, when it was classifying Calm songs correctly but misunderstanding the differences between Melancholic and Calm songs, classifying many Melancholic songs as Calm. It is reasonably comprehensive, since the two categories are very related for some similarities on the audio features. After increasing the data set, these two categories turned to be very badly classified, but the Uplifting and Aggressive songs kept the good results.\n\n\n    \n    Precision\n\n\n\n    \n    Confusion Matrix\n\n\nConclusions\n\nIt was a very important experience to understand the issues of classifying such sensitive characteristics as emotional content by automatizing a system that can distinguish subtle differences on human perception of emotion.\n\nThe system proved to be accurate classifying Uplifting and Aggressive songs, but inefficient when it comes to Calm and Melancholic categories. It can indicate that the features and machine learning techniques used were suitable for the identification of tempo, percussive dynamics and energy, but not as efficient to distinguish melodic and harmonic content.\n\nAlso, it is possible to see correlations between the features and that this system could have being split in\ntwo sections, one dedicated to classify arousal and the other for valence. The data set size has to be taking in consideration as well, a bigger data set would supply more data material for the system to be trained and provide consistent results.\n\nFurthermore, future solutions could involve choosing a different approach for the categorization of the emotions, excluding emotion labels as the ones used in this project, but instead considering the continuous distribution of the songs amongst the negative and positive valence and arousal values.\n",
        "url": "/machine-learning/2020/09/20/Music-Mood-Classifier.html"
      },
    
      {
        "title": "Classifying Urban Sounds in a Multi-label Database",
        "author": "\n",
        "excerpt": "How well does a convolution neural network perform at detecting multiple classes within a single sample? This experiment explores augmenting the UrbanSound8K database to test a well performing CNN architecture in a multi-label, multi-class scenario.\n",
        "content": "Environmental Sound Classification over Concurrent Samples\n\nGiven the short two week span to develop a machine learning model, I decided instead of beginning anew, to repurpose some pre-existing methods in approaching environmental sound classification.  Environmental sound classification (ESC) is a field that benefits well from machine learning techniques, as the data examined will always be unique and noisy. Two well known databases, UrbanSound8K (US8K) [5] and ESC-50 [9] provide recordings from Freesound.org, trimmed, labeled and grouped into categories for analysis. This system attempts to utilise a convolutional neural network (CNN) on an augmented UrbanSound8K dataset for multi-label classification.\n\nUrbanSound8K contains over 8000 sound files separated by categories of sounds typically found in an urban setting. Instead of exclusive categorical labels in its original state, the dataset has been recreated with the purpose of exploring how a successful architecture might perform on multi-label samples rather than simply uni-label, multi-class sounds. Thus, this project investigates how techniques in classifying an environmental noise database might generalize to a multi-label scenario with a new database composed of overlaid sound pairs.\n\n\n    \n    Spectrograms of three UrbanSound8K classes\n\n\nThe initial code that I forked was sourced from Aapid Saeed’s implementation which was in turn inspired by Karol Piczak’s 2015 paper that provides a scientific example of ENC using a CNN [4]. The experiment tested here provides some insight into the obstacles that appear when shifting this problem space both in terms of performance but also how features and parameters must be resituated.\n\nOverview of dataset and fabrication\n\nThe original dataset, UrbanSound8K, contains 8732 .wav sounds sourced from Freesound.org across 10 classes of urban sounds:\n\n\n  air conditioner\n  car horn\n  children playing\n  dog bark\n  drilling\n  engine idling\n  gun shot\n  jackhammer\n  siren\n  street music\n\n\nAll are separated into 10 folds (each containing an equal distribution selection of the classes). These sounds are all less than 4s but vary in length, recording device, sample rate and perceptual loudness. Many of the sounds were generated as slices from longer sounds - meaning that many sounds share the same sound file source.\n\nFor creating a new dataset, I composed of multiple tracks of audio from UrbanSound8K. Using the library pydub, I created a separate script to use a randomized list of each fold’s files and overplayed each file with a randomized gain reduction between -6 and 0. This enabled a more authentic mixture of sound in a live context and would also contribute to the robustness of the model and its consequent difficulty during training (10 classes to 45 conceptual classes (or 10 choose 2)).\n\n...\nfor j in range(multi_num): # number of samples to mix\n    print(f'Processing: {samples[j]}')\n\n    labels[j] = samples[j].split('-')[1] # get label\n    sounds[j] = effects.normalize(sounds[j]) # normalize\n    sounds[j] = sounds[j] + rand_gain[j] # add random gain reduction\n    p_ratio[j] = pow(10, rand_gain[j]/10) # convert to power\n    names[j] = f'-{labels[j]}({round(p_ratio[j],2)})' # label file with params\n\ncombined = []\ncombined.append(sounds[0].overlay(sounds[1], times=20)) # overlay sound (repeat if base sound is longer)\n...\n\n\nTwo tracks of audio were chosen after testing the capabilities with three concurrent sounds, which appeared even too difficult to discern by ear - however, the script was written with the possibility of any number of overlays. This brings up another point - some of these classes of sound like ‘air conditioner’ and ‘engine idling’ closely resembled white noise, something that every recording ultimately contains due to the nature of recording sound. The final “multi” database is a little less than half the size of US8K as some of the samples of US8K were unreadable and were skipped.\n\nImplementation of model\n\nIn addition to the creation of a script to fabricate and label a multi-labeled dataset, I augmented the scripts used to evaluate a CNN on the original UrbanSound8K database to suit this new multi-label scenario. The actual feature processing was identical to the one-sound-per-sample database. The audio data’s features (mel-spectrogram) were extracted and filtered with librosa.\n\n\n    \n    One example of the mel-spectrogram feature\n\n\n\n    \n    Another example\n\n\nMost of my efforts here went into changing how the labels were being processed. The network used to train and test the data was fashioned with the Keras wrapper for TensorFlow and other ML backends. Keras was chosen as it offers the ability to construcasdast neural networks at a lower level than the sklearn package whilst being fairly novice-friendly.\n\nModel tuning for new dataset\n\nParameters that were previously provided by Saeed needed to be adjusted considering the new multi-label problem space. The first major change needed to happen at the label encoding level as labels were no longer a number from 0-9, an array of n values from 0-9. To resolve this, the labels were transformed into one-hot binary encoding. This allowed multiple simultaneous classes to be represented in an array of the same length.\n\nAnother major change concerned the end of the network, where predictions and evaluations are made on the training data. The final dense layer was set to the softmax activation function which was inappropriate for a non-exclusive multi-label scenario. The softmax function outputs probabilities for classes that sum to one, making it impossible for multiple classes to reach a binary activation. In this case, sigmoid offers probability distributions that are unconstrained, enabling multiple classes to reach binary classification.\n\n\n    \n    Applications of softmax vs. sigmoid - Credit to Ashis Parajuli\n\n\nIn parallel, the loss function needed to be changed from categorical cross entropy to binary cross-entropy for this binary data.\n\nEvaluation of performance\n\nDuring training, the accuracy score increased to around 44% after 15 epochs while the loss continuously decreased. However, the last 10 epochs showed only a 4% increase in accuracy, suggesting the model was approaching convergence of the weight values and perhaps over-fitting (I had also attempted with longer training sessions with similar outcomes).\n\n\n    \n    Categorical accuracy over epochs\n\n\n\n    \n    Model loss over epochs\n\n\nIn testing the data, again the migration from a single categorical label to a one hot binary array means that the accuracy metric (a mean average of predictions on the test set) does not give us the whole picture. Moving to hamming loss, an estimate that shows use what percent of our answer’s elements were correct, is a much better metric when predicting on a multi-label sample.\n\nThe average accuracy was quite poor, as expected, sitting at 18%, however, the hamming loss (less is better) was 15%, meaning that 85% of the model’s label predictions were correct. These metrics are in sharp contrast to the performance of the untampered UrbanSound8K dataset, which was observed to have an accuracy of around 75-85% for most state of the art models [1, 2, 4, 8].\n\n\n    \n    Confusion matrix across the ten classes\n\n\nThe confusion matrix and classification report, of one training instance, also provide interesting insights into how the test data was predicted. One major point to note is the time scale of some of these classes. It appears that those sounds that appear briefly with high impulses like the car honk, dog bark, and gunshot (1, 3, 6) are some of the most precisely predicted classes (low-false positives).\n\nAnd as one might expect, the classes most difficult to label correctly turn out to be the noisiest and likely most organic and spectrally dynamic sounds within US8K: children playing, air conditioner and street music corresponding to 2, 0, 9. Of course, these metrics would need to be compared to the performance of the model on the raw UB8K database for one to make clear conclusions about how the shift into a multi-labeled dataset\n\nReflections\n\nNot all of the details of the network employed have been fully elaborated and it may be that some transformations of the input data have been overlooked. Given the provided window of time and simultaneous introduction to techniques in machine learning, this investigation fulfils, at least, a tentative exploration into the field of ML based ENC.\n\nOne obvious challenge in mixing sounds as was performed in this experiment is the perceptual presence of the sound within the sample. This is actually accounted for by a subjective estimate in the taxonomy of the UrbanSound8K database where they determine if the category was a foreground or background sound. For categories like “air_conditioner” and “engine_idling”, poor classification performance would be expected when mixing sounds of these classes due to their lack of a sonic shape - they mostly consist of white noise. One might predict then that these categories were over-predicted on average across all sounds. Indeed, that does appear to be the case and this can be seen through the confusion matrix.\n\nAnother issue tied to the source database was its sheer size. The total time required to load around 9000 .wav files makes it prohibitively expensive when tuning parameters, or in this case, adapting the processing of files for a new problem space. K-fold validation would have been helpful in estimating the average accuracy of the model and providing greater confidence in our reflections of the model but there was not enough time to do so.\n\nCode and instructions for setting up this project can be found here.\n\nReferences\n\n\n  \n    Abdoli, S., Cardinal, P., &amp; Koerich, A. L. (2019). End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network. ArXiv:1904.08990 [Cs, Stat]. http://arxiv.org/abs/1904.08990\n  \n  \n    Mushtaq, Z., &amp; Su, S.-F. (2020). Environmental sound classification using a regularized deep convolutional neural network with data augmentation. Applied Acoustics, 167, 107389. https://doi.org/10.1016/j.apacoust.2020.107389\n  \n  \n    Piczak, K. J. (2020). Karolpiczak/ESC-50 [Python]. https://github.com/karolpiczak/ESC-50 (Original work published 2015)\n  \n  \n    Piczak, K. J. (2015a). Environmental sound classification with convolutional neural networks. 2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP), 1–6. https://doi.org/10.1109/MLSP.2015.7324337\n  \n  \n    Piczak, K. J. (2015b). ESC: Dataset for Environmental Sound Classification. Proceedings of the 23rd ACM International Conference on Multimedia, 1015–1018. https://doi.org/10.1145/2733373.2806390\n  \n  \n    Saeed, A. (n.d.). Urban Sound Classification, Part 2. Retrieved 18 September 2020, from http://aqibsaeed.github.io/2016-09-24-urban-sound-classification-part-2/\n  \n  \n    Aaqib. (2020). Aqibsaeed/Urban-Sound-Classification [Jupyter Notebook]. https://github.com/aqibsaeed/Urban-Sound-Classification (Original work published 2016)\n  \n  \n    Su, Y., Zhang, K., Wang, J., &amp; Madani, K. (2019). Environment Sound Classification Using a Two-Stream CNN Based on Decision-Level Fusion. Sensors, 19(7), 1733. https://doi.org/10.3390/s19071733\n  \n  \n    UrbanSound8K. (n.d.). Urban Sound Datasets. Retrieved 23 August 2020, from https://urbansounddataset.weebly.com/urbansound8k.html\n  \n\n",
        "url": "/machine-learning/2020/09/20/classifying-urban-sounds.html"
      },
    
      {
        "title": "Support Vector Machine Attempt",
        "author": "\n",
        "excerpt": "Not fun\n",
        "content": "For SMC4047, I designed a Support Vector Machine(SVM) model which can learn to separate genres. The dataset used consist of a drastically different genre such as ambient and similar genres such as liquid drum and bass and hip hop. The reason for this choice of dataset was to hopefully combine a variety of feature extractions in efficiency with the most reasonable results at the end process.\nThis project revolves around music recommendation. Streaming companies such as Spotify use user history and activities to recommend music that might be of similar taste of its users (Engineering, 2020).\n\nFeature Extraction\n\nFor this project, I extracted nine features using Librosa. \nOne of the most crucial extractions use was spectral flatness. As said on librosa’s page on spectral flatness, it measures how noise-like a sound comparing to its tonality (Librosa.Feature.Spectral_flatness — Librosa 0.8.0 Documentation, n.d.). Heavier genres could be describe as noise-like because of heavily distorted guitars and heavy harmonic processing. spectral flatness would be very appropriate to use as a value to separate genres.\nThe other extraction includes RMS levels, spectral bandwidth, spectral contrast, spectral centroid, spectral rolloff and tempogram.\nThe feature extraction took a very long time to execute due to the huge collection of music and file size. The extraction was designated with a label as well from the file name. However due to the nature of the very long processing, I then saved the information into a csv file using a pandas data frame which I then converted it back into a numpy array.\nUsing a scatter plot through matplotlib, I first tried to experiment if I could find any meaningful correlations to ensure that I have the necessary feature extractions for this project. In the python file, there would be three scatter plots, plotting out correlations of RMS to spectral flatness variance, spectral contrast with spectral rolloff and RMS with spectral rolloff.\n\n\nfigure 1. RMS vs Spectral Flatness Variance\n\nML Algorithm\n\nFor this project, I will be using Support Vector Machine(SVM). The choice of SVM was rooted in the advantages in the way SVM handles multi-dimensionality as there will be nine features. The other reason for choice in SVM was to try to be memory efficient.\nThe ethos behind the choice was to learn SVM well enough and try to squeeze out as much accuracy I can with handling high dimensionality without dimensional reduction.\n\nResults\n\nThe accuracy mean is slightly above 0.8 . The value might not be useable in a real world solution which means there is still allowance for improvements.\n\nConclusion\n\nThe choice of SVMs was ideal in my situation. It had all of the advantages required for my project.\n\n  Smaller Dataset\n  Multi-Dimensional\n  Memory Efficient\n\n",
        "url": "/machine-learning/2020/09/20/Support_Vector_Machine.html"
      },
    
      {
        "title": "Postcard from The Valley of Despair",
        "author": "\n",
        "excerpt": "Well, that was fun.\n",
        "content": "The Setup:\n\nWell, that was fun. Relax and have a good summer! they told us. And so we did. The trauma and triumph of the first year of SMC slowly faded as we road-tripped, climbed mountains, ran rivers, and leaped naked into fjords. We were children then, careless like the wind, our sunburnt faces besmirched with brunost and bringebær jam as we grilled waffles in the park (who does that?!) and washed them down with can after can after can of Who Cares pilsner.\n\nAnd then, we came back.\n\nSMC4047 is a tough course. There’s no sugarcoating it. Machine learning is a hard, bitter biscuit, dipped in cold data gravy. Eat and despair, ye who dare tread these shadowed neural pathways.\n\nOr, maybe not. If you have a good background in maths, some programming skill, and a fully-functioning frontal lobe, it’s easy! How nice for you. As for me, all I can say is time will beat you like a rented mule. Add in a deep suspicion of robits and a pernicious latent Luddism in general, and I’m pretty much that old guy at the park shouting at the kids to get their damn computers off my park bench.\n\n\n\nfigure 1: Dunning-Kruger Curve\n\n\nFigure one maps my approximate location on the Dunning-Kruger curve. I got over the peak of ‘Mount Stupid’ quickly, once the first week of SMC disabused me of any notions of expertise, and plummeted straight into the Valley of Despair. It’s hard to get out of that valley, especially without wheels (brains), but I’m doing my damndest, and occasionally some progress is made.\n\nAt least SMC4047 is well-documented, with extensive preparatory readings and hours and hours of video tutorials. Enough to keep you busy for years.\n\nExcept there’s a catch: you only have two weeks to prepare. Hahaha! Good luck, suckers!\n\nAn Attempt to Explain My Project:\n\nI tried to come up with something reasonable, something I could get my mind around, something that wouldn’t compel me to fling anything precious into a volcano. A simple classifier or something. And so, I came across the Google datasets on spoken language. That looked fun, or something. Maybe I could discover something about the underlying musicality of language? Is that a feature one can extract? Well, we can try. Read on.\n\nThe Data Set:\n\nI made a dataset by cutting down Google’s extensive set (like 18,000 examples) of short, spoken English phrases into 100 examples each in 11 different classes, making a total dataset of 1100 examples. Files were all 48kHz, 16-bit Wave audio. Six different regional dialects were represented, from Great Britain and Ireland, separated into male and female speakers. Intrepid readers who can multiply will note that there should therefore be 12 classes, not 11; apparently all the Irish gals were too busy takin’ a whirl ‘round the Salthill Prom with Mr. Earle to be bothered with this nonsense, so Ireland was only represented by the dudes.\n\nTo make the examples consistent, and therefore more palatable to The Machine, I used Librosa’s onset detection to trim the dead air from the start of each file, then set the duration to two seconds, the approximate length of the shortest file in the set. There, that only took me 37 hours.\n\nThe Features:\n\nFeatures extraction is the most important part of Machine Learning in my opinion, which of course is completely meaningless. However, the features that one extracts from one’s data is what determines the success of one’s model, so choose poorly here and no algorithm will save you. ‘Garbage in, garbage out’, as my grand-pappy used to say as he ate gravy for breakfast. Those were his last words.\n\nMy thinking at this point was to try to get the algorithm to be able to identify dialects. This is what we in the Machine Learning space call a Classifier, a Supervised Learning technique in which we train the algorithm on a certain percentage of the data (usually 70-80 percent) for which it knows the labels (which classes are which), then turn it loose on the rest and see how it fares. When it performs better than random chance at determining the classes, we say that the algorithm has ‘learned from data’, and pat it on its ‘head’. Good algorithm.\n\nSince audio files contain tons of data we don’t actually need for this task, we perform a process known as Features Extraction.\n\nLet me see if I can explain. Let’s say we devise a system to distinguish Humans from Cats. We assemble a dataset culled from the most popular, trending images of cats and people on the internet. Now, we know that just because Harry Styles has four nipples, that doesn’t make him half an average housecat (or even two people), but our machine doesn’t know that. Especially if the only feature you extract from data is ‘Nipplegram’. Aside from the fact that this feature doesn’t exist in any ML library that I know of, apparently ‘number of nipples’ just isn’t a reliable predictor of species. Maybe we should try to detect the presence of fur. Wait, now Steve Carell and Zach Galifianakis are cats. Dang, this stuff is hard.\n\nOr maybe those guys are just outliers. If your data set contains 100 images (way too small, what are you thinking?), and it classifies Styles, Carell, and Galifianakis as cats, and that one Sphynx skin cat with shy-nipple syndrome as a human, you still got 96 percent right. Hey, that’s pretty good!\n\nThe Algorithm:\n\nOh, merciful science, there are so many. After dipping my toes into exciting-sounding neural networks like the Multi-Layer Perceptron and the encouraging Support Vector Machines, I settled on the aptly-titled Gaussian Naive Bayes classifier. It’s (relatively) easy to understand, if you understand Bayesian probability theory. Or, look at fig. 2. Pretend like you’re trying to find which category x falls into, A or B. Which curve does it go higher on? Bingo.\n\n\n\nfigure 2: X is probably A\n\n\nHow the system performed:\n\nSo well! After K-fold cross-validation, my system returned an accuracy score of about 95 percent. It seemingly learned how to detect dialects, so I was successful, right? Nah. Going back to the dataset, I realized that the speaker within each class was always the same person, most likely using the same microphone, in the same room. The system was most likely detecting the unique sonic signature of a particular person’s voice coupled with their environment and the equipment used to record them. They could’ve been speaking Russian or Xhosa or gibberish and the results would have been the same.\n\nConclusion:\n\nBack to the drawing board. See you in the Valley.\n",
        "url": "/machine-learning/2020/09/20/Valley-Of-Despair.html"
      },
    
      {
        "title": "Beneficial Unintelligence",
        "author": "\n",
        "excerpt": "In the future, when the robots take over the world, we all will be listening to 24/7 live streamed death metal until infinity\n",
        "content": "Artificial Intelligence\nArtificial Commonsence\nPractificial Frankinsence\nBeneficial Unintelligence\n\nMachine Learning, the buzz word. Since I started on the SMC master, I’ve been looking forward to these two intensive weeks of coding. I imagined building a whole choir of cute singing robots, performing Beethoven’s 9th symphony:\n\n\n\nOr to create a music buddy, who could be my new band mate, something like Dr. Squiggles:\n\n\nOr an android guitarist with 78 fingers. This is what I need in my band obviously. My Guitar Pro compositions from 2007 (with guitar tabs that demanded the guitarist to have at least 3 hands with 7 fingers each), could have it’s renaissance:\n\n\nImage source: http://edition.cnn.com/2014/03/14/tech/meet-the-robot-guitarist/index.html\n\nSo what’s machine learning? It can be many things. It can be something that learns, after you give it a lot of data with labels attached, and after some training, it tries to predict something (supervised learning). Or it can be an algorithm that on it’s own organizes loads of data into lumps, without labels, and it’s called clustering or unsupervised learning. \nIt can be something that replicates the network of neurons in a human brain: Artificial neural network.\n\nSo Machine Learning is a lot of things, really. It’s not just the robots that are about to get sentient (and soon will take over the world etc). And if you get really good at machine learning, you can make a youtube channel that live streams deathmetal 24/7 until infinity:\n\n(This is by the way, what we all will be listening to in the future, when the robots take over the world)\n\nIf you are not THAT good at machine learning, but had a two weeks intensive SMC Music and Machine Learning course, you could for instance do something like I did: make a system that identifies which vocal technique you’re using! Why, you might say, don’t we have vocal teachers that do that? Well, if this corona hell is going to last, we might really need some online tools for music instrument learning assessment. If your internet connection is bad, and you sound like Dadabot’s Death Metal livestream through Skype, what should your vocal teacher say, I mean. Therefore I present:\n\nThe Vocal Technique Classifier:\n\nTo make this tool you need:\n\n  1 dataset\n  A couple of feature extractors (E.g. MFCC and Melspectrogram)\n  1 pinch of Linear Discriminant Analysis (my favourite ingredient)\n  1 Support Vector Machine OR 1 Artificial Neural Network\n\n\nBefore you pour the whole datset into the algorithm, it should be processed with an extensive procedure of feature extraction. Add 1 MFCC and 1 Melspectrogram with Librosa. Now it’s time to add the emulsifying agent, the Linear Discriminant Analysis. Mix it all together until it turns to a delishious spaghetti code and pour it all into the Support Vector Machine (Optional: Artificial Neural Network). Turn on your CPU to 250 degrees celcius and wait for 5-10 minutes. Bon Apetit.\n\nGallery:\n\nThe delightful chaos before applying LDA:\n\n\nThe beautiful tidiness after applying LDA:\n\n\nUnicorn spaghetti in 450 dimentions:\n\n\nSpectrogram before LDA:\n\n\nBarcode of vocal features:\n\n\n",
        "url": "/machine-learning/2020/09/21/beneficial-unintelligence.html"
      },
    
      {
        "title": "Learning to sequence drums",
        "author": "\n",
        "excerpt": "Can reinforcement learning be a useful tool to teach a neural network to sequence drums?\n",
        "content": "Sequencing drums with reinforcement learning\n\nThe motivation for this project was to start to explore interactive use of machine learning in music, with the intention to continue the work in my master thesis. The specific goal of this project has been to train a neu- ral network to respond to incoming MIDI sequences of one instrument with appropriate MIDI sequences of another instrument.\n\nThe background for the idea was a personal desire for a system that could be able to respond to choices I make in my digital music production. Often, I find myself sampling from a variety of different random functions to produce interesting parameter combinations or patterns. However, I generally have to make quite a few attempts before finding something that sounds meaningful.\n\nIn this project I wanted to figure out if it was possible to create a software that could do a bit better than random, to augment the human-computer interaction in a production setting.\n\nSource code: https://github.com/ulrikah/rave\n\n\n\nReinforcement learning\n\nTo tackle the problem at hand, I decided to apply techniques from reinforcement learning, a form of machine learning that I have been wanting to learn more about for some time. Reinforcement learning is concerned with learning software agents an action policy that they should use to act in an environment with the goal of maximizing a cumulative reward. The reward is given by the environment in which the agent is acting. One example of reinforcement learning could be a game in which a software agent learns to play tic-tac-toe against a human opponent only by evaluating the reward (e.g. 1 for victory and 0 for loss) after repeated rounds of playing with a human agent. For a game such as tic-tac-toe, in which both the action space and the observation space are small and discrete sets, iterating through all the game states and calculating the optimal policy is a trivial task. In more complex environments, it can be either impossible or too computationally expensive to derive the policy explicitly. In such scenarios, neural networks can be used as approximators for the policy function, since they have the property of being universal function approximators [4]. This has led to recent breakthroughs for the field of reinforcement learning, such as the ability to learn agents to play several Atari games only by looking at the raw pixel values [5].\n\nImplementation\n\nThe code for the project can be found at: https://github.com/ulrikah/rave. After installing the dependencies in conda.yml, training the model can be done by executing\n\npython main.py train\n\n\nThis produces a checkpoint in the checkpoints/ folder. Subsequent testing of the model can be done by running\n\npython main.py test --checkpoint checkpoints/&lt;checkpoint&gt;\n\n\nModelling the environment\n\nTrying to build a custom reinforcement learning system comes with the cost of having to design and develop and environment in which your agent can act. To build the environment, I created a class which inherits from gym.Env (see the gym docs) and implements the required methods and properties.\n\nThe environment I built is more conceptual than the typical gym game environments. At its core, it has a function for calculating the reward given a (state, action) pair, and a function called step for performing iterations and updating its internal state. It also has a render function which simply prints the sequence as a string. However, render is supposed to be extended with some functionality for converting the binary sequence to MIDI and doing I/O operations over a MIDI port.\n\nThe metric I used to calculate the reward was simply defined as the normalised sum of the absolute difference between the two patterns. As an example, given the following inputs:\n\nx o o o x o o o x o o o x o o o [kick]\no x x x o x x x o x x x o x x x [snare]\n\n\nthe environment yields the maximum reward of 1.0. In other words, for most of the time I tried to make the network learn the snare pattern above. This metric is in itself not interesting in a machine learning context, since it would be easy to define a much simpler algorithm to optimize the program to generate such sequences. However, it was very useful to work with a simple metric to evaluate whether or not the network learned the way I hoped it would.\n\nI wanted to test out a variety of metrics in my project, but I didn’t get that far. I would like to try implement at least one of the techniques from Shmulevich and Povel [7] to incorporate a metric of rhythmic complexity. In that way, given a kick pattern as input, I could potentially be able to learn a neural network to play the most complicated appropriate snare pattern as the output.\n\nEvaluation\n\nTo evaluate my model, I made a script that would run n iterations of the environments step function and blindly take the argmax of the softmax output the neural network as the action. The softmax activation function returns a probability distribution over the different steps in the sequence. Taking the argmax of softmax just means picking the step in the sequence that the neural network has given the highest probability for. During training of the network, I sampled from the softmax distribution in order to explore different options.\n\nConsidering that my network only can toggle one step at a time, it should take some iterations before it finds the optimal sequence.\n\nI created two test scenarios which should yield rewards in the two extremes of the reward range (−1, 1) according to the metric I used. In both scenarios, the kick sequence is initialized to the standard 4/4 sequence.\n\nThe model that I used to test here was trained for 300 episodes.\n\nBest case scenario\n\nx o o o x o o o x o o o x o o o [kick]\no x x x o x x x o x x x o x x x [snare]\n\n\nThis scenario yields the maximum reward of 1.0 by default. This means that the network hopefully should avoid modifying the snare sequence, as it is already in its most optimal state. This very model was trained without the option to avoid toggling (i.e. by only outputting values for each of the 8 on/off steps in the snare sequence). As a consequence, it is expected that the network oscillates between the rewards 0.75 and 1.0.\n\nFigure 1 demonstrates that the network is able to avoid modifying an optimal sequence, which is what we wanted. And interesting side effect is that the network only uses one of the steps to toggle on/off (a bit difficult to see in the graph). It may seem like the network has learned that if it is in a balanced state, it should only toggle one of the steps repeatedly.\n\n\n   \n   Fig. 1: Reward over 10 iterations for the best possible sequence as input \n\n\nWorst case scenario\n\nIn the worst case scenario, I initialize the snare sequence to be equal to the kick sequence:\n\nx o o o x o o o x o o o x o o o [kick]\nx o o o x o o o x o o o x o o o [snare]\n\n\nThe expected result of this scenario is that the network is able to modify the sequence into a sequence that ends up yielding rewards 0.75 and 1.0.\n\nFigure 2 shows 10 forward passes through the network. The network manages to modify the snare sequence into a sequence that yields positive results very fast. After it has reached the optimal state, we can see the same oscillation between 0.75 and 1.0 that we observe in figure 1.\n\n\n   \n   Fig. 2: Reward over 10 iterations for the worst possible sequence as input \n\n\nReflection\n\nThroughout the project, I spent a lot of time trying to understand why the neural network I created failed to learn the representation I wanted it to learn.\n\nAt the very end of the project, I realized that I was only considering the average reward per episode as the metric of performance. This doesn’t make much sense given that I initialized the snare sequence randomly every time I reset the environment, which was done at the start of every episode. The problem with this approach is that it doesn’t measure how well the network performs in terms of correcting a bad sequence. Looking back, I probably should have measured the delta reward per episode, i.e. if the episode yielded a more positive reward at the end of the episode than at the beginning.\n\nFuture work\n\nFuture work on this project will involve exploring other metrics for defining the reward. I think an ideal solution would be to have the reward be defined by some weighted sum of multiple metrics, one of them being for instance to look at rhythmic complexity. In a production scenario, it would be interesting to let the user define which metrics that the agent should learn from, and this might be parameters that the musician could play with in a live context.\n\nThe system as it is today don’t render the binary sequences in a meaningful way. There is no formal relation to MIDI in the code, which obviously is something I would need to work on. Some of the next steps would be to formalize the agents a bit more, and integrate them with existing MIDI libraries such as pretty-midi or mido.\nI would also need to design the algorithm more carefully to allow for flexible sequence lengths. One idea could be to train the network on long sequences (e.g. 512 or 1024 notes), and to do some smart resampling of shorter sequences.\n\nExploring how to extend this program to multiple voices would also be interest- ing to look at. Ideally, I would like to have multiple agents all learning from each other in a swarm-like manner to investigate what those sort of interations might produce. For this purpose, looking at Multi-Agent Actor-Critic [12] might be interesting.\n\nAt last, the system would need to be redesigned in a way that makes it applicable. This would potentially mean to export the model into something that could be read by other environments, and e.g. use it as part of a Juce or Max plugin (through Node For Max) in Ableton Live.\n\nReferences\n\n\n  Qichao Lan, Jim Tørresen, and Alexander Refsum Jensenius. RaveForce: A Deep Reinforcement Learning Environment for Music. In Proceedings of the SMC Conferences, pages 217–222. Society for Sound and Music Computing, 2019.\n  Francisco Bernardo, Chris Kiefer, and Thor Magnusson. An AudioWorklet- based Signal Engine for a Live Coding Language Ecosystem. In Proceedings of Web Audio Conference (WAC-2019).\n  Lonce Wyse. Mechanisms of Artistic Creativity in Deep Learning Neural Networks. arXiv preprint arXiv:1907.00321, 2019.\n  Michael A. Nielsen. Neural networks and deep learning, volume 25. Deter- mination press San Francisco, CA, USA:, 2015.\n  Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n  Muhan Li. Machin. https://github.com/iffiX/machin, 2020.\n  Ilya Shmulevich and D.-J. Povel. Rhythm complexity measures for music pattern recognition. In 1998 IEEE Second Workshop on Multimedia Signal Processing (Cat. No. 98EX175), pages 167–172. IEEE, 1998.\n  Adam Paszke. Reinforcement learning DQN tutorial. https://pytorch. org/tutorials/intermediate/reinforcement_q_learning.html, 2020.\n  Chris Yoon. deep-q-networks. https://github.com/cyoon1729/ deep-Q-networks, 2019.\n  Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider,\nJohn Schulman, Jie Tang, and Wo jciech Zaremba. Op enAI Gym. arXiv:1606.01540 cs., June 2016. arXiv: 1606.01540.\n  Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alch ́e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.\n  Ryan Lowe, Yi I. Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in neural information processing systems, pages 6379–6390, 2017.\n\n",
        "url": "/machine-learning/2020/09/21/reinforcement-learning-drum-sequences.html"
      },
    
      {
        "title": "Scale over Chord using ANN",
        "author": "\n",
        "excerpt": "Try to learn scale over chord choices of great jazz improvisers\n",
        "content": "Scale over Chord using ANN\n\nIntroduction\n\nThe aim of this research is to propose a way of determining the usable scale(s) in any jazz harmonic context, according to what scale choice a particular great soloist would have made. It is to be proven that some chord components may favor a certain scale choice over another.\n\nMost of the music theory relies on Mark Levine’s jazz theorizing (Levine 1995), but only the heptatonic scale is considered. The simplified model used here consists of categorising chords and scales into six types. A different algorithm should be trained for each type and each missing degree, but the 6th of dominant chords (mixolydian and mixolydian b13) will be used as a proof of concept here.\n\nDatabase\n\nThe Weimar Jazz Database (Pfleiderer 2017) contains manual transcriptions of 456 famous jazz monophonic solos. One of them, Benny Goodman’s improvisation on Handful of Keys, is shown in fig. 1 as a PDF score.\n\n\n   \n   Fig. 1: Handful of Keys as a score \n\n\nIn the database, chords are represented as strings, and notes as MIDI pitches. Notes that are missing harmonic context (notated N.C.) are simply discarded. Fig. 2 reveals what exact data is being retrieved from Handful of Keys (song no 15 in the WJazzD).\n\n\n   \n   Fig. 2: Handful of Keys as a data set \n\n\nImplementation\n\nA regression algorithm is used here, where the target is a float between 0 (minor) and 1 (major) representing the 6th’s major alteration’s probability on a dominant chord.\n\nLibraries\n\nTo do the music theory analysis (parse chords, compare notes, construct scales, …), a custom library has been implemented.\n\nTo retrieve the data from the WJazzD database, sqlite3 is used.\n\nTarget Definition\n\nThe target float in this implementation represents the probability between two cases, major and minor alteration of the 6th (of dominant chords). As the same harmonic contexts (features) can lead to different outputs, data with similar features are used to calculate a proportion for each output.\n\nFeature Extraction\n\nThe features represent the harmonic contexts of any chord. Many notations and neighbour sizes have been tested, but the one that gave the best results uses pitch classes (Rahn 1980):\n\n\n  For any central chord numbered c in the progression, the harmonic context is defined as the relative pitch classes of the chord notes composing the chords numbered c-2, c-1 and c+1 (2 chords before, 1 chord after).\n\n\nThe relative pitch classes (rpc) are defined as the difference (%12) between the chord notes’ pitch classes and the pitch class of the current chord’s fundamental. Fig. 3 shows a sample of features associated to a target. Fig. 4 aims to represent the data set by showing the slight differences between major and minor 6th cases in the WJazzD.\n\n\n   \n   Fig. 3: Features and target \n\n\n\n   \n   Fig. 4: Proportion of each rpc for each neighbouring chord, classed by most probable alteration\n\n\nAlgorithm\n\nAn artificial neural network is used to train the data with, as it seems to be a scalable solution, in case this project goes any further. The underlying task of the model is to define which neighbouring relative pitch classes tend to imply a major or minor 6th alteration, particularly which are the defining relative pitch classes. The assumption was that some notes present in the neighbouring notes would influence the scale selection, and it seems to be the case as the algorithm gives coherent results.\n\nEvaluation\n\nAt first the testing gave r2 scores around 0.5, which is not too bad for a start. Neural networks were given a size of 2 layers containing between 4 and 16 neurones. Doing repeated k-Fold and grid search quickly revealed that parameters tweaking could enhance the results quite significantly. Splitting the data in 0.8 for training and 0.2 for testing, using an activation parameter ‘tanh’ and hidden layer sizes of [300, 300, 300, 300, 300, 300, 300], the testing gives a r2 score of around 0.65, which can be considered a significant increase. The variance is quite low, the training always gives r2 scores between 0.62 and 0.67.\n\nThe system is of course enhanceable, but the result seems to indicate that there is indeed a correlation between the harmonic context and the alterations of the heptatonic missing degrees.\n\nReflection\n\nOne of the main benefits of the research is its application to the existing AutoScale improvisation system (Jaccard 2020). Indeed, this software aims to predict and propose scales on any chord progression, which could now be done using machine learning trained on any great jazz artist.\n\nIf the project goes any further, models have to be trained separately to determine alterations of any degrees of any chord type (not just dominants’ sixths). Furthermore, the particular jazz artist could be considered as a new feature, and the scale proposition algorithm could be inspired by any improviser the user wants to imitate, or learn from.\n\nConclusion\n\nThis project has been a great pedagogic opportunity, as I learned so many basic aspects of machine learning and more specifically data retrieval and feature extraction. I really want to carry on in this direction, and use this first step as a start in my master thesis. I in fact consider creating a bigger database derived from audio recording, containing both chord and scale progressions. In such a process, the WJazzD could be considered as a testing data set.\n\nReferences\n\n\n  Rahn, J 1980, Basic Atonal Theory, Longman, New York.\n  Levine, M 1995, The jazz theory book, O’Reilly Media, Inc., Surrey.\n  Pfleiderer, M 2017, Inside the Jazzomat - New Perspectives for Jazz Research, Schott Campus, Mainz.\n  Jaccard, T 2020, AutoScale: Automatic and Dynamic Scale Selection for Live Jazz Improvisation, NIME.\n\n",
        "url": "/machine-learning/2020/09/21/scale-over-chord-ann.html"
      },
    
      {
        "title": "Multilayer Perceptron Classification",
        "author": "\n",
        "excerpt": "Multi-layer Perceptron classification. Big words for a big task. During this two-week course in machine learning all my brain cells were allocated in solving this task. Initially I wanted something simple to do for my project since wrapping my head around ML was a daunting enough task. I soon realized there really is no such thing as simple in machine learning.\n",
        "content": "Multilayer Perceptron Classification of Drums\n\nOVERVIEW\nFor my project I have worked with drum samples and respectively classified them into snare, kick, tom, ride, crash etc… There has been done quite a bit of research into this field, as drum classification is a vital part in auto transcription of music, as well as its commonplace within software to handle and organize large sample libraries.\n\nTHE DATA SET\nThe dataset I used for this site I collected from this site.\nIt consists of a whopping 10 600 files. Albeit some are these are different microphones of the same recordings. Every drum is struck from different positions, heights and intensities making it a great dataset to work with. I used the three 10 seconds of each sample, and loaded it at 44,1 kHz as I came to the conclusion that a higher sample rate would help better define the relatively short and large frequencies drums will get you.\n\n\nFEATURES\nWhat soon became apparent working with such a large dataset, was the amount of time spent on extracting features and training. Of course I just got to have it all, so the first time around I extracted literally every feature librosa had to offer. A whopping 99% accuracy! Looking at the CSV file of all my features, it was getting close to 2GB, so obviously I got to working with reducing the amount of features, and basically going back to scratch. I came up with a process where I tested each of the features to see what kind of accuracies, they alone would give me, for some indication as to their importance. Zero crossing rate, is for many a given when working with drums and it gave me decent results, ranging in the 80% accuracy just from the mean of the ZCR. The other two I identified as important were the melspectrum and the chroma energy normalized distribution statistics. I quickly also realized I were extracting a whole lot of unnecessary data of nothing and reduced the duration to three seconds. Using only these three features I reduced the amount of data to a fraction of my starting point but was able to preserve accuracies in the high 90’s.\nAs a learning experience this was a valuable course. But the time constraints really did limit the scope of the project. Despite this, the course was an excellent introduction to librosa, and how powerful it can be, if only put in the right hands!\n",
        "url": "/machine-learning/2020/09/21/MLPclassification.html"
      },
    
      {
        "title": "ML something creative",
        "author": "\n",
        "excerpt": "Intuition tells me that a larger network should be better. More is more, as Yngwie says, but that is definitely not the case.\n",
        "content": "10th of September\n\nWhen starting this course i decided on giving it my best, following 100% and trying to learn as much as possible. I knew that it would be hard, but I also knew that I would be very satisfied when completing it and handing in something i was proud of.\n\nSince the first step after having decided on what to do in my project is to gather a dataset, I did this. I also have to cut the dataset into\n\n11th of september\n\nI have finally gotten an algorithm to run on my own dataset!\nI have hesitated quite a bit to actually go ahead and try because I wanted to understand EVERY aspect of the code before I do so. Not that I understand nearly every aspect of it, but I feel a bit more confident now.\n\nI really wish that I had more knowledge of Python before starting this, right now I feel like I am trying to write map directions or a cookbook in German (I don’t speak German).\n\nThe algorithm that I fanally got tu run was the one from “Workshop 2a”.\nThe first time around I got the astounding result of 29% accuracy! Meaning that it is just slightly better than random guessing…\n\nWhat should I do to improve this?\n\nAllright!\n\nIt’s getting pretty late here and I have had a few beers, but things are starting to look better!\n\nI managed to get the “workshop 4a” -patch to work after just a few modifications. I don’† know why, but I get this weird error when trying to write to anything other than a .CSV file (like in “workshop3b”).\n\nValueError: could not broadcast input array from shape (13545) into shape (1)\n\nIt obviously has something to do with the formatting of the array that i am storing to, but I cannot manage to figure out what it is.\n\nOne very strange observation is what the “random starte” value does to the result of the algorithm. I changed it from the initial 50 -something to 3, then the accuracy dropped from 50% to arround 30% after that, I started with 200 and it raised up to over 60%. How could the seed for a random number have so much to say for the outcome of this?!?\n\n12th of September\n\nI wondered if why I got such bad results from my algorithm was because I had been sloppy with processing of my training data. So today I took the time to go through every single file, cutting them to the same length and normalize them all. Now, inconcistensies in the samples should not be a problem. Still I have very bad results! This is getting ridiculous.\n\nI now feel very stuck on this.\nMostly because my knowledge of Python is so limited that I don’t have the ability to see where the mistake is made in the first place. What to do now? Learn Python! Where to start? I have no idea.\nMaybe the best thing to do is to run through all the code in the notebooks and stop if there is anything that is unclear, learn it and go further. This way I will know where the problem is.\n\nSix hours later\n\nWell, have I learned Python yet?\nNo.\n\nBut I have come a whole lot closer!\nThere is still so much to learn before I feel comfortable with understanding what I am doing with these notebooks, so better keep at it.\n\n14th of September\nI have fiddeled around a bit with different features and structures of the algorithm and it struck me that the MLPclassifier in “tanh” -mode is SO much slower than the others. Being that much slower, one would guess that it is much better as well? Not the case.\nAfter leaving it for about 10 minutes it produced results that were slightly worse than those of the default “relu” -setting. Why?\n\nNumber of mislabeled samples 10 out of 21\nAccuracy: 0.5238095238095238\n              precision    recall  f1-score   support\n\n     0.0       0.00      0.00      0.00         2\n     1.0       1.00      0.75      0.86         4\n     2.0       0.50      1.00      0.67         2\n     3.0       0.50      0.67      0.57         3\n     4.0       0.75      0.60      0.67         5\n     5.0       0.50      0.33      0.40         3\n     6.0       0.00      0.00      0.00         2\n\naccuracy                           0.52        21    macro avg       0.46      0.48      0.45        21 weighted avg       0.56      0.52      0.52        21\n\n\nconfusion matrix\n[[0 0 0 0 1 0 1]\n [0 3 1 0 0 0 0]\n [0 0 2 0 0 0 0]\n [0 0 0 2 0 1 0]\n [0 0 0 0 3 0 2]\n [1 0 0 1 0 1 0]\n [0 0 1 1 0 0 0]]\n\n300,3000,3000,10000,2000,200), max_iter=2000, activation=’tanh’\n\nNumber of mislabeled samples 8 out of 21\nAccuracy: 0.6190476190476191\n              precision    recall  f1-score   support\n\n     0.0       0.00      0.00      0.00         2\n     1.0       0.60      0.75      0.67         4\n     2.0       0.67      1.00      0.80         2\n     3.0       0.50      0.67      0.57         3\n     4.0       1.00      1.00      1.00         5\n     5.0       0.50      0.33      0.40         3\n     6.0       0.00      0.00      0.00         2\n\naccuracy                           0.62        21    macro avg       0.47      0.54      0.49        21 weighted avg       0.56      0.62      0.58        21\n\n\nconfusion matrix\n[[0 2 0 0 0 0 0]\n [0 3 1 0 0 0 0]\n [0 0 2 0 0 0 0]\n [1 0 0 2 0 0 0]\n [0 0 0 0 5 0 0]\n [1 0 0 1 0 1 0]\n [0 0 0 1 0 1 0]]\n\n## Tuesday 15th of September\n Dang it!\n I had let the system sit on my computer overnight with these settings:\n #Import the classifier\n\n\n  mlp = MLPClassifier(hidden_layer_sizes=(300,10000,10000,10000,10000,2000), max_iter=2000, activation=’tanh’, verbose=True)\n\n\nIt took forever to go through just one cycle and I was really excited to check with it in the morning to see how it had done, but when I did it said that “the Kernel has stopped”\n\nToday I realized that I have to expand my dataset to include more samples. So I have gathered a lot more samples on some of the instruments and am very eager to see if it makes a difference!\nSince I eave downloaded more samples in only two categories, I predict that the results that I get from those categories will be much better than those from the others, it’s time to find out.\n\nSuccess!\nIt really got better for most of the samples.\nWhich kind of is to be expected since when more samples get classified correctly, that leaves less to chance.\n\nWednesday 16th of September\nSooo\nI have been very intreagued with the question of why an ANN does what it does. I have tried many different setups in the structure of the network and found some very interresting results.\n\nIntuition tells me that a larger network should be better, “more is more” as Yngwie says, but that is definitely not the case. Some of the best results that I have gotten has come from fairly simple networks of 20,300,300,300,20. This configuration has given me up to 80% success rates, which I think is pretty good for my case.\n\nMuch, much later the same day\nAllright!\nFinally I feel that I have some kind of grip on what all the aspects of the algorithms do. I have this thing where I can’t really settle for working with something that I don’t know exactly how works down to every little detail. This is a great ability when learning something that builds “on top of” prior knowledge on a subject, but in this project? Well…\nI don’t know Python™ that well, so stopping and looking up what a line of code does every two minutes is very tiresome. What is even more tiresome is looking up what an MCFF, spectral centroid or spectral rollof is/does and reading about it untill I have no more questions and can explain this to anyone if they ask. It takes so much time away from coding, it’s ridiculous.\n\nIt is very hard to try to conceptualize where the values are stored, how the arrays look, what the sequence of precesses are and so on before one has a good understanding of the language used. I now feel that I have the basic knowledge of Python™ that is needed to get someting useful out of this project, which is so frustrating! I really wish that I had this knowledge when starting the course, then I wouldn’t have to both understand how machine learning works and the language, I could have focused only on machine learning, making the learning outcome much greater.\n\nFound out that the “adam” -solver works much better than the “lbfgs” -solver. Which is weird, since the litterature states that lbfgs should be better for smaller datasets, which I judge my dataset to be.\n\nThursday the 17th of September\nInteresting.\nWhen I was thinking of what to do for my project I had the idea of making the algorithm recognize feedback to be an automatic feedback spupressor that will not effect the audio signal the way conventional feedback supressors do. After tweeking and training my algorithm a few times, I noticed that it almost always recognized feedback correctly. Maybe this would be the best application for what I have made here. A machine that shouts “FEEDBACK!!!” and the name of the channel that it feeds back on is a thing that would be really helpfull in a live audio setting.\n\nDeep into the night\nWell, I have done what I can on this project and will hope for the best.\nOne very interesting discovery was how much better I work and how much better I feel when I am on cmapus! Today I went to the portal to ask Stefano some questions and ended up sitting there with my fellow students for a little while. The feeling I had afterwards was so much better than the one I have had the last two weeks! It must be seing people. Yes, I do like working from home from time to time, but being in the physical presence of others really help me get better work done by putting me in a better mood.\n",
        "url": "/machine-learning/2020/09/21/Gaute-sv-Skynet.html"
      },
    
      {
        "title": "A Brief Workshop on Motion Tracking",
        "author": "\n",
        "excerpt": "Since our spring semester of motion tracking was a purely digital experience, a few of us got to together to quickly test out the OptiTrack system within the Portal.\n",
        "content": "A few of us were taught by RITMO post-doc, George Sioros, the basics of the motion tracking system (OptiTrack) hidden in the rafters of the UiO portal. While we only had two days, it was interesting to explore the technicalities involved in setting up and running an interface like this. The accuracy of the system was exceptionally high and robust even during rapid movement.\n\nWe ended up doing a simple recording on the lower half of the body, see below!\n\n\n  \n    \n    Jackson jumping around synced with Motive, the motion tracking software\n  \n  Jackson jumping around synced with Motive, the motion tracking software\n\n",
        "url": "/motion-capture/2020/09/25/motion-tracking-workshop.html"
      },
    
      {
        "title": "Pedál to the Metal",
        "author": "\n",
        "excerpt": "Another week, another audio-streaming platform to test. Here is Team A’s impressions of Pedál. According to their website, Pedál is a cross platform service to stream audio, speak, share screen, and record high quality audio together and the simplest way to make music together online.\n",
        "content": "Pedál\n\n\n\nAccording to their website, Pedál is “a cross platform service to stream audio, speak, share screen, and record high quality audio together” and “the simplest way to make music together online.”\n\nUser Interface Design\n\nAs the name suggests, this app is designed to look like a footpedal, the type you would use to add audio effects to your guitar.  Typically these pedals only have one button which enables or disables the effect.  This is great for physical pedals while you’re playing guitar, but for a desktop app? - not so much.\nIn order to keep with the pedal aesthetic, all the features of the program are hidden from the user. Even what the main pedal button does is not immediately clear.\n\nSo, lets go through a few things.\n\nHow do you think you would configure your input and output devices? Maybe the “settings” cog at the top right, that sounds reasonable.\n\n\n\nNope, that just lets you Quit, Subscribe and Share Screen.\n\nTo configure your input and output devices you actually have to click on your UV meter display.\n\n\n\nLet’s say you want to mute your audio for a minute.  We already know clicking your UV meter configures your devices so that can’t be it. But it kind of is… If you hover your cursor over your name it will change to the word “mute” allowing you to click your name to mute your audio.\n\nLet’s press on.\n\nTo actually connect to someone else, you click on the UV meter on the right, of course.  This brings up a contacts screen where you can add contacts to create a session with or go “On call” which allows anyone else using Pedal to request a session with you.\n\nSo what are the two windows below the UV meters?  The top one is your WAV/MIDI recordings and the bottom one is the WAV/MIDI recordings of the person you’re connected to.\n\nYes, there is a user guide (if you care to scroll down to the bottom of their website and click “knowledge base” and then scroll to the bottom of that webpage) but this is a simple app and it’s 2020, you shouldn’t have to RTFM.\n\nFeatures\n\nPedál spouts on their website - “Sample or send WAV directly between each DAW” which led me to believe this was some kind of audio VST, routing the other user’s audio directly into a channel in my DAW.  That’s not the case. So what does it do?\n\nWhen you click the big pedal button, it records either your audio input (if you’ve selected wav) or midi input (from a MIDI keyboard, not your DAW, when you’ve selected MIDI) and sends it to the other user’s Pedal app.\n\nSend WAV\n\nWhen you click the pedal on WAV mode, your audio is sent directly to the other user allowing them to drag the wav file from pedal directly into their daw. From there on it’s pretty clear, you can use it as any other audio file in your daw.\n\nSend MIDI\n\nThere is a “BPM” you can set when you’re sending midi, but there’s no click track when you hit the foot pedal to start recording, so assumably you need to provide this yourself. However, the audio file the other person receives has a BPM attached to it. It’s not clear what would this help with: a BPM it’s more useful while you’re playing and recording not afterwards.\n\nImpressions\n\nWhen we were testing this software we found that the latency of audio using the Pedal app was actually slightly higher than the latency of Zoom’s audio which is not great for a platform made specifically for streaming and recording high quality audio.\n\nRegarding the option to share screen to the person you’re connected to, as Windows user we encountered a surprising bug. Even after we stopped sharing the screen, the “shared screen” window didn’t close from the other person’s screen, it stayed there as a black window confusing and dark until manually closed.\n\nPedál’s usability is undermined by the impossibility to connect to more than one other person. We still haven’t figured out if it’s possible, and if yes, how - it’s not clear based on their user guide either. Latency issues aside, the quality of the audio is quite alright - that is, if you’re careful to select the proper drive! If not, then you get stuck into an infinite loop and end up having feedback issues.\n\nSummary\n\nIt’s hard to see who this app is targeted at.  When people record audio for someone, rarely do they want to send it to a producer in RAW form.  Typically they will want to redo sections, apply effects or just tidy it up a bit.  But for the sake of argument lets say you want to stream directly to someone else and let them have the unedited audio files immediately rather than saving them to your drive and sending it to them via some other means e.g. shared Dropbox.  How often is someone likely to do this? If pedal becomes a producer’s preferred method of working with clients, how easy is it going to be for them to convince their client to subscribe to pedal for $5/month for a one off session? Its probable best working scenario is when two producers are working on a song and want to share and monitor files immediately, it will also help if there are audio plugins which are not common to both the producers and want to use them in runtime on the audio while producing a song.\n\nBut if this is what you want, you can also use Audacity and save your files to a shared Dropbox, where you would then be able to have different versions saved. And if you want to use the app for jamming, there are free apps available with less latency, which allow more than two users at a time to join (see for example Jamulus).\n\nNote\n\nAll three of us are using Windows 10.\n",
        "url": "/networked-music/2020/09/26/alena-pedal-experience-team-A.html"
      },
    
      {
        "title": "Zoom - High Fidelity and Stereo",
        "author": "\n",
        "excerpt": "For the members of Team B, Zoom meetings are an everyday occurrence. Like most people during the covid era, we spend much of our professional time communicating from the comfort of our living rooms. These days, using Zoom feels akin to wearing clothes; we’re almost always doing it (sometimes even at the same time).\n",
        "content": "Will it hi-fi? Will it stereo? Let’s find out.\n\nFor the members of Team B, Zoom meetings are an everyday occurrence. Like most people during the covid era, we spend much of our professional time communicating from the comfort of our living rooms. These days, using Zoom feels akin to wearing clothes; we’re almost always doing it (sometimes even at the same time).\n\nHowever, this week we took the ‘Zooming’ one step further and checked out two features we’d been neglecting: ‘high fidelity music mode’ and ‘stereo audio.’ These are two features that, as music technologists, we should have great use for and were eager to test. We were sitting on both macs running Catalina, and computer running Windows 10, when doing these tests.\n\nWe were all pretty excited for these features, as working with sound via Zoom has been a tedious process for us all. For testing we went with some good old hard panned Black Sabbath, and for clean audio testing we listened to Gaslighting Abbie by Steely Dan. Probably as clean as it gets within music. The method for this was that Henrik shared sound from his computer while we all muted our microphones. We agreed on visual signals for when he was switching modes, so that we would all know what to listen for in the beginning. But before testing could start at all we needed to get the stereo mode going in Zoom. Turns out we had to manually download version 5.3.0 from their website, and after that you, as a host, have to enter settings via the browser to enable the stereo mode switch. Why does it have to be like that? We wouldn’t know, but we definitely think this was an unnecessary step in the process. Now, let’s talk testing.\n\nHigh Fidelity\nThe first thing we found quite disappointing was of course the fact that the high fidelity mode wasn’t the John Cusack face-filter-effect we expected it to be. Pun intended, you’re welcome.\nThe second most disappointing thing was the effect it had on audio - almost none. At least with stereo mode enabled at the same time. When playing mono, Willie was able to get some degree of effect from the high fidelity mode on his budget earbuds. The rest of us were listening through studio headphones and speakers.\n\nStereo\nThe stereo thing is pretty nice though. The one thing Team B all agreed on was that applying these two new settings really forced Zoom to work a bit harder. Some glitchy and laggy audio issues (things that rarely happen in our simple four member group meetings) were experienced by all of us, raising the question of whether the enhanced settings were worth the loss of smooth Zoom usage. Some group members also experienced fairly continuous clicking and clipping sounds in their right headphones. Why this would be we have no idea, but it clearly isn’t ideal. Obviously more experimentation will follow as we continue to experiment with the audio settings.\n\nWe recorded the zoom session explained previously, unfortunately, we realized that the recording Zoom feature does not take the signal as stereo. So another take was made using the built-in recording app in Windows intended mainly for games’ screen capture.\n\nThis time the recording worked and the changes between mono and stereo were noticeable. However, the Hi-fi feature was still in doubt because of a similar perception when it is activated or not. Therefore, an excerpt of the song was taken from the recording file made in Windows and we plotted the spectrum for when the Hi-fi feature was enabled and when it was not. The following graphs depict the results:\n\n\n    Let's lo-fi\n\n\n    Let's hi-fi\n\n\nNote the differences in frequency peaks. Especially between 300 Hz and 600 Hz. Here we see two peaks in the lo-fi, whereas the hi-fi mode shows a smooth descend as we move along the frequency axis. We can assume that this may be caused by a higher number of FFT windows in the hi-fi mode, and that what we see in the lo-fi is actually the hamming windows. But we wouldn’t know for sure, as we haven’t seen the algorithm that actually performs the high fidelity mode.\n",
        "url": "/networked-music/2020/09/28/henrikhs-teamB4021ZoomHifiStereo.html"
      },
    
      {
        "title": "The Joys of Jitsi",
        "author": "\n",
        "excerpt": "Jitsi is a venerable open source chat and video-conferencing app thats been around since 2003. It’s multi-platform, and runs pretty much everywhere. We were given the task of testing the desktop apps on MacOS and Windows, and the mobile apps on Android and iOS.\n",
        "content": "Jitsi is a venerable open source chat and video-conferencing app thats been around since 2003. It’s multi-platform, and runs pretty much everywhere. We were given the task of testing the desktop apps on MacOS and Windows, and the mobile apps on Android and iOS.\n\nLet’s start with Jitsi desktop. First impressions weren’t great - you are requested to log in with an account from Google Talk (a service whose retirement was announced in 2012), SIP (eh?) or XMPP. I had a Jabber account once, although I wouldn’t even know where you’d set one up now. I guess we’ve all been sucked into the proprietary services offered by the likes of Apple and Facebook.\n\n\n\nWe wanted to see if there was a way to chat together without us having to set up Jabber accounts, so we dismissed that window and had a rummage around the app itself.\n\n“Jisti wants to access your contacts. Allow?” - Deny!\n\nAh, there’s an “Add a chat room” option hidden under the File menu (where else?), maybe this is it? “Choose account” - ah, bugger.\n\nLets check the documentation. Hm.. a little thin on the ground here. There is a “How to download and install Jisti” video from erm.. 2012.\n\nOk, maybe we’ll have a bit more joy with the mobile apps. \nOh, hang on - whats this in the FAQ?\n\n\n  Is there an iPhone/iPad version of Jitsi?\n\n  No. Due to the restrictions imposed by the platform it is highly unlikely this answer is going to change.\n\n\nBugger.\n\nThankfully the answer has changed, and not only that, but the Jitsi experience on iOS was much better.\n\nAt least initially.\n\nJitsi Meet (the mobile version of Jitsi) doesn’t require you to sign in to other services, so we could jump straight into setting up a room.\n\n“This room is insecure, anyone can join. Press the security button” Um.. which security button? After not managing to find any security buttons anywhere, we decided to throw caution to the wind and start our video chat.\n\nAnd it worked! Latency seemed fine on iOS, although there was no direct way of measuring it like there is in Zoom. On Android it didn’t run quite as smoothly, with quite a lot of audio dropouts.\n\nWe were still a little nervous a stranger might jump into our cosy video chat, so the hunt for the ‘security button’ resumed. Tucked away 2 levels down in a menu, I found an option that allowed you to set a password for the room. We’d done it!\n\nJitsi Meet has a nice ‘recents’ list, which allows you to jump straight back into any rooms from previous sessions. Unfortunately we found that it didn’t remember the password we had set, and not only that, but it didn’t warn us this time.\n\nJitsi is free and open source, and we were really hoping for it to be a viable and more ethical alternative to the Zooms of this world, but for us it was a disaster. In terms of UX, it is user-unfriendly to the extreme and even worse, it’s clumsy attempts at security are leaving people vulnerable. Until the Jitsi team sort out these issues, our recommendation is to look elsewhere.\n\nUpdate\n\nShortly after we published this post, there was a big update to the Jitsi Meet mobile apps, fixing quite a few of the issues we came across. We have also been pointed in the direction of https://meet.web.net, a web based client for Jitsi that works really well, and offers a much improved user experience. We would still recommend avoiding Jitsi Desktop, but both the web and mobile apps are beginning to look like viable alternatives for online video meetings.\n",
        "url": "/networked-music/2020/09/28/the-joys-of-jitsi-team-c.html"
      },
    
      {
        "title": "Musicking with JackTrip, JamKazam, and SoundJack - Presentations",
        "author": "\n",
        "excerpt": "The class of 2021 presented on JackTrip, SoundJack and JamKazam and their networked musicking potential. Presentations are included in this blog post as pdfs.\n",
        "content": "SoundJack\n\n\n    \n    Our presentation on SoundJack\n\n\nJackTrip\n\n\n    \n    Our presentation on JackTrip\n\n\nJamKazam\n\n\n    \n    Our presentation on JamKazam\n\n",
        "url": "/networked-music/2020/10/08/telematic-musicking-presentations.html"
      },
    
      {
        "title": "Scientific Computing Midterm",
        "author": "\n",
        "excerpt": "Team A’s reflection on the midterm scientific computing assignment. Shortly, this had a purpose of creation of a general python program that would read audio files from a specified folder based on a csv file and would output another csv file with an added values for each individual audio file: the average Root Mean Square, Zero Crossing Rate, Spectral Centroid. Moreover, the program displays and saves to a file several scatterplots.\n",
        "content": "Assignment description\n\nInput components\n\nA folder with several audio files (.wav) and a CSV file containing the descriptions of each audio file (i.e. labels) - each file is a recording of one acoustic musical instrument of different length and formal (i.e. sampling rate and number of channels).\n\nTasks\n\nWrite a python program which creates (and saves) another CSV file with extra columns for each audio file:\n\n  the average Root Mean Square (rms)\n  the average Zero Crossing Rate (zcr)\n  the average Spectral Centroid (cent)\n  a positive integer number representing each label (e.g., 0 for all instruments X, etc.)\n\n\nThe program should also display (and save to a file) scatter plots with color code entries per instrument, labeled axis and a legend of:\n\n  Root Mean Square (rms) vs Zero Crossing Rate (zcr)\n  Zero Crossing Rate (zcr) vs Spectral Centroid (cent)\n  Root Mean Square (rms) vs Spectral Centroid (cent)\n\n\nSubmissions\n\n\n  Submit once immediately after class.\n  Submit one more time the whole code after a few days.\n  Submit the final version after reviewing the reference solution with documented additional changes (if any).\n\n\nAlena’s reflection\n\nI consider this assignment a good mix of the things we were taught so far. It also offered flexibility regarding the libraries to be used and the programming styles. This is particularly important since all members of our team come from different backgrounds and approached the tasks accordingly.\n\nLeigh’s used with software de development so his code was closer to what I’d call low-level programming - he used lists and did not try to put the CSV file into a DataFrame (such as pandas). It was educational to see how the same things can be done without many external libraries. Choubey is pretty new to programming so his style was more unstructured and repetitive (such as when calling the plots), but it was good to see how he, having a “fresh perspective”, solved the tasks. I sometimes feel like the more one programs the bigger their bias regarding programming styles, libraries, functions and other preferences. Having had some experience before with these libraries, this assignment was an opportunity for me to learn how to use new functions of the aforementioned libraries such as pandas.unique and to learn to think of arrays as audio files.\n\nAfter reviewing the referenced solution I realised it’s easier to use built-in matplotlib.style for styling the appearance of the plots instead of manually setting up colors, axis styles and markers’ shapes. I wrote my code with functions, so there wasn’t much else to change to make it more efficient.\n\nLeigh’s reflection\n\nChoubey’s programming has more of an unstructured programming style, with less functions overall which results in a bit of repetition of code, for example when multiple plots have to be drawn with different data. Alena’s programming style uses functions and appears to be from an analytical programming background, with more knowledge of the pandas library and plotting functions.  I come from a software development background so I prefer classes and functions, but have not used any analytical or statistical libraries, so my code tends to rely on built-in object types.\n\nAfter reviewing the example solution and my teammates original submissions I learnt about the usefulness of pandas when working with structured data.  I changed my code to stop using python Lists and instead to use Pandas Data Frames.  Pandas Data Frames have built-in functions for working with tabulate data, which made it relatively easy for me to transfer my knowledge of working with SQL databases to working with Pandas.  I was able to simplify my code for retrieving unique values from a column, filtering or selecting specific data, and in turn resulted in a much cleaner process for plotting the individual instruments on the graphs. Originally, there was no way for matplotlib to know which dots related to which instrument, as I passed all the plot coordinates at once and provided another array of colours that associated them with their instrument. This taught me how to create a custom legend but is much less reusable than providing different sets of plot coordinates for each instrument, as I did when using Pandas, and letting matplotlib handle the display itself.\n\nChoubey’s reflection\n\nThis was a fun assignment to work upon, perfectly summing up all the things we have learned midway through the course. There were three submissions in total, and my code improved a lot after every submission. After submitting a very abstract Jupyter with tons of things happening in it in the first submission, and putting all of my weekend in it, I felt a sense of fulfilment when the code finally worked. Since I don’t have an adequate programming background, I wanted to have a working program which satisfies all the conditions first not worrying about the efficacy much, and without taking help or inspiration from my teammates or other people’s code, therefore I made a choice of not looking at the codes of my teammates until I was able to do so. Which didn’t turn out to be a comforting decision, as I had to really work hard to convert my logic or the algorithm that I had in my mind into code. After having made the program to work and seeing how Leigh and Alena approached the assignment, I understood what the code I made needed in the next iteration (the third submission), the areas of improvement rightly noted by both of my teammates - a proper structure and the redundancy fact. I needed functions upon which I effectuated in the last submission. This assignment helped to realize where I am at, things I am good at and areas I need to work upon, also completing it have made me confident and optimistic towards the enchanting world of Scientific Computing.\n\nTalking about the programming style of my teammates, both Leigh &amp; Alena have a similar one in my opinion, whereas Leigh’s approach maintains a low level programming, Alena’s style is more open. I believe I have a lot to learn and my teammates’ high level knowledge will help me a lot to become a sound programmer (pun intended).\n\nOther thoughts\n\nOverall, the content of this course has been well structured to bring newcomers up to speed while still offering challenges and new technologies for experienced software developers.\n",
        "url": "/sound-programming/2020/10/14/alena-programming-team-a.html"
      },
    
      {
        "title": "Étude de téléphone portable et popsocket",
        "author": "\n",
        "excerpt": "Click to see a cute dog making strange music. Unbelievable. I think that sums it up.\n",
        "content": "Étude de téléphone portable et popsocket\n\nFor this project I wanted to use data from my phone to control “stuff”. I wanted to try to make some sort of Monome Ark speed control. After I experimented with hooking up gyro-values and acceleration to different parameters, ending up with some different results. Ended up with a delay that I built into a reverb with a simple sequencer and microphone providing the reverb with sound. I used Max/MSP and gyrosc on my phone.\n\nWhat is it?\nSo for the speed control I used the compass data from the phone. Using a popsocket on the back, the phone is able to spin pretty nice on a table, like spinning a record. By splitting the 360 degrees of compass data into four (divided by 90) points, I found this a pretty good way to make the phone «bang» four times per round. By splitting in different values (180, 120, 90, 45 - should be in range of 360) we can achieve different calibrations for how fast we’ll have to spin the phone. I used a timer to get the ms values between each bang, then I wrote a brief JavaScript averaging the four last numbers.\n\nFor all osc-values I used a ($1, x)-[line~] which is ramping from value to value, making the value transition less steppy. Also [change] was a great object to use with the compass data to get rid of wild trigger happiness.\n\nThe sequencer part has one hardcoded melody and another melody that is making 8 random notes within a pentatonic scale. The random one also has a subdivision of time, making it jump around the steady bass melody. This is an easy way of providing something random that can also be heard as something melodic.\n\n\n    Face of instrument\n\n\n\n    Inside of instrument\n\n\nBut why?\nControlling software in this way gave different results than I expected. I’m quite conventional when it comes to tweaking, really into knobs and faders, not the whole «waving with my arms to make a synth sound»-thing. But this pushed me somewhat closer to the edge, giving me a better view over the valley of body motion control in software. I immediately found it quite interesting working with as little as two parameters (time and feedback), and I realized how I was reacting differently than I normally would, using traditional knobs and faders.\nWhen mapping the values, I took elements of music cognition into consideration. Especially metaphors and affordances. This when deciding on how the delay reacts when you point it up and down, and how filters open when pointing upwards and close on downward movement. I wanted the interaction to correspond with how we initially, and generally, think of interaction and movement in sound. My experience was that this made it easier to understand where to look for certain sounds. Using compass is a little strange though, as north will always be north in the world, not north of your computer. This will give different results in different locations with the same movements, if you don’t always face north when using the setup. Maybe I should’ve built a sonic compass into the thing, so that you’d have to find the right position to get rid of extremely loud 13kHz sine waves 👹… I think not.\n\nSo let’s hear it in action\n\nI’m delaying in a room\nThis is a performance of me using the phone as modulation while typing on the computer keyboard. The computer microphone is picking up the sound, sending it through my patch.\n\n  \n    \n    Alternate Text\n  \n  \n\n\nDog D(el)ays\nMaybe it’s not summer at all anymore, but it’s still possible to feel some heat from the sun. This is a piece made by my dog (Sanna) wearing my phone, looking for dog-treats in our yard. There are different inputs and modulations for each act. This also shows the sequencer in action. Enjoy.\n\n  \n    \n    Alternate Text\n  \n  \n\n\nThank you\nTwo slightly different approaches to this instruments. Hopefully you leave this post with some understanding of what it does. I’m happy I managed to get communication between my devices, and it’s probably something I will explore further in music making. I’m happy to take any feedback or suggestions on this if you feel like saying something about it after watching this.\n",
        "url": "/interactive-music/2020/10/14/henrikhs-etude-de-telephone-portable.html"
      },
    
      {
        "title": "Making virtual guitar playing feel more natural",
        "author": "\n",
        "excerpt": "Can using sensors, buttons and joysticks to play a virtual Guitar resemble the experience of playing a real guitar and result in a more natural performance than using a keyboard for input?\n",
        "content": "The Task\n\n\n\nWe were given the task to create “an interactive system which you will present as a short musical performance” using any software that might suite our needs.  I had recently written a song using a VST called “Strum Session” which has electric and acoustic guitar sounds, which can either be played in the same way you would play a piano, or by holding down the chord you want to play and triggering an “up strum” or “down strum” using the lower “c” and “d” midi notes on the keyboard.\n\n\n\nThe Idea\n\nMy Idea was to use two phones; one in the left had to determine the type of chord you will play, and one in the right hand for strumming.\n\nIn the left hand, the user can press buttons on the phone’s display to determine the root note of the chord, then tilt the phone at different angles to determine the type of chord (major, minor, seventh).\n\nIn the right hand, the user would be able to hold the phone as though it is a giant plectrum and move their hand up and down to strum the guitar.\n\nThe Implementation\n\nI used oscHook on both phones. In the left hand (the phone determining chords) I am sending only the pitch orientation value of the Compass as OSC data to my computer and from there I send the appropriate Midi notes for the selected chord to loopMIDI which acts as a hardware midi device in Ableton.\n\nI found that when holding the phone as you would a fretboard, the compass pitch value would range from 0 degrees when the phone is horizonal to 90 when the phone is vertical. So, I simplified this value to three ranges 0-30, 30-60, 60-90 and chose to assign major, minor, and seventh chords to these ranges respectively.\n\nIn the right hand I had a bit more trouble since there wasn’t really an appropriate sensor in the phone for this, and data is only sent every 100ms from oscHook (the experimental 50ms option crashed pretty consistently). I tried to use acceleration on the x-axis, so when there was a peak in acceleration downwards I would do a down-strum, and when there is an acceleration upwards there is an up-strum.\nThere’s a few problems with this:\n\n  Most of the acceleration begins at the beginning of a stroke, not while it is “passing over strings” so going by peak acceleration would cause a strum before you would normally hit the strings.\n  Slowing down or deceleration at the end of a stroke is actually acceleration in the opposite direction of movement, so you generally get a peak in acceleration in the opposite direction of movement at the end of the strum, causing a “strum” trigger in the other direction\n  There’s no way to avoid strumming, for example if you only want to do downstrokes or upstrokes.\n\n\nThe second iteration was to not use “peak” acceleration values to trigger a strum, but instead when the acceleration changes direction (acceleration values switch from positive to negative and vice versa). This occurs around the middle-end part of the strum when the user has stopped building up speed for their strum and has begun to slow down.  This created much more accuracy but still didn’t solve the problem of not being able to avoid strumming entirely when you moved your hand.\nHere is an example of this implementation in action:\n\n\n  \n    \n  \n  Strumming with Phone Accelerometer\n\n\nFinding the right sensor\n\nI wasn’t happy with this strumming solution so I wanted to try a bunch of different sensors I have lying around to see if they could produce a more accurate reading.\n\nFor this I used a Teensy device (similar to Arduino) which I set up to behave as MIDI hardware device over USB.  I then connected a few sensors to it.\n\nFirst up is was what is commonly known as a “line tracing” sensor, but also functions as a proximity sensor.\n\nLine Trace/Proximity Sensor\n\n\n\nThis functioned well in that it was very accurate, but the sensitivity of the device was too low. Your hand had to pass within about half a centimetre in order to trigger it so a lot of the time you either hit the device or it doesn’t get registered as a strum because your hand is too far away.\n\n\n  \n    \n  \n  Strumming with Proximity Sensor\n\n\nMagnetic Door Switch\n\n\n\nIn testing this functioned the best. The distance was okay at up to 2cm and the switch was consistantly either on or off, unlike the hall effect sensor which had a mid-state and sometimes toggled between on and off when entering or exiting its sensitivity area.\n\n\n  \n    \n  \n  Strumming with Magnetic Door Switch\n\n\nMagnetic Hall Effect Sensor\n\n\n\nThis device had the largest range but was also the least accurate because of that.  It would retrigger multiple times for the same stroke (which was eliminated in code by implementing a “cooldown” timer after each stroke) but this particular sensor also has a “half-way” point where is constently toggles between the off and on states.  This made for a cool “continuous strumming” effect after I implemented a 10ms cooldown on each stroke in my code, but it made the strumming overall less accurate.\n\nA problem with all the sensors above is that they are only capable of detecting “strum” or “no strum”, there’s no possibility of detecting the direction of the strum.  In testing I did put two Hall Effect sensors next to each other, and if the top sensor was triggered first it would be a down strum and if the bottom sensor was triggered first it was an up strum. Unfortunately, my two sensors had different levels of sensitivity so the results were not very consistent.\n\n\n  \n    \n  \n  Strumming with Magnetic Hall Effect Sensor\n\n\nJoystick\n\n\n\nFinally, I felt like the main thing that was missing from this feeling of strumming was a level of tactility. Thinking back to my days of playing guitar hero on play station, I decided to replicate this strumming action using a joystick module.  This enabled both up and down strums as well as the ability to either play only up or down strums.  Since your thumb is always in contact with the joystick all proximity issues were eliminated, and the results are very accurate in comparison to the options tested.\n\n\n  \n    \n  \n  Strumming with Joystick\n\n\nThe performance\n\nWhat do you do when you have a guitar? You play Wonderwall, of course. For “Copyright Reasons” I’ve only included 4 bars in this final video, but it gives you an idea of the final product.\n\n\n  \n    \n  \n  Strumming Performance\n\n\nFinal Thoughts\n\nOverall, I feel like this was a successful project.  Some adjustments I would make would be:\n\n  Remove the tilting of the chord hand to modify the chord and just make the buttons customisable so you can set them up for each song.\n  Currently, changing a chord immediately stops the previous chord playing (it sends an “all notes off” midi command). I would change this so that the chord is playing until the next strum is performed.\n  Replace the phone in the chord hand with buttons. It’s hard to find the right chord without looking if there’s no tactile feedback.\n  Make the “strum” note-on, note-off midi messages have a slight delay between them, as I think having no delay causes the VST play a single note sometimes instead of the full chord.\n\n\nUltimately, I would try to replace this implementation with a guitar hero controller as it has the layout of a guitar and has the chord buttons, tilt sensor and strumming bar all ready to go.\n\nCode\n\nThe Arduino code for the Teensy Strumming can be found here:\n\nhttps://github.com/leighmurray/teensy-strum-session\n\nThe python code for the OSC to Midi for chord changes and strumming can be found here:\n\nhttps://github.com/leighmurray/strum-session-osc-to-midi\n",
        "url": "/interactive-music/2020/10/15/leigh-guitar-vst-interactions.html"
      },
    
      {
        "title": "Reflections on Scientific Computing (The B Team Bares All)",
        "author": "\n",
        "excerpt": "Team B reflects on a week of coding ups and downs.\n",
        "content": "A Midterm Reflection\n\nThe B team is a professionally diverse group of individuals. Even by the standards of the SMC program, we have unusually eclectic skill sets. Often this variety is a strength, but for this assignment that may not have really been the case. Read on for four very different perspectives on a brief adventure in problem solving.\n\nA bit of background. The task: creating a new csv file containing data from an original csv file and defining specific attributes of 60 individual wav files. Some of the knowledge needed for the task is easy if you have programming experience. It’s a matter of knowing what functions to use, and what functionality already exists.\n\nThe task wasn’t too clear on that we shouldn’t be aware what’s in the csv-file. We couldn’t see the file, so we went for retrieving the header data rather than printing it to tell us what the headers are, requiring a level of human interpretation.\n\nHenrik’s reflection\nAt first I thought the assignment was pretty overwhelming and I didn’t think I was ever going to be able to finish it. But after some lines of code I began to understand how this program should work, and then it was quite easy and fun from there. I got a little tangled up in the thought of modifying the existing csv-file rather than reading the old one and making a new one, which was a way easier approach. We had a quick chat after Friday’s class and Pedro showed how we write csv-files. Had a lot of fun thinking about solutions for this program and then testing them. Especially for plotting the data. Overall my thoughts are that this was a good task and that it’s a nice way to learn how to apply the concepts from class. I find working with tasks like this is motivating.\n\nPedro’s reflection\nAs an experienced programmer, I found this assignment reasonable within the development process of the solution and the suggested time to complete it. However, I realized that it was something complex for some classmates, not only for team B members. It made think about why it could be, and in my opinion, people that have little practice, or is just starting, do not have some patterns that are common when producing algorithms and they would take more time to solve them. For instance, for this assignment I used a dictionary, which was my immediate way to solve the problem because I have used it in similar situations many times, in fact, in programming classes you learn about HashMaps and Dictionaries (depending on the programming language that is used), and a typical problem to show how it could be useful is when you have to list and count the number of words in a text. At the end it could be a matter of practicing by solving algorithmic exercises.\n\nRegarding Stefano’s solution, it has what I was looking for, which was a way to search in the data collection without the manual intervention as in my solution, I mean, I kind of ‘query’ for grouping (like SQL or LinQ in C#). I am not fluent in python but I am always surprised about how things can be very compact compared to other static languages like the one I use the most, which is C#. I think Python is very useful as a data processing tool, however, I doubt if it can manage efficient solutions for critical real-time problems, like static language such us C and C++ do, but I am aware that it depends also on the programmer’s expertise in those languages to actually take advantage of their benefits.\n\nWillie’s reflection (aka in which I gently imploded)\nAs a complete newcomer to python and almost complete newcomer to the greater world of coding, this assignment felt a bit like my Everest. Before this, I’d successfully completed small tasks, but this was of a different scale. It did not go well.\n\nMy initial attempts to create useful code made it as far as seeing the csv file data before the train began to derail. Accessing the wav files was a significant roadblock for me, as was then calculating values via the data contained within them. Unfortunately, even though I had a vague sense of how to create the scatterplots that the assignment called for, that knowledge was useless without the data to fill the plots. Having spent far more time than was wise trying to complete this task, I turned to my fellow B-teamers for assistance and was very interested to see how different each person’s code was. I see that there are many ‘right’ ways to complete tasks like this. However, I can personally attest after much experience that there are also many, many wrong ways.\n\nLooking to the future! I have a lot to learn. The more masterful coders of Team B have identified some resources for me to utilize as I try to catch up. I hope to someday be coding in style and laugh at tasks like this, but, until that day comes, I will remain the most easily confused coder in the room.\n\nThen Anders enters …\n… giving Willie a real competition on being the most confused coder in the room.\nGiven the task, and starting to code the last hour of the class, wasn’t the best way to start the weekend, I’ll admit. What I achieved in 45 minutes, was mostly a pseudo-code, just to wrap my head around what should be done. But the next 6-7 hours was mostly googling my error-messages and reading things i didn’t understand on interwebz like geeksforgeeks, stackoverflow and a ton of other places. I also went through everything we had done in previous sessions, without getting far.\nI also understood the task as not looking into the csv-file, to write a program that would do what it should do, without me knowing anything about the contents of the original file. So when I quite quickly manage to read and display the csv-file, I felt like cheating.\nLooking at others solutions feels rewarding, especially when I understand what’s going on … I even feel like it might be something I can do in the future, but I still need a lot of practice. In good coding-practice I’ll steal a wise man’s words to end with:\n\n“Looking to the future! I have a lot to learn. The more masterful coders of Team B have identified some resources for me to utilize as I try to catch up. I hope to someday be coding in style and laugh at tasks like this, but, until that day comes, I will remain the most easily confused coder in the room.”\n  (W. Mandeville, 2020)\n\n     \n  \n",
        "url": "/sound-programming/2020/10/15/henrikhs-teamb-scientific-midterm.html"
      },
    
      {
        "title": "Team C's reflections on Scientific Computing",
        "author": "\n",
        "excerpt": "Despite our diverse professional backgrounds, three of us are pretty much beginner pythonista.\nAll of our code seems pretty readable. Joni’s code is very clean, possible because she’s a designer, with good commenting. Wenbo favoured a single cell for his code. Upon the review on everyone’s submissions, we agreed that our team activity in the future has to increase.\nMeister Stephen will have to be more patient with us!\n",
        "content": "Despite our diverse professional backgrounds, three of us are pretty much beginner pythonista.\nAll of our code seems pretty readable. Joni’s code is very clean, possible because she’s a designer, with good commenting. Wenbo favoured a single cell for his code. Upon the review on everyone’s submissions, we agreed that our team activity in the future has to increase.\nMeister Stephen will have to be more patient with us!\n\nWenbo\nAs a rookie programmer’s first practice exercise, the midterm assignment gave Wenbo a great sense of frustration. When he saw the problem, Wenbo thought it was very simple, but as he started to solve the problem, he found that everything was far from the documentation he saw on the website (but he accidentally discovered the magic of stack overflow and google). After spending 4 hours, Wenbo finally accomplished this task using an inefficient method - reading all the files in the folder and reordering the CSV files. After seeing Stefano’s solution, Wenbo once again realized how small he was in front of this HUGE mountain of programming. He thought that he needed to make more use of the functions he learned to solve real problems in his later programming studies instead of reading documentation like a novel. \n\nStephen\nThis was a fun assignment to do. I am an experienced programmer, so it wasn’t a huge struggle for me. The first thing I noticed was grouping the plot data by labels - I wasn’t sure how to approach this at first, but it turns out that DataFrames have a nice groupby method, similar to Ruby, so once I had this sorted, the rest of the assignment came fairly quickly. I tend to favour short and readable code, but I am also aware that I can become so pleased with finding a concise solution to a problem that I fail to see the issues with it - I am hoping that this isn’t the case with this assignment!\n\nJoni\n\nThe very first code I wrote during the class last Friday was originally better, at least I have printed out the csv file and plot in a better way. However, the more time I spent on the assignment, I got lost. Perhaps it was also because of my lack of understanding of the python functions and methods, as well as the lack of regular practice. Instead of continuing modifying the code I already had, I ended up looking everywhere on the internet for more alternatives. I learned a lot by looking into how others are doing and the ways they explained ‘why’ they did in a certain way and not the others, and ‘why’ that didn’t work or what could be better, etc. So, during the process, I also learned about how ‘programmers or engineers’ think. In fact, it is pretty much the same concept like reading different books and write an essay to a particular topic by mixing the knowledge by heart.\n\nIn the future, I would love to have more of this type of assignment, because it challenges us to think and practice all at once, by applying all the knowledge and concepts we learned to solve a particular problem with our creative skills. The process of exploration to solve an assignment helps to enhance the learning experience. The challenge per se makes it more appealing and engaging.\n\nTechnically, I overcomplicated the code and after reading the suggested solution, I realised that I should simply follow the instructions. As the question began with reading CSV file and asked to create a new one, I should just started with this step and then the next. Therefore, upon reading the suggested solution, I went back to the very first attempt I had already from the Friday class last week. I began reading the original CSV file and classify the labels/instruments from it. The suggestion is very well thought out and structured, with the very clever twists of the methods, I began to understand how one can be creative in programming! Programming approaches the problem in a linear, systematic way, very unlike design process, but both are creative in different perpectives. Interesting.\n\nAbout the styling, from the suggested solution, I learned that I could directly import that library at the beginning. Quite ridiculous of what I did, I added the sinewaves formula in my previous codes and I wondered what was happening with my head. It was absolutely irrelevant for this task. This mistake taught me that nothing is certain and I should not program my mind before I know the actual problem. And I created 3 scatter plots just to try testing the zcr, rms and cent methods. It was an absolutely silly thing I did!\n\nWith the .label.unique() methods to sort out the labels into a different category. I did something really silly before, I even create 5 different CSV file by using another method that I couldn’t remember now. Declaring the functions make everything way more efficient, for both human and computer. Before that, I tried to create empty string added new rows to the list. It didn’t work out, it worked, but it was an unnecessary step, showing I didn’t know what I was doing.\n\nIn the plt.scatter, previously I had separately printed out the labels and tried to assigned different colour individually. However, the plot didn’t show 5 colours, but it showed only 2. And I have repeated the setting 3 times because I didn’t know that there is a plt.savefig I could use to create png files.\n\nWhile fixing the solutions, I wrote some notes in attempt to explain what I have learned after reading the suggested solution and what I understood from there. I don’t think I can come up with better solutions because of my very limited programming knowledge. However, during this process of studying the code of the tutor was trully illuminated and I had an overview of the past 5 sessions.\n\nEnding with stealing a wise man from a wise man’s quote: “Looking to the future! I have a lot to learn. The more masterful coders of Team B have identified some resources for me to utilize as I try to catch up. I hope to someday be coding in style and laugh at tasks like this, but, until that day comes, I will remain the most easily confused coder in the room.” (W. Mandeville, 2020)\n\nLindsay\n\nAs a beginner in learning python and also programming, the task which was assigned to us was from my perspective, was a difficult one, atleast in the starting.\nI didn’t have a clue in how to start or which logic to use to solve this task. For the first submission, I only had 3 lines of code and I felt it would take me more than a week to get through this.\n\nGoing through all the previous notes, I decided to seek some help from my class mates, and find out how they are approaching the problem.\n\nAt the end, I could identify the main logic and approach which I found resourceful to me and executed it sooner than I thought. \n",
        "url": "/sound-programming/2020/10/15/stephedg-teamc-scientific-midterm.html"
      },
    
      {
        "title": "The voice of a loved one",
        "author": "\n",
        "excerpt": "Can AI really know what our facial expressions mean?\n",
        "content": "Prelude\n\nIt was quite a ‘heart attack’ when a design researcher was asked to do the prototype and implementation stages in just a week with little prerequisite knowledge of programming.\nIt was rather challenging to push me to think from my toe than the way I normally do - which is starting from the WHY.\n\nHere I am sharing a basic design process of how a designer normally works:\n\n\n\nFig 1: Basic Design Process\n\n\nWe start with understanding the problem by conducting a series of qualitative research, such as in-depth interviews, in order to gain insights.\n\nHowever, since the project was given a week time, grrr, what problem can I solve? Instead of keeping this ‘heart attack’ going, I had a quick idea to focus on my ‘heartache’ …the fact that I am stuck in a place since March 2020,  where I don’t know anybody, nor the culture.\nLike many novelists, artists etc, from history got their rather best ideas from their emotions, so I thought - why not? Just express myself through this project?!\n\nAria\n\nSo, the first stage of ‘understanding’ is done and here is my problem statement for this project, which is also the 2nd and 3rd stages -Define &amp; Ideation:\n\n Travel restrictions or countries lockdowns\nsperate many people from their loved ones. Online communication tools don’t solve the problem ever since the invasion of ZOOM-meetings. According to research, having long hours of digital meetings drain our energy and its effect is as bad as having chronic fatigue… I call this - ‘Zoom fatigue’ here.\nIt is because of the restrictions of non-verbal expressions, such as using our body movements to implicitly communicate things without words.\nThe attention span of an average person holds around 8 - 20s and that we need to pause once in a while. Anyway, if our energy drained already after long hours of Zoom meetings, do we still have the energy for our loved ones from afar?\nHow can we bridge these long-distance relationships issues? \n\nBased on research, such as the experiments by the University of Wisconsin, if you are curious, feel free to look it up yourself. The experiments aimed to find out if students from group 1) having regular phone calls with their loved ones, or from group 2) texting only with their loved ones can handle stress better during their exams.\nThe result from the blood test showed that group one’s students’ oxytocin level (a.k.a. the love hormone) rise and their cortisol level (a.k.a the stress hormone) fell. Group 2 did not have any changes in their blood chemistry.\n\nThen I got inspiration from the Japanese traditional aesthetics - MA, and I thought of a solo flute piece I performed when I was in Lyon, called : Voice by Toru Takemitsu. The memories of performing this piece had given me an eureka moment - Ah, I guess I know what to do for this project:\n\n An inner voice to remind you whenever you need emotional support \n\nChanson\n\nSo, back to the Zoom thing, if we are to continue doing this for another months or years (I hope not!) without physical contacts, I definitely need to hear the voice of my loved ones once in a while even during the Zoom meeting. For example, if I am bored, a voice cheers me up to carry on the meetings….\n\nSince Zoom is having a lot of new features and updates pretty regularly, I thought- why not? Let’s have a ‘Mindfulness feature’! If mindfulness is rather a trend in 21st century, why don’t we simply put this inside our favourite Zoom? See the suggestion below (and welcome to have a gaze on my cat collection):\n\n\n\nFig 2: Zoom's new feature\n\n\nOstinato\n\nFor the final stage of prototyping, since we had few options to collect OSC data or I did not have any tools my phone is also a 3rd-hand 50-euro phone…so the only few options I could choose were FaceOSC, or the Reaper thing which I found it was rather boring that restricts possibilities for creativity (just not my cuppa coffee).\n\nThen, I started with testing the original FaceOSC that was linked to Canvas, it was a rather complicated version where training my unskilled facial expression was impossible. I connected it with Wekinator with raw 132 inputs. In parallel, I ran with Processing and sound. I recorded the voice from one of my loved ones via Zoom, so I edited the wav files in audacity just to crop them into individual files.\n\n\n\nFig 3: Audacity\n\n\n\n\nFig 4: Wekinator 1\n\n\n\n\nFig 5: Processing\n\n\nThe first attempt at working with this version of FaceOSC failed. The sound did not come as I trained my facial expression. This version of facial expression capture is a very tricky one if I am not good at separate the movements of my e.g. eyebrows and eye movements, or if my mouth is not big enough (big enough to sip a shot of Jägermeister), I am not able to train proper data in Wekinator.\n\nSo, I discovered another version of FaceOSC with 14 inputs. The setup on Wekinator was way easier than the first one, see below:\n\n\n\nFig 6: Wekinator 2\n\n\nFinale\n\nWhat else can I write on? I have to admit that I am really not a programmer although am an interaction designer, to me, if the prototype can show the developers about my ideas, then we normally work together in order for running user testing iteratively…. I might be able to explain more about the tech part when I continually make progress in the SMC course. This few-day project was a ‘heart attack’ to me at first. However, upon making my ideas into some forms, I heartfeltly felt less heartache. Now I can fathom why Samuel Beckett wrote the way he wrote…\n\n\n\nPlease click to view the video.\nImagine if I am having quirky facial expressions during Zoom meeting and I turn my ‘mindfulness mode’ on, the voice of my loved one will communicate with me based on how I act in front of the camera. It could be heart-warming and creepy at the same time.\n\nThanks if you read until here of my ‘sleep-writing’, goodnight!\n\nSources\n\nFor the 14 inputs FaceOSC\n\n[https://https://github.com/kylemcdonald/ofxFaceTracker)\n",
        "url": "/interactive-music/2020/10/16/joni-creepy-voice-in-zoom.html"
      },
    
      {
        "title": "Musings with Bela",
        "author": "\n",
        "excerpt": "A tale of accelerometers, knobs, an EEG and the attempt to tame sound with my mind. Follow along!\n",
        "content": "For my project in the Interactive Music Systems course, I decided to work with a portable EEG reader and a Bela coupled with an accelerometer and potentiometers. Little did I know how much of a challenge it is to join both software and hardware within an interactive package.\n\nInspiration\n\nComing from a undergraduate degree in cognitive science, I’ve always wanted to work with (read hack) an electroencephalogram (EEG) for some kind of artistic performance or instrument. While I have worked with a more traditional systems that require head-caps and conductive gel, I have never had the opportunity to test out some of the many different portable systems that have come out in the last decade. After reaching out to Alexander Jensenius, I was able to borrow a system from a researcher at RITMO - which I’ll cover later in the hardware section.\n\nArtistic examples\n\nIn addition to my personal interest in finding an artistic meeting point between cogsci and SMC, I was also inspired by a number of performances and musical systems that employed EEG technology.\n\nThe first of these was Alvin Lucier’s “Music for Solo Performer” (1965) which was a landmark piece not only for its use of an EEG but sonification generally. Lucier had mapped the voltage potential from his electrodes into low-frequency tones that were able to excite percussive instruments in front of him.\n\n\n    \n    Alvin Lucier's \"Music for Solo Performer\" (1965)\n\n\nThe second performance that offered insights in using EEG’s in a sonic environment was Ouzounian et al.’s Music for Sleeping &amp; Waking Minds (2011-2012). In their piece, they asked multiple participants to wear EEG sensors as they spent a night in a collective slumber. Over the course of the night, their brain waves (in passing through the various oscillatory states of sleep) were represented in sound and light.\n\n\n    \n    Participants sleeping for Music for Sleeping &amp; Waking Minds (2011-2012)\n\n\nIn addition to these, there were a number of other interesting takes on EEG sonification such as MoodMixer (Leslie and Mullen, 2011), a collaborative installation where two participants navigate a shared musical space via EEG as represented by a 2D visual space. Another implementation comes from the Multimodal Brain Orchestra (Le Grouz et al., 2010), a collection of musicians whose sheet music was generated on the spot as a product from a reading of their collective cognitive response. And recently, Chris Chafe worked on sonification of seizure data recorded from EEGs, providing an illumination of hidden neural activity.\n\nHowever, in all of these examples, it’s not obvious what the sensor data is actually being mapped to - a confusing experience from both the audience as well as someone trying to find inspiration for an IMS of their own. For a much more clear framework on how one would go about building and evaluating an IMS, I turned to the literature.\n\nAcademic support\n\nTwo articles held my interest during the time I spent conceptualizing and designing my system, the first from Birnbaum et al. In their article Towards a Dimension Space for Musical Devices, the authors lay out a visual representation for describing aspects of IMS (Birnbaum et al., 2005). They identify 7-axes that might characterize new interactive music systems and in parallel, provide some representative space of which to locate an author’s proposal for their own IMS.\n\n\n  Required Expertise\n  Musical Control\n  Feedback Modalities\n  Degrees of Freedom\n  Inter-actors\n  Distribution in Space\n  Role of Sound\n\n\nThese principles were helpful as a means of comparing my proposed instrument against others but also for a class of features to focus on as I developed it. For Musings with Bela, this is how we might visualize its capacity as an instrument:\n\n\n    \n    Musings with Bela's dimension space\n\n\nThe second paper, A Framework for the Evaluation of Digital Musical Instruments by O’Modhrain takes up a similar issue with the absence of well defined lenses through which we can consider, criticize and explain a musical instrument (O’Modhrain, 2011). O’Modhrain suggest that taking the perspective of not only a musician or designer when building an IMS, but also that of an audience member or even a manufacturer. These novel perspectives force an author to consider their instrument from angles that are not typically confronted until after the instrument has been built. In both articles, it is clear that building a musical interface is a project whose treatment must be considered with others in mind.\n\nHardware\n\nThe device actually made use of the breadboard it was wired to as a frame to hold and rotate the device. The Bela was placed in between the accelerometer and two knobs, allowing for it to easily sit in one’s hands. The idea was to keep the design uncomplicated as the EEG might require the performer to have their eyes closed.\n\n\n    \n        \n        Bela and friends\n    \n    \n        \n        The Muse\n    \n\n\nA Muse (2016) portable eeg headband, graciously borrowed from RITMO, was another major hardware device incorporated within my IMS. I had read through my preliminary research that this device might be easily hackable. Imagine my sadness after finding out all developer resources for the device were discontinued (ARJ you liar!). Unbroken, I pushed forward and ended up modifying a completely unknown Python package to finally interface with the device.\n\nNevertheless, the device’s specs are quite impressive with 4 electrodes recording at 256Hz and an all-day battery life (no joke). Unfortunately, the fact that the device streamed through Bluetooth meant that my laptop would necessarily be involved (I wouldn’t dare attempt it on the Bela (Okay, maybe if I had another week!)).\n\nSoftware\n\nInterpolating between tables with an accelerometer\n\nAt the core of my system was a method for interpolating between short audio grains. Audio files were read into an array of 1024 samples and these arrays were then interpolated using the external iemmatrix. More typical methods of reading through arrays would be to step through each index and read the sample, apply whatever operation you wanted and then store it. In my case, however, I wanted to tie the accelerometer to degree each sound file is interpolated into one another (via arrays) which makes it challenge to read through these arrays sequentially when the sample rate of change needs to be very fast. iemmatrix instead, allows for operations to take place on the array as a whole (like numpy vectorization) meaning this is a much more efficient method of morphing between these arrays.\n\n\n    \n    A shot of the main matrix operation sub-patch\n\n\nUpon reflection, another alternative would be to get two readings, do the matrix operations, and slide between them with a [line] object. As I was looking into this, this is an external (list-abs) that allows for linear interpolation between lists. However, this might be a slightly more costly object to use - perhaps a combination of both techniques would have worked best.\n\nThe two physical knobs control an oscillator to read the resultant table (a morphed sound grain) into the DAC. These knobs are also read at audio-rate but the operations they control are far less complicated. In Musings with Bela, these knobs serve as tuners for the synth that can explored (sonically) by rotating the device.\n\nReading arrays and remaining calm\n\nFinally, the last piece to my glorious IMS puzzle, the Muse! As I mentioned, this device was tricky, thorny, and a general struggle to work with, especially considering I had to set the Bela up as a WiFi hot-spot to pass the samples from the Muse, to my PC and then off to the Bela.\n\nAnother major piece of working with the Muse was actually testing to see the behavior of the electrode readings, if they are consistent, their fluctuations, and whether or not I would be able to reliably reduce the noise and relative intensity. I made a subpatch to test for this reason, allowing me to record and playback samples from the Muse even without its connection.\n\n\n    \n    Data from the Muse\n\n\nMusings with Bela takes this EEG stream and modulate the amplitude of the read table so that, in theory, a wandering, active mind would lead to a disrupted synth. The configuration was technical to say the least and, in retrospect, something I wish I had tackled earlier in the building process so I could use the EEG signals in a more complex mapping.\n\nReflections\n\nBuilding an IMS is a whirlwind of an experience and one that is especially difficult to achieve in two weeks. Working with hardware and software turned out to double the time I expected any individual task would take. However, I feel like I had successfully built an interesting system that touched on my history in cognitive science and applied it within this an acousmatic environment.\n\nHere is my short, final performance for SMC4045.\n\n\n    \n    Musings with Bela\n\n\nAnd my final presentation can be found below as well!\n\n\n\nReferences\n\nBirnbaum, David, et al. Towards a Dimension Space for Musical Devices. 2005.\n\nFan, Yuan-Yi, and F. Myles Sciotto. ‘BioSync: An Informed Participatory Interface for Audience Dynamics and Audiovisual Content Co-Creation Using Mobile PPG and EEG.’ NIME, 2013, pp. 248–251.\n\nHamano, Takayuki, et al. ‘Generating an Integrated Musical Expression with a Brain-Computer Interface.’ NIME, 2013, pp. 49–54.\n\nLe Groux, Sylvain, et al. ‘Disembodied and Collaborative Musical Interaction in the Multimodal Brain Orchestra.’ NIME, 2010, pp. 309–314.\n\nLeslie, Grace, and Tim R. Mullen. ‘MoodMixer: EEG-Based Collaborative Sonification.’ NIME, Citeseer, 2011, pp. 296–299.\n\nO’Modhrain, Sile. ‘A Framework for the Evaluation of Digital Musical Instruments’. Computer Music Journal, vol. 35, Mar. 2011, pp. 28–42. ResearchGate, doi:10.1162/COMJ_a_00038.\n\nOuzounian, Gascia, et al. ‘To Be inside Someone Else’s Dream: On Music for Sleeping &amp; Waking Minds’. New Interfaces for Musical Expression (NIME 2012), 2012, pp. 1–6.\n\nParvizi, Josef, et al. ‘Detecting Silent Seizures by Their Sound’. Epilepsia, vol. 59, no. 4, 2018, pp. 877–84. Wiley Online Library, doi:10.1111/epi.14043.\n\nStraebel, Volker, and Wilm Thoben. ‘Alvin Lucier’s Music for Solo Performer: Experimental Music beyond Sonification’. Organised Sound, vol. 19, no. 1, Cambridge University Press, Apr. 2014, pp. 17–29. Cambridge University Press, doi:10.1017/S135577181300037X.\n\nWu, Dan, et al. ‘Scale-Free Brain Quartet: Artistic Filtering of Multi-Channel Brainwave Music’. PloS One, vol. 8, no. 5, 2013, p. e64046. PubMed, doi:10.1371/journal.pone.0064046.\n",
        "url": "/interactive-music/2020/10/16/musings-bela.html"
      },
    
      {
        "title": "The Dolphin Drum",
        "author": "\n",
        "excerpt": "My granular synthesis percussive instrument from the Interactive Music Systems course.\n",
        "content": "The Dolphin Drum.\n\nFor my project I have turned a vase into a percussive tactile instrument based on granular synthesis.\nIt is a hollow vase with different textures making it ideal for a range of use as a sound generator.\nI placed a speaker inside the vase, so all the output acoustically feeds back into the system picking up qualities from the vase. A piezo is mounted inside the vase which is used as the sound input for the system. \nAccompanying the vase as a part of the instrument is the bela breadboard which has a 9DF sensor and a force resistor. By turning the breadboard on three axis I can simultaneously play the drum and interact with multiple parameters. From the 9DF sensor I obtain the fusion pose, which I mapped windowsize, pitch, and delay. The force sensor is mapped to control feedback. With a large amount of delay and feedback in the system, the user continuously reacts to the system output to explore a sonic landscape.\n\n\n\nFig 1: Circuit and sensors used\n\n\nI identify this instrument as percussive, but not rhythmical.\nIt is pitch based, but is not tonal.\nTo me it’s a great dynamic toolbox for creating interesting sounds.\nInstrument has a large dynamic range. Heavily reliant on the initial acoustic sound input. Strike vs slide, material as a drum stick. Material on the drum itself and the shape allows various acoustical properties from the instruments shape. The rotation of your hand is quite Low effort, but the results are high expression. It heavily relies on buildups, risers, noise, and percussive sounds. Controlling the parameters becomes a balancing act of shaping the sound, and has some inherent intuitiveness to it.\nWhile the instrument has a range of dynamics, these dynamics are easy to control and as such its not an instrument bound by skills or practice necessarily. It is more of an exploratory journey that anyone playing the instrument could attain.\n\n\n\nFig 1: Circuit and sensors used\n\n\n“a perspective of interactive music systems that focuses on the shared creative aspect of the process in which the computer influences the performer as much as the performer influences the computer …An interactive system has the potential for variation and unpredictability in its response, and depending on the context may well be considered more in terms of a composition or structured improvisation rather than an instrument.” (Drummond 2009)\nI think this resonates quite well with the instrument. There is a large element of continuous feedback, between performer and system due to the amount of delay as well as actual system feedback. I found myself continuously adjusting to the system response to aim for a coherent soundscape. The name of dolphin drum somewhat emphasizes this. While the dolphin uses echolocation, this system equally provides feedback in how to navigate, also it sounds a bit similar at times.\n\n\n    \n    Demo of instrument\n\n\nReferences\nDrummond J “, Understanding Interactive Systems”\nOrganised Sound, Volume 14, Issue 2\nAugust 2009 , pp. 124-133\n",
        "url": "/interactive-music/2020/10/16/simonrs-the-dolphin-drum.html"
      },
    
      {
        "title": "Real-time audio processing with oscHook and Reaper",
        "author": "\n",
        "excerpt": "Fun and not too complicated interactive audio processing.Using oscHook to transmit sensordata from an Android phone to OSC Router and then to Reaper to control the values of certain effects’ parameters.\n",
        "content": "Introduction (task + idea)\n\nFor our Physical Computing project we had to use sensor data to control certain parameters of audio processing using already-built software, in order to get a basic understanding of data sources and the linkup to musical parameters.\n\nHaving had no previous experience in this field, and (very) limited knowledge on how to use a DAW, my idea was to transmit data from my phone’s sensors, using oscHook, through the OSC Router to Reaper and controll some of the effects applied on different tracks of a song in real-time. A simple enough idea… if you know what you’re doing, which I was not.\n\nFor this assignment I used the following softwares:\n\n  oscHook (for Android)\n  OSCRouter\n  Reaper\n\n\nSetup (technical)\n\nAs a novice regardig both parts of this task - on one hand seding sensordata to a DAW in real-time to control parameters’ values and on the other hand tweaking parameters and effects in a DAW for audio processing purposes - I had to approach things systematically.\n\nFirst step was to send data from my phone to the OSC Router and then in Reaper. After installing the necessary software and with some help, I managed to do it successfully, but I found it important to document the process - if nothing else, for future reference since it’s likely that I will forget some steps and get frustrated if I ever try to do it again from scratch. Also because it was a substantial step, taking me almost as much time as the other part of the task. So, in this section I will write a few important steps that shouldn’t be skipped and some troubleshooting notes.\n\noscHook\n\n\n  \n    Go to IP/port setup and write the IP address (e.g. 192.168.1.51) and the port you want to use (e.g. 7400). An easy way to find this is to open a command window (on Windows) and type “ipconfig” and it will show there.\n  \n  \n    Go to OSC address setup to select which sensor data you want to transmit (e.g. the Compass and Light data) and see the related paths/parameters for each sensor (e.g. /orientation/azimuth).\n  \n\n\n\n\nOSCRouter and Reaper\n\nThe OSC Router interface shows the incoming data on the left (info about the oscHook app) and the outgoing data (info about Reaper) on the right.\n\n\n\nIncoming IP\n\n\n  Defining the Label and the Incoming IP is optional.\n  The port has to be the same as the one used in the oscHook app (e.g. 7400).\n  The path depends on the sensor and the parameter data you’ll be sending from the  oscHook and can be found under OSC address setup (e.g. /orientation/azimuth)\n  \n    The Min and Max values refer to the scalling of the sensors’ parameters (e.g. for Compass, the azimuth parameter is scalled between -180 and 180).\n  \n  Add another line for each different sesorsdata (e.g. if you want to use all parameters of the Compass you need to fill them in separately).\n\n\n\n\nReaper settings\n\n\n  Open Reaper, go to Preferences and then to Control/OSC/web.\n  Add a Control Surface Mode = OSC (Open Source Control).\n  Give a name to the device (e.g. “Test”) and set the mode to Local Port.\n  Set a port for the local listen port (e.g. 8000) and write your Local IP (e.g. 193.157.251.241).\n\n\n\n\nOutgoing IP\n\n\n  Use the 127.0.0.1 IP address for the Outgoing IP (that’s an “internal IP”, because you’re sending the data between two apps/softwares on the same computer).\n  Write the port you chose in Reaper in the Port (e.g. 8000).\n  In the folder you saved Reaper (probably in the C: partition) find a document with setup information for defining paths for editing audio effects of certain tracks. The only one I used is “n/track/@/fx/@/fxparam/@/value”, where you will need to remove the “n” at the beginning and for each “@” write the number of the track, the effect and the parameter respectively. See the photo with the OSC Router Incoming IP example from earlier, the Outgoing IP details are also visible.\n  For the Min and Max write the scaling of the effects’ parameter (e.g. between 0-1).\n\n\nTroubleshooting and other notes\n\n\n  \n    For a more reliable data trasmission try to change the OSC timing (e.g. re-sending data every 100 ms) in the oscHook app.\n  \n  \n    To minimalize the delay between the movement of the phone (so the sensor data) and the change in effect parameters (in Reaper), only send the data for the sensor that you need. This can be chose in OSC address setup in the oscHook app (e.g. only the Compas data).\n  \n  \n    Remember to connect both devices (your phone where you have oscHook and the computer where you have the OSCRouter) to the same wi-fi.\n  \n  \n    If you are changing the wi-fi you’re connected to, you most likely have to restart the OSCRouter, to change the network permissions.\n  \n\n\nDAW audio effects, parameters and scaling\n\nAs I mentioned before, I have no previous experience with audio processing, so I couldn’t create a song from scratch. I searched for one on Mixing Secrets and downloaded Sun Drenched by Mike Skalandunas.\n\nIn terms of effects, I downloaded the Sound Hack Delay Trio and used the bubbler, the delay and pitch delay. I also used Reaper’s built-in Cockos ReaPitch. I used certain sensors and parameters transmitted by oscHook to control parameters of these effects. See below more details of each effect, how it was controlled and the scaling.\n\nAudio tracks with their effects\n\nChecking each track individually, and knowing the effects I wanted to use, I decided which effect would be paired with what track. The scaling of each parameter was decided experimentally - setting up the (scaling) boundaries for each parameter allowed me to controll more parameters at the same time. See the “sheet” presented further down for more details on what effects were used at what times.\n\nTrack 1: French Horn 2\n\n\n  Effect: Pitch Delay (Sound Hack) — Pitch Factor\n  Sensor: Rotation Vector — r4\n\n\nTrack 4: Malaysian Djembe\n\n\n  Effect: Pitch Delay (Sound Hack) — Octave\n  Sensor: Rotation Vector — r4\n\n\nTrack 5: Percussion 1\n\n\n  Effect: ReaPitch (Cockos) — Volume\n  Sensor: Rotation Vector — r1\n\n\nTrack 11: Psaltery\n\n\n  Effect: Delay (Sound Hack) — Resonance\n  Sensor: Light\n\n\nTrack 12: Pad 1\n\n\n  Effect: Delay (Sound Hack) — Time\n  Sensor: Compass — azimuth\n\n\nTrack 13: Pad 2\n\n\n  Effect: Bubbler (Sound Hack) — Grain Size\n  Sensor: Compass — roll\n\n\nTrack 15: Violas\n\n\n  Effect: ReaPitch (Cockos) — Volume\n  Sensor: Rotation Vector — r1\n\n\nScaling\n\nFor a smoother control of the parameters’ values, I changed the scaling manually for each effect. Below it’s a screenshot of my final OSCRouter settings.\n\n\n\nPerformance demo + (time axis sheet)\n\nImportant note! After scaling the parameters, try to keep the exact same light conditions before recording or working on it again. You might notice that the scalling is totally off if you scale the parameters in the middle of the day, near a window, and then record a demo at midnight with only a lamp for light (as it was my case).\n\nHere is the video of the final performance. On the front camera view you can see the way I’m moving the phone to control the parameters’ values. In Reaper you can notice the effects’ windows and how the bars are moving based on the phone’s movements.\n\n\n  \n    \n  \n  Interactive OSC-Reaper Performance\n\n\nHere is the “sheet” I used to know when to use what effect, based on the time axis of the song. In order to minimize the delay between transmitting the data to the OSC Router and then to Reaper to control the effects’ parameters, I sent the Compass data and the Rotation Vector only when needed it - aproximatively halfway through the song I went to the OSC address setup, unchecked Compass and check Rotation Vector. The Light sensor was always on, since it was not taking that much bandwidth.\n\n\n\nUsability\n\nOriginally, I thought of a way to make classical music performances more interactive and interesting (especially for the youth). In the example described in this blog post I didn’t use a classical song, but the concepts are the same.\n\nThe way I imagined it, each audience member would get the oscHook app and get control over an instrument. Then, during the performance, there would be specific times when the can send data to the OSC Router on the (let’s call it) conductor’s compuetr who has everything set up from before: what sensor controls what effect and parameter.\n\nOf course, there would be hard to have control over when the audience members are using their phones to influence the performance. However, this can be solved if the effects controlled by the sensors would be time-bounded - for example the Rotation Vector would have control of the bubbler effect on track 5 only between second 1:21 - 1:35. In my view in would make for a highly interactive, messy, and maybe sloppy but amazing performance.\n\nLimitations and final thoughts\n\nThe biggest limitation of this project was probably my own lack of knowledge regarding audio processing. Have I known more about audio effects and how to use them, the tasks would have been easier and the demo nicer. While playing with it I had many ideas of how I wanted to make it sound like, but I didn’t know how to do it. That’s why the order of the steps I took might seem a bit  strange: I found some effects I liked and then looked for a track they would fit on, intead of thinking “ah, this track needs a bit of this and a bit of that to sound better”.\n\nOverall, this project was a gread opportunity for me to dip my toes in the audio processing world as well as exploring how to use accessible technologies to apply real-time interactive systems for music performance.\n",
        "url": "/interactive-music/2020/10/16/alena-oscHook-oscRouter.html"
      },
    
      {
        "title": "Improvised electronica with TouchOSC",
        "author": "\n",
        "excerpt": "In this project, I wanted to explore the options available when performing electronic music live with no pre-recorded / pre-sequenced material.\n",
        "content": "In this project, I wanted to explore the options available when performing electronic music live with no pre-recorded / pre-sequenced material.\n\nIn a band or orchestra, you have multiple musicians, each focusing on a particular instrument. For electronic music makers, we have an entire suite of sound generators (instruments) and sequencers (the performers) in our laptop, and the question becomes - how can we translate an electronic composition on a laptop into something that can be performed in a live situation? The obvious answer would be to assemble a band, with each person controlling a single part on their laptop or with a synth, in order to keep the one-to-one ratio of performer and instrument. Another option is to look upon the laptop as an orchestra of musicians, with each sequencer / synth combination being a part controlled by an individual ‘performer’. In this way, you become the conductor, directing the musicians (sequencers) at a higher level. Such a system, designed correctly, could allow for a great deal of compositional and improvisational freedom in a performance context, and my project was a first attempt at looking into some of the options that would be available to such a performer.\n\nAs I wanted individual control over all parts at the same time, I limited the number of parts to four - two melodic parts, a harmony part and a rhythm section. I put together a system using Reason Players (Dual Arpeggio, Scales &amp; Chords, Beat Map and Note Echo) which I could feed with single notes, and let them generate the melodies, harmonies and beats.\n\n\n    \n    I used Reason Rack to turn singles notes into melodies\n\n\nIntroducing TouchOSC\n\nAfter an initial test using a Playstation 4 DualShock controller, I decided to build a custom TouchOSC interface. TouchOSC Editor allows you to put together a control interface consisting of wide variety of sliders, buttons, rotary knobs and various other controls. Each control can be configured to send an OSC message, MIDI data including note on/off, or a keystroke.\n\nMy custom layout included buttons 1 - 7 for triggering the notes in a D mixolydian mode (hard coded in this example), plus part specific controls for the melody, harmony and rhythm sections.\n\n\n\nThe two melody parts were handled by Native Instruments Monark and U-he Repro-1 soft synths. The note received from TouchOSC was turned into a 4 note chord and sent through two arpeggios set at two different rates and lengths in order to generate an interesting rhythm.\n\nThe first group of sliders controlled these melody parts - one I assigned to the Note Echo (MIDI echo) device, allowing me to introduce additional notes which would vary in pitch (a subsequent player mapped these notes back to the D7 scale). The other slider I assigned to the Monarks third oscillator, allowing me to shift the octave up and down, in essence introducing an additional note/voice to the melody.\n\nThe second group of controls were for the harmony part. This was a synth pad played by U-he Repro-5 soft synth, and the note received from TouchOSC was mapped to a chord using the Chords &amp; Scales player. I used the sliders to adjust sound parameters (cutoff frequency and the introduction of a tremolo effect) rather than notes for this part.\n\nThe final group of controls controlled the bass and drums, with a slider for a beat-repeat effect, and a button for muting / unmuting the part.\nI used Ableton Live as the host for the VSTs and the Reason rack plugins. TouchOSC on the phone connected via the TouchOSC Bridge app to the Max for Live Connection Kit device running in Ableton, which then allowed me to map the TouchOSC controls to Ableton.\n\nIn practice\n\n\n    \n    Performing with TouchOSC\n\n\nI feel there is a lot of potential in a setup like this. Once you have an idea of how you would like things to work, creating a TouchOSC layout and mapping it to elements in an Ableton session is quick and straight forward. I could easily imagine a whole live set being performed using this method. TouchOSC lets you store multiple pages of controls within a layout, and so makes it easy to switch layouts depending on the needs of the song. I used a mobile phone for this particular project, but I suspect the screen real estate of a tablet would feel more comfortable and offer more space to organise the layout.\n\nThe future\nThe basic idea presented here could be extended in a number of ways:\n\n  sending notes individually to each part\n  allowing the choice of root note and scale/mode, for on-the-fly key changes\n  being able to tap in rhythms for the melody parts to follow\n  the ability to change the flavour of the chords - adding 7ths, 9ths, suspended 4ths, inverting chords, etc.\n  use the Beat Map player to generate and manipulate beats on the fly\n  adding more sound sculpting parameters for each part\n\n\nI feel there is a danger of getting carried away when adding more controls however - I would be worried it could increase the distance between me and the performance, as the number of controls I can manipulate at one time is limited, and so the temptation could be to start to automate more and play less. For me, the key would be selecting a small but effective number of parameters to control, and for the focus of those parameters to be on manipulating the melodic, harmonic and rhythmic elements of the song, with a few well chosen sound sculpting controls thrown in.\n",
        "url": "/interactive-music/2020/10/16/stephedg-phys-comp-project.html"
      },
    
      {
        "title": "Chance Operations, Rudimentary Pure Data (PD), and a Bunch of Spinning in Circles",
        "author": "\n",
        "excerpt": "Sometimes you want to compose and get your workout. Experience a chance composition that may leave the performer sweating.\n",
        "content": "This is Sweaty Music\n\nIn recent weeks the SMC 2020 intake has been exploring physical computing ideas and tools. As a concluding activity, each one of us was instructed to develop a composition or tool utilizing some of the materials about which we’d learned.  While in more normal times that might mean hands-on manipulation of specialized equipment, the Covid era forced some real adjustments. Individual projects instead became an exercise in determining what could be done with a phone, ipad, or whatever else each student might have in his or her home. Necessity is the mother of invention, of course.\n\nFor me, it was clear that this assignment was a chance to compose something quirky and outside of my traditional comfort zone in acoustic art music. While electronics have not been explicitly my musical enemy historically, neither have they been an ally.\n\nThe Composition:\n\nI decided to produce a five-minute chance composition structured around pitch shifting in three musical elements (or voices) created via manipulation of non-musical data. Less technically, I wanted to build the composition solely around materials and gestures coming from my own physical motion (because I’ll take any excuse I can get to run in circles). The data I used came from a Garmin running watch and consisted of a .fit file and a .csv file generated on a run in Bymarka and subsequently stretched and altered for aural aesthetic purposes. The pitch variations in these voices were then controlled by different types of Gyrosc motion data generated by the movement of my phone. Since the phone was worn in a tight vest pocket on my body, the pitch-controlling motions were essentially the motions of my  torso. One voice of the texture was functionally static, with pitch level assigned by GPS latitudinal data from an ipad, while the other two ‘melodic’ voices were modified by compass data and gyroscopic data.The compass voice was additionally modified by inputted heart rate data drawn throughout the performance from the running watch responsible for the original files. If perceived as two voices engaged in counterpoint over a drone accompaniment, the piece makes a weird sort of sense as stochastic polyphony. At least that’s what I told myself…\n\n\n    The \"Score\" and Pure Data Process\n\n\nFor the selection of movements (essentially the score in this case) I chose five bodily movements and three numbers of iterations that would likely yield compelling sounds and then wrote a simple program for random selection of these variables. A more PD-capable human would likely have been able to plant this code in the Pure Data ‘score’, but for my purposes having it in Python was acceptable.\n\n\n    Random Motions Assigned\n\n\nTo perform the piece (see the ‘score’ image above for further context), the performer (me!) starts the audio files, enters his or her heart rate in the relevant PD object and runs the python program to randomly determine the first minute of motions. This sequence of heart rate, python program, and enthusiastic movements is then repeated four additional times or until the audio completes (human motions may lead to inexact 1-minute intervals).\n\nThe Performance:\nSound ridiculous? Yes, it does! However, as an exercise in process/chance-driven music, I actually felt that the composition could succeed if performed in the proper way (maybe not in a dimly lit living room). Enough elements are performer-specific (location, relative fitness, body shape and motion) that there would certainly be a level of audible, varied personality in further performances by different individuals.\n\nEnjoy this footage of a 1-minute subset of this piece. The sounds are fairly compelling, and how often do you get to watch a “composer” run in circles in the name of art?\n\n\n  \n    \n    Alternate Text\n  \n  In which a composer/performer flails about. Willie performs an excerpt from Sweaty Music.\n\n",
        "url": "/interactive-music/2020/10/16/sweaty-music.html"
      },
    
      {
        "title": "cOSmoChaos",
        "author": "\n",
        "excerpt": "Cosmos and chaos are opposites—known/unknown, habitated/unhabitated—and man has through all times been seeking to create cosmos out of chaos. But what has this to do with GyrOSC controlling my hardware … well, everything.\n",
        "content": "I love hardware when making sounds. I’ve had several goes on softsynths, MIDI-stuff and also hardware with menus meant for scuba-diving. But I always get back to one-knob-per-function style hardware, at least when performing.\n\nI never got comfortable with using MIDI data, and just assigning some sound to the data, although I appreciate the flexibility and control it offers. But the way I’ve used MIDI and softsynths has always felt unnatural and to clean to me, and my experience is then projected to the whole concept of softsynths and MIDI.\n\nBut recently I have started connecting hardware to each other again via MIDI, just to use the control changes of it, to say have «more hands» in a performance. The hardware is producing the sound, but knobs are operated via MIDI.\n\nAnd in this project knobs and sliders are operated via osc-data from GyrOSC (iPhone), via OSCulator where the osc-data is translated to MIDI Continous Controller (CC) messages.\n\nSo what I wanted to achieve was to make a setup that can be a little chaotic, a little uncontrollable, and try to have some kind of control over it using different features in GyrOSC only. My setup for this was as follows:\n\n\n   \n\n\nAs you might notice, this is not the normal way to connect audio, as there are feedback loops created between the TR-8 and the Moog (marked with green and red pointers). So the sound coming out can potentially be a little chaotic and for some ears also unpleasant, but processing any audio through the external input on the Moog Voyager is always nice in my ears, and was also one of the more chaotic elements to try to control and make cosmos out of.\n\nGyrOSC to OSCulator\nCommunication between GyrOSC and OSCulator is very easy to set up and get going. Just make sure to put in your computer’s IP fin GyrOSC and make sure the port on both GyrOSC and OSCulator matches. Preset on OSCulator was port 8000, so I used that. When you activate different features in GyrOSC, they immediately show up in OSCulator, where you the can scale the MIDI-data you’re sending out, and of course choose MIDI-channel and what MIDI-data to send. You can also easily invert the datas, but what you can’t do, is to make your own labels for it. This can make everything a little chaotic to start with, because you may not immediately understand which sensors are at play all the time. So that would be an improvement to implement in OSCulator, I’d say. And maybe I should actually contribute to the development by buying it, so I don’t need to have it stop every 20 minutes to tell me to do so …\n\nNext step is to start setting up GyrOSC and OSCulator to control different parameters on both the TR-8 and the Voyager, and try to make some sort of action-sound control, or sound gesture resemblance, in the sense that there is analogy between the sounds and the movements, even though I’m working with electronic instruments here.\n\nPerformance\nI wanted to emphasize the instrument I was facing, i.e. facing the Voyager, should favor the sound from it and get the drum-machine more in the background, and vice versa. This is—at least in theory—obtainable by simply rising/lowering the volume of the respective hardware, or giving different effects to them. I was also trying to filter the Voyager heavily when facing down, as well as using something towards infinite feedback delay on the TR-8 at the same time, with less kick and snare coming out. And so on.\n\nWell, after spending quite a lot of time tweaking parameters in OSCulator, to provide me with the control I wanted, I had to realize that there will always be many surprises. The calibration in GyrOSC/iPhone is not very stable, even though you can only press one button in the app to re-calibrate. So at last I just had to a little performance.\n\nNote: I started on a kind of composition sending MIDI-notes from Logic to the Moog, but it really didn’t work, since the three oscillators started to tune with my movements, due to some OSC sent from GyrOSC. So I went with a drone only, with some fiddling on top from the guitar … please bare with me.)\n\n\n   \n   Cosmos … or still chaos?\n\n\nConclusion\nI think it worked quite well as a performance tool, and it was fun using it. But I experienced that what worked one day, didn’t work quite as well the next day, and tweaking it wouldn’t help for the next time I turned it on. But anyway, with the scaling of the outgoing data in OSCulator it still worked alright, and I really enjoyed playing with it. Unless I grow more arms in the near future, I can see myself actually implementing something like this in a live performance setting.\n\nFinal note: I could also see this as a fun addition to add to some guitar-FX pedals that take MIDI-cc messages, but it might also take away too much of the shoegazing …\n",
        "url": "/interactive-music/2020/10/16/chaotic-cosmos.html"
      },
    
      {
        "title": "A Live Mixer made from mobile devices",
        "author": "\n",
        "excerpt": "How does it look to control audio effects in real-time using gestures?\n",
        "content": "The expectation\n\nFor this project, my dream of using my phone as a sensor to perform electro-acoustic music was ended by the frustrating stability of the latest OSX. So I tried to build a system that could change the audio effects with gestures for real-time interaction. I think it would make the EDM live concert even more frenetic if I could incorporate more physical performance into the shows.\n\nThe Demo\n\n\n    \n    Demo with Chi Dao\n\n\nThe construction of a simple interactive system\n\nIn general, I use the OSCRouter and TouchOSC Bridge on the computer to implant the Reaper as a central hub for transmitting OSC information from the mobile device. On the mobile side, on the one hand, I use my Android phone’s oscHook to transmit light sensor and gyroscope data as part of signals, controlling the bypass of some plug-ins and the amount of reverberation of the track; on the other hand, I use my iPad’s TouchOSC to control parameters that need to be precisely controlled, such as Pan, Pitch, EQ, etc. In the Demo, you can see that my left hand primarily controls the TouchOSC on the iPad, and my right hand holds the phone’s light sensor and gyroscope.\n\n\n\nMore specifically, I connected oscHook to the desktop and linked the OSC data to Wekinator, a user-friendly machine learning application for classifying data; the reason for this is that the range of data emitted by the sensor is very large; also, delays usually higher than 100ms which make real-time control difficult to achieve. But in the Wekinator, the software processed a wide range of data in different lighting conditions classifying the data as 0 and 1, corresponding to plug-ins on and off. At the beginning of the music, my hand covered the sensor, and this bypassed the EQ and compressor, making the drum sounds soft; and as the development of the music, I released my hand, and the EQ and compression were applied to the track, thus making the music more exciting; Besides, the change in the phone’s gyroscope data corresponds to the amount of track sent to the reverb track, and when I flipped the phone over, the amount of reverb increased with the phone’s rotation.\n\n\n\nI originally wanted to control more of the effect through sensors. Sadly, I found out through practice that the application latency made it challenging to guarantee the result, so I abandoned that idea in favor of controlling more of the tracks in real-time through TouchOSC, a sophisticated OSC control app with multiple presets. It allows user to control the parameters of the PAN and some of the plug-ins precisely with the button. The sound image and pitch shifts in the Demo are all controlled by TouchOSC.\n\n\n\nThe problems\n\nAs an audio engineer accustomed to using Pro Tools and Logic Pro X, neither of the two well-known DAWs could support different OSC applications well. Obviously, Reaper and Abelton Live are better answers for this project. Meanwhile, I found that when passing OSC data using different platform applications, it often failed to link (e.g., Android to Mac OSX), and my attempts to use FaceOSC to recognize facial movements failed due to my OSX system version. Moreover, when using the light sensor, data that is used during the day does not usually work well in the evening. After spent a lot of time testing different application. I realized the sensors and applications on consumer electronics don’t work well for this kind of professional needs, so perhaps using more common tools like Wii controller, digital pad, and custom sensors would be a better choice for this project.\n\nThe reflection\n\nHonestly, this project didn’t work out the way I wanted it to for some reason, and when I using gestures to modify sound effects, results was never quite what I was looking for. From my perspective, lower latency and better ways to handle OSC data is what I need to improve in the future. As a fan of electronic music, in my imagination, I’m very looking forward to the possibility of collaborating with a visual designer to design a set of visually aesthetically movements to match the various parameters of music mixing, combining electronic music and dance, which will make electronic music live concert even more exciting!\n\nAll in all, the project was quite an interesting experience for me this week, and I hope to have more time to complete a more interesting project in the future!\n",
        "url": "/interactive-music/2020/10/16/wenbo-live-mixer.html"
      },
    
      {
        "title": "The Psychedelic Journey of an Unexpected Spaceship",
        "author": "\n",
        "excerpt": "An electronic music performance in an audio-visual digital environment. Let’s go through the galaxy in a crazy spaceship and have an experience full of color and funny turbulence.\n",
        "content": "The Psychedelic Journey of an Unexpected Spaceship\n\nFirstly, I want to encourage you to watch the performance related with this project before diving into the details of what is actually happening behind this audio-visual experience. You will find a video below.\n\n\n   \n   The Performance\n\n\nThe Concept\n\nThis project is the result of the task required for the Physical Computing module at SMC4000 course: “Shape an interactive system and present it as a short musical performance”. Considering this description, I though firstly in a technological solution to support such application before the actual performance.\n\nI have experience in game development and multimedia systems by using the Unity game engine. A widely used platform for interactive systems, mainly intended for game development but not limited to it. So I proposed to implement two applications, a mobile controller for an Android device, and a PC application (in my laptop) that receives the data from that controller and generates synchronized sounds and visuals.\n\nHowever, It just raised the question of what I actually would do in a short time. At that point, I started to think about the performance. My intention was to have a base percussion track and build musical layers at the top of that, manipulate some sound and effects parameters in real-time, and reflect it in an ethereal visual environment according to the music.\n\nTo some extent, I think that I managed some of my vision, but it was a quasi-improvised process, mainly regarding the visuals because the combination of graphic effects and objects results in a strange experience in the space. In fact, the central object, which is a flattened sphere, looks like a spaceship (an UFO some mates said) and I was not aware of that.\n\nIn addition to the unexpected spaceship, some classmates highlighted the use of the blending of colors and shapes. Therefore, after a “deep introspection” I came to the conclusion that it was like a psychedelic journey of an unexpected spaceship. Maybe some of you agree.\n\nThe Implementation\n\nAs I mentioned before, this system was implemented in Unity and is composed of two applications, an Android mobile controller, and a PC software for a Windows laptop.\n\nBoth applications are communicated through OSC messages. In order to achieve this, I used a script from UnityOSC, an open source project developed by Thomas Fredericks. The mobile application sends messages and the laptop receives them.\n\nThe Mobile Application\n\nThis applications is composed of a simple UI interface and its purpose is to send sensors’ data as well as specific commands through OSC.\n\nThe sensors used are the light sensor and the attitude sensor whose support is given by the relatively new Unity Input System. The commands are managed by the user interface depicted in the image below.\n\n\n    Mobile Application Interface\n\n\nThis interface has the following elements:\n\n\n  A text box to provide the local IP address for the machine where the data is going to be sent.\n  The SET button that reinitializes the app with the provided IP address.\n  A Calibrate button to take base measurements for light and attitude sensors in order to normalize this data.\n  The Drums1 checkbox to activate or deactivate a drums sample.\n  The Tempo checkbox, that is actually a command to activate or deactivate the influence of the attitude sensor over the drums sample (It is called Tempo because that was the first intention, but actually it controls the cutoff frequency for a Low Pass filter in the drums track).\n  The Synth1 and Synth2  checkboxes that are grouped to activate or deactivate the influence of the virtual keyboard over one of the two tracks associated with those elements.\n  The virtual keyboard which is a set of buttons that send MIDI data (note_on and note_off) routed over Synth1 or Synth2.\n  The Record button that is used to record a segment of MIDI commands from the keyboard considering the Synth (1 or 2) that is activated in that moment. Basically it is for the interaction of a simple looper.\n  There is also a small label under the Calibrate button which represents the value of the light sensor.\n\n\nAll of theses interactive controls send OSC messages and none audio or graphics processes are carried out in the mobile device. You will find a picture of the Unity development environment for this solution below.\n\n\n    Unity Mobile App\n\n\nThe PC Application\n\nThis piece of software is more complex and all the heavy process happens here. This solution deals with both, sounds and graphics processing. A complete schema regarding the communication between the mobile and the PC app is shown below.\n\n\n    System Messaging Architecture\n\n\nThe previous figure describes the mapping between the mobile device parameters sent through OSC and the PC modules that interacts with those messages. Note that one parameter is able to control two or more modules in terms of audio of graphics. For instance, the light sensor manipulates the pitch for Synth1 (audio) as well as the color for a Bloom camera effect and the color for particles (graphics effects and objects). Taking this example, refer to the picture to relate all the messages between the mobile app and the PC app.\n\nI want to focus more in the audio segment of the PC application, in which the following elements are important for the operation of this software:\n\n\n  Audio Files Playback: Unity, as many other game engines, is generally intended to use audio samples for the soundscape, that is, sound effects and music, which are key elements in such interactive systems and are represent as components called Audio Sources. In this case, the only element that is used in this way is the Drums Track, which is pre-recorded and has a strict length of two bars. The tempo for the performance is based on this track.\n  Synthesis Processing: Unity is able to attach custom filters to Audio Sources in order to generate new sounds or manipulate an existing one considering mathematical calculations over audio samples. I used this feature to implement a synthesis chain structure for generating sounds. For this case, I used only three modules for each chain Synth1 and Synth2, which are: A sine wave -&gt; ADSR Envelope -&gt; Output. The sine wave used the sine math formula over samples, the ADSR envelope receives the MIDI data from notes sent by the controller and allows a simultaneous reproduction of notes wrapped in an amplitude envelope with default ADSR values, and finally the Output receives the last element in the chain and put it in an Audio Source through a custom filter.\n  Audio Effects (like filters): Another audio feature in Unity is the possibility to use its built-in filters, which are audio effects over Audio Sources. That is why, despite Synth1 and Synth2 use the same synthesis chain, they sound different, because Synth1 has a reverb and a delay filter attached, while Synth2 has a chorus filter.\n  Looper: In order to have a multi-track experience, I implemented a very simple looper by recording just the MIDI commands in a segment of time defined by the performed when he or she activates or deactivates the Record checkbox. It only happens for Synth1 or Synth2, one in a time.\n\n\nIn terms of graphics. I used the post-processing queue for the virtual camera and basic objects like spheres or simple particle systems. Some effects are just in there (like the moving background) and others reacts according to the performance because they are associated with sensors or controls (as shown in the previous image).\n\nThe PC development environment in Unity is illustrated below.\n\n\n    Unity PC App\n\n\nConclusion\n\nI really enjoyed to implement this system despite it does not reflect my complete vision, but I understand that it was very ambitious to try an implementation of such digital instrument. However, It was fairly enough for the time that we had to solve the task.\n\nAmong all the elements that I developed, I realized that it is complicated to make a looper, I can imagine that there are tons of work in tools like Ableton to achieve something like that.\n\nRegarding the platform that I used, I think Unity is good for fast prototyping of complex interactive systems, nevertheless, in terms of audio generation, there is still some things to solve to rid of some artifacts that are present in the outcome. That is why I will explore to integrate an actual synthesis engine to Unity (maybe PureData, SuperCollider, etc) and process all the audio through those platforms. The result will be better if some calculations are performed by the right tools.\n\nFinally, I want to say that the unexpected could lead to new possibilities, just be open to experiment several approaches.\n",
        "url": "/interactive-music/2020/10/16/pedropl-pyco-unexpected-spaceship.html"
      },
    
      {
        "title": "SamTar",
        "author": "\n",
        "excerpt": "An interactive music system exploring sample-based music and improvisation through an augmented electric guitar\n",
        "content": "Sample-based music is no new phenomenon, and I’m no stranger to it. For me, the love for sample-based music is mostly due to 90´s French-house and the various hip-hop acts I listened to as a kid. More recently, I’ve been interested in how we usually interact and “play” with samples, often through MIDI-based beat controllers, and if there are other ways we can interact with samples that can provide new perspectives on sample-based music-making.\n\nIn light of these reflections, I decided to prototype an augmented electric guitar (SamTar) for playing and exploring sample-based music. The idea being that interacting and playing samples through a guitar interface could generate such new perspectives, and maybe a few novel affordances. The system uses a Bela micro-controller, some Pure data software and 5 sensors all mounted on the face of an electric guitar.\n\n\n  \n    \n  \n  A fully functioning SamTar prototype loaded with Daft Punk and Kayne West samples. I apologize for the audio and video quality but time was of the essence. \n\n\nDesign\nTo trigger samples on the SamTar you basically hit the only string currently available on the instrument. Simple enough. The hits do not only trigger the samples but also adjusts the gain and amplitude of the samples played. Another feature that is controlled by the main trigger mechanism is a sample-and-hold function. If you play two consecutive “notes” at a certain speed, the current sample will hold.\n\nSample Space\nTo categorize and organize samples, I created a 2D sample-space in Pure Data. The purpose of such a space was to enable easy and dynamic navigation of sample clusters from the SamTar itself. This space consists of multiple scenes (tracks) each with n-number of segments (section within a track), filled with x-number of audio files/samples. Also, as a general principle, I wanted to focus on how minimal input could control complex processes, so essentially how “one-to-many” mappings could benefit “one-man-band” type instruments (Morreale, McPherson 2019, p.2). The result was a simple button, mounted on the guitar, that skips segments and scenes when pressed.\n\nIn my prototyped version I was able to implement an LED array in p5 visually displaying the “location” of the player in the current sample-space. Optimally, this would be implemented in a physical Adafruit NeoPixel LED array mounted on the instrument itself.\n\n\n   \n   The SamTar's sample-space is inspired by Ableton Live's session view. However, the implementation is one step more high-level meaning the green circles represent segments containing multiples samples each. \n\n\nRandomness\nWhen in a segment, the system chooses a random sequence of two samples that are looped when hitting the string of the guitar. To enable a exploratory interaction I added a rotary knob (0 - 100) that adjusts the probability a trigger has of changing the current sample sequence to a new one. For instance, if the knob is at 40 the next trigger will have a 40% chance of changing the sample sequence and a 60% chance of staying on the same two samples.\n\nEffects\nFor some further variation, a designed a few effects to facilitate some sound manipulation. As the players left hand is completely free during play I put a soft potentiometer on the back of the guitar neck that controls the overall pitch of the samples. Furthermore, a chorus distortion was added for the ability to go full rock n roll, enabled by a simple switch on the mounted Bela board.\n\n\n   \n   Protoyped Bela implementation of the SamTar with 3 of the instruments 5 sensors visible.\n\n\n\n   \n   The use of Command Strips is a cheap, strong and effective way to mount micro-controllers in gravity-defying locations.\n\n\nEvaluation\nTechnically the prototyped system performed surprisingly well with negligible latency and reliable runtime. Also, the performance of the instrument indicated that the conceptual idea and intent of the system was more or less successful. With this, I mean that the balance between its “mastery range”, playability, and general interactivity was such that it warrants further exploration and prototyping. If this work is to be undertaken then several things would have to be tested and considered:\n\n\n  More strings and updated sample-space control.\n    \n      It struck me that the relationship between the sample-space and the sample triggering could be optimized and extended. Maybe it would be better to keep all 6 strings, map the signal pitch to individual samples and the envelope of the signal to pitch or duration of the samples played. Or something along these lines.\n    \n  \n  Re-develop the trigger system.\n    \n      As mentioned above, the system performed technically just fine. However, the trigger system does exhibit unwanted cut-offs and the occasional glitch which suggests re-development or improvements are in order.\n    \n  \n  More diagnostics.\n    \n      Additionally to the proposed LED array, more diagnostic displays should be explored that give users more insight into the current state of the system.\n    \n  \n\n\nRelevant Sources\nCook, P. R. (2006). Re-Designing Principles for Computer Music Controllers: A Case Study of SqueezeVox Maggie. in Proceedings of the 6th international conference on New Interfaces for Musical Expression.\n\nGonzalez Sanchez, V. E., Martin, C. P., Zelechowska, A., Bjerkestrand, K. A. V., Johnson, V., &amp; Jensenius, A. R. (2018). Bela-Based Augmented Acoustic Guitars for Sonic Microinteraction. In Proceedings of the International Conference on New Interfaces for Musical Expression. Zenodo. https://doi.org/10.5281/zenodo.1302599\n\nHunt, A., Wanderley, M. M., &amp; Kirk, R. (2000) Towards a Model for Instrumental Mapping in Expert Musical Interaction. in Proceedings of 2000 International Computer Music Conference\n\nMorreale, F., Guidi, A., &amp; McPherson, A. P. (2019). Magpick: An Augmented Guitar Pick for Nuanced Control. In Proceedings of the International Conference on New Interfaces for Musical Expression. Zenodo. https://doi.org/10.5281/zenodo.3672868\n\nO’Modhrain, S. (2011). A Framework for the Evaluation of Digital Musical Instruments. Computer Music Journal, 35(1), p.28–42.\n\nAcknowledgement\nA final thanks to Paul Koenig for the lending me the guitar and Thibault Jaccard for a quick JavaScript p5 introduction.\n",
        "url": "/interactive-music/2020/10/17/aleksati-samtar.html"
      },
    
      {
        "title": "Voice augmentation with sensors",
        "author": "\n",
        "excerpt": "Trying to achieve a choir-like effect by augmenting microphone input with sensory features\n",
        "content": "For this project, I wanted to create an interactive music system based on my own voice. Being inspired like Holly Herndon’s SPAWN, I tried to investigate ways of making one monophonic voice sound like a full choir. Spoiler alert: it ended up sounding more like a detuned alien than anything else.\n\nDesign\n\nThe figure below shows a rough sketch of the signal flow in the application.\n\n\n\nIn short, the microphone input is routed through an effect rack containing five individually tuned voices of a pitch shifter, a distortion and a delay line with feedback. The pressure sensor is mapped to the dry/wet mix of both the pitch shifter and the distortion, with the idea being that the more force you apply to the microphone, the more extreme the sound. A soft membrane potentiometer was mapped to the delay time of the delay effect. At last, the piezo microphone triggers an envelope when it picks up signal above a certain threshold. The threshold was set so that you have to hit the surface on which the piezo is lying relatively hard in order to trigger the output. The envelope was mapped to the dry/wet mix of the delay line, creating bursts of a delay/feedback effect.\n\nResult\n\nThe short timestamped snippet below shows how the system sounds.\n\n\n    \n    \n    Small snippet from the performance\n\n\nThe figure below show how the hardware was connected in my system.\n\n\n\nReflection\n\nDue to various circumstances, I didn’t get to work as much with this project as I would like to. One of the things I realised too late that I should have set aside more time to design the physical system. Almost until the deadline, I worked only with the software, and with the sensors loosely coupled together on the breadboard, as you can see in the image detailing the hardware connections above. For the next physical interface I’m making, I think I will try to start in the other end - by first sketching out how I want the system to look and feel, and design the audio engine based on the affordances of that physical interface.\n\nOverall, I wasn’t too satisfied with the otucome of this project. I would love to spend more time on making more natural sounding choir effects, even though this wasn’t the main goal of the course. I also realise that I should have explored more embodied ways of interacting with this instrument, as the current setup don’t afford a lot of playability.\n",
        "url": "/interactive-music/2020/10/18/ulrikah-the-choir.html"
      },
    
      {
        "title": "MIDI and Effects: the Musicpathy ",
        "author": "\n",
        "excerpt": "The magic of controlling instruments from 1000km apart\n",
        "content": "Introduction\n\nSince SMC began, we both were looking for an opportunity to make music together and we finally did it on our physical computing project. We both are from India, but still live 1000 km apart (Abhishek in Central India &amp; Lindsay in South of India), this brought a series of challenges.\n\nOur main idea was to play live music collaboratively with the tools that we were introduced to in the physical computing module, to input sensor data and control various aspects of the setup.\nWe approached it with a hands-on physical controller that was self-made for another project and also the OSC router which takes inputs from the mobile phone. Ableton Live was our preferred DAW as both of us are well versed in using it. Abhishek was the host, he used his phone as an input device and Lindsay used his MIDI Controller to send MIDI data over the network.\n\nTools and Setup\n\nSo to make music together live over the internet and still be technologically engaged, what do you need? A couple of computer applications and a decent internet connection, if you have a cool custom-built MIDI controller, that’s like a cherry on top.\n\nTools:\n\nTo jam live together we used the following software/tools :\n\n1. Ableton:\nThe Digital Audio Workstation of our preference, any DAW can be used.\n\n2. rtpMIDI:\nrtpMIDI is a virtual MIDI driver that sends MIDI data over the network.\n\n3. Hamachi (VPN):\nSince rtpMIDI sends data over the network, you need to have a LAN connection to be able to do so, or else you need to do port forwarding. Hamachi here fits as it makes a virtual LAN like network enabling the exchange of MIDI information.\n\n4. OSChook (Android):\nThis is the android application that reads data from different sensors and sends OSC messages to the OSC receiver.\n\n5. OSCRouter:\nOSCRouter receives the OSC messages and routes it to the application on your system that uses them to control things.\n\n6. LiveGrabber:\nThis is free MAX for Live plugin and is used to take data from OSCRouter and then control different effects in Ableton.\n\n7. Midi controller(Arduino Micro):\nAny MIDI controller works but we used the self-made MIDI controller that is based on the Arduino Micro with an analog multiplexer for many inputs, it has 18 buttons, 8 rotary potentiometers, 2 linear potentiometers out of them one acts as a channel controller and two buttons are octave shifters.\nThe program was written in C, for Arduino and sends MIDI data as output. The buttons are Sanwa arcade, which has a super-fast reaction time.\n\nSetup:\n\nThe basic flow of the setup is as follows: Lindsay sends MIDI data to Abhishek’s Ableton, and Abhishek controls the Ableton effects from his phone via OSC messages, achieving this was a little tricky. There are two aspects of this setup, one is the MIDI connection and the other is the OSC connection.\n\nMIDI:\n\nFor controlling the instrument setup into Abhishek’s Ableton using Lindsay’s MIDI controller we tried sending data over the network and since we both were on a wired connection, the latency that we always fight with at SMC, was very decent at about 37ms.\nThe MIDI controller is directly plugged in via USB to the system, rtpMIDI takes the input data and sends it to Ableton’s instrument rack to the receiving port setup in Ableton, over a virtual VPN network made through Hamachi. There’s a flexibility of controlling multiple channels and playing them all at the same time by mapping the controller inputs to different instruments and parameters.\n\n\n\nOSC:\n\nTo control the audio effects of Ableton via the smartphone, OSChook app installed in the phone takes data from different sensors and converts it to OSC messages that are sent to the OSC router’s receiving port, this messages are directed towards LiveGrabber plugin. LiveGrabber takes the data and uses it to control the effects mapped to the ParamGrabber plugin.\n\n\n\nLive Jam\n\nTo have a structure in the song we agreed to make a basic arrangement with drums and keys, which gave us a base track to jam upon. Here’s a video from one of our sessions:\n\n\n    \n    Jam Session\n\n\nConclusion: Individual Reflections\n\nLindsay’s Reflection:\n\nI really enjoyed making music with Abhishek although I did not have any experience in making chill LoFi type of music. The whole procedure of setting up and getting our conceptual idea into a working model was super fun and exciting. An advantage of being in the same time zone, we spent a whole night trying to solve the challenges and figuring out ways to bring out our idea, making music with the least latencies, and during the night time, we had latency levels as low as 20ms. The whole process of composing music on the go and live was thrilling and the night well spent. Finally, I would like to say that making your own controller and your DIY setup with the least latency to connect and play collaborative music is a very satisfying experience.\n\nAbhishek’s Reflection:\n\nThe whole process was super fun and felt like we were in a hackathon or something, setting up connections and then finally making music was a different experience, although it still cannot replace the physical jams, this one was the best and closest to the physical jam session we used to have in the pre-covid era. Nevertheless, the best part of this setup is that once everything is fixed and connected, the next time you want to jam again you don’t need to do it from scratch, just plug in the controller, start the DAW and you’re pretty much ready to go, making it super easy to have more jams with Lindsay. The next iteration of this setup for me will be to include acoustic instruments in the jam, use FaceOsc to control the effects, and increase the precision and accuracy of OSC controls.\n",
        "url": "/interactive-music/2020/10/18/abhishec-Musicpathy.html"
      },
    
      {
        "title": "Plunged Into Chaos",
        "author": "\n",
        "excerpt": "Wherein the lowly Oompa-Doompa assumes its ultimate form.\n",
        "content": "A Little Context\n\nBeloved by plumbers, sexual deviants and trombonists, the venerable toilet plunger (or ‘force cup’) was invented in 1874 by a New York confectioner named John Savage Hawley. Hawley was a man of many talents, holding patents for, among other things, an improved type of corn-cob pipe and a method for the manufacture of chocolate cigars.\n\nWhether or not the consumption of too many chocolate cigars is what led to the invention of the force cup is question lost to history, but it’s easy to imagine a rough sort of timeline. As the Greek philosopher Plato was fond of saying, “necessity is the mother of invention”.\n\nIndeed. I already had in my mind a hazy idea for the type of instrument I’d like to invent for SMC4045 (Interactive Music Systems), and rest assured, dear reader, it wasn’t a musical toilet plunger. What I had was a roll of Nichrome resistance wire which I intended to wrap around a dowel of some sort and thereby create a crude sort of variable resistor, which would form the basis for varying the pitch of the instrument. But I had no dowel, so after moping about the house for a bit, I decided I’d better get a move on and make a run to Clas Ohlson for some crafting materials.\n\nIt was then that serendipity struck. As I reached into the closet for my jacket, my gaze fell upon the utilitarian form of our lowly toilet plunger, purchased new and hence abandoned, unused, at the back of the closet. Apparently, they put some mighty strong-flushing crappers in SiO housing, because I’d forgotten it was there. Jackpot! Not only was its handle a perfect sort of dowel for wrapping the wire around, I could stick it to the table and use it as a sort of joystick. All I needed was a way to read the x/y axis . . .\n\nNow, the prudes among you may be asking just what sort of antisocial misanthrope must I be to bring an oompa-doompa to a master’s-level university course of a serious nature, but I do think Hawley would approve, and he was a religious man. In his landmark work, the catchily-titled Job, His Old Friend and His New Friend; also A Study of What the Book of Job Means Spiritually, to All Mankind, by a Plain Man, Who Has about Finished With What is Called Business, and Writes from Experiences, Not Entirely Unlike Those of Job (Hawley, 1912), the author describes the many labors of the biblical Job, who was forsaken by the almighty God while he was on the toilet. Therefore, the musical device which I have developed could be considered to have a long and religious pedigree, and might very well be used for the playing of devotional music in serious churches and cathedrals.\n\nI myself used it to play ‘Sunshine Of Your Love’ by Cream, as well as the riff from ‘Slow And Easy’ by Whitesnake. It’s my religion, ok?\n\nOk, let us fast forward a bit, and get to how the damn thing works. The components of the system as I imagined it are as follows:\n\nPhysical Interface:\n\n\n  A Plunger, Plumber’s Friend, or Oompa-Doompa. An unused one is preferred but hey, it’s your funeral, weirdo.\n  A wire-wrapped homemade variable resistor (see fig. 1)\n  Force sensitive resistor\n  X/Y accelerometer\n\n\nSound Generation:\n\n\n  Three simple oscillator engines\n  Bandpass filters\n  Whatever madness lies in your heart\n\n\nMapping:\n\n\n  Pitch: variable resistor\n  Volume: force-sensitive resistor\n  Accelerometer: y-axis, bandpass. X-axis, volume of secondary voice\n\n\n\n\nfigure 1: Slow And Easy\n\n\nFig. 1 shows the aforementioned homemade variable resistor. The ‘wiper’ is a brass pinky slide ring made for guitarists. The ‘wiper’ is drawn along the resistance wire’s length, thereby changing the wire’s effective length, and therefore its resistance. Just trust me. Because the overall resistance of my makeshift potentiometer was very low (between 1-50 ohms), I utilized an LM317 voltage regulator circuit to read these values without shorting out the system. Blah blah blah, science. I don’t expect you to care but it’s rather quite interesting, really.\n\nFig. 2 shows the rather quite interesting circuit I used to measure values from the low impedance resistor, for those that do care.\n\n\n\nfigure 2: Blah Blah Blah, Science\n\n\nFig. 3 shows the mounting of the volume sensor. It’s a very sensitive little beast, but with practice the player can control onsets, volume swells and tremolo-like effects, at least until it breaks, which it most likely will. To avoid this, I zip-tied it down with the zeal of a cartoon villain tying a damsel to the railway tracks. Done.\n\n\n\nfigure 3: Help! Ooohh helllpp!\n\n\nThe x/y accelerometer was mounted to the top of the handle. Do you want to see it? Ok, fig. 4.\n\n\n\nfigure 4: Just So\n\n\nI ran the wires very neatly down the back of the handle, secured with double-sided tape, and soldered and shrink-tube wrapped my wiring pigtails for a professional look. That is, if a musical plunger can be considered professional. But anyway, have you seen some of the things people bring to the NIME conference? Ridiculous. I rest my case.\n\nAnd, lastly. How does it sound? Well here’s the video:\n\n\n\n\n  Something not quite approaching Chaos\n\n\nAnd for those that are still interested, my (slightly more serious) slide presentation on the interface:\n\n\n    \n    Presenting: The Presentation\n\n\nReferences:\n\nHawley, John S. (1912) Job, His Old Friend and His New Friend; also A Study of What the Book of Job Means Spiritually, to All Mankind, by a Plain Man, Who Has about Finished With What is Called Business, and Writes from Experiences, Not Entirely Unlike Those of Job, Reprint: Franklin Classics (2018), ISBN-13 978-0343209469\n\nFor more references, see full presentation, above.\n",
        "url": "/interactive-music/2020/10/18/Plunged-Into-Chaos.html"
      },
    
      {
        "title": "The singing shelf bracket",
        "author": "\n",
        "excerpt": "Pure Data (PD) has a lot of possibilities, but when getting the opportunity of putting together all of those digital features into the real word: with real wires, real buttons, real sensors - I must admit - I got a little over-excited!\n",
        "content": "Pure Data (PD) has a lot of possibilities, but when getting the opportunity of putting together all of those digital features into the real word: with real wires, real buttons, real sensors - I must admit - I got a little over-excited! It’s a new dimension that reveals in front of you in terms of audio programming. It’s like playing with LEGO, but you can make music with it. Just so fun.\n\n\n\nI have been playing around with Arduino and breadboards before, bought a lot of wires, leds, resistors and other electronic stuff online, and I have had some projects with Pure Data and Raspberry Pi, but never really completed anything appliable. Bela is neat, as it is designed to be used together with PD. The difficult thing is that you cannot use messages in Bela, everything should be audio signals. Also, the Bela has not the best CPU capacity, so you have some restrictions on what you can make. But when compared to Arduino and other single-board microcontrollers, Bela is exceptional regarding low latency. It can perform latency as low as 0.5 ms.\n\nIn my project, I used an object called Sigmund~, that converts audio signals to midi note float numbers. My idea was to be able to control a synth with the voice, so that hands could be free to do something else. I was thinking about a digital saxophone (Akai Ewi) I heard some years ago at a concert with Panzerpappa. That was the most awesome thing I’d ever heard, and I wish that I had one, and hey, maybe this could be a chance of building something that would never even be CLOSE to sounding like that instrument, but could be a step forward in that direction? And with a twist: using your voice to create the melodies. Call it a Kazoo if you want, I call it … something else (I will come back to that later).\n\n\n\nIn my PD patch, I tried to create some great many-to-many mapping, because that’s how the traditional instruments are mapped. Initially, I wanted to add so many features for this instrument. A drone note function, a low frequency oscillator, a volume control, a pitch bend, bypass filter, a loop function, recording function, effects such as reverb and other effects were some of the features I initially wanted to include. And of course, some AWESOME synth sounds. In addition to this, I also imagined that you could use the microphone in a traditional way without converting the audio to midi notes. So that you in one small instrument could create a synth loop which you could perform over with your voice. This is definitely something I would love to possess and use myself!\n\nAnd this whole thing was supposed to be wrapped inside a 3d printed body without any of the electronics visible, and there were supposed to be dozens of LEDs covering the whole body so the instrument could look like a christmas tree on speed …\n\nBut what did I end up with? \nThis:\n\n\n\n…. A shelf bracket. A singing shelf bracket that I found in my locker (don’t ask why I had a shelf bracket in my locker - I really don’t know why!)\n\nNo loop function, no raw voice function, no reverbs, no filters. But it has a couple of oscillators, a drone and a low frequency oscillator. It is a start really. The shelf bracket shape Is OK, as you can hold it with your both hands and still control the sensors with your fingers! But perhaps not a christmas tree. Definitely no christmas tree. More like a … chistmas foot? :P\n\n\n\nI wish this course lasted for more than just TWO WEEKS!!! And on the last day, we had to disassemble the whole instrument  - no chance of realizing further plans at all. If I don’t buy my own bela - perhaps I will. I’m addicted!\n\nThis diagram shows the mapping of the instrument:\n\n\nDemonstration of the instrument:\n\n\n",
        "url": "/interactive-music/2020/10/18/singing-shelf-bracket.html"
      },
    
      {
        "title": "HyperGuitar",
        "author": "\n",
        "excerpt": "An exploration of limitations and how to create meaningful action-sound couplings.\n",
        "content": "Introduction\n\nThere have been lots of interesting projects done when it comes to augmenting and extending acoustic instruments with the use of electronics. One of my favorite examples of this is Morten Qvenild´s Hy(Personal) Piano. My initial idea was to create something that would mimic the aesthetics of the sounding output from his project. With this holistic approach, emulsification as Qvenild describes it as. The acoustic sounds from the instrument blends perfectly with the processed sounds. The different parts are intimately interconnected.\n\nI had this idea of creating an instrument where you had this subtle interaction between the two entities. Instant subtlety, music later. However, it is difficult to accomplish this in only 10 days when your cognitive abilities are under par. We should all listen to Perry Cook. But at least I made an effort and managed to create something that made some sound.\n\nInteraction\nAt the core of this creation you find the “maker platform for creating beautiful interactions”: Bela. As you can see in the picture below the Bela and breadboard are attached to the headstock of my guitar using some string I stole from my girlfriends sowing kit and a capo. Up until the last day my system looked a little bit different. Instead of a sad looking rotary potentiometer I had a ultrasonic distance sensor attached to the body of the guitar. However, due to some last-minute complications this was removed and replaced with a rotary potentiometer.\n\nThe observant reader will also notice that there is a piezo-element fixed to the body. I will put a video at the end of the blog where you can see what this does. The piezo-element was there as an attempt to explore the percussive properties of the guitar. However, this was rather unsuccessful. Maybe I will explore this interaction more at a later stage, but for this blog post the focus will be on something more interesting. On the breadboard I have also connected a 9 DOF IMU LSM9DS1-chip from SparkFun. This is an IMU which houses a 3-axis accelerometer, a 3-axis gyroscope and a 3-axis magnetometer. Which gives you nine degrees of freedom (9DOF). If you analyse the output from the three sensors and fuse them together you can then calculate the roll, pitch and yaw.\n\n\n   \n   The instrument in action!\n\n\nIn my project I have mapped the pitch and the roll of the 9DOF to different parameters of a spectral delay and a granular synthesis-patch. All of the sound-programming is done in Pure Data. The famous visual programming language which will give you RSI and tinnitus in a heart-beat. In the GIF below you can see how the pitch is mapped to the bin-position of the spectral delay.\n\n\n   \n   PD-snippet\n\n\nAnyways, the main issue when creating an interactive music system where you involve a guitar, is that you don´t have much spare bandwidth. Both of your hands are involved in the sound creation and letting go of the strings will stop the musical output. Therefore a motion sensor fits perfectly in this context.\n\nGesture-sound\n\n\n  \n    \n  \n  Roll mapped to pitch\n\n\nAs you can see in the video above i have mapped the roll-movement of the guitar to the pitch of the granular synthesis. One of the features I wanted to achieve was a clear link between the movement I made and the sounding output. The gesture or action had to be related to a meaningful sonical output. Creating a link from action to perception. Bringing the audience perspective into the design of the instrument. In a way trying to trick the spectator into believing that the sounds are a natural part of the instrument. Or at least making it obvious where the sound creation is coming from. When you play the guitar you might have a lot of movement in your upper body. However, this might not effect the actual sound. So the movement is in a sense already there for many players. It´s just not utilized. Unless you have a B-Bender installed on your guitar. Therefore I found this type of movements to feel quite natural while playing, and it did not disturb my playing in any particular way. It was quite easy to adapt to this part of the instruments gestural acquisition.\n\nLast thoughts\n\nIf I were to do this all over again. I would have spent less time on the sound-synthesis and more time on exploring what I could have done with only the granular synthesis. Spent more time exploring and fine-tuning. It requires some  practices if you want to convey something meaningful. There is a lot of expressive possibilities in this instrument, but it also has its limitations. And some parts are more fulfilling than others. The piezo-element is perhaps the least successful part of the creation. And has very limited expressiveness. However, I feel that there is still a lot to explore and learn about the instrument. I have not been able to spend too much time just playing with it, but here is a video of me exploring the instrument.\n\nSome ideas:\n\n\n  try to include the yaw movement(?)\n  build a  more lightweight patch with less latency\n  make it function with a distance sensor\n\n\nHere is a video you can watch. I recommend you to watch through the whole thing. Some very neat projects made by very clever guys:\n\n\n    \n    Exploration of unfamiliar territories and tunings\n\n\nThis is how it sounds in a more controlled environment:\n\n\n\nAnd this is what the piezo did :):\n\n\n\nAnd if you are really interested you can have a look at the presentation I made. See if you can find the hidden treasures:\n\n\n",
        "url": "/interactive-music/2020/10/19/thomasanda-HyperGuitar.html"
      },
    
      {
        "title": "The Ring Synth",
        "author": "\n",
        "excerpt": "Exploring speed as sound shaping parameter\n",
        "content": "Introduction\n\nThe Ring Synth is a monophonic subtractive controlled by voice and finger movement. The finger data acquisition is done using an ATSAMD51 based microcontroller by Adafruit and the Trill Ring capacitive touch sensor by Bela. The sound engine is implemented in Pure Data.\n\nDesign\n\nOverview\n\nThe system consists of three parts, the audio peripheral (sound card, microphone and heaphones), the finger gesture peripheral (microcontroller, Trill and LEDs) and the computer (mapping and synthesis in Pd). You can have a look to the pin diagram below.\n\n\n  \n\n\nInputs\n\nFrom the voice, the fundamental frequency and the RMS are extracted in Pd.\n\nFrom the Trill sensor, the touch surface (pressure) and the finger displacement speed are extracted using a custom program implemented in the Arduino IDE (in C). These data are sent to the computer using MIDI CC over USB.\n\nFeedback\n\nAn Adafruit NeoPixel 24 LEDs Ring surrounds the Trill Ring, and gives a useful feedback in order to develop expressive skills on the instrument. The touch surface is mapped to the size of an appearing strip and the speed to colour (see video below).\n\nSynthesis\n\nThis subtractive synthesizer has five parameters to control:\n\n\n  Subtractive synthesis (phasor~ -&gt; vcf~)\n    \n      Harshness (q of vcf~)\n      Vibrato (moving f of vcf~)\n    \n  \n  Noise (also subtractive, pink~ -&gt; vcf~)\n    \n      Dry/wet\n    \n  \n  Volume\n    \n      Continuously changing envelope\n    \n  \n  Reverb (freeverb~)\n    \n      Dry/wet\n    \n  \n\n\nBelow is the subpatch doing the first synthesis step.\n\n\n  \n\n\nSee video below to hear it in action.\n\nMapping\n\nThe frequency extracted from the voice is mapped to the fundamental synthesis frequency in a one-to-one fashion. The rest (RMS, surface and speed) interact in a more complex many-to-many mapping shown below.\n\n\n  \n\n\nDemonstration\n\nHere is a video of the system in action. Last third of the video presents the instrument being used with random noise as audio input, so the pitch jumps all the time.\n\n\n\n\n\n\nReflection\n\nThe interaction is interesting due to the speed parameter, it kinda ressembles a bow in a way, and is fun to play with. But the sensor’s sensitivity is high, and it is hard to stabilize the sound. Many improvements could be done on the data filtering process.\n\nTaking advantage of the possible voice virtuosity has proven to be a straightforward way to test the instrument in any track, genre and key.\n",
        "url": "/interactive-music/2020/10/20/the-ring-synth.html"
      },
    
      {
        "title": "Breathe the light, scream arpeggios!",
        "author": "\n",
        "excerpt": "Multisensorial music interface aiming to provide a synesthetic experience. Touch, light, breathe, scream - make sound!\n",
        "content": "Concept\n\nMy intent with this project was to provide a synesthetic experience involving breath activations, light feedback, tactile sensations and some screaming while performing. (The screaming part happened not only during the performance, but also a lot during the development process, in order to deal with the unpredictability of dealing with sensors, Bela and Pd!)\n\nThe interface was designed to allow the user to have all the senses mentioned interacting simultaneously, creating expressivity and an interesting relation between body movement and sound production.\n\nThere are basisc two modes, one is the [ standard mode ] which is a simple synthesizer of 5 oscillators having the pitch controlled by a flex sensor, acid filters and the light controlled by blowing a small microphone. The second mode is the [ scream mode ], when the user instead of blowing screams to the microphone, activating an arpeggiator to this synth.\n\nDesign\n\nThe design process started when I realized the importance of sketching the ideas with pen and paper. It helped a lot to clarify some toughts and visualize the ideas in practical way. Drawing, listing components and possible mappings and noting alternatives, were extremely important as a starting point for this project.\n\n\n   \n   Sketch\n\n\nAfter that I started to design a prototype digitally, that way I could organize the ideas and make the draw more clear and simple to understand in order to visualize the disposition of the sensors, wires and material used for the interface.\n\n\n   \n   Prototype\n\n\nThe next step was to design a scheme for the connections between the sensors and Bela using a software called Fritzing.\n\n\n   \n   Fritzing\n\n\nComponents\n\nFlex Sensor 2.2”\n\n\n  Tactile sensory experience\n  Controls Pitch when in [ standard mode ]\n  Controls Root note octave, scale, and sequency pattern when in [ scream mode ]\n\n\nElectret Microphone\n\n\n  Breath and Scream activations\n  LED lights up like a flame! [ standard mode ]\n  Acid filters and Evelopes open up [ scream mode ]\n  Activates arpeggiator when scream! (total madnassss!) [ scream mode ]\n\n\nFlex + Electret Microphone [ scream mode ]\n\nThat combination of sensors was a creative solution that I had to come up with at the last hours before the presentation. It happened because my Accelerometer sensor stopped working around 01:30 of the night before the performance. Some of the functions bellow require a sequence of interactions and range of values.\n\n\n  Touch and Scream! (multisensorial)\n  Arpeggiator rate (if you press the flex enough)\n  Acid filters / Evelopes open up\n  Pitch and Scale\n\n\nThe System\n\n\n   \n   The IMS\n\n\nResult\n\nThe results in the performance were way distant comparing to the experience that I had during the development and testing phases. The main reasons were:\n\n\n  \n    I had to deal with the loss of one sensor\n  \n  \n    The microphone during testing phase didn’t capture any sound feedback since I was using earphones as monitors for testing. So the funcions like lighting up the led blowing the microphone, opening-up the filters and envelopes and activating the arpeggiator got all messy during the performance, since it was capturing the loud sound coming from the monitors and interpreting as the user blowing the microphone or screaming.\n  \n\n\n\n    \n    Performance\n\n\nReflection\n\nTime management. The best that I can say here is that time management is essential to any project, and of course it was a key point for my project. As I spent much time on the theoretical conceptualization, ideas and studying the subject, I didn’t have time enough to spend testing the system out of my house bubble. In real action. That lack of management made me go from a stable and interesting performance at home, to a messy and unpredictable performance in public.\n",
        "url": "/interactive-music/2020/10/20/rayaml-breath-and-scream.html"
      },
    
      {
        "title": "Shaaape",
        "author": "\n",
        "excerpt": "Distorting signals with ghost signals.\n",
        "content": "\n    \n\n\nIntroducing shaaape, a tanh distortion effect unit based on an idea of driving a signal with an added “ghost signal”. The inspiration is based on how loud low frequencies effect higher frequencies in speakers. How a loud sub bass can cause fluttering in the treble (trouble in the treble). So I went about trying to recreate the effect as a plugin so it could be used musically.\n\nProcessing\nThe input signal is being distorted summed with a “ghost signal” (in this case a sine wave and a filtered random oscillator. Hereby referred to as GS.) which is being phased out of the output signal. The benefit of dealing with digital distortion is that a signal can be distorted identically in two places. So the GS is not only being distorted in the main distortion, it is also being distorted equally much in it’s own distortion channel. The purpose of this is to create a distorted GS that is equal to the GS distorted in the main distortion so that it can be phased out after the main distortion. It’s a tounge-twister, so hopefully this illustration will help understanding what’s going on.\n\n\n    Illustrating the shape of shaaape we see that it is an quite easy way of processing\n\n\nInput and mod have individual amps before they are being summed for the main distortion channel. Also mind that the signal chain is stereo for both carrier and modulation. I chose the tanh type distortion due to it’s “analog roundness” compared to the hard clipping of a clip type distortion. Future possibilities in trying different distortion types - even difference in distortion types between the two channels (+left/right). I have dont some testing of this, but without any convincing results. Usually, due to the phasing towards the end, the result is an audible sine wave playing at the given frequency. Presumably this phenomenon occurs because it isn’t getting phased out properly.\nBefore outputting I included a Low Pass Filter for the purpose of cleaning up heavily distorted signals. It’s controlled by the distortion amount to keep things simple. At the end, and beginning, there’s a gain balance which let’s you tilt the gain-staging between input and output. Kind of controlling how hot the signal chain is. This gives more flexibility in how hard you want to go with the distortion and filter, and will also affect how the mod-amount is behaving.\n\nDemos\nVideo showing what shaaape does, from subtle to extreme and some tweaking. Sound source is a stereo drum loop.\n\n  \n    \n    Should show a video...\n  \n  \n\n\nWhite noise through shaaape to give an impression of how it flutters audio:\n\n    \n\n\n  \n    \n    Alternate Text\n  \n  \n\n\nUsed as bass synth on guitar sample:\n\n    \n\n\n  \n    \n    Alternate Text\n  \n  \n\n\nShort example of stereofication. Mono drum sample going “stereo” halfway through:\n\n  \n    \n    Alternate Text\n  \n  \n\n\nShaaape, shaaape - what is it good for?\n/-hopefully not nothing-/\n\nWhy not just use a regular ringmod, amp it and then add a distortion? Great question. It would do for the same purpose, though I found there are interesting random glitching happening when turning the knobs and trying different levels. Also putting everything together in one package like this opens for mappings and also a different approach for working with the parameters.\n\nI have been using the Max/MSP prototype for a while. This new version is more aggressive, but also has more interaction between parameters. That being different knobs adjusting the intensity of others and control mappings.\n\nPersonally I like to use it on drums and bass, and as a stereofication tool. The stereo modulation is quite transparent on low amounts and with random mod-source, adding width to a track. Somewhat inspired by the brainworx channel strip plugins, where they use TMT (Tolerance Modeling Technology) to mimic the marginal differences between the 72 tracks on a mixing console. Applying two different analog channel strips to a mono sound source will spread it out slightly, due to the different wear on analog components, especially in old mixers. Sort of spreading to stereo due to differences in dynamic information.\n\nFinal thoughts\nThe plugin is in my opinion a fun little tool to use. Distortion and filtering sounds quite nice and the modulations open for weird, glitchy sounding stuff. Shaaape does a good job of making sounds more unpredictable and to some degree more “organic”. Like all distortion and overdrive it sounds good on drums, but I’ve been getting interesting results on synths as well as vocals.\n\nThere is some interesting potential in looking into playing shaaape as a keyboard instrument on an audio input carrier, like the guitar sample example. Though this is a slightly different instrument/effect than this intends to be. So it would be for another project.\n\nI have done my best to set proper gain-staging for the signal path, though I experience that there are still improvements to be done here. For now I have gained/lowered signals outside the plugin. Took a stance at not having both input and output knobs to keep number of dials down, but maybe some refinement can be done to the “in out” dial to make it more effective. Kind of curious about AI, or even just simple algorithms, for things like this in plugins. Could be cool.\n\nIf you want to try it, please do so. Below are the vst/vst3/au files to download. Mind that if you’re an AU user, you should also include the “shaaape_01.csd” in your Components folder. Else it won’t work. It’s a Cabbage thing. Have fun.\n\nDownload shaaape for free!\n",
        "url": "/sound-programming/2020/11/02/henrikhs-shaaape.html"
      },
    
      {
        "title": "Kovid Keyboard",
        "author": "\n",
        "excerpt": "A web based CSound Synth letting you play together online!\n",
        "content": "\n\nPlay Together Now!\n\nCheck it out at the link below and send the link to a friend to play together!\n\nYou can use the lower two rows of your computer keyboard to play notes after you click the on screen keyboard, or use a USB keyboard if you have one.\n\nhttps://midi.leighmurray.com\n\nOverview\n\nSince people are home a lot more I decided to make an online synth to let people play together from home.\n\nThe Kovid Keyboard is a polyphonic midi synth that is synchronised between all users, that is, any changes you make to the Kovid Keyboard on your computer are applied to all the other Kovid Keyboard users in real time.  In this way, rather than sending audio data between users, only note on and note off events are sent between users and all sounds are generated locally for each user using CSound. USB Midi keyboards and sliders/knobs are automatically mapped to the knobs on screen, but MIDI learn can also be used by right clicking on a knob/slider and clicking learn.\n\nThe Kovid Keyboard has 12 individual CSound instruments which are each bound to their own MIDI channel (1-12). The user can select their instrument using the Select an Instrument dropdown at the top of the screen and all note-on/note-off events generated by the user will be sent to their selected instrument in CSound. Other users can select different instruments and their note-on/note-off messages will be sent to the instruments they have chosen.  You can see which users are playing what instruments in the table on the right.\n\nAll instruments are single oscillator instruments except for instrument 10: Subtractive Synth Wave which has two oscillators.  Some instruments have more knobs than others which is determined by the amount of parameters than can be passed through to the oscillator’s opcode in csound.\n\nThe Start button is only necessary on some web browsers to initialise midi but if your browser works without it you don’t need to press it.\n\nThe PANIC! button turns off all MIDI notes, in case some note off messages get lost and sounds won’t stop playing.\n\n\n\nFrontend\n\nI’m using the CSound for Web library which is a standard Javascript library available from the CSound downloads page under “Mobile and Web”. It’s poorly documented but by using the examples found here I was able to piece it together.\n\nI found a web library that has cool looking knobs and automatically binds to MIDI command messages from USB MIDI devices, and also provides a MIDI keyboard input, so I hooked that up.\n\nI then send the values from these knobs into the different CSound instruments to change various parameters.\n\nBackend\n\nI’m using Nodejs as my webserver and Socket.io for sending simplified MIDI and control data messages between all the users in realtime.\n\nInstruments\n\nYou can select an instrument using the Select and Instrument dropdown menu.\n\nThe Gain and Limiter are local parameters, they aren’t sent to other users. The Gain lets you adjust the overall volume of the synth and the Limiter switch limits the output to -0.9, 0.9.\n\n\n\nAll instruments have Amplitude AMP, Low Pass Filter LP, and Attack aA, Delay aD, Sustain aS, Release aR parameters.\n\n\n\n\n\nInstruments 1-7 all use the vco2 opcode to generate different wave shapes, but Instruments 5 and 6 have a Pulse Width PW parameter to change the ramp or pulse width of the Triange and Square waves respectively.\n\n\n\nInstrument 8 uses the oscil opcode to generate a standard sine wave.\n\nInstrument 9 uses the gbuzz opcode to generate a sine wave with a variable amount of harmonics.\n\n\n\nInstrument 10 has two oscillators.  The user can choose the shape of each oscillator using the Osc1 and Osc2 sliders. The top position is a Square Wave with variable Pulse Width PW, the middle position is a Triangle wave with variable ramp PW and the third position is noise.  The signal from these oscillators goes through a filter with its own ADSR fA, fD, fS, fR and resonance value fR.  The Low Pass Frequency LP is now passed to the filter to act as its upper limit.  You can change the Octave of each oscillator using oct1 and oct2 and fine tune each oscillator using cent1 and cent2.  I got most of the code for this instrument from the floss manual here.\n\n\n\nInstrument 11 and 12 are simple Saw waves using the vco2 opcode.\n\nResources\n\nIf you want to check it out yourself you can get the code here: https://github.com/leighmurray/csound-socket\n\nYou need to install NodeJS. Then clone the code and you should only have to run:\n\nnpm install\nnode index.js\n\n\nThen go to http://localhost:3000\n",
        "url": "/sound-programming/2020/11/03/kovid-keyboard.html"
      },
    
      {
        "title": "'El Camino de los Lamentos': A performance using custom Csound VST Plugins",
        "author": "\n",
        "excerpt": "This performance is using two VST plugins produced in Cabbage through Csound. A synthesizer based on elemental waveforms, frequency modulation, and stereo delay, and an audio effect for pitch scaling and reverb.\n",
        "content": "As part of the project for Digital Audio module regarding SMC4000 course, you will find below a video for a performance that uses an effect and a synthesizer implemented in CSound, which were exported through Cabbage. Besides, I used the looping capabilities of Ableton Live together with a specialized Ableton controller (AKAI APC40), a MIDI keyboard controller (AKAI MPK mini), an electric guitar, and my voice.\n\nI called this song in Spanish “El Camino de los Lamentos” (The Road of Lamentations) considering the mystery and feelings that, personally, they produce in me. I hope you like it. Later, I will explain the work behind the development of these plugins.\n\n\n   \n   The Performance\n\n\nThe Performance\n\nThis performance is a looping session that starts with a pre-recorded drums base, then, it continues with arpeggios from an electric guitar with a reverb given by the custom VST plugin effect with a little pitch scaling amount. The next part is a melody with the same schema, but in this case the pitch scaling is more noticeable because it has less reverb and a little modulation effect that makes a slightly change of pitch in time.\n\nAfter some guitar playing, I continue with the synthesizer VST plugin by using a square wave. Firstly, a chord base is played with a small value in the cut off frequency of the low pass filter and a high resonance, then I change those values to have more frequencies into that base. Here, the panning effect can be listened slightly.\n\nThe next loop track is about a simple melody with a very noticeable moving panning regarding the delay. Here I change some parameters in real-time to demonstrate the effect of changes in the low pass filter and frequency modulation.\n\nFinally, I used the effect over my own voice by scaling the pitch on a fifth (+7) or a complete octave (+12) and sometimes back to normal (0). The performance ends with the same percussion as the beginning and the volume decaying slowly and manually.\n\nPlugins Development\n\nThe Synthesizer\n\n\n    Synthesizer Interface\n\n\nThe image above depicts the interface for the synthesizer I implemented. It can be controlled directly with the mouse pointer or through a MIDI controller when used in a DAW and mapped properly. The following elements compose this plugin in two Csound instruments:\n\nElements of Instrument 1:\n\n\n  \n    Waveforms: The user can choose among a Sine, a Sawtooth, or a Square waveform as the sound source. For producing the last two, I used the ftgen opcode to obtain global variables and give them to an oscil as a parameter. It means that those waves are shaped by the sum of sine waves considering the right harmonics to generate them. The UI in Cabbage is a group of radio buttons that allow the user to select just one out of three options.\n  \n  \n    ADSR Envelope: The madsr opcode is used to modulate the amplitude of the wave generated every time a key from a MIDI controller is pressed. Only the attack and release parameters are exposed as knobs, while the decay and sustain are fixed.\n  \n  \n    Frequency Modulation: A sine LFO works as a modulator for the frequency of the source wave and is controlled via two parameters: Mod Amp and Mod Freq, which correspond to the amplitude and frequency of the modulation signal, respectively.\n  \n  \n    Output and Display: The resulting signal is added to a global signal that is processed by instrument 2 as well as displayed in the interface as a waveform plot Time vs Amplitude.\n  \n\n\nElements of Instrument 2:\n\n\n  \n    Delay: Here the vdelay opcode is used with an implementation strategy that considers feedback. In this case, instrument 2 is running constantly and parallelly to instrument 1 that is instantiated every time a NoteOn MIDI message is received. Instrument 2 defines a feedback signal that is weighted by the delay feedback amount provided by the user and is built considering the delay time also from a knob.\n  \n  \n    Panning LFO: A sine LFO for panning the delayed signal is used with an amplitude and frequency of 1. This is mathematically manipulated such way the delayed signal is moved between left and right. After this process, a new signal is composed by the sum of the original signal from instrument 1 and the panned delayed signal.\n  \n  \n    Resonant Low Pass Filter: The opcode moogladder is used for filtering the composed signal mentioned before. The cut off frequency and the resonance parameters are given by the user through knobs in the interface. This filtering is applied to both stereo signals and then sent to the output device.\n  \n  \n    Global Output: It is important to keep a global output signal for the delay effect that can be used for both instruments. Also, this signal must be reset to zero after every execution in instrument 2 to receive data from new instances of instrument 1.\n  \n\n\nThe connections architecture for this plugin is shown below. Note that some arrows are colored differently in order to avoid confusion when they are crossed one each other and for the clear identification of the final part in which everything converge to the stereo output device.\n\n\n    Synthesizer Connections Diagram\n\n\nThe Effect\n\n\n    Effect Interface\n\n\nThe image above shows the interface for the effect I developed. It is built by using one only Csound instrument and consist of basically two main components that affect an input audio signal: the pitch scaling, and a reverb effect. They are explained as follows:\n\nPitch Scaling\n\nThe idea behind pitch scaling is producing a signal that mimic the input with a different pitch regarding a scale factor. For this case, the scale factor is a number between -12 and 12 which represents semitones below or above the original pitch.\n\nIn order to do this, the scale factor (kPscale) is transformed to a different factor. This factor tells us that, when pitch scale is 2, the pitch will be one octave above the original, and when 0.5 an octave below. Numbers in between are given by a formula based on the relative semitone in which the user wants to scale. Thus, for values between -12 and 0, the new scaling factor is 1.5 – 0.5 * 2^(-kPscale/12.0), for the rest of values (0 to 12) is 2^(kPscale/12.0).\n\nAfter scaling, there is a modulation wave with an amplitude and frequency given by the user that is summed to the result pitch scale factor.\n\nThis resulting modulated factor is included in the pvscale opcode that is used together with pvsanal and pvsynth  as described here.\n\nAfter this process, the pitch amount parameter is used to weight the quantity of the signal effect that contributes to the output. Also, a gain scaling is applied to avoid the clipping of the signal.\n\nReverb\n\nThe signal explained in the previous section is sending to the reverbsc opcode. This same signal is used for stereo processing. The feedback and cut off frequency for the low pass filter associated with this reverb opcode are given by the user.\nThe result is the generation of two signals for stereo. These signals are summed with the original signal and weighted according to the Dry/Wet parameter that allows to increase or decrease the presence of the reverb effect in the original signal.\n\nThe connections for this plugin are illustrated below. Note that some calculations are computed in one only box to avoid an overwhelming representation for this solution.\n\n\n    Effect Connections Diagram\n\n\nConclusion\n\nThe development of a digital instrument or effect is a challenging task because it needs a mental capacity to think in terms of a real-time system, that is, a process that is constantly running in time. This is easier after training your mind about the way of thinking solutions that runs in a loop, and it happens in Csound for audio.\n\nIt could be difficult at the beginning to think in this way, but it is important to know the basis of the platform that is being used and explore the tools that it provides in order to build interesting applications.\n\nI tried to implement these plugins as general-purpose components for a performance. Despite they have particular elements, I think they are useful for several compositions. In the case of the synthesizer, various type of waves can be used and shaped in a smooth or harsh way, or even deformed it completely by the frequency modulation feature. The effect also can be regulated according to the needs of the musician, both can be easily modified for live performance or a recording session.\n\nGet the Plugins\n\nYou can download these plugins for Windows here. (Copy the two folders in your VST folder). Users from other OS can use the .csd files from the Windows folder and export the plugin in Cabbage\n",
        "url": "/sound-programming/2020/11/03/pedropl-caminolamentos.html"
      },
    
      {
        "title": "Ondes Martenot's brother - Orange Cabbage",
        "author": "\n",
        "excerpt": "The evolution of Ondes Martenot.\n",
        "content": "Introduction:\n\nThe Ondes Martenot was an electronic instrument invented by Maurice Martenot in the 1920s. It was used by many 20th century composers such as Oliver Messiaen and Edgar Varèse. In popular culture, the English rock band Radiohead, J.P. Jeunet’s Amelie and many other Sci-Fi movies are still using it. Inspired by this invention, I have made a simple synthesiser with 3 variables: tone,vibrato and waveform. It is a SynthVST plugin and I made in both Cabbage and Csound. The eventual name of this project becomes - Orange Cabbage.\n\nAnalogue-Digital:\n\nThe analogue interface of an Ondes Martenot was originally a metal ring. Players slid up and down on the metal wire with their right index finger to create ‘theremin-like’ sweeps in tones (through oscillations in vacuum tubes). The later generations added moveable keys that allow players to create vibrato when the metal wire was wiggled. Martenot later invented a new wooden frame that featured a drawer containing a haptic-sensitive glass ‘lozenge’ to control the sound.\n\nIts amplification system is interesting. In the later development, it had 3 additional loudspeakers, one of which had the speaker cone replaced by a gong. Another had a resonance chamber laced with 12 tuned strings. These modified speakers would add a metallic timbre from the gong along with resonant, clinking tones from the strings. A question is how can I mimic this with Csound?\n\nThere are three controls in the interface.\n\n\n  \n    Tone:\nThe first knob ‘Tone’ allows players to add harmonic partials to the fundamental block, and in sequential order. I discover an interesting opcode called buzz that does the work.\n  \n  \n    Vibrato:\nThe second knob ‘Vibrato’ controls the “send” level to the chorus effect. It is the most doable, yet similar effect I found to mimic the haptic-sensitive glass feature on ondes martenot.\n  \n  \n    Waveform:\nThe last, but not the least important knob, ‘Waveform’ lets players select what type of waveform they like to use as a base. Earlier I mentioned the wooden drawer in the revised Ondes, but I haven’t mentioned what is inside it. It is a drawer with a group of switches controlling the timbre. These select sine, triangle, square and pulse waves, or pink noise. In this project, I prototyped sine waves, pulse waves and square waves.\n  \n\n\n\n\nFig 1: Brand and UI Guideline\n\n\n\n\nFig 2: Code structures\n\n\nVideo demo:\n\nThe following video is a demo to show how the synthesiser works.\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream\n    \n=======\n    \n&gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes\n    Video Demo\n\n\nNow, we can imagine that the pulse waves and square waves become rather dissonant as the ‘tone’ knob turns up. It becomes even more obvious in the upper octaves. Buzz seems to be built to handle sine waves. So it starts getting ‘mad’ when you ask it to add another pulse or square wave at the frequency. It does make some bass sounds, though, especially when you turn the ‘vibrato’ knob all the way up.\n\nLesson I have learned:\n\nIn this course, I have learned the basics about building a SynthVST plugin in Cabbage and Csound. The synthesiser is intended to mimic the sound of ondes martenot. However, without having the proper understanding and skills it was difficult to turn this idea into reality. It ended up to be a simple, cute orange monster. In the process of finding solutions, I have gained new understanding about waveforms, about frequency and amplitude in a more scientific way. In the meanwhile, I learned some interesting facts in terms of the infrastructures, architecture and components. This process of making has given me a lot of valuable science lessons that I was once concerned about.\n\nIn the future, I would like to explore ways to combine Csound in software engineering tools.\n\nReference\n\nhttps://forum.cabbageaudio.com/t/understandig-how-cabbage-csound-works/219\n\nhttps://www.wikiaudio.org/adsr-envelope/#:~:text=An%20ADSR%20envelope%20is%20a,often%20its%20loudness%20over%20time\n\nhttp://www.csounds.com/chapter1/\n\nhttps://gist.github.com/juniorxsound/33f75fc4dadde18942e561670c80f3f0\n\nhttps://www.musicworks.ca/featured-article/featured-article/ondes-martenot-making-new-waves\n",
        "url": "/sound-programming/2020/11/05/joni-DA-orangecabbage.html"
      },
    
      {
        "title": "Beethoven under Moonlight",
        "author": "\n",
        "excerpt": "Creating a CSound project that uses Frequency Modulation synthesis (carrier and modulating oscillator) and plays the first few bars of the Moonlight Sonata by Beethoven based on a score.\n",
        "content": "Beethoven under Moonlight using CSound\n\n\n\nIntroduction\n\nDuring the Digital Audio module we’ve been given a basic introduction to CSound, a domain-specific programming language for audio programming. In class, we covered the basics of this language: the syntax, structure, a few syntesis techniques, and effects based on delay, distorsion and filtering. For the individual projects we each wrote a functioning Csound program in the form of an audio generator and/or a processing unit for incoming audio.\n\nWith these freshly-acquired knowledge about CSound and the assignment instructions I designed my idea. Sound effects are fascinating to me, but still a bit of a mistery; on the other hand, I have quite some experience with acoustic instruments (especially pianos). Having said this, I decided to ease into the world of digital audio by writing a CSound program that would imitate as closely as possible the sound of an acoustic instrument while “performing” a classical piece of music. For me, this was a theoretical assignment as much as it was practical.\n\nIn 2020 people all around the globe are celebrating the 250th anniversary of the birth of Ludwig van Beethoven (1770-1827). During the years I performed classical music, I played several pieces composed by him for piano and I always found them extremely… fullfilling to play and full of emotions. This, together with the special significance of this year inspired me to choose his “Moonlight Sonata” for the present assignment.\n\nFun fact:\nThis sonata was completed in 1801 and dedicated in 1802 to Countess Giulietta Guicciardi, his pupil. Its original name is The Piano Sonata No. 14 in C-sharp minor, marked Quasi una fantasia, Op. 27, No. 2. The popular name of Moonlight Sonata dates back to the remark of a musical critic sometime after Beethoven’s death.\n\nPreparations &amp; Resources\n\nThe principal resource of this assignment was CSound. I tried several methods of running it, for example the frontend CSoundQT with which CSound comes when installed, Cabbage and Winxound. Although I managed to make Cabbage work on my computer, it crashed quite a lot which got frustrating, so I ended up using a Visual Studio Code extension.\n\nCSound has a pretty consistent manual, which proved to be an important resource. In order to understand more about this language programming and digital audio in general, I had to start with the basics and read about sounds, waveforms, pitch, frequencies and other realted concepts. This proved to be very useful for the next steps: understanding the basic mechanisms of sounds allowed a smoother transition to digital.\n\nProgram development\n\nTheoretical background\n\nIn 1971 he wrote an article about stimulating real instruments.\n\nWhile at Stanford University, John M. Chowning, an American composer, musician and researcher worked on finding ways to stimulate real instruments digitally. He developed something called Frequency Modulation synthesis (FM synthesis) which is a way of synthesising sound based on its frequency and amplitude: the frequency of a waveform is changed using a modulator such that the frequency of an oscillator is altered “in accordance with the amplitude of a modulating signal” (Chowning, 1973).\n\nUsing this synthesis method, both harmonic and inharmonic sounds can be created - for the former, the modulating signal has to be harmonically related to the original “carrier signal” (i.e., the sound grows more complex the more the amount of frequency modulation increases), while for the latter, modulators with frequencies that are not integer multiples of the “carrier signal” have to be used.\n\nImplementing FM synthesis through analog oscillators can result in pitch instability, so the digital implementation quickly became standard practice. This is how digital FM synthesis began to be the basis of digital instruments starting from 1974. It was implemented using phase modulation(a form of angle modulation).\n\nCSound implementation steps\n\nImplementing a FM synthesis in CSound proved to be doable after understanding the concepts and the formulas. Below is a step by step explanation that leads to the final program (based on an example found online).\n\nFirst step:\n\n\n  Creating an instrument inside &lt;CsInstruments&gt; that plays sine tones.\n  Creating an oscillator that gets the amplitude, frequency and wave shape. The parameters are all determinted by p-fields in the score file &lt;CsScore&gt;. This is also playing the role of the carrier oscillator.\n\n\n\n\nSecond step:\n\n\n  Creating another oscillator, the modulating oscillator, that will modulate the frequency of the carrier oscillator by adding the output of this oscillator to the frequency of the other one.\n  The rate of the modulation (the vibrator rate) is determined by the frequency of the modulator, whereas the depth of the frequency deviation is determined by the amplitude of the modulator.\n  A subtle vibrato - deviation depth of about 1/5 of a semitone and a rate of 6Hz - in the carrier oscillator’s frequency is caused this way, by adding the low-aplitude, low-frequency sine wave as the modulator oscillator.\n\n\n\n\nThis far we’ve seen an example of FM (frequency modulation), more specifically using a LFO (low-frequency modulating oscillator) to create a slight vibrato. When the modulating oscillator has an audible frequency and larger amplitude, something else happens: sidebands (other frequencies present around the carrier frequency) are created.\n\nThis sidebands’ frequencies depend on the frequencies of the carrier and modulating oscillator: c+m, c-m, c+2m, c-2m, etc (where c stands for carrier and m for modulating oscillator’s frequencies). This relationship represents the ratio of carrier frequency to modulating frequency (Fcar/Fmod) = the harmonicity ratio.\nThe number of sidebands is determined by how strong the modulating oscillator’s amplitude relative to its frequency. The ratio Amod/Fmod = modulation index.\n\nThe harmonicity ratio and the modulation index are the main factors in determining the number, placement, and relative strength of the different partials of the resulting sound = the timbre.\n\nThird step:\n\n\n  Create a high-frequency modulating oscillator to create sidebands (in addition to the LFO for the vibrato). In order to control the timbre of each note, we specify the modulating index and the harmonicity ratio as separate p-fields in the score file.\n  Doing the necessary calculations as described above, obtain the ratios needed for this new modulating oscillator (in the instrument design).\n  Harmonicity ratio = Fcar/Fmod =&gt; Fmod = Fcar/harmonicity ratio. Fcar is p5 and the harmonicity ratio is p7 =&gt; the frequency of the modulating oscillator = p5/p7.\n  Use an amplitude envelope: the linseg generator. This traces a series of line segments between some specified points - in this case the shape of the envelope is made to be directly dependent on the amplitude value given through p4. The same envelope is used to shape the amplitude of the modulating oscillator: the lounder a sound gets, the more partials is contains (the modulation deviation increases), giving a more realistic timbre.\n\n\n\n\nAlthough defining frequencies for the notes to be played is alright, it can get quite messy the more complex the score. Luckily, it’s fairly easy to use pitch notations, as CSound has a function called cpspch() to translate pitch to cycles per second. Pitch = actave plus a number of semitones: e.g. A above middle C = middle C plus 9 semitones = cpspch(8.09).\n\nForth (and last) step:\n\n\n  Adding a variable ifreq in the instrument design that will save the carrier frequency based on the pitch notation in the score =&gt; replacing p5 with ifreq.\n\n\n\n\nCSound final score and demo\n\nHere is the final CSound program. Using a Tempo and Note Length Calculator I calculated the durations of each note based on a BMP = 51 (Largo). Knowing the frequencies of notes was also useful at different stages of this project.\n\nHere is a demo video of the final “performance” of the first few bars of “Moonlight Sonata” by Beethoven using a slightly peculiar sound - somewhere between a marimba and a vibraphone.\n\n\n  \n    \n  \n  CSound Moonlight Sonata\n\n\nOverall impressions\n\nIt was a nice experience to read so much about oscillators and ways to module sound and creating the score was really fun, even if a bit tedious as well.\nThis assignment helped me understand more about the basis of CSound. Before this, I knew nothing about sound modulation and how we can use the frequency and amplitude of sounds, so even the simple notion of creating an oscillator slightly confused me. I can now say that I familiarized myself with the concept of creating different types of oscillators and I know how to use the “interface” of CSound (e.g. I know how to create an instrument and how to controll it from the score). I even got the chance to use the manual when looking for linseg.\nHowever, for me, the most interesting part was to read the theory behind these techniques, and finally understand (at least a few of) the concepts behind digital audio.\n\n\n\n\n\nReferences\n\nChowning, J. (1973). “The Synthesis of Complex Audio Spectra by Means of Frequency Modulation” (PDF). Journal of the Audio Engineering Society. 21 (7).\n",
        "url": "/sound-programming/2020/11/05/alenacl-beethoven-under-moonlight.html"
      },
    
      {
        "title": "Phade - a phaser delay FX",
        "author": "\n",
        "excerpt": "A swirly, shimmery shoegazey multi-fx.\n",
        "content": "Introduction\n\nFor my Digital Audio project, I decided to build a multi-effect. Initially, it was going to be a delay with lots of processing on the delays repeats - added chorus, pitch instability, degrading of the repeats, etc. As things progressed, I decided to separate out the effects to give the user a bit more control. I have always been interested in the shoegaze and dreampop genres, so it made sense to combine as many types of modulation and delay as was humanly possible in order to create that swirling wash of sound. And so Phade was born.\n\n\n\nFig 1: Phade\n\n\nThe main signal is split into three, and fed in parallel into a pitch shifter, a delay and a chorus.\n\nPitch shifting\nThe pitch shifter is implemented using pvsanal to convert the signal into the frequency-domain, upon which we carry out the necessary pitch shifting using pvscale. The signal is then transformed back to the time domain using pvsynth, and fed into a huge reverb. I do this twice - once where I pitch the signal up an octave, and again to pitch down an octave. The resultant signal could be quite rich, so I ended up putting it through a high-pass filter to clean it up a little.\n\nThe delay\nThe delay takes a mono signal, and passes the repeats through a phaser. I also used some filtering to clean things up a little. Providing sliders to control the delay time and feedback proved tricky at first - delayr and vdelay like to have their variables set during initialisation, so I had to tap the delay using deltapi in order to be able to adjust the parameters in real time.\n\nThe chorus\nFor the chorus I used two vdelays controlled by two different LFOs set at different rates for a nice wide stereo effect.\n\nFinally, the outputs from all three sections were mixed together, and sliders were added to control the relative levels of each.\n\nThe following diagram shows the signal chain.\n\n\n\nFig 2: Signal chain\n\n\nSome examples\nAll examples start with a dry sound, and then different elements of the FX are introduced gradually.\n\nThe first is a mellow electric piano going through Phade. I bring up the delay, then chorus, the lower octave of the pitch shifter, before finally bringing in the higher octave. An additional lead synth, also going through Phade, is introduced as the piece progresses.\n\n\n  \n    \n    Alternate Text\n  \n  \n\n\nNext we have a guitar loop that’s goes from dry to completely saturated in effects, allowing you to hear the differences that each section of the FX introduces.\n\n\n  \n    \n    Alternate Text\n  \n  \n\n\nFinally, here is a short video of Phade in action.\n\n\n    \n    Phade in action\n\n\nFuture opportunities\nAs to future developments, I would like to play with the pitch shifter and reverb order, so that the reverbs signal is fed into the pitch shifter, giving a shimmer-reverb type effect. Secondly, the phaser didn’t add the bite that I was hoping it would. Rolling my own using an all-pass filter could be an interesting next project.\n\nDownload Phade VST\nYou can download Phade VST here\n",
        "url": "/sound-programming/2020/11/05/stephedg-dafx.html"
      },
    
      {
        "title": "Spaty Synthy",
        "author": "\n",
        "excerpt": "Attempt at modelling a Delay Repeating Spatial Synthesizer using Csound\n",
        "content": "Introduction\n\nIntroducing you to an attempt at creating a phasor modulation based Delay-Spatial synthesizer VST Plugin implemented in Csound and Cabbage.\nThe synthesizer is tested out as a plugin synth on Ableton Live DAW, I’ve used two MIDI controllers, one to control the parameters and the other to play the notes.\n\nBelow is the demo of the VST and its development.\n\nDemonstration\n\n\n\nDemo Video\n\nHow it looks like\n\nThe synthesizer has an oscillator  which generates 4 types of waves Sine, Saw, Square, Pulse where each can be selected from the drop down combo box.\n\n\n\nThe oscillator signal is routed to ADSR Envelope(Attack, Delay, Sustain, Release).\nI mapped the MIDI controls to Attack and Sustain. The sustain values can go really high for as long as 5 seconds making the notes repeat for longer times.\nIt is then routed to the Butterworth band pass filter which has parameters of centre frequency and bandwidth, which are also mapped to midi controller knobs.\nNext in the chain is the phaser modulated signal used as the delay or repeater envelope, it sounds similar to an arpeggiator but with the same notes.\n\nThe output of all these envelopes is fed into the spatial effect which has a numerous parameters, but I have decided to keep only one controllable variable which is the degree of rate of stereo change between the left and right.\n\nFinally, a Reverb is added at the end of it all.\n\nWhat’s inside\n\nAs a Whole, the sound generator and the effects are bundled into one instrument in Csound using opcode ‘instr’ which has all the effects and the oscillator in it. The oscillator opcode used here is the ‘poscil’ which is a high precision oscillator.\n‘madsr’ is the opcode used as the ADSR Envelope described above which is multiplied with the band pass filter signal with opcode ‘butterbp’.\n\nThe phasor signal opcode ‘phasor’ acts as the modulating wave to generate a delay signal or repeating signal, with a particular rate assigned to a knob which the user can change.\n\nFor the logic of spatial sound, the opcode ‘spat3d’ is used which allows positions of the input sound in a 3D space, with optional simulation of room acoustics, in various output formats, spat3d allows moving the sound at k-rate, the parameters involved in this are X, Y, Z axes, the distance from sound source to microphones and other room size parameters.\nI have nulled out the Z axes for intensive purposes.\n\nThoughts and Conclusion\nThe conception of this synth idea is from the basic knowledge of Csound and Cabbage, and I would say that this VST is still under development and there’s definitely going to be an advanced version of this later.\nI may not have reached the goal which I had in my mind about the output in terms of sound and design but this VST is a good basic synth for a space kind of sounds which can be a compliment for scoring or adding a bit of spatial touch to your compositions or mixes.\nI am still exploring the sound textures from it, however it overloads my PC, probably due to the inefficient code or just too many effects routed all in one block.\nCsound is a very good tool which is particularly functional to my knowledge of Audio programming compilers.\n\nHere’s the files to the VST and the CSD file (make sure you place both the .csd and .dll files in the same folder with the same name)\n\nClick to Download!\n",
        "url": "/sound-programming/2020/11/06/lindsay-spatialsynth.html"
      },
    
      {
        "title": "A fake Steinway made by Csound",
        "author": "\n",
        "excerpt": "How to use Csound to create a digital Steinway?\n",
        "content": "Introduction\n\n\n\nThis VST plugins is a sound synthesizer for simulating real acoustic piano in CSound environment. I was inspired to make this synthesizer by the SMC entrepreneurship course where Group C was trying to develop a remote music therapy App, and the technical core of this app was playing with music using a built-in lightweight sound synthesizer. Therefore, I tried to use Csound to simulate a real instrument in a way that would use the least amount of resources.\n\nBelow is my Demo:\nDemo\n\n\n    \n    Demo with Scale\n\n\nMethod\n\nAfter referring the works of others, reading some papers and CSound documentation, I decided to use some real piano sound files as samples to reconstruct the sound. This was done for two main reasons: First, CSound has excellent tools for analyzing and reconstructing FFTs to construct very realistic sounds. Second, instead of requiring 88 samples, this method requires less than 10 samples to build, which is enough lightweight to implement in an App.\n\nIn more detail, I used Pvanal to analyze the sample files from C4 to B4 in the piano to get the .pvx file, and then used the Pvoc opcode to restore the tone. The specific method is that through the IF Statement, all the C note except C4 are reshaped through FFT files according to C4, all the D note are reshaped according to D4, and so on, thus reshaping the entire piano sound range. However, the FFT reconstruction is rather stilted and not realistic enough in the release part of the piano sound, probably due to the fact that real pianos have echo sound from piano cavity reflections, and in order to mimic the residual sound of the piano cavity, I used the delay to add a brief echo effect to make the Release part of the piano sound more realistic.\n\nIn addition to the sound timbre, the sound image of a real piano need to be imitated as well, otherwise the piano will change from a surface sound source to a point source. After some guidance from teacher, I chose to use notnum to get the midi note number input from the midi keyboard, and through a simple calculation, I controlled the pan range from 0.1 to 0.9, arranged linearly from left to right in order to make it more realistic.\n\n\nLast but not least, In order to give the VST plugins more plasticity and independence, I used Cabbage to add interactive and adjustable reverb and delay plug-ins, using opcodes of reverbsc and deltapi controlled by slides in Cabbage, which can shape the sense of spatial for the synthesizer piano, more closely resembling a real piano in its natural state.\n\nReflections\n\nAll in all, the sound of fake “Steinway” is not as smooth as I expected at the beginning. Obviously the sound is more realistic around the sampling points (C4-B4), but the further away from the sampled source the higher the distortion of the sound is, especially in the high and low regions of the piano. It still far away from a real Steinway.\n\nAfter discussing this with instructor, I learned that using Sprintf to match the sample file to the keyboard without limiting the file size is definitely a more mature solution. If it sill need to use partial samples, pvscale also seems to be a better solution than Pvoc, because it fixes the harmonic ratio and makes the sound more realistic. Also, I have read in some papers that synthesis piano can also be modeled using algorithms to simulate piano hammers hitting string vibrations to make the piano sound more realistic, and I think this is a great direction I could improve on in the future.\n",
        "url": "/sound-programming/2020/11/06/wenbo-fake-piano.html"
      },
    
      {
        "title": "Butter: a multi-effect plugin",
        "author": "\n",
        "excerpt": "An easy to use and fun plugin to make your sound smooth as butter.\n",
        "content": "Butter: A tool to make your sound smooth as butter.\n\nIntroduction:\n\nSince the time I got introduced to the world of DAW’s and producing music electronically in general, a part of me was always fascinated by behind the scenes of the tool that I used, to create something palatable to the ears on the blank canvas of my DAW. I wondered how would the song transfer from one effect to the other or how are these sine waves and square waves getting generated? Coming from an engineering background the signal flow of the sound behind those easy to manipulate looking knobs was always at the back of my mind, while the front end of it was busy creating creative things.\n\nThe short module on Digital Audio was a nice introduction to the behind the scenes I always wanted to know about. So I took this opportunity to peak into the scenes and create a tool of my own, which had all the effects that I love and have easy to manipulate knobs, and after a lot of trial and error, plus with a little bit of help, I made Butter\n\nBUTTER is a Multi-effect plugin which intends to make your sound better and smooth. It has four effects in it, Distortion, Chorus, Reverb &amp; Filter. Each effect can be used individually as well as in combination with each other.\n\n\n\nProcessing\n\nThe signal flow between the opcodes and processing of the audio is as follows:\n\n\n\nThe input dry signal goes to the effects one by one and gets processed according to the parameter set by the users. The opcodes used here are Clip for Distortion, vDelay for Chorus &amp; Reverbsc for Reverb. Clip here clips the input signal upto a predefined limit in a soft manner giving a nice warm Distortion, vDelay is an interpolation time delay function, it basically delays the sound with a longer delay time and a very short feedback to make the effect of Chorus. Reverbsc is my favourite opcode among all, its based on the Valhalla reverbs created by Sean Costello, and the user is manipulating the size of the reverb by the slider.\n\nThe output from all these opcodes gets added and is then passed to the Filter effect which uses moogladder. Moogladder opcode is based on the architecture of the famous Moog synthesizers. The ouput from the filter is then sent as the final output from the plugin.\n\nDemo:\n\n\n   \n   Butter in Performance\n\n\nFinal thoughts &amp; Future Developments:\n\nIt’s a fun plugin and can have multiple use cases. Since this is just the first version of the plugin there is a lot of room for improvement. Nevertheless I believe it has big potential. I am not fully satisfied with Distortion &amp; Chorus, but its okay for now considering the short duration of the course. I will definitely work on them along with perfecting the gain staging and smooth signal flow.\n\nFor future developments I believe it could be great to integrate this with a synth or maybe even develop it as a pedal which has more controls, enabling me to create multiple ready to use presets, that butters the audio with very less tweaking. You can download the latest version of Butter down below and drop me a E-mail if you have any feedback. Enjoy!\n\nDownload it here:\nButter\n\nNote: To run the plugin keep all files in folder in your plugin directory i.e. the plugin file &amp; .csd file should be in the same folder, it’s essential for smooth running otherwise it will crash.\n",
        "url": "/sound-programming/2020/11/06/abhishec-butter.html"
      },
    
      {
        "title": "Everything's Out of Tune (And Nothing Is)",
        "author": "\n",
        "excerpt": "Who says that Csound and early music can’t mix? Building a VSTi for swappable historical temperaments.\n",
        "content": "Tuning Up\n\nWhen thinking about interactions between music, communication, and technology, I am usually starting from a much different place than most other students in the SMC program. I respect, fear, and expect to learn much more about coding and all the technology that makes the world go round, but my ultimate goals lean toward preservation, education, and transmission of older, decidedly lower-tech music.\n\nWith this in mind, our recent shallow dive (that’s what I’ll call a deep dive that’s done in just a few packed hours) into the world of Csound presented me with an opportunity to create a nifty little tool for explorers of early western music. I developed a VSTi for use in Reaper that allows the user to toggle his or her midi keyboard between a handful of the different tuning systems commonly used in Europe within the last 500 years.\n\nAn Extremely Oversimplified History of Tuning\n\nEqual temperament, the division of the octave into 12 equal parts to produce the chromatic scale is very convenient. People can play things in any key and have them sound ‘correct,’ making transposition and movement of music materials quite simple. However, the dirty little secret of equal temperament is that it is achieved by pushing every single note slightly out of tune. Our oversaturated ears are very used to hearing this tuning system, but, when examined more closely, it really ensures that every interval (except for the octave) has a small amount of rhythmic beating.\n\nBefore equal temperament became widespread (some semblance of it has likely always been around in less codified forms) music theorists and performers of different eras utilized a variety of different tuning systems. Renaissance music typically remained in keys with fewer sharps and flats, allowing for tuning systems in which certain chords sounded delightful while others sounded horrendous. The same applied somewhat to Baroque music, however, as harmonies grew more complex, the rougher qualities of far-flung keys likely actually became a compositional and expressive tool that keyboard players could exploit. Today, bowed string players, singers, and other instrumentalists who can adjust intonation while playing may color notes in pursuit of purer intervals, but the constant presence of keyboard and fretted string instruments really drives everything toward equal temperament.\n\nFor the purposes of this blog post (since as much as I’d love to go full tuning nerd, the assignment is to write about Csound and not tuning histories) let’s stick with an oversimplified outline of tuning systems. Tuning approaches moved from concepts like Pythagoras’ system (developed in the 6th century BC and based on pure intervals from the harmonic series) toward systems like just intonation and meantone temperament (based on sequences of pure or almost pure intervals) before settling (from roughly the tail end of 18th century onward) into today’s general adherence to equal temperament.\n\nThe VSTi\n\n\n    Temperament Toggling\n\n\nSince I currently still know far more about tuning systems than I do about Csound, it is fair to say that my VSTi is a fairly rudimentary one. It allows a user equipped with a midi keyboard to select equal temperament, just intonation, meantone temperament, Pythagorean tuning, or a mixture of any or all of these tuning systems for use within Reaper. As would be likely on a historical keyboard, the tuning system begins on C, an important distinction in these non-symmetrical temperaments.\n\nI used Cabbage to assemble a series of if statements pertaining to each note in the two-octave compass of my midi controller keyboard. By simply giving the proper ratios to provide the correct adjustments away from equal temperament for each note in each tuning system and then assigning each system to a toggle button in the interface, it was possible to construct a basic but serviceable VSTi with a clickable interface. Currently, the VSTi has only been tested in Reaper, but I see no reason why it wouldn’t work in other similar DAWs.\n\n\n   \n\n\nFor my purposes, this little VSTi is useful for exploring early keyboard and plucked string music. A desire to hear different temperaments in musical contexts isn’t convincing enough to drive most keyboard players to retune their instruments every five minutes, however being able to toggle between them makes it much easier to consider how tuning decisions could affect a piece or repertoire. Basically, it’s a fun toy for making nerdy activities even nerdier.\n\nWatch the video below for a taste of the different temperaments in action.\n\n\n  \n    \n    Alternate Text\n  \n  Playing example chords and excerpts in different temperaments.\n\n",
        "url": "/sound-programming/2020/11/06/short-williakm-vst.html"
      },
    
      {
        "title": "SYNTHa",
        "author": "\n",
        "excerpt": "While this is not a clone of the classic EMS Synthi A, it might have a trick or two up it’s sliders, this one to: A softsynth with hard features.\n",
        "content": "Trying out Csound for the first time, was both fun and frustrating. Actually I have previously looked at Csound, but at that time I quickly realized I’d rather make noise the old school way (even though Csound might be consider pretty old school). So I was a little excited on how this should go.\n\nThe coding itself was pretty understandable to me, the logic feels more, well, logic to me than other coding I’ve seen. But there were some bumps in the road. I had, and still have, too many issues with the front ends and what-not, and Cabbage, as I’ve been using to make this softsynth, needs to restart every time I have runned the code and used my MIDI-keyboard. Very annoying, of course, but at least it’s rebooting quickly.\n\nI started out very ambitiously, and planned to make cool plugins for Logic and other DAWs. A sound on sound-looper with degrading loops was my first thought, because I want one after selling my Strymon Volante. But I quickly found out I’d rather make a softsynth, since I even didn’t get the filepath to my testloop right.\n\nSYNTHa\nSo SYNTHa started coming to life. The name is of course inspired from the classic EMS Synthi A, but other than that, there are little resemblance between them.\n\n   \n   SYNTHa (top left) vs Synthi A (the rest).\n\n\nSYNTHa is built using vco2 for the saw and the square, and like many cool analog synths, it’s got a pulse width modulation (PWM). I had many issues with the PWM, first I spent some time trying to figure out how to get the PW-slider to move along with the modulation, and I kind of got the idea how to it, but I thought I’d spend the time on the synth itself first. But the main issues with the PW and PWM is that I could not get rid of the click in the modulation, even though I scaled the PW to not go into flatline.\nBut anyhow … these waveform are feed the moogladder-filter, and then into the ADSR-section (with up to 180 seconds releasetime, no need to rush it …). Before the master this is fed into the reverb, where you can control the amount (Reverb-knob) and the frequency of the reverb (Dampen-knob).\n\nHardware\nI’m still a hardcore hardware-guy, so I always think in terms of how this synth could be fun as a hardware-synth, sitting on my desk. And one thing I’m a big fan of, is inputs no hardware-synths. Like a vocoder, like the input on my Moog Voyager as well as the input on my TR-8 drum machine. There are always some exciting possibilities when you can put an instrument into another.\nSo I equipped SYNTHa with two inputs, channel 1 for a microphone, and channel 2 for a guitar. So the pitch of the incoming signal is turned into control signal, using the pitchamdf-opcode. And I have used this to play a synth-tone, ranging from saw- to triangle- to ramp-wave, using the vco2 again.\n\nReverbera-a-a-ti-o-o-o-o-o-o-o-on\nIn addition I use the pitch of my voice to control the amount of reverb, when I activate the VOICEctrl. This function runs even with no volume on the VOIsynth, and I also only run the normal part of the synth through the reverb, to keep things somewhat in line. But as you might hear if—you’re patient enough—in this way to long demovideo, the reverb gets a little to eager sometimes, even when scaling it down to 95%. But hey, what’s music without any noise and hiss?\n\n\n   \n   No rush, huh?!\n\n\nHaving my hardware oriented brain, I didn’t spend time getting this to work properly inside Logic with the inputs, only the supersoftsynth-part of it. So I’ve been using it for live-playing only, and filmed this session using Quicktime Player, and Photo Booth, to also show what happened outside the computer.\n\nTo be continued\nTo conclude this very brief introduction course we’ve had in digital audio: it has been very rewarding and interesting in my point of view, and I will spend a lot more time trying to learn more csounding, as it seems to me there are no limits in what I can do with it (when I learn it), and this is a way to actually experiment with soundrouting, without having to sell my house to buy the hardware.\n",
        "url": "/sound-programming/2020/11/06/anderlid-SYNTHa.html"
      },
    
      {
        "title": "Zoom + Ambisonics",
        "author": "\n",
        "excerpt": "Talking together in an online room is an interesting topic, as the mono summed communication in regular Zoom can be tiring when meeting for several hours a day. Could ambisonics in digital communication be the solution we’re all waiting for?\n",
        "content": "The dynamics of talking together in an online room make for an interesting topic, as the mono summed communication in regular Zoom can be tiring when meeting for several hours a day. Comparatively, playing around with HighFidelity on the first day of SMC 4021 was a real eye-opener. Suddenly it was way easier to follow who-said-what and the feeling of organic conversation was immediately more natural. I (Henrik) have been testing this with different friends the last months, and the feedback has been more or less the same - that this way of communicating feels more natural than the traditional Zoom/Skype/Teams.\n\nIn light of these and similar technologies, Team B thinks that there is huge potential for reviving the effectiveness of online communication with easy steps toward more humanizing. It’s easier and more natural to be in the immersive field of ambisonics, and also there is a level of “togetherness” in having this common room feeling. It’s obviously how we would talk if we were in the same room, and we think that adding acoustics to the digital platform will make it more user-friendly for people now working long online hours.\n\nBefore a recent SMC4021 class we experimented with using BlackHole for routing sound. We then had success with routing sound from Zoom into Reaper for ambisonics, but less success doing it the other way around. The culprit of this difference appeared to be Zoom’s inability to deal with the BlackHole sound driver consistently. We’re not sure why they made it like this, but “it’s just the way it is”, as Tupac once said. Nevertheless, routing this way with aggregate (for Mac) turned out to work fine for Team B at least. Combining the audio interface with ZoomAudioDevice enables us to send the stereo signal from Reaper where the ambisonic magic happens with the IEM plugins. Using binaural decoding of course. But sadly this wasn’t going be a ready solution for everyone in the class of 2020.\n\n\n    \n\n\nThe class experience of getting a dozen users, roughly ⅔ Mac users and ⅓ Windows users all up and running on the stereo, manipulable setup we’d been hoping for was not particularly smooth. Even some users running identical Catalina operating systems to those used by successful participants were unable to efficiently get everything working. It seems that in an ideal world this approach (creating an aggregate device and routing through reaper to zoom) would be a strong improvement on the current reality of zoom meetings, however if the first 45 minutes of each meeting needs to be fighting with technology it is unlikely that anyone would enjoy this. Our class success rate over 90 minutes was around 45%.\n\nIn the case of a Windows machine, there were some issues to solve. Pedro used ASIO4ALL as the driver for Reaper and the virtual cable VB for routing. This ASIO driver steals the device from other programs, that is, it is used exclusively for the platform it is working on. The configuration is a little tricky regarding the driver since the inputs that are used for the DAW must not be used for Zoom and vice versa, and yet problems persist. Sometimes  Zoom or Reaper has to be restarted, other times Zoom works just by deactivating and activating the Mic, so there is not a fail-safe way to claim that everything will work on Windows. Other options to be explored would be using different drivers than ASIO4ALL or even the audio interface driver and performing some further tests by exchanging devices in Zoom and Reaper.\n\nAfter talking for a while with the room function, we found that there wasn’t a significant difference from regular talking. It had some effect, but it wasn’t immediately revolutionizing the way we talked. It’s still a static and locked environment, and we got used to it quite fast. When we got used to it, we actually somewhat forgot about it and reverted to almost the same level of engagement as with the flat audio of a typical Zoom meeting.\n\nFor wider use there is also the problem that people at home offices don’t necessarily use headphones. Long work days with headphones are not very comfortable. And not everyone likes to wear headphones. And they might not have speakers. Which leaves us with the built in computer speaker. This will, for obvious reasons, make it hard to get any impression of an immersive online space.\n\nAn issue with online meetings can be the high bar for getting involved in a discussion. When meeting participants are ‘stepping upon’ one another aurally often only the most persistent speakers will be able to make their points heard. This is a key area in which a less flat digital meeting space might be an improvement for meeting effectiveness. When mimicking a room, could this affect our behaviour on the digital platform? Would we be able to feel more comfortable taking part in discussions, and could discussions be more lively? Lively in the sense of dynamic discussions where other participants react instantly with short comments, or confirmations and objections, while the main speaker is talking. With the room mapping of sound, these short comments don’t feel as interrupting as they do on the traditional mono-platforms, as participants can easily navigate from where and from whom the comments are being delivered. This is a natural part of our way of talking and communicating together, so neglecting it with muted microphones might be a bigger online-social handicap than what we initially think.\n",
        "url": "/networked-music/2020/11/10/henrikhs-teamb-zoom-ambisonics.html"
      },
    
      {
        "title": "Ambisonic as a ‘mental’ management tool in Zoom?",
        "author": "\n",
        "excerpt": "An immersive audio as a tool to keep our state of mind peaceful.\n",
        "content": "Ever since the start of the Covid pandemic began affecting  to everyone’s life and the society at large, Zoom, or other online communication tools such as Teams have become a part of our lives. No matter if we are having business meetings or communicating with families and friends abroad, these online communication tools bridge the gap in ways that the old-fashioned communication methods such as the phone and email couldn’t do. Despite those methods being quite good at helping us communicae, they did not do so in such a complete way as Zoom or Teams.\n\nSince March 2020, we all face some kind of difficulties in different aspects in our lives. It could be the lockdowns causes different types of transportation frustrations. Now most of us need to meet business partners, colleagues or families in the digital realm. WIth the restrictions of social distancing, nearly everything else become physically-impossible.\n\nZoom or other tools are functional in most senses, from seeing the faces of your co-workers to listening to stereo quality audio. However, in terms of the psychological needs of users, this is the aspect Team C thinks  that Zoom can be improved and become more accessible in the future.\n\nEveryone has a life to deal with, no matter how big or small the issue is. However, meeting online with others can be stressful for some. Whether you are an introvert or a social butterfly, meeting others digitally can drain our energies much faster than physical meeting does. According to recent research, Zoom meetings are taxing our brain and many users suffer ‘Zoom fatigue’.\n\nWho wants to sit in front of the computer and communicate all the time that way? Especially with people you never met in real life? In a working environment, for example, negative emotions of colleagues are sometimes inevitable, but we are not responsible for others’ emotional baggage if we have a lot on our plates. That is also what psychologists are here for. However, sometimes, people cannot discern the differences situationally. Some might let their negativity spread in the Zoom room. Unfortunately the audio is completely one-dimensional and flat. If someone takes over and does all the talking, everyone has to listen to that voice, while other voices in the room become harder to hear. In such a situation, it is even harder to make the Zoom meeting experience enjoyable, and we experience more mentally stress.\n\nSo, how can we keep our sanity during an important meeting and not to let others affect our peaceful state of mind? How can we focus on what matters the most while putting those noises aside? Zoom has no function for this particular issue. It is still not personable enough for users. We are drivers of our own lives, technology is supposed to help not make us feel worse.\n\nThis is where Ambisonic can be the potential to solve this problem.\n\nDuring our SMC 4021 first session, we explored High Fidelity. It is an online immersive platform that allows us to socialise with 3D spatial sound and explore the location of others through auditory experience. The idea behind it is to illustrate the real-life situation, where the further the person stands from the microphone, the lower the volume of their voice. If someone stands right next to you, for sure, their voice is clearer and audible to you. If it is on the left? Or on the right. Ambisonic allows us to explore the sound distances in a digital environment. It has a huge potential to create a better user experience that feels ‘real’. For Virtual Reality(VR) is focusing majority on visual and spatial dimensions, why don’t we focus on audio as well?\n\nThe future possibilities for implementing ambisonic (3D spatial audio) in Zoom or other online communication tools are enormous. In the meanwhile, if developed carefully, it could be a tool that nurtures our psychological needs.\n\nDuring the ambisonic workshop we had tried to set up the aggregated devices and different settings in Reaper, Zoom and audio midi. This section aimed to test out if that kind of ‘room’ mode has positive future potential for better user experience in Zoom meetings.\n\nSMC4021 Ambisonic workshops:\n\nIn the ambisonic session with 13 participants, approximately  ⅔ Mac users and ⅓ Windows users. We followed an instruction video given by the tutors. We ran on the stereo and configured setups without fully reaching our expectations for the session. It seems like the ideal future of Zoom meeting to implement ambisonic audio is merging aggregated devices and routing through Reaper to Zoom. The setup successful rates were around 50%. In Team C, we discovered the reason why the setting didn’t work after class. In our personal Zoom account, under the audio section, there is an option for us to enable, this is where we can allow the Reaper and aggregate device settings to perform in our Zoom.\n\nImpression:\n\nIt could be an interesting idea and is fun, however, what is its practical application?. Imagine if we are in a serious business meeting, why would we need such an effect? Why do we need to mess around in the 3D spatial audio? Who has the time to do this unless they are interested in the sound field?\n\n((I((Joni here)) think the reason it is useful is because it lets the mind use less effort in determining the location of people in the virtual space. I imagine that having six floating voices is not a natural condition whereas we have cognitive systems evolved to place people in the space we share. Mobile ‘phone users in cars use too much brain power while talking compared to drivers chatting with a person in the car)).\n\nPurpose of this:\n\nSo, if we create meaning with this feature, it becomes a possibility for our mental health well-being. As we said earlier, it could be a tool for users to control the audio input of colleagues in a large-scale meeting.\n\nPotential Jamulus supplement?\n\nIn the 2 Jamulus sessions we had in the SMC 4021, we also tried to combine with Zoom. We tried to test out how we could make music-making online more real as in the physical world. However, during the Team C sessions, the flute was too loud and drowned out the other instruments, despite the fact there were settings in the Jamulus server in which we could turn the volume down. The one-dimensional, flat audio outputs in our computer meant the experience was not so enjoyable. If Ambisonic applies in this situation, it allows us to control the distances of a sound source, it seems possible to enhance the future Jamulus-Zoom experience.\n\nHigh Fidelity on Zoom?\n\nIn the first session as we mentioned earlier, High Fidelity (HF) was a fun tool for us to socialise with sound. If this platform emerges in Zoom directly, we can envision our ‘main Zoom room’, where we see all our co-workers at once. With HF here, we can imagine to move or scale the size of the co-workers, in the meanwhile, scale their voices and volume around. It is possible to enhance the user experience. However, first and foremost, we need to test further with more users. It is rather subjective if we make any judgement here.\n\n\n    \n\n\nThe idea creating a virtual meeting table in the Zoom, it allows us to choose where we sit in the room. In the meanwhile, it gives flexibility that we can move the other participants around. The further we move the participant, the less we can hear their voice. This is an idea of mimicking the real-life meeting situation.\n\nFinally, ambisonic is possible to create meaningful experience in Zoom in terms of our psychological needs.\n",
        "url": "/networked-music/2020/11/14/team-c-ambisonic.html"
      },
    
      {
        "title": "Zoom here & Zoom there: Ambisonics",
        "author": "\n",
        "excerpt": "A more natural way of communicating online, wherein it feels like all the members are in the same room talking.\n",
        "content": "Zoom here &amp; Zoom there: Ambisonics\n\nEver since we started this digital semester as part of the SMC program, we knew we were in for a ride. With each of us sitting at our homes in different parts of the world, riding on the Zoom ferris wheel, resisting that urge to have snacks midway the meeting (sometimes giving up and having them), we kept trying to make the experience more immersive. With our first class in HighFidelity we tried to host ourselves in a more natural way, as it created a spatial map of our voices making it easier to follow each other. After trying multiple software to make the jamming experience more than just an exchange of audio, we tried to do a similar thing during the workshop in Ambisonics, using Zoom as the exchange platform.\n\nWorkshop Task:\n\nThe task seemed easy: integrate an ambisonic “effect” within Zoom, to simulate the organic way of having conversations. With the voice of each member coming from a different direction and fooling the HRTF into believing that our course mates are sitting right beside with their faces getting displayed in Zoom it was a pretty weird but an interesting experience. Put simply, the aim was to explore a more natural way of communicating online, wherein it feels like all the members are in the same room talking.\n\nSetup:\n\nAll members of Team A lie at the Windows end of the spectrum and spoiler alert: this side of the spectrum isn’t as smooth as it could (should?) - we had our fair share of fails and wins while setting up the Ambisonical Zoom.\nSo, let’s start detailing: to integrate ambisonics into the otherwise flat and mono sound of Zoom you need an external software which does this, i.e. maps the input audio into the spatial audio field  and then passes it to a binaural decoder and to Zoom via a routing driver.\n\nThe first part of this signal flow is achieved by using Reaper as the host DAW and IEM plugins, to manipulate the audio into an ambisonic sound. The input audio from the microphone is received in as an audio track in Reaper, and is processed by the RoomEncoder plugin. This audio is sent to the master track in Reaper which has the Binaural decoder by IEM loaded and then outputted to Zoom via the audio driver Virtual Cable by VB Audio. A scheme of the process can be seen below.\n\n\n\n\n\nDon’t forget to select Cable Output driver as the input device in Zoom’s audio settings.\n\n\n\nOur experience:\nWindows has always struggled with multiple software that use a single audio device. Since we all had different sound devices, a mix of internal and external sound cards, we faced different issues and ultimately were unable to get this system working between the three of us.  For example, for one of us, initialising the soundcard in Reaper would cause all Zoom audio to be lost, and this could not be rectified without a restart, making it difficult to communicate with other teammates while trying to get the system working.\n\nSome of our classmates kept their ambisonics settings activated when returning to the main communication room in Zoom, and, after several months of hearing only single channel/centred voices, it was a nice surprise to hear some of the voices from different directions. In real life it is never the case that the exact same sound waves hit both eardrums, even if the sound comes from straight in front or back of you. Because of this, the unnatural way of listening while in a Zoom meeting needs more focus, so, even slightly offset audio felt better and required less mental effort.\n\nConclusion:\nIt would be great to have this ambisonic feature as a built-in of Zoom, or as a standalone program that does all the processing for you.  Having to get the soundcard to play nice with Reaper, with the virtual audio device (VB-CABLE Virtual Audio Device) and with Zoom, as well as setting up the correct ambisonic routings in Reaper for the master tracks and input tracks is definitely not an user-friendly solution.  It would also be interesting to try other solutions like Røde’s soundfield VST, which requires less configuration and doesn’t need a DAW that supports multiple audio channels per track. Overall, it was a nice and fruitful experience that helped understand the theory behind ambisonics.\n",
        "url": "/networked-music/2020/11/14/team-a-zoom-ambisonics.html"
      },
    
      {
        "title": "Peace in Chaos: A Spatial Audio Composition",
        "author": "\n",
        "excerpt": "Our spatial audio composition highlighted the ways audio can represent both peaceful and chaotic environments and transitions in between.\n",
        "content": "A Composition\n\nThe SMC4044 course happened right in the middle of an increasing lockdown of the society due to the corona virus pandemic, and just the day before the workshop weeks began, the whole structure of the course was turned upside down. We were not supposed to be in the portal, then we were supposed to, but in smaller groups, and the requirements and due dates of the assignments were changed. In the middle of all of this we were supposed to collaborate on a huge spatial audio composition, putting our hands on the recording equipment (“but is it safe to touch it?”), but also follow the intensive workshop that really was a handful for some of us (4-5 hours in front of your laptop screen everyday), and learn how to use the DAW Reaper (the challenge occurs when you only have one screen on your laptop and you try to follow a course on Zoom, try it!).. To some of us, the whole situation was severely chaotic and confusing, and when some of us got sick (“This must be corona!”) on top of all of this: it wasn’t really making things easier.\n\nSo the main theme behind our composition was inspired by this stressful situation, and we decided that we wanted to explore the search for “peace and calmness in a chaotic world”.\n\nListen to our track\n\n    \n        \n    \n    Beginning with Thomas, then Jackson and finishing with Mari\n\n\nBeginning with Thomas\n\n\n    \n    Inside a practice room\n\n\nMy piece was done by doing recordings with the Soundfield SPS200 microphone in six different locations. Two indoors and four outdoors. Due to bad weather and lack of time the recordings where done around campus on Blindern. As a consequence of this the transitions between the different scenes are very subtle. In addition to the Soundfield recordings some of the guitar was recorded with an AKG-C414 microphone inside the Portal. The piece mainly consists of self-recorded material but contains some samples gathered from BBCs´incredible sound effect library.\n\nConcept\n\nIn line with our overarching concept and dramaturgy this part can by seen as an exploration of the concept of calmness from an introspective point of view. What are my own associations to the word and how do you capture and convey it in a convincing manner? Throughout the piece there is a sense of tension and inner turmoil portrayed by a static noise fading in and out. Repetition and cyclic motion is also a central part both in the musical and spatial output.\n\nAesthetic and technical choices\n\nThere are two main “attractions” throughout the piece. It´s a voice and a guitar. For the first part of the composition you can hear the guitar through the Soundfield microphone staying quietly in the background, but as the piece progress it fades in to the recording from the AKG. The AKG-recording has been treated with a RoomEncoder with automation lines that moves the sound around the listener. In addition to this there is also an mcfx convolver with an IR from a hall to make the source a bit more ambiguous. Which according to Barry Blesser could portray increased anxiety since it give an inability to localize the sound.\n\nSince most of the recordings were done on the Soundfield microphone the spatialization on those comes from the placement of the mic´s and some rotation to the recordings. The technical execution choices made are quite banal and straight-forward. Due to sickness during the process I only had one day to record and edit the composition. I would have liked to go deeper into the mixing stage and manipulation of sounds but there wasn´t time to that in this instance. So therefore this composition is more about the conceptual musical ideas rather than the spatial audio aspects.\n\nFinal thoughts\n\nThe technical parts of this course would of course be interesting to explore further. Investigate spatialization techniques on a deeper level. Working only with ambisonic microphone recordings is also a challenge in itself. It takes time to find interesting places to record and in this project that wasn´t possible for me. As a personal reflection on the topic of spatial audio in a musical context I guess that for me the interesting part of music is what the sound is, and not where a sound is placed in a virtual space or how it moves through space. So in hindsight if I were to do this again I wouldn´t have cared about trying to make some “music”  and instead just record some interesting soundscapes and find some godd samples I could manipulate.\n\nAn interlude by Jackson\n\nBeginning with the sound of a backpack unzipping, my section collects a number of ambisonic recordings with the Soundfield microphone and a trio of AKG mics. My piece features a series of transitions that are centered around the theme of the cinematic use of space, a concept that takes a filmatic approach to sound recording and composition (Roads, 2015). These transitions would often be impossible in reality and provoke confusion and resolve quite quickly from one moment to the next.\n\nThe sections of this piece are as follows:\n\n  A backpack unzipping\n  Elevator arriving\n  Drumsticks tapping across a room (inside a metal mug)\n  A drawer opening\n  Thomas playing guitar (with convolution)\n  The high end of many plastic bags\n  Footsteps ascending and descending a spiral staircase\n  An elevator rising and opening\n\n\nHere are some images of the various locations recorded:\n\n\n    \n        \n            \n            Top of the staircase\n        \n        \n            \n            Looking up\n        \n        \n            \n            Looking down\n        \n    \n    \n        \n            \n            Inside the portal\n        \n        \n            \n            The three AKGs used\n        \n        \n            \n            Mics placed in a closet\n        \n    \n\n\nWorking with three microphones instead of one allowed the sounds to become actual images (as opposed to single point mono sources). This provided a wonderful sense of depth when the three mono tracks were encoded into ambisonic space. Automating the movement of a group of these sounds created very dense, dynamic spaces like being inside a drawer as it’s opening or the eerie high end of plastic bags as they shift around one’s head. I was also lucky to have Thomas play a short melody for me that served as a great piece to test out matrix convolution using an impulse response from the building in which we have our classes. Recording Thomas’s guitar with three mics meant convolving each of those channels with the four channels recorded from the Soundfield mic. Those my impulse response was a simple clap, the end result nicely blended into the layered recordings of me running up and down the stairs.\n\nThough an explanation of every transformation would take too long for this post, many screenshots of the Reaper session can be seen in our presentation listed below.\n\nMari concludes\n\n\n    \n    The passing train\n\n\nMy part of the composition is a mix of ambisonic recordings that were done in Nationaltheatret Train Station together with Jackson. In the west entrance of Nationaltheatret train station, there is an acoustic sculpture. Some special kind of architecture in this room is creating a special “flutter” effect, and the reverberation time is remarkably long.\n\n “The acoustic sculpture”)\nMost of the recordings were done in this room, in addition to one recording at the train platform. The scene is starting with an announcement that was spoken from the speakers: “Take cautions because of the coronavirus, keep distance, and wear a face mask”. This announcement is kind of setting the mood for the whole scene; we’re in the middle of a chaotic virus apocalypse, it reminds us that it is not really safe to be out here these days. Then the staccato, mechanical beat slowly comes in, and an upgoing pitch resembles an engine that is starting up. The train is moving towards you , and the listener gets the feeling of being run down by the train. The train is then taking you on a meditative journey, with angelic voicing that appears as a contrast to the mechanic beat in the background. \n “Pitch automation in Reaper is converting the four channel ambisonics down to stereo format, and is not recommended if you want to keep the multi channel surround sound.”)\n\nIn reflection\n\nOf course Corona restrictions were the major limitations in our collaboration. And even with brief sickness in the group, we were able to persevere and create something pretty interesting and quite abstract. Working with ambisonics was certainly a learning experience in itself. It’s one thing to learn about the theory behind spatial audio and another to work with recordings and encoded samples. This was compounded by the fact that this was the first time that many of us were working with Reaper. Reaper, while being a fantastic tool that appears to be able to do just about anything in sound, is a bit clunky and obtuse to work with. With all of the sounds we were manipulating, the plugins we employed had to support multichannel processing (as to not disrupt the ambisonic encodings) which further restricted our post-production abilities. Routing was quite a mess as well (working with 16+ channels per track!) but enforced a clear understanding of the processing sequence.\n\nOur final presentation can be viewed below\n\n\n    \n    Our presentation on audio\n\n",
        "url": "/spatial-audio/2020/11/16/spatial-chaos.html"
      },
    
      {
        "title": "Music for dreams",
        "author": "\n",
        "excerpt": "We tried to make a spatial audio composition based around a dream\n",
        "content": "Composition\n\nConcept\n\nWe departed from the idea of making music for dreams. We all had to make individual contributons, i.e. separate parts of the composition. As we were four students in the group, we split the composition into ~ 1 minute long pieces, where three of them were different parts of a dream, and the fourth part was the part where our main character falls asleep and wakes up again. The division of labour went as follows:\n\n\n  Iggy was in charge of creating the introduction scene when the first person goes to sleep\n  Ulrik made the first dream scenario\n  Thibault made the second dream scenario\n  Simon made the third dream scenario\n\n\nIggy also made the outro to the composition where the main character wakes up from his dream.\n\nThe figure below shows an animation of the idea behind our composition.\n\n\n\nThe final composition can we listened to in binaural below. Make sure to use a headset when you listen to it.\n\n\n  \n    \n    Binaural rendering of the final composition\n  \n  \n\n\nIggy\n\nUlrik\n\nThe goal I had for my part was to compose music for abstract dream-like space, where different types of sounds move along trajectories above and around the listeners head. I got inspiration from how dreams suddenly appear and disappear without any, in a seemingly unpredictable manner.\n\nRecordings\n\nI did a few recordings with the Zoom H3 VR ambisonics microphone when I was in Trondheim the first week of the course. Two of them were used in the composition.\n\n\n\nRecording of some construction work in Trondheim\n\n\nThe first recording was used as a background element in the composition. It was recorded outdoors in an space with sound sources coming from multiple directions and distances, as well as the general noise of the city. The recording helped providing a spatial context for the other elements in the composition.\n\n\n\nRecording of a metal stick scraping the floor in the Portal\n\n\nI also recorded a metal stick that I dragged around the microphone in clockwise order. Iggy had a cool idea of adding a delayed version of this recording on top of the original one, and rotating it the other way around the listeners head. It created an interesting whirlwhind effect that I ended up including in my composition.\n\nSpat\n\n\n\nSpat experiments\n\n\nAfter having heard very promising words from Natasha about the software, I was curious to look into Spat. I decided to process one sample with different variations of a spectral delay, record six slightly different versions of it, and use these six samples in a sound cluster in my composition. This was my take on destructuring a sound in space, a concept that I borrowed from Natasha Barrett’s WoNoMute talk last year. I really liked this way of working with spatial audio, and I’m definitively going to use Max and Spat a lot more the next time I work on a spatial audio project.\n\nOther sources\n\nThere are a couple of other sound sources that I put into my composition as well, such as the drone in the background and the drum breaks that are approaching the listener around 01:26. I just experimented with positioning them around in space, and automating the position until I felt that it sounded somewhat natural.\n\nThibault\n\nThis part was composed in a layer-by-layer fashion, improvising 6 voices on a violin (plus one bass note). All voices fit in an A Dorian scale, and stay on one chord (Am).\n\nThe idea was to experiment with layer-by-layer spatial automation recording. Indeed, the displacements have been recorder in the same order as the violin tracks, and the aim was to give it a comparable intention as the notes. Speed and proximity are used to reflect bow speed and pressure, but not in a proportional manner. Musical intention may be more intense with a softer sound, and in that case the track sounds very close to the listener.\n\nFor violin recording, a piezoelectric pickup was used. For ambisonic encoding, the IEM RoomEncoder was used. All was done in Reaper.\n\nSimon\n\nIn my scene the listener enters a warzone in the midst of a bad dream.\nHe is heard running through a corridor together with other soldiers and as the doors open we can hear guns blazing from different directions. We enter a larger area where some futuristic cyborg monstrosity is unleashing its attack.\n\nMy primary drive was that I wanted to work with diegetic sounds. I have previously worked with sound design and quite enjoy it. As many of the sound libraries I have collected over the years are game related, I had a lot of fitting material on hand for a warzone scene.\nI visually sketched out a scene before starting at the beginning and adding sounds.\nThe IEM suite became my most important tools. In particular, the RoomEncoder, to bring all my monotracks into ambisonics. The plugin allowed me to automate both listener and emitter position which gave me decent control over directionality and movement. One of the challenges that became apparent, was that this plugin is limited to one sound source per instance. This forced me to use an increasing amount of this plugin as I kept adding more sounds to the soundscape. The futility in that approach both became apparent in CPU cost and a non-optimal workflow. Another problem I faced was the dynamic range in my soundscape. Having minor sonic details such as breathing and the rustling of clothes noticeable while guns are blazing in other side of the room became a never-ending struggle of balancing the mix. While I think the overall result was decent, I would not have approached such a task in the same manner if I were to do sound design within ambisonic again. I would believe working in a game engine such as unity and fmod/wwise would give a better visual representation of localization and would be better suited for such a task especially considering the increasing amount of spatialization plugins available.\n\nWorking at Notam\n\nIggy and Ulrik got the chance to work in Notam’s spatial audio studio. The studio is equipped with 24 loudspeakers in a hemisphere, evenly distributed approximately 2.5 meters from the listener. It was a great experience, and made it easier to become fully immersed in the world of spatial audio.\n\n\n\nSlides for presentation\n\nIf you’re more interested in our project, you can have a look at our slides below. Feel free to contact us about the project as well.\n\n\n    \n    The slides for our presentation\n\n",
        "url": "/spatial-audio/2020/11/16/team-trondheim-spatial-audio.html"
      },
    
      {
        "title": "How To Save The World In Three Chords",
        "author": "\n",
        "excerpt": "Don’t run! We come in peace!\n",
        "content": "Brought to you by Aleks ‘Ack-Ack’ Tidemann and Paul ‘Ray Gun’ Koenig\n\nThe Dramaturgy, if you will\n\nFor SMC4044 (Spatial Audio), we were tasked with the . . . task of designing, recording and mixing a spatialized audio composition. The composition was expected to have a clear dramaturgy, a unifying and coherent concept and realization, loads of action, excitement, and a mind-blowing climax that will simply blast your socks into next week. (I’m paraphrasing the requirements here.)\n\nShamelessly ripping off the 1996 cult-classic film ‘Mars Attacks!’, we decided to tell our harrowing story-in-sound as a dramatic tale of alien invasion and eventual repulsion.\n\nBecause the unrelenting lethality of Slim Whitman’s voice is by now well known, and also because neither I nor Aleks can yodel, we decided that our fearless (and perhaps clueless) hero would take a slightly different form.\n\nIn the guise of a possibly-homeless street performer, our doughty busker repels the aliens with a combination of harmonica, guitar, and questionable lyrics about snorting coke off of someone’s garage floor (we take no responsibility for his poor life-choices).\n\nTo effectively tell the story, we paid super careful attention to its Aural Architecture as described by Blesser (Blesser, Barry et al 2008), in particular the concept of Navigational Spatiality.\n\nIn other words, we strove to create an aural environment whereby the listener could visualize the represented space through sound. Nifty, eh?\n\nFigure 1 shows our storyboard/timeline.\n\n\n\nfigure 1: A Dramaturgy, if you will\n\n\nSound-Field Recordings\n\nWe made a total of seven sound-field recordings in outdoor environments around Blindern and Marienlyst. Each recording was approximately two-and-a-half minutes long, and captured an array of environmental sounds ranging from basketballs bouncing, children shrieking, birds yelling, and cars larking about to lunatics stomping through puddles while playing the harmonica.\n\nThis seemed like just the perfect, idyllic sort of scene to be obliterated by malice-filled alien beings.\n\nFigure 2 shows one of our microphone setups, just outside the backdoor of the Fysikkbygning. Since we couldn’t find a deadcat (wind-shield) for the rather expensive Soundfield microphone, we improvised with a lua, or “deadhat”, which was loosely-woven and somewhat hairy, for a lua. It worked well enough, and no doubt better than an old wool sock, though this was not tested.\n\n\n\nfigure 1: Not An Old Wool Sock\n\n\nMaking The Spaceships\n\nUsing a Korg MS-20 synth, Aleks created some great spaceship sounds for the project. Ever keeping in the forefront of his mind the concept of immersivity through spacial presence (Roads 2015, p. 278) he toiled for days or minutes to fashion immersivity from the rank dust of insensate sound-oscillator ejaculate, via the technique of having multiple sound files for a single source.\n\nFive different mono audio files were created to represent the same object at different locations, as follows: Sound 1: Appearing Craft. Sound 2: Hovering Craft. Sound 3: Craft Near Listener. Sound 4: Craft Flying Away. Sound 5: Craft Plummeting Towards Ground.\n\nChaos Ensues\n\nWhat would an alien attack be without shrieking humans? For this sound effect, we used samples of actual shrieking humans that Iggy (the Samplemaster) sent along, including one of Barney from The Simpsons, episode 142 (we suspect). To create the aural illusion of people running about in blind, abject terror, we implemented collective rotation (using the master azimuth feature of the IEM multiencoder) to make the voices move around. To further increase tension, we increased the volume and proximity of the alien presence to create a more oppressive effect.\n\nEnter: Our Savior (Not Jesus… OR IS HE??)\n\nFor the hero guy, I made a simple mono recording in a specially designed acoustic chamber (my bedroom) of a suitable family-friendly song appropriate to the project. We then created the sonic effect of our hero-musician traveling through the soundscape through the use of source-panning automation, EQ and volume. Everywhere our hero goes, the music-intolerant aliens lose control of their spacecraft, crash and explode, much like the composition professors at my old alma mater when I approached them with a piece of tonal music written purely for the pleasure of ‘note-based’ music.\n\nIn this way we immerse our subject in the environment through processing (Barrett 2020; Roads 2015).\n\n(Explosion sounds courtesy of Freesound.com.)\n\nThe World, Saved\n\nAnd so, the world is yet again saved by the power of music. We enjoyed the project, and felt like we were successful in telling a fairly action-packed story in just over three minutes. Had we had more time, there’s a lot more we could have done to increase the realism of the environment, from more careful (re. very time-consuming) automation of eq and effects, to adding more sounds and smells (burnt alien in 3rd order smell-o-sonics, anyone?) to complete the illusion.\n\nBut, much like our hobo hero, I’m afraid we gotta move on. There’s a garage floor somewhere with our name on it.\n\nReferences\n\nBarrett, Natasha. (2020) “Spatial Music Composition,” n.d., 15.\n\nBlesser, Barry, Spalter. (2008) “Aural Architecture: The Missing Link.” The Journal of the Acoustical Society of America 124(4). doi: 2525–2525. https://doi.org/10.1121/1.4782966.\n\nRoads, C. (2015) Composing Electronic Music: A New Aesthetic. Oxford University Press.\n",
        "url": "/spatial-audio/2020/11/17/how-to-save-the-world.html"
      },
    
      {
        "title": "Slicing and Dicing Audio Samples",
        "author": "\n",
        "excerpt": "Slicing up two songs and re-joining them with RMS and Spectral Centroid values.\n",
        "content": "The Task\n\nThe task for the final exam for scientific computing was to create a python program that splits up two songs into segments, analyses particular\naudio qualities of these segments, then uses this analysis to determine the order when reassembling these segments back into a single audio file.\nThe final audio file then needs to be displayed in a plot, with some indication of the audio samples used and audio feature values.\n\nThe Implementation\n\nMy Idea was to combine two features, RMS and Spectral Centroid.\n\nThe program calculates the Spectral Centroid value (brightness of a sound) and average RMS (loudness)\nof each segment. It then multiplies these values together so that we can order the samples from\nhigh loudness and high brightness to low loudness and low brightness.\n\nI chose to use two songs I created myself. They are both cover versions of the same song, but in very different styles,\nso the tempo, chords and overall structure are the same but all the instrumentation is different.\n\n\n  \n    \n    Song One\n  \n  Song One\n\n\n\n  \n    \n    Song Two\n  \n  Song Two\n\n\nRather than just reconstructing the samples from bright/loud to dull/soft, the samples are re-orderd so\nthat each consecutive sample is a bright/loud sample followed by a dull/soft sample. In this way it is a\nbit more “musical” since the final output has a kind of loud beat followed by soft off-beat feel.\n\n\n  \n    \n    Output Song\n  \n  Output Song\n\n\nIt’s interesting to note that there is no clear indication of what sound is on the beat and what sound is on\nthe off-beat, allowing the listener to change their focus, which creates quite a different experience\nbased on whether you choose the soft or loud segments as the beat.\n\nIn the visualisation I chose to colour code the samples based on which song they originally came from.\nOverlayed on top of the samples is the RMS * Spectral Centroid values for each sample, and since this value\nalternates from high to low, the vertical line this creates indicates the end of the previous sample and\nthe start of a new sample.\n\nThis visualisation can be seen below. Open in a new tab or download to see the full resolution image.\n\n\n\nFinal Thoughts\n\nIt’s interesting to notice the difference in “dynamics” between the two versions of the song.  As can\nbe seen in the output image, song two is mostly placed at the start of the output audio file and\ncontains significantly brighter/louder and duller/softer segments than song one, which has much less\ncontrast between its bright/loud and dull/soft segments.\n\nThis was also interesting for me to hear how many segments of my two songs are literally identical.\nWhen placed within the context of the original songs this isn’t as noticable but when similar segments of a song\nare grouped together based on audio features that they have in common, it really stands out how repetitive\nsome of these beats are. I think I will take this knowledge on board and try to incorporate some more variables\nin my future tracks to make them sound a bit more “human”.\n",
        "url": "/sound-programming/2020/12/02/leigh-slicing-and-dicing.html"
      },
    
      {
        "title": "Dark Tetris",
        "author": "\n",
        "excerpt": "Scientific Computing assignment for a non-programmer, please don’t expect anything special when you see the title about DARK Tetris, even I really want to create that…\n",
        "content": "What?\n\nThe final assignment in the Scientific Computing module was to take out self-chosen audio segments from two audio files, to analyse their features  and to combine them into one audio file. We also needed to visualise both songs and the slicing points and create a new csv file with the features that we have analysed with the help of the Librosa library.\n\nWhat resulted?\nI merged a piece of music from the game Tetris and a song called Veil of Elysium, originally by a power metal band called Kamelot.   Originally, I wanted to merge “Tetris” with the other song so as to give it a heavy metal quality, meaning that the rather cute sounding Tetris would become darker and heavier. Very unsurprisingly, as a novice Pythonista here, I ended up putting the “epic” (a style of Heavy Metal) beginnings of the two audio files into one. In addition, I focused on the downbeats (onsets) and set as a ¼ tempo.\n\nThe version of Tetris I used was performed by London Philharmonic Orchestra and the recording of Veil of the Elysium was performed by the American power metal band called Kamelot. Both songs are in the Allegro to Vivace range (120 - 156 bpm), the average tempo is at 130 bpm. The expressions of both songs are quite different despite the fact that they both share rather epic introductions. Tetris is playful and Veil of Elysium is tense and a bit dystopian. The genres of both songs are obviously very different, however, from the music psychology point of view, the complexities of both genres are very similar and attracted similar listeners, such as introverts, whose minds crave for complexities and enjoy swimming in their train of thoughts. I have extracted the most epic beginnings from both files and simply put them together. (Feel free to laugh, it is a non-programmer way of doing programming).\n\nProcess and Why:\nAs I just mentioned with the onsets and tempo, these are the segments I have sliced, in a hope that it will make the downbeats clearer. The visualisation of this is, to be honest, quite boring, as you can see as below:\n\n\n\nThen, I extracted their average root-mean-square (RMS) and spectral centroids levels and created a new CSV file. While RMS quantifies the average level of a signal, RMS correlates more closely to our hearing, in this sense, I considered the timbre of both audio files. The spectral centroid is simply a measure used to characterise a spectrum of the digital signal it processes. In a nutshell, it indicates where the core spectrum is located - the brightness of a timbre. So, I tried to make the downbeats of the remix file clearer and brighter in its timbre.\n\nLessons Learned:\nI have to admit that I didn’t understand the array until this assignment, I finally learned how to play with it and twist it in order to program the ideas I have in my head. There is a lot to be explored in the audio field. Coming from the social sciences, humanities and architecture and design backgrounds, it took quite a while before I began to grasp the computing languages and communication styles. I noticed how terms have different meanings in different things in other fields, I also gained new insights by communicating with Team C.\n\nEnding with this small reflection of my final assignment for Sci-Com, I want to say thank you for the teaching program that was constantly improving and tailored for different types of learners. I have to admit that as a creative learner, thinking about spaces and visual terms are way easier than following verbal concepts step-by-step. I feel that the program changed along the way, and it helps my learning. Last but not least, I want to thank Stephen from Team C, who took his time and patience to help and explain with the codes in the past few weeks. Without these collaborations, I would have found it hard to  even finish the task. (Even if there will be a lot of weird errors, it is better than an incomplete assignment.)\n\n\n    \n    The output\n\n",
        "url": "/sound-programming/2020/12/02/joni-dark-tetris.html"
      },
    
      {
        "title": "Segmentation and Sequencing from and to Multichannel Audio Files",
        "author": "\n",
        "excerpt": "The strategy explained here considers the slicing of stereo audio files and the production of a new stereo output file based on a multichannel solution. The spectral centroid is used to reorder and intercalate segments in an ascending or descending order according to some rules.\n",
        "content": "Overview\n\nThe segmentation and sequencing solution proposed here takes multiple audio files of any number of channels, then it extracts segments per each channel based on the detection of onset points. The mean of the spectral centroid is calculated for each segment, which is used as a parameter to sort the slices in an ascending or descending order. Those segments are organized in a data structure that is used later for a sequencing process that merges the segments in an intercalated way to compose a new multichannel output file.\n\nAn implementation of this strategy was developed in Python. Two audio stereo files were used for testing purposes, corresponding to the following songs:\n\n\n    \n    L'Impératrice - AGITATIONS TROPICALES\n\n\n\n    \n    a-ha - Take On Me\n\n\nThe resulting output is the next one:\n\n\n  \n    \n    Output Audio File\n  \n  Output Audio File\n\n\nThe Strategy\n\nSegmentation and Analysis\n\nThe whole algorithm is developed to take into account multiple channels, that is, the input could be mono, stereo, or have more than two channels. Moreover, the output could be a multichannel audio file built from the input channels. It is possible to have different number of channels for the output regarding the input files.\n\nThe algorithm takes every channel and manages it as a single signal. Firstly, segments and relevant data are extracted per each channel by following the next procedure:\n\n\n  Obtain onset points.\n  The user could decide to reduce the number of onsets, which means a kind of merging of contiguous segments. They could be merged in groups of N segments. This step is indented to reduce the number of segments if needed (which reduces execution time).\n  The collection of onsets is iterated to define the segments by the start and end points. It considers that onsets could not be exactly in index 0 or the last point of the signal. Segments are accumulated in a collection.\n\n\nAnalyzing:\n\n\n  While onsets are iterated and segments are defined, each segment is analyzed by getting its spectral centroid mean.\n  The centroids are sorted from lowest to highest, nevertheless, it could be configured to sort from highest to lowest depending of the rules explained later in the sequencing part. The output of the sorting is an array that contains the indexes ordered according to the selected criteria, which is useful for accessing in that order to the whole data structure in which the segment is associated.\n  Useful output parameters are obtained for the later sequencing process(segments, start points from onsets, spectral centroids, sorted indexes)\n\n\nStructuring the whole segmentation\n\nA data structure collection is shaped in this step. This collection contains the relevant data for each channel from each audio file. For each channel, it obtains the output data given by the process explained above, that is, segments and relevant data. The sorting is inverted (from highest to lowest) when the target input signal is in an odd position (starting from zero) considering the order in which it is processed, that is, the first file (zero) will not be inverted in the sorting of its segments, and the second file (one) will be, and so on.\n\nAlso, this process collects the maximum number of segments that could be found in a channel, which is used in the sequencing strategy.\n\nAt the end, a complete data structure will contain all the segments and their data organized properly in order to be sequenced later.\n\nSequencing Algorithm\n\nThe number of channels for the complete output signal is defined initially, in the example was established a stereo (2 channels) signal. A new output signal is constructed for each channel. This strategy takes all the segments from the same channel that corresponds to the input signals, that is, assuming that channels are numbered from zero, if the output channel is 0, then all the segments to be merged will be from channel 0 of the input signals, in case that the number of channels mismatch, the strategy will take channels in a circular way, for instance, if the output channel is 3 and there are only 2 channels in the input, channel 3 will take segments from channel 0.\n\nThe segments are taken from the complete data structure. As the exact number of segments in this particular iteration process is not known initially, the maximum number Of segments in a channel calculated previously is used, then each signal is iterated and the segments and their data is retrieved by using the number of the channel that is being filled. Segments are accessed via the sorted collection of indexes based in the spectral centroid, which ensures a merging order considering the criteria explained previously. Note that indexes must be validated to be within the range of the number of segments since the general segment index per each iteration is limited by maximum number Of segments in a channel and not all channels has that number of segments, they could have fewer of them.\n\nThis merging process intercalates between signals for each channel, for example, segment 0 from file 0 is merge with segment 0 from file 1, then segment 1 from file 0 and segment 1 from file 1, and so on. Also, the order of the segments is inverted for odd channels, which gives a very noticeable effect in the channels when they are heard simultaneously.\n\nThe effect of this strategy is reflected in the image below. It represents the output generated that used the input songs mentioned previously. Here, each slice is differentiated within the waveform according to the corresponding input, also it is noticeable how the spectral centroid is increasing or decreasing per each channel. Considering that this is a vector-based image,  you can open a new tab and zoom in here\n\n\n    Output Visualization\n\n\nReflection\n\nThis strategy is an interesting process to perform a kind of concatenative synthesis over multichannel files and for generating a multichannel output. The feature used (Spectral Centroid) can be combined with other features, or the sequencing solution could be modified in favor of a better aesthetical result.\n\nThe output is highly variated in both ears when it is heard, I do not consider that is aesthetically appealing, but that is a subjective judgment. However, it is noticeable that the effect of slicing by using onsets is present per each segment in the resulting output, and the influence of the sorting regarding the spectral centroid could be appreciated when there are some smooth segments when values are lower, or a high concentration of segments and more rich sounds when values are higher, at the end, this feature is related perceptually with the ‘timbre’, that is, the ‘brightness’, and one can argue that variations of this parameter are changing in a more or less way across the output.\n",
        "url": "/sound-programming/2020/12/04/pedropl-multichannel-segmentation-sequencing.html"
      },
    
      {
        "title": "My Bloody Zerox",
        "author": "\n",
        "excerpt": "So the plan was to take two audio files, chop them up in some random way, mix up the pieces and stitch them back together again in a totally different order. Doesn’t have to be musical, they said. Well, just to make sure, I decided to chose two pieces of music that weren’t particularly musical to begin with - Only Shallow by shoegaze band My Bloody Valentine, and Zerox, by post punk combo Adam and the Ants.\n",
        "content": "So the plan was to take two audio files, chop them up in some random way, mix up the pieces and stitch them back together again in a totally different order. Doesn’t have to be musical, they said. Well, just to make sure, I decided to chose two pieces of music that weren’t particularly musical to begin with - Only Shallow by shoegaze band My Bloody Valentine, and Zerox by post punk combo Adam and the Ants.\n\n\n    \n    My Bloody Valentine - Only Shallow\n\n\n\n    \n    Adam and the Ants - Zerox\n\n\nFirstly, the slicing. Being predominantly white noise, Only Shallow is pretty much lacking in any transients, so using the tracks bpm and length to slice by beat made sense here.\n\nAs shoegaze bands tend to sound fuzzy and noisy, and post punk is often described as spiky and angular - a more tonal-leaning noise perhaps - I thought it could be interesting to get some spectral flatness involved in the analysis. Spectral flatness, aka tonality coefficient, measures how tone-like a sound is, verses how noise-like. Despite being pretty noisy itself, I expected the Adam and the Ants song to veer towards the more tonal side of things, if only in relation to the My Bloody Valentine track. So the sliced segments were sequenced using RMS and spectral flatness, which gave a nice smooth increasing curve when plotted.\n\n\n\nAs you can see from the plot, Zerox is mostly concentrated at the loud, tonal size of the spectrum. It makes sense that Only Shallow is seen as less tonal, but its also lower in energy / loudness. I guess this is due to the compressed nature of the track, whereas the spikiness of Zerox means it cuts through so much more easily.\n\nFor those that are interested in the resultant car-crash of an audio file, it is embedded below. They did say it didn’t have to be musical.\n\n\n\n",
        "url": "/sound-programming/2020/12/04/stephedg-my-bloody-zerox.html"
      },
    
      {
        "title": "Chop it Up: Merging Rearranged Audio",
        "author": "\n",
        "excerpt": "Taking beautiful music and making it less so.\n",
        "content": "The Task\nAfter several weeks of learning the basics of Python in the Scientific Computing module of SMC4000, the reckoning finally arrived: the final exam.\n\nThe mission, should you choose to accept it (and you better):\nCombine and reorganize segments of audio from two distinct audio files in an order dictated by a specific trait. Here’s how it went for me.\n\nAs the aural qualities of the final reorganized audio file didn’t matter within the context of this assignment, I didn’t spend a whole lot of time considering the musical properties of my two files. My only aural interest in these selections was ensuring that the instrumentation between my source files was noticeably different.  I selected a Kyrie from a Mass by Johannes Ockeghem and a Fugue for piano by Dmitri Shostakovich. Each less than two minutes long, these selections are both fairly monochromatic in texture, but I find them pleasing.\n\nHaving selected these two pieces, I then decided to divide each into 5-second segments and reorganize these segments by ascending maximum amplitude within each segment. All fairly straightforward once I wrapped my head around the right steps.\n\nBecause these audio files are, of course, copyrighted I won’t be including them here in my post, but do yourself a favor and go look these pieces up. They’re the opening Kyrie from Ockeghem’s Missa prolationum and the second Fugue from Shostakovich’s 24 Preludes and Fugues.\n\nThe rearranged final file is not particularly interesting sounding as a piece of music, but does yield some interesting points for consideration, particularly when its visualization is examined.\n\n\n    Comparing Amplitudes\n\n\nBecause the segments were ordered by peak amplitudes within each segment, there was no guarantee that the general aural aesthetic of each one would actually trend higher amplitude as the file progressed. This can be seen in the visualization above particularly toward the end of the combined piece where there actually is a visible segment characterized by low amplitude. Since this segment also happens to contain an accented downbeat, it has a moment of high enough amplitude to appear late in the rearranged file. A rearrangement of the 5-second segments by average amplitude within each one would likely have yielded a noticeable different structure.\n\nAnother element of this combination of files that I find interesting, and the driving force behind why I wanted files with different instrumentations, is the matter of how sonic structure is actually a key element in how ensemble music is shaped. Although I’m certainly not an expert in recording strategies for vocal music or piano music, it seems obvious that different approaches must be taken. The Kyrie, coming initially from the mouths of several singers, in comparison to the Fugue, featuring multiple voices that all emanate from a single source, raises questions for me about musical shape. Once the multiple voices get involved in the Kyrie, the amplitude shown in its visualization never seems to dip to the same degree that the Fugue’s amplitude does in the course of its duration.\n\n\n    Kyrie Shape\n\n\n\n    Fugue Shape\n\n\nFinal Thoughts\nSince the point of these blog posts isn’t to list line-by-line the specifics of each line of our code but to instead briefly outline some thoughts concerning how and what we produced, I’ll leave these last thoughts here. Is the audio I produced here very interesting? No, not at all. Would it be a victory for anyone who had been using Python for more than a few weeks? Definitely not. Was it a victory for me? Absolutely.\n",
        "url": "/sound-programming/2020/12/04/williakm-merging-ockeghem-and-shostakovich.html"
      },
    
      {
        "title": "Graphic score no. 7",
        "author": "\n",
        "excerpt": "So … it’s 00:01 in Oslo, December 1st, and the final SciComp assignment is here … And … it’s a though one … 2345 words … AI must be involved here …\n",
        "content": "Even though it initially stroke me as a hard task, it also seemed to be some fun ways of handling audio in here, slicing two songs and rearrange all slice into one new.\n\n\n   \n   Situations\n\n\nGraphic score no. 5\nI decided to go for a program that outputs audio-files inspired by John Teske’s Six Graphic Scores, no. 5, starting with heavy loudness moving towards silence. Teske’s no 5 also evolves around a center pitch, which I didn’t work with, I mainly wanted to make my output waves look similar to his graphic score, which essentially should give a somewhat recognizable audible output.\n\n\n    \n    John Teske - Six Graphic Scores, no. 5\n\n\nSo my plan was slice my two songs in as small segments as total program execution time allowed me, calculate the RMS for each segment, and put them together in descending order from highest value. Reason I wanted smallest possible segments was of course to make the new song a composition in its own, rather than a mix of the two sources.\n\nDecisions\nAs always when presented a task, I spent a lot time on deciding what to do, in this case I spent that time remixing one of my own songs, or rather making a «radio-edit» of the 9 minutes original. During the time I spent radio-editing, I had my plan, and chose Arne Nordheim’s «Den Lille Prinsen» as the other song. Within these two songs there should be both high values RMS as well as low values.\n\nChoosing songs could be crucial, since I still didn’t know how long the segments would be. The shorter segments, the less it would matter, I figured. A sample is the smallest possible segment, and then the songs itself wouldn’t matter too much, but moving towards several seconds, it could be an issue.\n\nSlicing\nAfter brainstorming for five hours, staring out in the air for four, 17 hours of sleeping, 26 hours of googling, walking around my neighborhood for five hours, and15 hours of writing and testing my code, I ended up with a program that sliced my songs in segments of 150 ms, sorted and played them back in descending order from the RMS-values.\n\n\n   \n   Screendump of waveform from Logic\n\n\nThe short length of the segments should indicate that whatever input I use, the output will be a new composition, rather than a remix of the two songs. And to me the sonic result is actually very appealing to me, which brings me to the …\n\nConclusion\nThe very last words in the assignment text are «The sonic appeal of the output of your program does not contribute to your grade.»\nTo motivate myself, I had the sonic appeal of the output as one of my goals for this task, but what’s appealing to me, might not be appealing to you, so leaving it out the grading is probably a good idea.\nAt the end — apart from some very frustrating moments — I had some good 72 hours working with this, and despite all the things I know should be done better and more effective, I’m very happy with the result.\n\n\n  \n    \n  \n\n",
        "url": "/sound-programming/2020/12/04/anderlid-graphic-no-7.html"
      },
    
      {
        "title": "Merry slicemas",
        "author": "\n",
        "excerpt": "Let it slice, let it slice, let it sequence.\n",
        "content": "The task for this program was making piece of music/audio based on slices from two chosen audio files. It being the 1st of December, christmas songs simply felt quite natural. Though I might not consider the generated outcome an instant christmas classic.\n\nThe first song i used is a bossa nova version of Let It Snow\n\nAnd the second is another bossa nova of Up On The Housetop\n\n\n    Both performed by the one and only Jack Jezzro\n\n\nThey’re both in the same key, so I expected the generated song to be somewhat musical, which it kind of is and isn’t. The bossa nova arrangement does introduce some modulations and slick harmonizing, so it jumps in and out here and there.\n\n\n  \n    \n    Should show a media player\n  \n  \n\n\nThe slicing is determined by BPM, either it’s given in the audio file name or calculated by onsets and the librosa.beat.tempo. It is pretty accurate, but putting the the tempo in manually provides greater accuracy of slicing. But then again, you’ll need to know the BPM. This is audible in the output file as well. Song 1 i put as 125BPM and it’s also quantized, resulting in quite tight slices. The other one is analyzed in the program and is not quantized, so you can hear how it slices a little looser around the beats. Song 1 is the one with most guitar, song 2 is the saxophone smooth one.\n\nI went with the BPM type slicing because it struck me as a musical way of slicing a song. It worked better with newer music which tend to be quantized to BPM grid than older analog music. Even though the task said stick to two song and don’t switch, I couldn’t help but trying different combinations. It was a nice additional challenge to make the program run with different song, making it more flexible and not too much “hardcoded”.\n\nThe way it is sequenced is that the mean spectral bandwidth is calculated for each slice of both songs. Since this program is working in stereo, I solved analysis of both left and right channel by appending the left and right slice for slice[x] and then calculate the mean spectral bandwidth of all the samples. This plot shows it all more accurately:\n\n\n    The large dots show how slices are distributed, showing global slice index on increasing spectral bandwidth. I also added a \"line\" of dots (the small dots) showing the order of slicing. This is not referencing slice indexes, but indicating what song is playing at the Nth position in the song.\n\n\nThe CSV file holds the accurate details for the plotted data. I had an idea that sorting after spectral bandwidth would contribute to putting slices with spectral equalities together. Looking at how the slices are picked from the songs, we there is a similarity in how slices from the first half is picked for the beginning and the last slices are put towards the end. There are wildcards all over, especially for song 1 where the intro actually is put at the end, but the tendency is there among the dots still.\n\nEnding note\nI think the output file has nice qualities, though it’s far from perfect. To me a program like this could contribute to generate new ideas for chord progressions and melodies based on references. There are lucky accidents here and there. Sometimes it also pretty funny, especially with songs having vocals in them, when the lyrics fall into new meanings.\nA nice experience from working with this task was to use something else than random for putting things together. Analyzing the audio and then using that to determine what to do makes more sence, unless you want the randomness. It could also serve as a control for random values. I have tried to work with reordering samples as well, like we’ve done with the slices here. Applying these calculations and manipulating samples might be something interesting to play around with when christmas gets too chistmassy.\n",
        "url": "/sound-programming/2020/12/05/henrikhs-merrySlicemas.html"
      },
    
      {
        "title": "Chaotic Battle of Music Slices",
        "author": "\n",
        "excerpt": "Slicing up the music and reorganize them with four kinds of Librosa features.\n",
        "content": "Task Overview\n\nThe task for the final exam for scientific computing was\nrequires us to write a Python program which sequences audio segments extracted from two selected songs according to arbitrary analytical criteria. The program takes as input two audio files, each containing a song you select, and it produces as an output audio file, a visualization of the sequenced audio, a CSV file with details of the all audio segments, and several metrics printed on screen.\n\nSolution Overview\n\nMy solution was to import two audio files into a panda data frame after slicing them rhythmically, sorts and reorganizes slices using the four kinds of calculated Librosa features as the principle.\n\nThe segmentation strategy: Slicing an audio signal in segments of fixed and duration related to the tempo and arranging them on Numpy array.\n\nThe sorting strategy: Four audio features(spectral_contrast, spectral_rolloff, spectral_centroid, rms) are computed using Librosa for each slice, and the results were summed at 30%, 30%, 35%, and 5%, listed in ascending order to reorganizes slices.\n\nI chose to use two songs I created myself a long time ago, and I chose two pieces of music with completely different styles and rhythms in order to make the reassembled music clueless! The material of the first song was only a piece of knocking on the glass, and it was an electro-acoustic music created by Kyma and GRM Tools. The other song is a more “popular” EDM, which is an undergraduate classwork 4 years ago.\n\n\n  \n    \n    Glass.Wav\n  \n  Glass.wav\n\n\n\n  \n    \n    Runaway.Wav\n  \n  Runaway.wav\n\n\nRegarding reorganized music, I was aware of the challenges of slicing and recombining two pieces of music in terms of audibility, so I simply tried to construct new music in a more chaotic way.\n\n\n  \n    \n    Output Song\n  \n  Output Song\n\n\nAs messy as possible!\n\nTo be honest, after tried to calculate multiple features, it was difficult to completely decomposition two different pieces of music in reorganized music(features of the slices from one music will be more similar). Therefore, I tried to use different weights for multiple types of data to make the fragments as messy as possible. After I added these data proportionally, It is not particularly clear what it means in terms of hearing. But two different styles of music mixed together in a seemingly clueless way!\n\nIn the solution for visualising audio files, I chose to use different colours to mark the source files of the music clips. In order to make the source of each slice clear enough, I used matplot to output a really huge png file, and the original png file can be download in here.\n\n\n\nMy reflection\n\nAs a beginner in programming, this semester’s python course was undoubtedly the most difficult and time consuming course for me to learn. However, the completion of this final exam went relatively smoothly overall, and I was able to complete the program’s goal in a relatively quick time (although stuck on how to sort audio sources visually for a long time)\n\nI also wrote two versions of the program during the exam, the first version was a linear build of the program as I had done before but the program was fairly hard to read and did not fit the reusability of the program. So, after finishing the goal, I tried to learn to construct the program like an experienced programmer, writing various functions to achieve different purposes for program. Obviously, this is a much more efficient and clearer way of building programs. For me, the next step in my learning might be to learn how to build programs from this kind of “bigger view” from the start.\n\nAll in all, this semester’s study of python has certainly been a very fascinating and rewarding course for me, and I hope to be able to go further in the future.\n",
        "url": "/sound-programming/2020/12/05/wenbo-slices.html"
      },
    
      {
        "title": "Chopping Chopin and Kapustin preludes",
        "author": "\n",
        "excerpt": "Chopping a prelude by Chopin and one by Kapustin and then merging the slices based on their loudness (RMS) and tonality (Spectral Flatness).\n",
        "content": "Chopping Chopin and Kapustin preludes\n\nThe Scientific Computing module within the SMC4000 course was the longest and most complex. I had some experience with programming from before, but not with anything audio, so this module provided a good introduction to audio processing with Python. The final assignment was a good combination of straight-forward and complex tasks: we had to choose two songs of relatively similar length, slice them and then concatenate all the segments based on a certain feature (this will become clearer in a moment).\n\nThe Initial Songs\n\nThe melodicity of the final song - can we even call it a song? audio sequence? - didn’t matter, I didn’t overthink the selection phase. I thought it would b a good idea for the two initial songs to be 2 minutes or less (for computational purposes), so I went searching through Frédéric Chopin’s Preludes. I remembered than Nikolai Kapustin, inspired by the way Chopin composed a prelude for each key, did the same in opus 53, with his “Jazz Preludes”, so an idea formed: I would chose one prelude by Chopin and one by Kapustin! Pretty smooth, eh? I wanted to chose either the same key or a major/minor combination, but the pairs I checked out were either too different in length or too different in rhythm, so I ended up choosing the Prelude in do minor by Chopin and the Prelude in fa minor by Kapustin - both minor keys, just the way I like them. These are the versions I used:\n\n\n    \n    Chopin - Prelude op 28 no 20\n\n\n\n    \n    Kapustin - Prelude no 53 no 18\n\n\nNote: For simplicity, I used an universal sample rate of 44100 Hz. For this, I preprocessed the songs in Audacity and changed their original sample rate. I also made sure to normalize the audio and cut off most of the “dead audio” from the beginning and end of the songs, to avoid as much as possible having slices contain no sound (e.g., it would influence the “score” of each slice).\n\nThe chopping strategy\n\nWe were allowed to slice each audio file in an arbitrary number of segments based on any criteria we chose. Before trying anything fancier, I wanted to have a basic, working program, so I started by writing a function that slices an audio input every 2 seconds (every 88200 samples). That worked out nicely from a programming point of view, but the final audio sequence wasn’t anything special, it was chopped up without retaining anything melodic from the input songs.\n\n\n    Chopin and Kapustin Waveforms\n\n\nObserving the waveforms of the two songs, I realised that Chopin’s prelude was nicely structured in chunks and this gave me the idea to try to split it based on this. So next, I wrote a function what would slice an audio input based on the onsets of the songs, the “peaks” in the melody. I tried the same with Kapustin’s prelude, but that ended up chopped into 500 something slices, very very short slices, which wasn’t nice at all for the final audio. In the end, I decided to slice Chopin’s prelude based on its onsets - it provided 73 slices - and Kapustin’s prelude every 2 seconds - it provided 61 slices.\n\nThe analysis criteria\n\nEach slice was supposed to be analysed according to some criteria that would provide the basis (of the order) in which they would be merged in the final audio sequence. For this step I had multiple ideas; I thought about using the amplitude of each slice, the spectral centroid (brightness of sound), the root mean square (loudness of sound), or the spectral flatness (tonality coefficient). All these were good ideas, but I wanted something more relevant to the two songs I chose.\n\nGiven that both are classical pieces, played on the piano, I thought the spectral flatness, the tonality coefficient, was most relevant. The way I understood it, the higher thespectral flatness, the less “melodic” the sound is; it would be more similar to white-noise. I thought this coeffficient would be particularly interesting when combined with root mean square, the loudness of a sound. So, I multiplied these to scores, for each slice, to get a score that would represent how loud and noisy they are. A high score after multiplying would mean the slice was particularly loud and noisy, and a low score would mean a quiet, melodic slice.\n\nThe final sequence and its visualisation\n\nThe final sequence consisted of all slices from the two initial songs mixed up, merged based on their spectral flatness multiplied by their root mean square score, in descendant order. The noisiest and loudest slices first and the most melodic ones last. Interestingly, the two preludes proved to be quite different with regards to their loudness and tonality; however, considering that Kapustin’s prelude is jazzy while Chopin’s all for slow nice harmonies it is understandable.\n\nIt has to be mentioned that I tried to concatenate the slices based only on their spectral flatness and only on their root mean square, but the end result was not particularly nice-sounding or visualy more appealing than the version I settled on. But slicing chopin based on the onsets in the song definitely helped both with the audio and visual output. The final visualisation can be seen below.\n\n\n    Output Sequence\n\n\nFinal thoughts\n\nAt a first glance this assignment seemed so complex that I didn’t know where to start with handling it. But once I started to go through the guidelines (2345 words! but I always prefer too many details instead of too little especially when it’s about a graded, important assignemnt) it was a matter of figuring out how to do it one step at a time. I was nicely surprised to realise I knew how to do most of the stuff - after “transforming” the instructions into programming steps - for example, get an empty numpy array where you will append each slice or go through each row of the pandas dataframe and then update it with the relevant scores, etc. - it started to be clear and a matter of how to handle numpy arrays as audio files. Overall, I quite enjoyed the assignment!\n\n",
        "url": "/sound-programming/2020/12/05/alena-chopping-chopin_and-kapustin.html"
      },
    
      {
        "title": "A new season of chopped music",
        "author": "\n",
        "excerpt": "The arbitrary mixing of two seasons (two songs representing seasons)\n",
        "content": "The Task\n\nThe task was basically to chop pieces of two existing tracks and then rearrange them in a manner defined by a property of those chops into one single file!\n\nThe idea\n\nI started with two songs called sender and receiver by Patrice Baumel, released in his EP called Transmission. Both these songs are fairly similar in there technical as well as aesthetic values yet so distinguishable which is what I love about the EP and so, I thought it would be interesting to hear an arbitrary remix of the songs, but the processing time to remix these songs was turning out to be way above the permissible limit so I had to drop them in between my code. I thought I was back to the blank plate, but came two songs that I produced for art piece installation to save me, sometimes back I made songs that represented four seasons in the year, and so I took winter and autumn and tried to make a unique season out of them.\n\nYou can listen to both the songs down below:\n\nWinter Texture:\n\n\n  \n    \n    Should show a media player\n  \n  Winter\n\n\nAutumn Texture:\n\n\n  \n    \n    Should show a media player\n  \n  Autumn\n\n\nThe work\n\nI started by making three-second slices of the songs, after that, I calculated RMS values of each of these chops and stored them into a CSV file, which also had information about the slicing points or the starting point of the slice. The sorting of these segments according to the increasing RMS values of these segments frustrated me a lot, as it would sort the segments but the final output did not merge into one another, just one after the other. After a lot of google searches and brainstorming plus a broken laptop charger that I had to repair and solder myself and the help from some of my peers, I understood that since I made the songs, one is less in volume than the other(as the client wanted it that way), and since RMS represents the average level of the signal my winter texture had low values than the autumn texture so my final output became a concatenation of two songs remixed within themselves played one after the other, and I kept thinking for a very very long time that something is wrong with my code. The final output was nevertheless a good mix of the songs but didn’t quite fit mine and assignments expectation. But it was interesting to see the waveform, as it is clearly seen in it how the RMS values are increasing. After the submission, I ran the program on the First two 6 and half minute songs and the code processed them as intended too, which mixes them one after the other and created a 13-minute output file.\n\nyou can listen to the final season output and the 13 minute output down below:\n\nSeason Output:\n\n\n  \n    \n    Should show a media player\n  \n  The Hybrid Season\n\n\nThe 13 minute Transmission Output: (Receiver &amp; Sender by Patrice Baumel - Mixed)\n\n\n  \n    \n    Should show a media player\n  \n  The sent &amp; received\n\n\nThe season mix waveform with increasing RMS values:\n\n\n   \n   \n\n\nFinal Thoughts:\n\nThis assignment taught me a lot, which also includes life lessons, that bad things can happen anytime, and sometimes you should take a step back and see the situation from a broader view. At the first look, I felt that assignment was very deep and it would be hard to complete it, but finally, when I started working on it some time I felt confident about it, and sometimes I got afraid that I would have to submit an incomplete assignment. Coming from not so good coding background I felt the scientific computing module was the best introductory module to the fascinating world of audio programming, and the assignment was a good amalgamation of the things taught in it. I had my ups and down and although I am not satisfied with the final result, I feel the experience as a whole was worth it.\n",
        "url": "/sound-programming/2020/12/05/abhishec-slopy-chops.html"
      },
    
      {
        "title": "Strange Fragmented Music",
        "author": "\n",
        "excerpt": "A strange song from Sequencing fragments of stranger things and Astronomia\n",
        "content": "Task\n\nThe task given to us was to segment two audio files in an arbitrary criteria and then sequence the segments back\ninto a single audio file using a spectral feature to put them back in order, followed by the visualization of the audio and a CSV file containing the details of the audio segments\n\nStrategy applied\n\nThe songs I chose were two covers I made:\n 1.Stranger Things, 2.Astronomia.\n\nBoth were my favorites to play. They had lots of synth sounds and had the same length but varied in tempo and key.\nThe slicing done on them was based on seconds, every slice was 1 second long on both the songs.\nI computed the spectral feature RMS(root mean square) of each segment and used the ascending order of the RMS values of both songs to sequence them back into one audio output file.\n\n\n  \n    \n    Should show a media player\n  \n  Stranger things.wav\n\n\n\n  \n    \n    Should show a media player\n  \n  Astronomia.wav\n\n\n\n  \n    \n    Should show a media player\n  \n  Output.wav\n\n\n\n\nReflection &amp; Learnings\nThe slicing section took me less time to implement, but rejoining or sequencing took some time to code and getting them to produce a track was quite a task for me.\nOn the second day evening, an unfortunate event took place, where my PC crashed, all to find out that my hard drive had failed.\nI tried solving the issue early next day, but failed to do so.\nSomehow I used my sister’s PC, installed Python and started coding from where I left off and thankfully finished before the deadline.\nKeeping in my mind the output I was looking for, I initially sliced the segments according to the tempo, but the code didn’t seem to work so, I went ahead with slicing with time as 1 second.\nThe output track sounded very Strange.\nI learnt to be prepared for contingencies.\n",
        "url": "/sound-programming/2020/12/08/lindsay-strange-fragmenty-song.html"
      },
    
      {
        "title": "Algorytme",
        "author": "\n",
        "excerpt": "A proposed application for generating personalized playlists based on emotional state.\n",
        "content": "\n    \n    \n\n\nIntroduction\nOver the past few months, we have been working on a project for ACX Music aimed at generating personalized playlists based on emotional states, specifically designed for therapeutic and clinical purposes. The overall project goal was to develop a novel computer-based application that promotes mental health and well-being based on modern-day music therapy research. The application/system we propose seeks to accurately predict musical preferences by collecting emotional data through user-profiles.\n\nIn this post, we will talk briefly about Music Therapy before discussing our proposed solutions, development process and some challenges met.\n\nTable of Contents\n\n  Introduction\n  Music Therapy\n  User Interface\n  Back-end System Design\n    \n      First Approach\n      Second Approach\n      Classification and Labeling of New Music\n      System Evaluation\n    \n  \n  Challenges\n   Ethics and Privacy Issues\n  References\n\n\nMusic Therapy\nBefore developing an app that was meant to be used in music therapy, we had to go through some literature to understand what music therapy really is, and how music therapy has been used for purposes that our app will cover. Music therapy is a broad field; a big discipline with many practices and usages. Music therapy has been defined as “Development and change through a musical and interpersonal collaboration between therapist and client” (freely translated from musikkterapi.no). The other definition defines music therapy as an academic discipline: “The study of the relationship between health and music” (ibid.).\n\nFairly anyone can benefit from music therapy in some sort of manner, but perhaps the most common practice is music therapy used in interaction with people with learning disabilities, mental illnesses, dementia, etc. Often here, the active approach (Trondalen &amp; Oveland, 2008, p. 437) is used, when the clients are actively engaged in musicking together with the therapist.\n\nThe app we are developing falls into the other main category of music therapy: The receptive approach (ibid.). The receptive approach involves the act of listening to music. Studies have shown that music listening could reduce anxiety before hospital procedures (Gillen et. al., 2008) and reduce pain during childbirth (Browning, 2000), but the most documented approach has traditionally been to use music for relaxation (Trondalen &amp; Oveland, 2008; Ruud, 2010; Grocke &amp; Wigram, 2006). It’s also commonly known that people use music to get in the right mood for different kinds of situations, e.g. for exercising.\n\nThe goal for the app we are developing has been to involve any kinds of potential music therapy purposes into the same system. That’s how we came up with the idea of using interpolation between different emotional states induced by music.\n\nUser Interface\nFor this project we decided to make a protoype of how a front-end solution might potentially look. This UI-design was made in Adobe XD, therefore it is a purely visual solution with no back-end integration. The main purpose of this part of the delivery is to give some visual reference to how the system might function.\n\n\n    \n    Overview of Artboards in Adobe XD\n\n\nIn the overview above you can see two different prototypes (marked with Flow 1 and Flow 2). The first “flow” corresponds with the first system we have suggested. Where the user inputs age and a nickname, rates their general mood and then proceeds to input their current mental state and their desired state before generating a playlist. The different emotional tags are takend from the GEMS scale. The GEMS scale is a list emotions typically elicited by the sound of music. Through research they have tried to pinpoint the most common emoitions evoked by music listening. And this we have used througout our project.\n\nIn the picture below you can see all of these emoitions listed with an emoij to go along each emotion. These emoji are use to both input your mental state, but also when you react to the music you are listening to. To either tag the music (System 1) or to tag the features (System 2).\n\n\n    \n    Symbol Guide\n\n\nIn addition to this we have made some extra artboards where the user can input some more information like musical taste, time restrictions, option for a therapist to use the guide and access to some additional information. This can be seen in the GIF provided below.\n\n\n   \n   Tour de force!\n\n\nBack-end System Design\nIn the end, we settled for two different approaches concerned with playlist generation and user-data collection. We agreed to this based on the quality of the ideas proposed and our unwillingness to settle for just one. This is not to say that we have to settle for just one system, they can and sould be used in a complimentary way after thorough testing of their strengths and weaknesses.\n\nFirst Approach\nThe first back-end system design features a “one-to-one” approach of logging user data together with an experimental approach of generating playlists. For simplicity’s sake, let’s imagine that a user requests a playlist from this system. The user must first specify his/her current and desired emotional states together with a either a time-limit or a number of tracks to include in the playlist. The system then takes steps to find corresponding annotated tracks logged in the user’s profile. If no matches are found, the system looks at other user information before diving deeper into the dataset to find matches.\n\nWhen the system has found two tracks corresponding to the user’s current and desired states, it will locate audio features associated with each of these tracks. As illustrated in image below, this requires us to we have extracted audio features from the dataset tracks.\n\n\n    \n     Each track is associated with emotional annotations and certain extracted audio features\n\n\nNow the system has two large lists of numbers, each representing audio features. This is where the linear vector interpolation comes in. If the user requests 8 tracks in his/her playlist, the system will generate 8 additional lists by interpolation, as demonstrated below. These additional lists can be referred to as “imaginary audio tracks” as they represent audio features from imaginary sources.\n\n\n    \n     A python demonstration of the linear vector interpolation. \"From state\" refers to the users current emotional state while \"to state\" refers to the users desired emotional state.\n\n\nThis interpolation process constitutes a “musical journey” through a high-dimensional space, visually represented in the below illustration.\n\n\n    \n     A more visual representation of the linear vector interpolation, a journey through a high-dimensional audio domain. The red circles represent the \"imaginary audio tracks\" discussed.\n\n\nThe purpose of these “imaginary audio tracks” is to be used as input to a regression algorithm that predicts which real audio tracks (in the dataset) best correspond to these imaginary audio features. As the visual overview below shows, the output of such an algorithm is track ID numbers corresponding to tracks in the dataset. Together they form a playlist.\n\ns\n    \n     Regression algorithm that predicts track ID's (target) based on audio feature input (label).\n\n\nFinally, the user will rate the playlist tracks given based on the available emotional categories. Then the tracks and associated ratings are stored in the user’s profile as the cycle continues.\n\nSecond Approach\n\nThis approach differs from the first one in that it stores what the user associates with features and metadata from the music in stead of tagging the songs directly. The idea is that since taste in music and what emotions music evokes is very personal; making a personal database of what aspects of the music evokes what emotions might be a good way of quantifying this process.\n\nEvery person has different conotations to different music.\nI, for instance get very energized, almost extatic when listening to a certain type of heavy, rhythmic metal. For people who haven’t had very positive, energizing and social experiences to this type of music (most people, I reckon) might have angry or depressing associations with this genre. Like the mirror from the first Harry Potter book; what the viewer sees when looking into it is completely personal and always different from person to person.\n\nIs there such a thing as universally accepted happy or sad music? Maybe.\nWhen first starting out building the database of what features and metadata the user associates with the different emotions, using pre-tagged happy, sad, melancolic etc. music can be helpful, but these tags should way in less and less as the user’s profile is built up over time.\n\nSimply explained:\nIf you hear a song that contains features like 100-110bpm, acoustic folk music, from 2010-2020 and so on (the song can have as many features as you like, the more, the merrier) and tag it with “happy and energetic”; the system will store that you associate these features with being happy and energetic. Whenever you want to evoke that emotion again, we simply have to find the song that most closely matches those features.\n\nThe beauty of this is it’s simplicity: the system only needs to store a data-table of features on the device and fetch music and metadata containing features from a database, via an API. This maintains both light weight performance and privacy.\n\n\n\nClassification and Labeling of New Music\n\nWhen launching the alpha version of the app, we believe that there might be an advantage for AXC Music to instantly be able to label their whole music library with emotion tags. To provide this, we have created an algorithm that uses machine learning to label music mp3-files with emotion tags. The starting point of this algorithm was the Emotify dataset, which consists of 400 songs in four genres, and uses the GEMS scale for emotion labeling.\n\nWe believe that the act of annotating emotions to music is both an individual and complex process, so this model is not meant to replace the human interaction at all, but rather to work together with the users. The users will constantly update the music labels by giving feedback on their personal preferences, but this information will also be given back to the server to improve the dataset that feeds the algorithm. The algorithm will only be used to label new music that is added to ACX’s music library, that hasn’t been listened to by any users yet.\n\n\n  \n   The human/machine feedback loop \n  \n\nThe prototype model consists of two separate Jupyter Notebook files, written in Python and Markdown (the whole folder is a part of our main github repository and can be downloaded from this link ). The first notebook ( ACX_algorithm.ipynb ) is the machine learning algorithm which imports metadata from the Emotify dataset and extracts features via Librosa. If ACX later are in possession of a bigger and better dataset than Emotify dataset, this file can easily be edited with the new dataset. And also, if ACX wants to use other emotion annotations, this should also be an easy thing to adjust.\n\nThe Librosa feature extractors used are the tempogram and mfcc. The features are the sound information retrieved for each of the songs, such as BPM, energy and tonality. Tempogram was chosen because tempo often is an indicator of the energy in the song. MFCC is a great feature extractor to detect similarities between sound files, and it is commonly used in speech recognition. The Linear Discriminant Analyzer projects the features and reduces them to a number that successfully separates the different music files from each other by emotion tags.\n\nThe machine learning algorithm chosen was the Support Vector Machine, and it performed with an accuracy of 100 %, which means that all of the tracks in the testing set were labeled correctly.\n\nThe other Jupyter Notebook file ( ACX_input.ipynb ), imports the algorithm and reuses it to label new music files with emotion tags. When running the Jupyter Notebook, the user is asked to type in the folder that contains the music files (instructions on where and how to place the files is described in this readme file). The result is an excel file that belongs to that folder, with emotion annotations to each song.\n\nSo, does it really work?\nThe algorithm was apparently working very well on it’s own training set, but it is difficult to say whether or not it works well when tested on new music content. This is due to the fact that the dataset was very small to begin with, and only contained 4 different genres. When listening to the newly labeled music, there seems to be similarities between the songs within each of the label, but there are also some songs sticking out as obviously wrong labeled. It is also difficult to do any objective evaluation of the model just by listening, as musical emotion annotation is highly individual.\n\nIt can also be discussed to which degree the model performs wrong, when misclassifying songs, as some of the emotional categories are close to each other (eg. nostalgia vs. sadness, or power vs. joyful activation). If a “sadness” song was classified as “nostalgia”, did the model perform 100 % wrongly, or could you argue that it was 50 % wrong?\n\n\n  \n   Based on the overlapping user tags on the Emotify dataset, this matrix was made, which shows how the emotion tags could be organized on a linear scale. \n  \n\nWhen evaluating the model it might be an idea to range the emotion annotations on a linear scale. The linear way of thinking around the emotion annotations also fits better to the way the rest of the app is working, and should also perhaps be regarded when importing the dataset into the machine learning model. So instead of using the mean emotion annotations, each song should be tagged with a percentage correspondence to each emotional category (e.g. track 1 is 0.20 nostalgia, 0.3 calmness and 0.5 sadness). This way of thinking will probably give a more nuanced and precise classification of songs.\n\nBelow, you find a short demonstration video of music that was labeled with the algorithm. The music samples were borrowed from the Emotion in Music Database:\n\n\n\nSystem Evaluation\nOur project development did show promising results considering the scope of the project. We succeeded in our goal and managed to prototype various solutions (components of a system) that will generate playlists based on the emotional states of its users. However, a two things are worth discussing.\n\nFirst, we could have explored using multiple datasets. At the end of the project period, we had acquired multiple equally applicable datasets with emotionally annotated music. Therefore, a solution could have been to fuse multiple datasets. However, we had little time or knowledge on how to best compare them to one another. Such a dataset would have given us more and a wider variety of data that could, in turn, have rendered our systems more accurate.\n\nSecondly, the positive thing about having multiple main system solutions (system nr.1 and system nr.2) is that we address the uncertainties inherent in developing systems without proper testing procedures. The negative side of this approach is not being able to deliver a fully functional system, only a collection of prototypes. When evaluating both back-end system designs now, it seems clear that a combination of the two probably would be the most optimal solution.\n\nGroup Challenges\nOne of our main challenges in this project was the fact that non of us are experienced programmers or designers. We have no prior experience in building such a system from a technical point of view. And it is therefore difficult for us to evaluate how easy or difficult it would be to integrate our solution in a functioning application.\n\nAnother challenge we faced was the inability to use third-party streaming services and their API. The external partner has their own database of music, but it lacks usable metadata.Therefore, the database with musical features has to be built from scratch. Which can be very time consuming.We never got access to this database so we were not able to test the system with real music. Since we only used the Emotify dataset, we only had 400 songs with 4 different genres. Which is not enough for building a good ML-algorithm. A potential solution in the future could be to fuse several datasets. However, this would force us to abandon the emotional tags we have used since these are not part of universal standard.\n\nEthics and Privacy Issues\nLike all systems that analyze user interaction data for improvement, privacy is a huge concern.\nThis concern is amplified by the fact that this is a system party intended to be used in a clinical setting where sensitive patient information must be protected as good as possible. The theme of privacy on digital platforms has had center stage in many public conversations in the past years and become more relevant by the day. To use search engines as an example: If you have had a Google acocunt for some years, big brother has hundreds of gigabytes of data on you stored on their servers. This includes location history, searches, all files you store (yes, they never actually delete them), emails… If you put on your tin foil hat and start to dig in to this, it gets really scary really quickly.\nBut have you ever tried to use DuckduckGo as your main search engine? The experience is mediocre at best.\nThe difference between seeing internet filtered and tailored just for you and plain search engine query is stark, but that’s the prize we pay for privacy. We want to build a service that gives the user the privacy they deserve while still providing a highly customized user experience, without selling your dreams and dirty secrets to satan.\n\nWe do this in a couple of ways:\n\n  By presenting the user with a very clear switch for turning on or off the stream of data that goes to analysis.\n  By encouraging the user to use an alias instead of their real name.\n  By having two “modes”, one for use as a tool for therapists and one for personal use.\n\n\nReferences\n\n\n  Aljanaki, A., Wiering, F., &amp; Veltkamp, R. C. (2016). Studying emotion induced by music through a crowdsourcing game. , 52 (1), 115{128. doi: 10.1016/j.ipm.2015.03.004\n  Browning, C. A. (2000). Using music during childbirth. Birth, 27(4), 272-276.\n  Eerola, T., &amp; Vuoskoski, J. K. (2012). A review of music and emotion studies: Approaches, emotion models, and stimuli. Music Perception: An Interdisciplinary Journal, 30(3), 307-340.\n  Gillen, E., Biley, F., &amp; Allen, D. (2008). Effects of music listening on adult patients’ pre‐procedural state anxiety in hospital. International Journal of Evidence‐Based Healthcare, 6(1), 24-49.\n  Grocke, D., &amp; Wigram, T. (2006). Receptive methods in music therapy: Techniques and clinical applications for music therapy clinicians, educators and students. Jessica Kingsley Publishers.\n  Lieberman, E. J. (2007). Sacks, oliver. musicophilia: Tales of music and the brain. , 132 (15), 76{77. (Publisher: Library Journals, LLC)\n  Norsk forening for musikkterapi. (7 December 2020). Hva er musikkterapi? Retrieved from: https://www.musikkterapi.no/hva-er-musikkterapi\n  Ruud, E. (2010). Music Therapy: A perspective from the Humanities. New Hampshire: Barcelona Publishers\n  Trondalen, G., &amp; Overland, S. (2008). The Bonny method of guided imagery and music (BMGIM). Perspektiver på musikk og helse: 30 år med norsk musikkterapi.\n  Zentner, Didier &amp; Klaus Scherer. Emotions evoked by the sound of music: Characterization, classification and measurement. Emotio (Washington, D.C.), 8:494-521, 09 2008.\n\n",
        "url": "/applied-project/2020/12/10/aleksati-acx-music-applied.html"
      },
    
      {
        "title": "Expanding Collaboration in Pedál",
        "author": "\n",
        "excerpt": "For our external partner, Pedál, we wanted to document and expand how multi-user interactions took place.\n",
        "content": "A Plan for Pedál\n\n\n    \n        \n    \n    \n        \n    \n\n\nTable of contents\n\n\n  A Plan for Pedál\n    \n      What is Pedál?\n      Initial Proposals\n    \n  \n  How would it work? - User evaluation\n    \n      Initial Study\n      Case Study 1\n      Case Study 2\n      Case Study 3\n      Reflections from the case studies\n    \n  \n  How would it function? Experiments with WebRTC\n    \n      WebRTC\n      Development and evaluation of naive multi-user P2P\n      Possible system architecture for multi-user in Pedál\n    \n  \n  How would it look? - Prototype of UI/UX\n    \n      A new concept for a listener gallery\n      What’s new?\n      More users, more problems\n    \n  \n  Conclusion\n\n\nWhat is Pedál?\n\nPedál is a software that enables musicians to remotely collaborate in real time. Some of its main features are the ability to stream both voice and system audio (from a DAW) between two collaborators. There is also the ability to drag and drop audio and MIDI clips to send from one collaborator to another, as well as a recording functionality to record directly into Pedál. Pedál is minimal and has a design reminiscent of analog guitar Pedáls. It’s not intended for live rehearsal (like JamKazam, JackTrip, etc), but instead is optimized for collaborative beat-making, songwriting, and recording sessions.\n\nA platform for musicians to collaborate:\n\n\n  Audio stream from mic &amp; session\n  Drag and drop file sharing\n  Recording audio and MIDI clips\n  Contacts for friends\n  Minimal, unobtrusive\n\n\nInitial Proposals\n\nFrom the initial proposals we received from the Pedál team we selected two concepts we wanted to pursue, one to document the workflow of Pedál and suggest improvements, and the other, to draw from this experience and propose a prototype that would allow more than 2 users to collaborate.\n\n\n  Research the workflow of Pedál by doing a collaborative project, document the process and suggest improvements for the partner\n  Research and describe, and possibly prototype a multi-user (2+) functionality for Pedál\n\n\nWe further refined these proposals after discussing it with the Pedál team and decided to take a three tiered approach, where each layer would provide insights unto the next. Each of us tackled an angle of this project that we each had experience in and met throughout to discuss our findings and developments. Our research question was:\n\nHow can Pedál develop towards a 2+ user functionality from the perspectives of workflow evaluation, user design, and implementation?\n\nThe B-team approached Pedál from the viewpoint of researching and identifying areas for\nimprovement to its functionality, workflow, and user interface. This involved multiple sessions\nusing the app both with each other and with musicians outside the SMC program. Multi-user\nimplementation was explored, as well as UI/UX design of the GUI itself. We divided the tasks among us like so:\n\nBreakdown of work\n\nWorkflow - Iggy, Paul\n\nEvaluation and improvements through collaborative musical project\n\nUI/UX Design - Jackson\n\nEvaluation of current design and development mock faceplate for multi-user interface\n\nImplementation - Ulrik\n\nWorking with WebRTC to develop and evaluate P2P functionality with multiple users\n\nThis translated into three broad questions that we attempted to answer through our solution and will be the order in which we layout this blog as well.\n\n\n  \n    How would it work in practice? (User evaluation)\n  \n  \n    How would it function? (Experiments with WebRTC)\n  \n  \n    How would it look? (UI/UX prototyping)\n  \n\n\nHow would it work? - User evaluation\n\nInitial Study:\n\nIn our initial study of Pedál we set out to examine the app as it exists, using a couple of different\nscenarios so that we could properly evaluate its pros and cons.\n\nThese different scenarios - we call them case studies here - were\n\n\n  Producer to Musician with technological competency\n  Producer to Musician with low technological competency\n  Multi user experience\n\n\nThe issues we experienced were pretty similar across the board in each of the use-cases. Here’s a\nquick summary:\n\nCase Study 1:\n\nThis was a one-on-one session with Iggy as producer (session host) and Paul as session guitarist. Iggy played tracks from his DAW and Paul recorded guitar parts in both Pedál directly and in his own DAW that Iggy could then add to his production.\n\nWhat we found were two main problems. When recording directly into Pedál, Paul could not use any effect on his signal, which ruled out the use of an electric guitar. (Un-amped electric guitar = uninspiring.) Of course, he could use external effects or even mic up an amp, but this both wakes the neighbors and severely limits the producer’s ability to re-amp, effect, or otherwise manipulate the track.\n\nAnother problem is there was no easy way for Iggy to time-align what Paul had recorded within his own DAW. Basically, he had to use his ear and hope it was what was intended. This works ok if what is recorded is very rhythmic and on the beat, but in the case of improvised or syncopated material it’s anyone’s guess where to place the track.\n\nCase Study 2:\n\nSince Pedál is designed for simplicity and ease-of-use, we felt it was important to attempt a session with a musician outside of our group, someone with minimal technological expertise, and see how they did with it.\n\nThis session was between Paul and a mysterious, unnamed collaborator who is both a classically- trained violinist and also a self-described technophobe.\n\nAfter everything was properly set up, the non-technological violinist had no problem with the workflow, although she had to press the record button well in advance of where she wanted to play in order to give herself time to get ready. Paul felt that it would be great to have the ability to press the record button for the musician, so all they would have to do is listen and play.\n\nOtherwise, the findings were very much the same as the first study. The violinist didn’t like her sound in the headphones, and it was very difficult to align her tracks. Several things were tried to help with this, such as having the violinist count in or tap the top of the instrument for 8 beats before her entrance, but this was still quite inaccurate.\n\nCase Study 3:\n\nUsing the information we had from the first three studies, it was Tom’s job to create a scenario where Pedál would be magically transformed into a multi-user experience using a three program solution.\n\n\n  Pedál - Used as the main seat to exchange files and conversations.\n  Listento -Used to broadcast the signal from the DAW.\n  Discord - Used for group conversations. Very flexible\n\n\nThis would allow the group to create the necessary environment to give the multi-user experience of what we presume Pedál with multi-user capabilities would be like. The results were ideal for this particular use case. However the user in the main seat had to ensure they turned off their output in Pedál and use Listento and Discord as the main source of receiving the broadcast and talking on discord.\n\nReflections from the case studies\n\n\n  Point of Connections\n    \n      No Processing Flexibilities\n      No fold-back listening for the producer while musician is recording\n      Pedál lacks effects\n      Lack of Plugin format - The flexibility of a transmission and receiving plugin node would allow for easier integration of SMPTE and file sharing.\n    \n  \n  Transmission and receiving points\n    \n      Lack of transmission to multiple listeners\n      Multiple participants means multiple licenses\n    \n  \n  Lack of MIDI control or SMPTE integration\n    \n      Recording artists needing to press record adds an extra step.\n      SMPTE integration would allow for the recording to be snapped into place in the DAW\n    \n  \n  UI/UX\n    \n      Very compact, consolidated\n        \n          Not much room to expand\n        \n      \n      Simplicity, few labels\n        \n          Balance between function and simplicity\n        \n      \n    \n  \n\n\nHow would it function? Experiments with WebRTC\n\nIn the experiments with WebRTC, our goal was to find out how the WebRTC underpinnings of Pedál possibly could be extended to work for multiple users.\n\nWebRTC\n\nWebRTC is an web API for streaming multi-media content (text, audio, video). It wraps around multiple standards, protocols and other API’s.\n\nWebRTC works peer-to-peer (P2P). P2P networks require all peers to create independent connections to all other peers. This means that the total amount of data that each peer has to send and receive increase with the size of the network, and many of the streams will be duplicates. Also, the latencies depend on the respective network connections which result in larger synchronization problems.\n\nThe most common workaround for larger P2P networks is to have a set of specialized relay servers that has the role of receiving and forwarding data from and to peers. This takes the load of the peers, but in turn centralizes the transmission. It is also costly to maintain these servers as you have to have many of them.\n\nDevelopment and evaluation of naive multi-user P2P\n\nWe developed a functioning P2P video and audio streaming application that handles multiple users. It’s a rudimentary implementation of a WebRTC service that just connects all the people who joins the session. It can be tested by going to https://handshakers.herokuapp.com/. The code can be found here: https://github.com/ulrikah\n\nThe purpose was to learn more about WebRTC, but also to be able to evaluate between us how P2P networks scale with more than two participants. The screenshot that you can see is from an evaluation session we did a couple of weeks ago.\n\n\n    \n    Screenshot from an evaluation session of our developed multi-user P2P application\n\n\nWhen there were only two of us connected, the service worked exactly how we hoped it to work. The latency was approximately as good as it is when using Zoom or Discord. When we were three in the session together, it become noticeably more difficult to communicate with each other. When all four were connected, it was practically not possible to maintain a conversation. This evaluation only reinforced the theory that naive P2P networks don’t scale very well.\n\nPossible system architecture for multi-user in Pedál\n\nBy using some of the results from the case studies as well as examining the properties of Pedál a bit more in detail, we sketched out a theoretical suggestion of a possible multi-user system architecture for Pedál. Below is a diagram showing how we envisioned this system architecture.\n\n\n    \n    Possible system architecture for multi-user in Pedál\n\n\nThe core idea behind this architecture is that the host controls which participant to collaborate with inside the GUI of the Pedál. Only the collaborator and the host are connected through a P2P connection since this connection is the most critical one in terms of latency.\n\nThe host and the musician send two duplex audio streams, one for the actual music and another for talkback. All of these channels are aggregated within the host’s Pedál and sent as one mono stream to the relay server. This stream contains the music and the talkback channels from both the host and the collaborator, and is forwarded from the relay server to the listeners. In return, the listeners send their talkback channels to the relay server. The server aggregates these channels and sends it as one mono stream to the host.\n\nIn this way, all the peers are able to hear each other. The P2P connection between host and the musician can be prioritized, and can be of high audio quality.\n\nHow would it look? - Prototype of UI/UX\n\n\n    \n    Pedál\n\n\nA new concept for a listener gallery\n\nConsistency with Pedál UI\n\n  Minimalism, essentialism\n  Conserve space maintain functionality\nCollaborator/listener user state\n  Promotion function by host\n  Follows Ulrik’s work\nA collapsible audience\n  Thin, unobtrusive\n  Listeners cannot add files or share\n  Can be muted, kicked\n\n\nAs mentioned before, our varied angles were able to inform and benefit each of our tasks. For UI this brought to mind a concept of separating collaborators and listeners, where collaborators are still two users (host and another person) and listeners could be additional guests that have access to the audio streams of the collaborators but cannot share files or record. This attempts to simplify a complex problem, both from a technical perspective and design perspective and builds off of the suggestions and potential optimizations that Ulrik mentioned in his architecture.\n\nWhat’s new?\n\n\n  A new “Invite” button appears to add new guests to the session. It switches between feature sets when 2+ users are present and keeps unused elements hidden\n  An audience gallery row is filled with listener dots. The luminance of dot as VU meter (speaker volume)\n  A new “End” button appears that allows a host to close the session for everyone\n\n\nMore users, more problems\n\nThere was a major difficulty that came along with more users, how would a host go about managing them?. The solution was a modal menu for each user that was activated upon click.\n\n\n  Host has control of management\n  Individually mute, add friend for all listeners\n  Only host can kick (don’t be rowdy!)\n  Promoting is how listeners can become collaborators, able to share files, send tracks to the host, as usual\n\n\nThis is a basic prototype of bringing this idea into fruition without rearranging the entire UI or making dramatic changes.\n\nConclusion\n\nOur research, both in the workflow and the technical implementation and design of Pedál. In our experience and documentation with Pedál, we proposed a number of improvements regarding the design and future mechanics, like native DAW integration, that would make using Pedál a more fluid experience for the end-user.\n\nMoving on to the experiments pursued by Ulrik with WebRTC, we can conclude that while WebRTC is an incredible technology for serverless, low-latency communication across two peers, as the number of the users increase, the technology is unable to cope with the network demand. Pedál will need to answer the questions, “Who needs to hear what? At which quality and at which latency?”, define clear use cases, and finding methods to optimize network capacity thereafter.\n\nAs for the UI, Jackson’s work suggests a prototype design that accommodates Pedál’s design language, but there will still be challenges in unifying this experience across all participants (collaborators and listeners).\n\nPedál fulfills a sweet spot for many musicians and beat-makers and we’re excited to see the company develop.\n",
        "url": "/applied-project/2020/12/10/pedal-applied.html"
      },
    
      {
        "title": "Sound Design for IMTEL",
        "author": "\n",
        "excerpt": "For the applied project 2, we worked on enhancing the sound components of a VR language learning application in Unity\n",
        "content": "Project Overview\n\nFor the Applied Projects 2 course, the team consisted of Thibault and Simon.\nWe were given IMTEL as our external partner, a research group focusing on developing VR and AR applications for learning purposes.\n\nThe project was a language learning application. In it, several people can connect together with a teacher in a multiplayer setting. They can talk together and the players can interact with objects in the game and by doing so the words are read out loud and players are asked to repeat them back through speech recognition.\n\nThe application consists of two scenes, a Kafé scene and a Forest one. Each scene contains different objects (e.g. table and cake in the Kafé, tent and campfire in the forest).\n\nOur solution\n\nOur solution for the project was to introduce audio middleware for the game, FMOD, and provide basic documentation of workflow to utilize this.\nWe improved the sound design for the game and added spatialization for an improved immersive experience.\n\n\n    \n    \n    Snippet from the Kafé scenes\n\n\nFMOD\n\nFMOD is a middleware and engine to handle game audio. Most who have previous experience with working with audio will immediately find familiarities with the UI being similar to that of a DAW and having many of the same functionalities.\nUsing audio middleware has certain advantages. FMOD allows\nyou to create a timeline for each event that can be triggered.\nHere you can lay out multiple or individual tracks much like in a DAW and\nprocess these in a similar way. This allows\nlooping and skipping back and forth to different timestamps. It also provides\naccess to VST’s which can all be automated through game parameters. You can let\naudio tracks trigger with different parameters e.g., pitch each time they are\ntriggered to make the event sound unique each time they are triggered, reducing\nthe need for multiple audio files within the game project itself, which can be one\nof the bigger contributors to game size and loading times.\n\n\n    \n    A project in FMOD\n\n\nSteamAudio\n\nSteamAudio is a spatializer that can virtualize the audio through the ambisonics format and decode it binaurally, and add geometrically and materially correct reverberation greatly improving immersion, which is important for VR-enabled games. By using the steam audio source, material, and geometry components within Unity we can achieve a realistic and immersive soundscape.\n\n[INSERT IMAGE]\n\n    \n    SteamAudio plugin for FMOD\n\n\nProcess\n\nThe first step was to dig into the existing project, and try to understand the existing implementation. The general architecture was done using the Unity GUI, but a lot of the logic was hiding in C# scripts. We therefore spent hours reading and meeting with programmers to make sense out of it.\n\nThen, what turned out to be the most time consuming process has been setting up FMOD and SteamAudio. Indeed, we had to try various versions of each, and follow step-by-step tutorials many times to make those pieces of software work.\n\nThen came the most interesting part, the actual sound design and implementation. We switched off the Unity audio engine, therefore disabling the existing sounds, and implemented all our sounds using the FMOD sound engine. All the words were created using the Google text-to-speech services, exported and integrated as wav files. We also did some sound design, especially for the environmental sounds, such as the Kafé background music and crows croaks in the Forest.\n\nIssues we faced\n\nOriginally we were 4 team members, but losing Rayam and Jarle along the way left us having to reduce the workload and our ambitions.\n\nThe game was originally a VR application but during the course, a desktop version was made available for us to work on. This solved a lot of issues for us as we did not have access to the necessary VR hardware.\n\nThe project was a challenge for the team, as we did not have any previous substantial experience working with Unity and C#. It became a learning experience and we relied on documentation which there was plenty of.\n\nCreating the necessary scripts for triggering the sounds were for the same reason one of the harder tasks for us to achieve. While the FMOD API is easy to understand, referring to the correct object within unity can be a challenge.\n\nLast remarks\n\nWe did not reach all the goals we wanted, such as enhancing VoIP capabilities or implementing different languages. But given the Covid situation and half of the team leaving the project, we managed to still have a working prototype, and most of all we learned a lot about gaming sound design!\n\nOur main contribution was establishing a new workflow, and documenting it extensively. We will see in the future if IMTEL is going to use it in other projects!\n",
        "url": "/applied-project/2020/12/10/sound-design-for-imtel.html"
      },
    
      {
        "title": "Interconnecting Modular Synthesizers Using the Web",
        "author": "\n",
        "excerpt": "In my thesis project, I present an approach to interconnecting modular synthesizer systems using a prototype multi-channel audio network solution made with Max/MSP. The research explores emergent affordances of such a system in the context of telematics and network music performance. At the bottom of this post there is a video demonstrating the prototype in action.\n",
        "content": "Introduction\nIn my thesis project, I present an approach to interconnecting modular synthesizer systems using a prototype multi-channel audio network solution made with Max/MSP.\nThe research explores emergent affordances of such a system in the context of telematics and network music performance. At the bottom of this post there is a video demonstrating the prototype in action.\n\nThe project involves developing a prototype network interface with Max/MSP and routing scheme for interconnecting a virtual modular synthesizer, in this case VCV Rack, with an analogue Eurorack modular synthesizer equipped with DC-coupled interface modules. The thesis contextualises itself in a historical perspective on modular synthesizers and hybrid analogue digital music studios, as well as give a literature review of four relevant papers on networked music systems. The prototype is tested with regards to latency and network in the context of network music performance and evaluated in terms of its affordances to music practice and telematics. Lastly, the thesis discusses the expanded high-level modular system and affordances provided by such a solution, and gives examples of possible ways to further research this topic.\n\nInspiration\nInspired by the historic connection between computers and electronic instruments in electroacoustic music, this project aims to expand on this approach by having the relation between the computer and analogue hardware be bi-directional and able to connect to peripheral systems, both digital and analogue.\n\nModular synthesizers are musical instruments consisting of separate modules with specific functions which a synthesist can arrange and connect, giving it an open signal path architecture. In this section I will provide some historical background on modular synthesizers, their basic function, resurgence and their connection with other hybrid analogue digital synthesizers used in early electronic and computer music.\n\n\n\nThe hardware modular used in the project\n\n\nMethods\nTo explore the affordances of the system, I made a network interface with Max/MSP capable of connecting to a another instance of itself through a network. This patching hub takes in local and remote signal channels and outputs them to local and remote destinations. In the machine running the virtual modular synthesizer, the interface connects using Dante Via, and on with the hardware modular synthesizer, a MOTU Ultralite mk4 audio interface is used to send signals to the synthesizer with ADAT.\n\n\n\nImage of the patching hub's user interface\n\n\nResults\n\nBy testing round trip latencies of various parts of the system, I was able to reveal some of the main technical issues and challenges with making a telematic music system like this.\n\nI also found that the system offered new and unique affordances for patchability, playability and connectability with modular synthesizers, letting me use more available modules, play with the network as a new environment, and make possible connections with remote locations.\n\nVideo demonstration of the system in action\n\nThe video is made using OBS, Open Broadcaster Software, to record the screen of the Windows machine running VCV Rack and Max/MSP together with video of the hardware modular synthesizer filmed with a GoPro Hero5. Audio is tapped from various places in the full patch and mixed within the MOTU sound card before being recorded, specifically the sixth input on the ES-6 module on the hardware modular synthesizer and from VCV, channels 7 and 8 are used as final output to the mix.\n\n\n",
        "url": "/masters-thesis/2020/12/15/networkedmodular.html"
      },
    
      {
        "title": "Clubhouse",
        "author": "\n",
        "excerpt": "Just a new phone or could it actually contribute to our online lives? This is not part of any course or anything, but I wanted to put down my thoughts on this new app. After all, it’s all about audio and streaming which is very SMC, so I hope it’s OK.\n",
        "content": "\n    \n\n\nThe new audio streaming app Clubhouse brings podcasting to the people, making it possible for anyone to host speeches and talks on different subjects. It’s, like other social medias, built on following different people, though everything but audio is let out - despite profile pictures, names and a brief bio. In a way it feels refreshing not being able to write anything directly to anyone, giving you the feeling that this is not yet another messenger.\n\nHere and now\nJust having audio streamed makes it entirely about the moment. Nothing is, as we know of, recorded anywhere, so what you do will only exist right when you say it. In that way it’s similar to Skype and our beloved Zoom. Still, not seeing each other calls for a different experience when communicating. It lowers some expectations on appearance and also lets you keep it going in the background while doing other things. So you might wonder why we wouldn’t just call each other? It’s different than that too. A first is that audio quality is quite impressive. Though it might have to do with people using this app in quite locations, for now at least. Practically you don’t call anyone, but make a room where people can join. Even public rooms are possible. When entering a room you can be selected to talk, or more correctly you may be granted the opportunity. So far I’ve seen this being used by artists for connecting with their fans, letting chosen fans ask them questions directly. That way you can talk to strangers without giving them personal information. A nice feature. Other usages are sure to come.\n\nMingling\nWe have soon lived socially distanced for a whole year, and parties are getting rarer and rarer for every week. At times I find myself being socially awkward with the person behind the counter in the grocery store, as it’s like I’ve forgotten what it’s like to talk to strangers in real life. I got an invitation to Clubhouse, clicked it and all of a sudden I was in a room with my friend, who invited me, and other friends she put in the room. Friends of hers, not of mine. Scary. I immediately felt an urge to say something as the room was called ‘Welcome to Henrik’ so I started talking asking about these people and getting to know them. More people entered the room and suddenly it felt like a good old mingling party, but online. It was a strange feeling, but it kind of worked. Here was a room of friends and strangers talking about the weather and good food. Just like in the pre covid world. A nice feeling.\n\nTogetherness\nFirst impressions and reflections on Clubhouse is that this minimalistic and simple approach to communicating and listening to people chat is a thing we could need. It adds a mobile, slightly fresh and organic way of talking together in a world where Zoom is tiring and still quite stationary. It’s fascinating how these simple functions have grown so popular over a vast amount of time, even more fascinating how we see these modernized phone calls as a new and exciting thing. Another element is the “podcast feeling”. Podcasts are popular and have entered the «attention market» as a mighty competitor to music. Quite often I my self prioritize listening to a podcast when I’m strolling the streets, even though I possess a particular passion for music. Not only do we want to listen to podcasts, we also want to make them. So Clubhouse enters as the platform for everyone to host their own live podcasts where they can feel funny for saying mediocre jokes. But I do not mean that as a bad thing. Not at all. Laughing together is important.\n\nNot that SMC - a reflection on segregation\nThis morning I opened Clubhouse to see what was going on, and I found a room where hip-hop producer Boi-1da was active. The room was about UFOs, so naturally I joined in. I find the subject interesting, but I’m highly source critical towards information given on the field. The room was moderated by Dr. Steve Greer who’s a retired physician and an active ufologist. For what I know, he’s a reliable source to some point, but the point I wanted to point out (point point), is the point that platforms like these could become huge contributors to conspiracy and segregation in our societies. And as much as we have experienced so far in 2021, segregation and division is not what the world needs right now. We can’t expect everyone to check references on everything that’s being said on a platform like Clubhouse, and we already know that people read fake Facebook feeds as real news. So could this contribute to utterly threaten democracy by giving us, the people, more rabbit holes to dig ourselves deep in? I’m probably overestimating the power of Clubhouse, but I still think it’s something we should take into consideration when offering people the opportunity to say whatever to whoever. To make this section slightly SMC Blog friendly I will end on stating that we have to make sure that audio streaming applications help to make the world a better place.\n\nVery SMC\nWe know that this can be used for talking, but then comes the question: will it music? I wasn’t expecting latency to be better than any other wireless connection. And it wasn’t. Luckily I guess, as it would be so typical that a new social media platform was the one to get zero latency going for everyone. So the question of will it music remains the same as with the other streaming platforms - not for rhythmic music, “great” for experimental and ambient.\n\nThank you.\n",
        "url": "/other/2021/02/02/henrikhs-clubhouse.html"
      },
    
      {
        "title": "Funky Balls",
        "author": "\n",
        "excerpt": "Want a more organic and dynamic way of mixing and applying effects? Experiment with funky balls!\n",
        "content": "\n\nThe idea\n\nWe started by considering the effects of music on the human brain and all the amazing music therapy techniques discovered so far. Then we looked for some examples of studies about music therapy and stress to understand more about how they are correlated (and if?). We found the MISTRESS project by Rudi Agius, part of the Danish Sound Cluster where they analysed the effects of music intervention on stress reduction - and the results are that of course it helps! We also found a paper about a study where they used music as a non-invasive technique of identifying stress in people (they used EEg and noticed that the brainwaves were substantially different before and after listening to music).\n\nNow, of course these studies are not enough for undebatable evidence, but we took it at face value that music is good for de-stressing and we extrapolated until we came up with the idea of creating a de-stressing audio tool. It would allow people both with and without a musical background to play around with some instruments / tracks and several effects, and in general make music in an unconventional way. It would facilitate communication through sound, without semantic content, among people who would mix / play together.\n\nTo concretise, we decided to create a web interface for any type of user for the purpose of de-stressing through making / mixing / playing around with music. It would include blobs/shapes of different colors: some would represent effects (e.g., reverb, harmonise, delay) and other would represent instruments or tracks (e.g., drums, cellos, synth, piano) and the users would then be able to move these blobs around and they would interact with each other and voila! mix / make new music. See the next sections for more details about the further inspiration and implementation.\n\nInspiration\n\nThese platforms inspired us:\n\nYUME - HELIOS\n\nChrome Music Lab\n\nGøy\n\nFinding Love\n\nBouncy Notes\n\nImplementation\n\nOne effect and one track\n\nWe started by using sliders to implement a simple version. The example directory of csound for web contains an example containing effects applied to a synth and an example that lets you upload a midi file to play.\n\n\n\nWe combined these to allow the user to upload an audio file, and apply the effects to that audio instead.\n\n\n\nMany effects and tracks\n\nNext step was to implement more tracks and keep the same possible effects for each. 6 effects per track with their own variable parameters. We used loops because of the size of wav files.\n\n\n\nEffects:\n\n  Moog Diode Ladder Filter: a digital emulation of this filter. Slider changes the cutoff frequency (moogvcf).\n  Reverb: it reverberates an input audio signal with a “natural room” frequency response. Slider changes the reverb time (reverb).\n  String Resonator (our waveguide!): a string resonator with variable fundamental frequency. Slider changes the fundamental frequency values (streson).\n  Delay: it delays an input signal by some time interval, with feedback. It’s a combination between delayr and delayw. Slider changes the feedback ratio (delay).\n  Bit-crusher: Slider controls the bit depth (custom opcode) found in the example code.\n  Harmoniser: it analyzes an audio input and generates harmonizing “voices” (or frequencies) in synchrony. The slider controls the ‘lowest expected frequency’ of the input (harmon).\n\n\nSimple interface\n\nWith all the tracks and effects shown in the image above, it is easy to become overwhelmed. So we wanted to create a simple user interface that hides this complexity and lets the user focus on exploring the different sounds and effects.  We found the Paper JS metaballs example and it seemed to suit our use-case perfectly.\n\n\n\nWe wanted a minimalist user interface that doesn’t immediately let the user know what circle represents which instrument or effect. This is to encouring exploring and relying on the audio feedback to influence the user’s choices.\n\nDynamic interface\n\nAfter implementing the PaperJS metaballs example into the project, the user experience and visual feedback was quite intuitive but the interface was still quite bland and static.  We decided to have all the effects as the same colour so that there’s no way to distinguish them, you have to interact with them to know what kind of sound they make.  The instruments all have their own colour.\n\nTo combat the static look and sound when the user is not interacting, we implemented the ability to have the balls move slowly back and forth virtically or horizontally, or combined to move in a circle.  This brought much more life to app and creates a much more pleasing visual and audio experience, allowing the user to create a dynamic “artwork” with audio feedback.\n\nFinally we wanted to allow different multi-tracks for different user experiences, so we chose a second song the user can experiment with, and made it very easy to implement more through the use of a url parameter.\n\n\n\nGetting started\n\nWhen you select a ball, you have control of it until you click again to de-select. To initially enable an instrument you must select it and increase its volume by scrolling up. The size of the instrument represents the amplitude of the instrument.\n\nThere are several controls you can use when a ball is selected:\n\n\n  space = hide the instructions\n  scroll = instrument amplitude\n  v = toggle moving vertically\n  h = toggle moving horizontally\n  d = change horizontal direction\n  up = increase vertical distance\n  down = decrease vertical distance\n  right = increase horizontal distance\n  left = decrease horizontal distance\n  f = move faster\n  s = move slower\n\n\nWhere to go and play?\n\nThere are two “interfaces” you can choose from: the sliders (mostly useful for debugging, it’s not very pretty) and the funky balls (not the best for debugging but very pretty). You can also choose between two songs. You can download the entire repository from github or you can go to these links:\n\nGithub: https://github.com/leighmurray/csound-gui-mixer/\n\nSliders, sliders, sliders: balls.leighmurray.com/controls\n\nFunky balls - first song: Leigh’s\n\nFunky balls - second song: Alena’s:not the epic movie soundtrack, but a funky drum&amp;bass loop extracted from Human Mistakes by Skelpolu\n\nFuture improvements\n\nLet’s play together! Although in the beginning we wanted to create a multi-user platform, behind the scene it would have been somewhat similar to Leigh’s Kovid Keyboard. That’s why we decided to work on other features, such as the dynamic effects instead of taking the time to integrate the already existing code. For the future, however, this connectivity feature is important to implement.\n\nWanna see something cool? Another feature that helps with the connectivity side is to be able to save the project and send it to someone - and enable them to pick it where you left off. Sharing work and being able to save it for later is important!\n\nCan I play with my own music? Enabling a feature where users can upload their own tracks to play with. This complicates things of course, because we would have to maybe consider a flexible number of instruments/tracks.\n\nWhat if I remember the positions? To avoid people using the same effects on the same instruments, it would be good to randomise ball positions and colours.\n\nWhat about loops? As per Henrik’s idea, it would be great to let the user record their own audio to create their own loops.  There is a csound for web example that takes input from the user’s microphone so it would be fun to implement this feature.\n\nHow else can I controll the effects? It would be interesting to enable camera control. By that we mean that head/hand movements would control the position of the instruments or tracks.\n",
        "url": "/sound-programming/2021/02/17/alenacl-funky-balls.html"
      },
    
      {
        "title": "Trinity: Triple Threat",
        "author": "\n",
        "excerpt": "Trinity: What happens when you combine Grain shimmer + Chorus + Stereo Width? Click to find.\n",
        "content": "TRINITY\n\nTRIPLE DSP FOR TRIPLE THE FUN\n\nCONCEPT\n\nWhat is Trinity?\n\nTrinity is a Granular Shimmer/Chorus effect programmed to create a distinctive space manipulation through the use of granular processing and modulation.\n\nIn developing the effect, the workload was spread out into three sections allocated to each of us.\n\n1.\tShimmer Reverb - Tom Ignatius\n2.\tChorus - Abhishek Choubey\n3.\tStereo Spread - Lindsay Charles\n\n\nThe eventual individual processing would be then summed accordingly through the various ins and outs of the Csound code.\n\nSIGNAL FLOW:\n\n\n   \n   Caption Text\n\nThe signal flow of Trinity is simple and linear with a few quirks to give it its signature sound. The inputs of the DAW go into the shimmer then into the chorus and lastly processed through the width matrix at the end. However, what makes the effect complex is the feedback of the granular processing is fed by the chorus output and not the first pass of the shimmer.\n\nThe chorus operates with four voices. Two voices are assigned to each of the two inputs. The resulting output of the chorus is 4 channels that are then fed to the width and pan control that allows for those 4 voices to be spread in the stereo field.\n\nTHE ELEMENTS\n\nShimmer:\n\n\n   \n   \n\n\nThe Shimmer effect’s engine is based on Øyvind Brandtsegg’s Partikkel Reverb. It’s not meant to sound like a replication of a true space but a creative stretch of the original audio files. To increase the resolution of the sampling, I modified the code to allow for 8 waveforms to be stretched out. To be able to compensate for restrictions of the partikkel, I duplicated the partikkel opcode to allow for the 8 sampled sections.\n\nThe feedback input is processed post to the chorus effect. The 4 signals are summed and sent back to the granular element allowing for an even richer palette of sounds.\n\nChorus:\n\n\n   \n   \n\n\nThis is an extension of the chorus effect in my plugin Butter, which I created in my physical computing module in the first semester. Although it uses the same opcode for delaying the signal the process is very different in this one. The chorus now combines four instances of the opcode called vDelay which is an interpolation time delay function. The delay time is modulated by an oscillator and the frequency of this oscillator is controlled by the user. Each instance uses this combination and produces four chorused signals for a deeper output. The phase of the oscillator which is controlling the delay-time is different in each instance that spawns a slightly varied signal creating a lush chorus.\n\nThe four outputs of the chorus as mentioned before are fed back into the reverb producing a richer sound. The final output coming out of the chorus unit is then fed into a highpass filter which can be controlled by the user. There is also an option to phase the dry signal feeding into the delays, by utilizing the phase knob.\n\nStereo Width:\n\n\n   \n   \n\n\nThe initial idea for this effect was to have 2 incoming channels which will be processed into a stereo field, each having Width and Panning parameters respectively. But we decided that as  the chorus is sending out 4 delayed channels excluding the dry signal, the width will use these  4 channels and place them at different positions into a stereo field. The position of the signal can be controlled by the width parameter. The Pan knob pans the signals to right or left.\nEach signal is placed so that the chorus delays are equally distributed and the dry signal is in the center or in mono.\nWhen the Width is to the extreme left or zero position, the signals are squashed at the center making it mono, on the other hand when the Width is on the other extreme or One Position, the signals are spread out in the stereo field.\nThe Panning effect can only be noticed when the width is less than maximum as the signals will be spread out the farthest at maximum width causing no change in pan.\n\nARCHITECTURE &amp; DESIGN\n\n\n   \n   \n\n\nThe design of Trinity is based on Retrowave aesthetics rooted in the 80’s. It goes from the top left, bottom left and then into the width and wet sections.\n\nThe architecture of the program was to restrict the effects from being rearranged and used purely as a chorus or just as a shimmer. Trinity is therefore not a multi-effect plugin but purely an effect of it’s own character. Therefore, it only ranges from the original signal to the “trinity” effect adjustable between them through the dry and wet.\n\nDEMO\n\n\n   \n   Trinity_Demo\n\n\nDownload Trinity here Trinity\n\nFINAL THOUGHTS: REFLECTIONS\n\nIggy:\n\nThe process of getting a grasp of the partikkel opcode was almost impossible in the time frame. At the final stages of this process, I am still in the process of trying to understand the code used. However, within the two weeks of active programming, I have been able to translate some of my reaktor knowledge into Csound to allow me to add the finishing touches of the code.\n\nAs a recreational UX/UI designer, it gave me an opportunity to give the plugin a design language. In this case, it was a retrowave aesthetic. However, thinking back, I would love to be able to implement filmstrips into the plugin allowing for a more distinctive look compared to the standard knobs available with Csound.\n\nThere was an initial lack of communication in my group resulting in a misunderstanding of the infrastructure of the plugin however we’ve managed to pull through in finishing the plugin.\n\nAbhishek:\n\nThese two weeks were exciting, I learned a lot about DSP and Csound and made a combination of some of my favorite effects, also working with Lindsay and Iggy was fun and interesting, they both taught me a lot about audio in general. We had a communication gap initially which made the direction of our project ambiguous but in the end, we managed to work towards a common goal and it turned out pretty well.\nThis time it felt much easier to work with Csound in comparison to the short module we did in our first semester because of the basics we learned then. I am not fully satisfied with the output, I think we can fine-tune it a lot more. I feel if the course was longer we would have managed to make a more polished instance of Trinity, but that’s what we will continue working on to develop a better version of our retro-designed plugin. On another note, I feel the chorus in itself produces a very rich and wide sound so I will also be making a standalone part of it.\n\nLindsay:\n\nSince the last semester, although working on Csound has gotten easier, the increase in learning and usability curve has taken more time than I thought it would.\nWorking on this project has given me more insight into the spatial world and a better understanding of how the signals are developed for stereo. The integration between the effect chain was not as difficult as I imagined, which gave me time to understand the blocks.\nThe “Trinity” is definitely going to be my favorite plug-in to use among other shimmers, I think the output that we have attained is quite close to the one we planned to execute, despite some setbacks with Stereo Panning.\n",
        "url": "/sound-programming/2021/02/18/abhishec-Trinity.html"
      },
    
      {
        "title": "Get unstressed with Stress-less",
        "author": "\n",
        "excerpt": "Acoustically-triggered heart rate entrainment (AHRE)\n",
        "content": "1. Introduction\n\n1. 1 Background to the project\n\nThe heart is one of the most important parts of the body. It is not by chance that it was already studied during the ancient times, for example by Aristotle (The History of the Heart, n.d.). Since William Harvey’s discovery in the 17th century, there have been even more interests in studying the heart (Ribatti, 2009). One of the interesting features of the heart is the heart rates. The heart rate is an important indication of the status of a person’s health, and there have been efforts to show how the heart rate could be entrained to acoustic stimulus (Saperston, 1993). More recently, fast paced music was shown to entrain the heart rate (Hong et al., 2011). However, these outcomes have been debated and experimentally illustrated (drum beats were used as a stimulus) that there is no strong relationship between acoustic stimulus and heart rate (Mütze et al., 2018). Before we settle on that, it is also important to note that the fetal heart was able to be entrained to the mother’s heart rate through the fetal auditory system (Ivanov et al., 2009). Perhaps, the studies where unnatural stimulus was used, such as complex and fast paced music and drum strokes, should be re-evaluated against experimental studies where more natural acoustic stimulus (i.e., heartbeat) was used. To the best of our knowledge, there has not been a study where real heartbeats or realistically simulated heartbeats through (either recordings or artificially constructed through audio programming) were used to entrain the heart rates.\n\nThis is where Stress-less comes in as an audio environment and therapeutic device where users can tune into the desired pace and sounding heartbeats. The audio parameters will give the user control to manipulate the pace and certain qualities/characteristics of the audio. It first collects the heart rate from the user remotely with a video processing and plays back heartbeats at the rate according to the captured data. The pace gradually changes towards the desired (e.g., resting- or energized-state) heart rate. This is developed and aimed to be used as a therapeutic device privately and also with health professionals in their practice, preferably with full-range loudspeakers for more effective entrainment.\n\n1.2 The timeline and overall plan\n\n\n   \n   Figure 1. Project Timeline\n\n\nThe workshop lasted for two weeks (Figure 1). We planned to spend the first week doing research on the topic (heart rate entrainment) and go through the ideation process (Figure 2) while running some basic experiments on Csound. Since everyone in this project was new to Csound, we planned to spend a decent amount of time understanding the basic elements (e.g., structure, syntax) of the language in the first week. With this limitation, we realised that we won’t have enough time to have our “dream program” fully working within two weeks. So by the end of the second week, we planned to have a basic prototype completed. Also, considering our background (a mixture of music technology and design) and limited amount of time, we decided to divide our work into segments depending on our expertise while taking up enough new audio programming challenges for everyone.\n\n\n   \n   Figure 2. Ideation process\n\n\n1.3 Concept development\n\nWe aimed to develop a program that can:\n\n  Capture the user/patient’s heart rate\n  Play simulated heartbeat at the captured rate\n  The simulated heartbeat gradually (controllable) synchronises to the desired rate (controllable) over time\n\n\n\n   \n   Figure 3. Concept development 1\n\n\n\n   \n   Figure 4. Concept development 2\n\n\n2. Key features in the prototype\n\nWe used Csound and Python for our prototype. The two languages are connected via Open Sound Control (OSC). Here we discuss the key features, and also some of the challenges and solutions we dealt with during the two weeks.\n\n2.1 Capturing heart rate\n\nAs illustrated in Figure 3, we needed to be able to capture the user/patient’s heart rate. We have adapted the Eulerian video magnification (EVM) as our tool to capture heart rate remotely. EVM is a computational technique for visualising subtle colour and motion variations in ordinary videos by making the variations larger. EVM helps to detect small changes that are usually not possible to see with naked eye. Some of the other applications can be recovering sounds from detecting vibrations of objects in distance and characterise material properties. This technique can help to enhance digital healthcare experience for both patients and doctors. More information can be found on this website.\n\nFor our prototype, we have adapted EVM in Python. After analysing a video recording (.mov) of the user/patient’s face, EVM outputs a heart rate (in BPM). This value is then sent to Csound via OSC.\n\n2.2 Phase-locked loop (PLL)\n\nPLL is used as our control system in Csound (Opcode) where the synchronisation processing takes place. PLL user defined opcode in our case takes a clock pulse, initial frequency, frequency adjust gain, and phase adjust gain as inputs. By adjusting the frequency and phase adjust gains, we could control how slow or fast it takes until the input pulse is synchronised with the clock (Figure 5).\n\n\n   \n   Figure 5. Frequency and phase gain adjustment in the PLL opcode\n\n\n\n4. The prototype demonstration\n\n\n    \n    Synchronisation demonstration\n\n\n5. Sonification\n\nIn order to synthesize a heartbeat sound, we needed to understand some of the basic acoustic characteristics of real heartbeat sound. This was an important step as we wanted to create a sound that resembles a real heart.\n\nThe first step we took was to run Fourier Transform to find the fundamental frequency from our acoustic model. We concluded that the most important frequency content is around 75~85 Hz as illustrated in Figure 6.\n\n\n    Figure 6. Fourier Transform result\n\n\nWe then studied the envelope of our acoustic model (Figure 7). For the time being, we were able to imitate this on another audio programming language (MAX/MSP, Figure 8), as we needed more time to implement this in Csound, and created a demo.\n\n\n    Figure 7. Waveform of a recorded heartbeat\n\n\n\n  \n    \n    Real Heart Beat\n  \n  Real heartbeats\n\n\n\n    Figure 8. MAX/MSP patch: Heartbeat simulation demo\n\n\n\nThe result of the demo (Figure 9) resembles the real heartbeat to some extent, but there is more work to be done (e.g., the attack time of the second pulse) in the future to sculpt the sound.\n\n\n    Figure 9. Waveform of the simulated heartbeat\n\n\n\n\n\n  \n    \n    Song One\n  \n  Simulated heartbeats\n\n\n6. Reflection\n\nDongho:\n\nConsidering we were very new to Csound, we have made great progress within the given time frame. We managed to develop a working prototype using a new language! This prototype will be a good foundation and model to develop the program further in the future. More in depth understanding of the core features of the prototype will help the future development which will require more time and effort.\n\nIt would have been great to spend more time on examining the acoustic sound quality and characteristics of real heartbeat sounds. The direction we took (FFT and qualitative envelope analysis) and its result seems like it’s a good starting point. Perhaps, there is a room for other musical features in the future development than only imitating the real heartbeat sound. This should be explored and experimented further.\n\nAlthough there is disagreement among in the current literature whether or not acoustic stimulus has an impact on heart rate entrainment, there seems to be a great research opportunity. As mentioned earlier, more range of controllable stimuli should be explored. Also, there seems to be an opportunity for development of a therapeutic device for private use and healthcare environments.\n\nJoni:\n\nWith the limited time, we were able to capture the heart rate using video analysis in Python and perform synchronisation through PLL in Csound. What we have established in this workshop has given us a good starting point to develop our app idea in the future (Figure 9). The heart rate synchronised technology will have more features, such as allowing users to change the EQ and set the standard heart rate of their own. Additional features include suitable mindfulness music or soundscapes to users whenever their heart rate is detected as faster than average.\n\nFurthermore, the aesthetic aspects of the heartbeat sound could be better and that we could add more layers. In our app idea, it is designed for users who can keep track of their heartbeat by taking a video.\n\nFinally, in the past two weeks, I have learned a lot and my brain fried in a good way. Now, I am able to interpret and modify codes more efficiently and gain understanding of the fundamentals of digital audio signal processing.\n\n\n    Figure 9. Our app idea as a video heart rate capture tools\n\n\n7. Our vision\n\nIn our future vision of this project, this idea of using video heart rate analysis technique can be implemented in online GP consultation, so that the patients and doctors do not need to necessarily meet physically. It can reduce a lot of time spent in the whole travelling journey  from home to the doctors. In addition, with an on-going pandemic, and economic downturn, we believe that this idea can accelerate the evolution of healthcare ecosystems. As we move forward, organisations can consider ways to use healthcare ecosystems to improve patient experience and health, while reducing total costs.\n\n\n   \n   Figure 11. Future digital healthcare systems \n\n\nReferences\n\nHong, C., Hsiao, W., Wang, H., Huang, S., Shao, K., Luo, S., Chiu, W., Lee, Y., Hou, M. C., Chao, S., Tseng, C., &amp; Chen, W. (2011). The Sustained Exhilarating Cardiac Responses after Listening to the Very Fast and Complex Rhythm. 2011 Second International Conference on Innovations in Bio-Inspired Computing and Applications, 53–56. https://doi.org/10.1109/IBICA.2011.18\n\nIvanov, P., Ma, Q., &amp; Bartsch, R. (2009). Maternal-fetal heart beat phase synchronization. Proceedings of the National Academy of Sciences of the United States of America, 106, 13641–13642. https://doi.org/10.1073/pnas.0906987106\n\nMütze, H., Kopiez, R., &amp; Wolf, A. (2018). The effect of a rhythmic pulse on the heart rate: Little evidence for rhythmical ‘entrainment’ and ‘synchronization.’ Musicae Scientiae, 24, 102986491881780. https://doi.org/10.1177/1029864918817805\n\nRibatti, D. (2009). William Harvey and the discovery of the circulation of the blood. Journal of Angiogenesis Research, 1, 3. https://doi.org/10.1186/2040-2384-1-3\n\nSaperston, B. M. (1993). Method  for  influencing  physiological  processes  through  physiologically  interactive  stimuli (Patent No. 5267942). https://www.freepatentsonline.com/5267942.pdf\n\nThe History of the Heart. (n.d.). Retrieved February 19, 2021, from https://web.stanford.edu/class/history13/earlysciencelab/body/heartpages/heart.html\n",
        "url": "/sound-programming/2021/02/19/joni-stress-less.html"
      },
    
      {
        "title": "Tapeloop DT-20",
        "author": "\n",
        "excerpt": "For the SMC4048 we wanted to make some looper with layers and FX. But what did we get? The DT-20, a 4-track inspired digital tape looper with FX channels and a ‘WASD’-mixer. Enjoy! - Stephen, Pedro, Anders &amp; Henrik\n",
        "content": "\n    \n\n\nThe Idea\nThe initial idea was to have a looper keeping track of each loop as a new layer, so it could be possible to browse your way through the history of the loop by muting and unmuting layers on the fly. In addition the idea included slicing of the chosen layer. Slicing based on some MIR in the style of the Scientific Computing exam from 2020. The reason would be that the looper then could «listen» to the loop layer and, based on what you played, reorganize the audio in slices giving you, the user, a new musical suggestion.\nAfter discussing ideas as a team, and after a week of work, we figured that we would have to aim for a slightly different approach to actually get the instrument working in time. After the first week we had the basic looper function plus nearly done effects to put in. What we wanted to implement was a way to send the loops into the effects and then feedback the effects back into the loop recorder so that it would gradually get more and more effected. An interesting looper approach seen in Mood by Chase Bliss Audio. We figured the best thing would be to simply start by including the effects as FX-return channels receiving send audio from the loops. A simple start. After some thinking it all worked, and we started playing around with the looper and figured this simple functionality was quite good. Considering the time we had left it felt like a better idea to focus on getting this idea to work properly rather than stepping away from it and maybe end up somewhere not so inspiring. So we did.\n\nThe DT-20\nThe result is the DT-20, a 4-track style looper taking inspiration from tape looping in a digital manner. The recording and playback is driven by a «digital motor» which can be controlled by pitch and with wow and flutter. It’s a digital tape degrading audio in a digital way. To avoid sample drops and copies when recording the motor records 44100 samples/sec at double speed, meaning that when driving it at «normal speed» it records 22050 samples /sec. Calling it «normal speed» as you can decide yourself where 1:1 playback speed is by adjusting the speed parameter before starting the first recording. At lowest speed, half time, the sample rate is 10025. And then there is all the in-between values where the digital tape really shines with recorded sample degradation happening, adding charm and love to the recordings. The instrument has a stereo mixer/preamp (with overdrive on high volumes) with FX on sends. The effects will be described in separate parts of this post.\nThe interface has a «semi hardware» approach. Since the instrument is based on old school lo-fi approach towards recording and music making, we felt that this should be reflected in the interface. So everything is computer keyboard controlled inspired by Commodore 64 way of controlling the computer. We used the ascii-numbers for the keys to identify what key is being pressed, and the ascii is reflected in the interface of the instrument which is solely based on ascii as well. It’s an interface that allows you to monitor what you’re doing with the keys. Highly convenient. We slowly managed to use the keyboard without the interface, but it’s a tedious process of confusion and doubts. Especially when debugging other functionalities at the same time.\nThe keyboard actions are mainly based on the position of the left hand around the ‘WASD’-keys. The mixer part is enabled by holding the shift key. We tried a lot of different approaches to this, also managing to make regular keys act as modulator keys, and really wanted this distinctly retro approach to interfacing feel natural in the century of touch screen. We’re not the ones to judge this, so we can only hope that you feel the same as us about this.\n\n\n\nDemo of the DT-20 prototype\n\n\nThe Philosophy\nNothing is perfect, but some things may still be considered perfecterer than others. Many of us would argue that digital hi-fi is more perfect, in terms of audio quality, than an old gramophone record. And objectively many of us would kind of agree. But how these different different mediums color and affect the sound complicates the question of perfectness, lifting it to the domain of subjective opinions and musical taste or preferences.\nSo for the DT-20 the approach to this topic is that degraded audio can have a positive and inspiring effect on recorded audio, playing back a slightly different sound quality than the one you put into it. It doesn’t sound like a good idea in writing, but the experience of it is playful and creative. Sometimes perfect isn’t the most interesting thing necessarily, sometimes you just want to go back to easier times - degrading in both time and audio.\n\n  \n    \n    Should show a media player\n  \n  An audio demo of the DT-20\n\n\n\n  \n    \n    Should show a media player\n  \n  Another audio demo\n\n\nThe Effects\n\nGranola\n\nThis effect is a Csound unit that uses time-based granular synthesis techniques with additional effects. The diagram for the signal flow and parameter modification is shown in the following figure. The implementation details are given below.\n\n\n    \n\n\nThis unit is wrapped as an opcode that receives an input signal with the possibility to modify six parameters. The main process is performed by the opcode Partikkel which allows a wide flexibility to manipulate time-based granulation. Most of the input data that this opcode receives is the default and other fixed parameters in which some important ones are: the envelope with its corresponding ratios, random positioning for the source wave, and a mask to distribute grains in three outputs. The user can modify the grain frequency and grain duration for the Partikkel.\n\nAfter getting the three output signals from the granular processor, each one is scaled in terms of amplitude by considering the overlapping of grains to avoid distortion, an ‘Amplitude factor’ is calculated and applied to the outputs for this purpose. Then, each signal is scaling by pitch randomly between semitones -12, -7, -4, - 3, 0, 3, 4, 7, 12 as well as displaced by the semitone factor given by the user. The results after the pitch process try to mimic a kind of chord that is obtained by the addition of the three signals and the right scaling. The next step in the chain is the application of a low pass filter in which the user can change the cut off frequency and the resonance, after this operation the signal is passing through an expander due that the previous amplitude scaling could be more than necessary because of the envelope, so this is a way to bring to better levels. Finally, a sine LFO is used to produce two signals with a moving stereo effect, which will be the final outputs for this unit.\n\nMilk (AKA meshverb)\nMilk, the DT-20s reverb unit, started out as a waveguide based spring reverb emulation. When we realised the potential of the waveguide approach, a multitude of meshes of different shapes, sizes and complexity emerged. The configuration possibilities seemed endless, being able to route audio into and out of any point on the mesh, and being able to use varying delay times to give different nodes of the mesh different physical properies. All this complexity had to be tamed, not least because it was so easy to drive the thing into uncontrolled feedback. So we decided on a preset system, with each preset using a particular shaped mesh and pre-configured input/output locations, delay and feedback times.\n\nBread\nCombining grains makes a bread. Initially Anders wanted to work with effects inserted inside the looper, for experimenting with different degrading over time, but as we didn’t get to implement that function, he decided to work with the ‘partikkel’ opcode, and make some time-skewed presets with long grains (breads). So he ended up with five pretty nice presets (IHO at least) where the two of four breads where delayed (one of them fixed at 300 ms, the other one from 5 to 5000 ms), and reverberated, as well as panned. Anders being in love with the EHX DMM, the presets are very audible, but pretty subtle at the same time. You can easily use them without disturbing your original signal, according to him. He describes the five presets:\n\nNo1\n0.5 grain/sec, long grains with no enveloping/windowing, some grains reverberated and delayed 900 ms. Try plucking instrument.\nNo2\n1.6 grain/sec, slow attack, some grains reverberated and delayed 2750 ms. Try droning.\nNo3\n2 grains/sec, slower attack, some grains heavily reverberated. Try drones and plucking.\nNo4\n5 grains/sec, slowest attack, low reverberation. Try plucking and droning.\nNo5\n15 grains/sec, middle attack, some grains reverberated and delayed 5 sec. Try drones and plucking.\n",
        "url": "/sound-programming/2021/02/19/henrikhs-DT20.html"
      },
    
      {
        "title": "Twinkle Twinkle (sad) Little Mars",
        "author": "\n",
        "excerpt": "Finally the B-boys found some hours one evening to spend in the portal, Willie up north, and Pedro, Henrik and Anders down south. This was the first day on their Mission to Mars. Enter our B-oyager, and join us.\n",
        "content": "\n    \n\n\nThe Mission\nOn the evening of February 22nd, the very first sounds ever recorded on Mars were made available to the world. Just some blowing in the wind, but still Anders was inspired to go on a Mission to Mars, sampled the sounds on his tiny Pocket Operator K.O! and called up the rest of the B-boys, who of course immediately commenced countdown, engines on. So on February 23rd, we took our protein pills and put our helmets on.\n\nGround control\nBut as usual, there were some issues on the ground, feedback among them. We had more issues with feedback when speaking from city to city than expected. Our goal was to first try to set up and gain our portals to make talking and communicating verbally easier than we so far have experienced in classes, but even after gaining every connected microphones nicely, we didn’t quite achieve what we wanted. So we did some compromising and moved on to connecting some instruments.\n\nIn Trondheim, Willie played a pickup-equipped western guitar routed through a series of AmpliTube effects and connected to the Midas M32. In Oslo, Pedro brought his guitar, his PC, and his AKAI midi-board (everything connected via his sound-interface and lined in to the Midas M32), while Henrik played his sweet little Teenage Engineering OP-1 (also directly into the mixer). Anders, mostly sitting behind the control-panel (M32), connected his K.O! with a looped wind-sample from Mars.\n\nTwinkle twinkle\nThe countdown started, and soon we set out into the unknown, without knowing where to start, but it didn’t take long before we saw familiar constellations, and also heard the saddest version of «Twinkle Twinkle Little Star» from Henrik’s OP-‘ on top of the floating reverberated and delayed plucking of Willie’s guitar, and some otherworldly sounds from Pedro’s computer along with his guitar picking. Mars is far away, even when passing the moon, but when we listened closely, we could also hear the wind blowing in a rhythmic pattern on our supposedly desolate red destination.\n\n\n\nIn space no one can hear you scream, but they might hear Twinkle Twinkle… sad version.\n\n\nBack to earth\nBehind the control panel of our B-oyager we discovered a severe error that could possibly explain some of our feedback issues before take-off. We returned to earth to start debugging and solve the issue. Even when no local instruments at UiO were sent to the master, they were very much there. That could be expected after sending our mix to Trondheim, and then possibly getting it back via microphones there. But as it firstly didn’t sound like room-sound, and secondly since all microphones at NTNU were muted, we dug deeper.\nWhat we discovered was that our signal sent to NTNU (stereo track via LoLa) was internally routed back to our mix, on the same tracks we received NTNU’s LoLa.\n\nBefore we figured out a way to try and solve the issue, the NTNU-portal was abandoned due to budget cuts at NASA. All American astronauts in the B-oyager program are now expected to fund further expeditions via NASA’s new dog-walking business. Once Willie has finished walking the dogs of various wealthy senators, we will explore this further, and hopefully our next attempt will take us one small step closer to Mars.\n\n\n   \n   Oslo, we have a problem !!!!\n\n",
        "url": "/networked-music/2021/02/28/anderlid-mars1.html"
      },
    
      {
        "title": "Liebesgruß or we can put that 'Liebe' aside",
        "author": "\n",
        "excerpt": "It’s simply a gruß from Team C with our first telematic setup.\n",
        "content": "\n    \n\n\nThe following section presents Joni’s reflection on Team C’s direction in Portal connection. Lindsay and Wenbo reported the setup in Trondheim and Oslo.\n\nTrondheim’s call for love\n\nOn 24th February during the winter break,  the most introverted C-team from Trondheim and Oslo creeped into the SMC Portal in the middle (not of the night) of the day. The solo Trondheim crew was able to rock the whole Portal by himself (Well-done Lindsay!).\n\n‘’I connected the Midas M32 to Ableton live on a Windows computer, so the M32 audio interface drivers needed to be installed. At Trondheim we only have one person playing the harp on the piano as MIDI. For some reason the ceiling mics were not functional, so we used an AKG Mic as a talk-back mic to communicate with Oslo. Overall only 2 channels were being sent to Oslo via LoLa. We had problems communicating with each other at first, there were many sudden cuts and outs from the Oslo side but we managed to use discord and sort things out. Once we got the line to work with a stable connection, we focused on sending an appropriate mix to each other, more so from Oslo as they were 3 instruments with 3 mics and a lot of EQ. We recorded the session on Ableton live in Trondheim. Hey you guys there from Oslo?’’, wrote Lindsay.\n\nOslo’s acceptance of love\n\nWenbo then replied: ‘’Group C tried to connect Midas M32 to Macbook as an external audio interface and used Pro Tools for multi-track recording. Midas does not need to install drivers in the OS X environment, only needs to set DN32 as the default audio device in the audio settings of the system and DAW. At Oslo, we have three musical instruments: bass, flute and electric piano. Bass and flute are acoustic instruments, so we used two AKG C414 condenser microphones and selected cardioid direction for pickup. At the beginning, we tried to connect the piano directly to the mixer through the phone out, but considering the complexity of monitoring and recording level settings, we chose to use another microphone to pick up the sound directly. To avoid feedback noise between Oslo and Trondheim, we set EQ for each microphone. But during the rehearsal, we found that the flute’s spot microphone would have unexplainable low-frequency noise at around 80hz even when it was quiet. After eliminating the equipment problem, we used an exaggerated high-pass setting to solve this problem. In this rehearsal, we still have some doubts about the routing of the BUS track and AUX track of the mixer corresponding to the DAW. We hope to explore further in the next session.’’\n\nFinally, we learned that the feedback issue was caused by having multiple platforms running together at the same time. In order to avoid those mysterious, unnecessary sounds, we should bear in mind that muting the right platform or bus in the mixer can do the magic. It is all about muting the right audio signal senders to the receivers and vice versa. We learned from Team B on the 1st March session - cheers Henrik!\n\nQuick comment about the music\nMusically-speaking, we should discuss more about phrasing and expression of the song. This piece was written by Edward Elgar as an engagement present to his then-wife Caroline-Alice Elgar. Therefore, besides having an ensemble with low latency, one of the most important elements to interpret this piece is timbre. In order to express the strong emotion of love, we need to understand the music and phrasing well before we can ‘talk’ about the love in different tone colours.\n\nNevertheless, we sight-read this for the first time and improvised alongside the performing process. Although two of us haven’t played music for some years (Joni hasn’t played for 8 years and Wenbo for 3 years), we had quite a fun rehearsal on that day.\n\nHowever, there is nothing set in stone with the choice of music yet. Who knows? Although it was originally written as a ‘Liebesgruß (A loving greeting)’, it might end up as a jazzy greeting or classical greets the jazz. Feel free to share your thoughts with us if you have. :)\n\n\n  \n    \n    Song One\n  \n  Audio Recording of the Rehearsal\n\n\n\n    \n    Video Recording of the Rehearsal\n\n",
        "url": "/networked-music/2021/02/28/joni-teamcjam.html"
      },
    
      {
        "title": "First Weeks Of Portaling With Team B",
        "author": "\n",
        "excerpt": "Starting to figure out the Portal and what we did/found out in the first weeks using it. How to play and some feedback fixing.\n",
        "content": "\n    EQ-ing the master bus\n\n\n          The following section\n                    presents Henrik's reflection\n                                  on Team B's direction\n                                                in Portal connection\n\n\nPlaying together\nMy experience with telematic music performance has been close to nonexistent prior to joining the SMC master’s program. Not that doing music online is something I find totally uninteresting, but I think it has to do with my line of work and the relevance of doing music online instead of in the same physical space. So the experience has been quite new to me. Last fall we jumped into playing online together from our homes using JamKazam and such, which was a bumpy ride for many of us. Much because of struggles getting servers up and running, but also then having everything connected and figuring out how to navigate musically without seeing each other properly. It quickly became messy.\n\nSo with this experience in our telematic backpacks, playing together in the Portal seemed promising as the room is set up to be more audiovisual than explicitly audial. With everything running, which took some time, the first obstacle was getting the sound right. Sending many people’s instruments on a stereo connection, without a proper soundcheck, is a pretentious project for new Portalers. Trondheim had issues hearing us with well balanced levels, and we had issues hearing them when playing. So we figured trying fewer people playing and sending two instruments as one on the left channel and one on the right channel gave more flexibility. That gives each campus the ability to mix two instruments individually as a «multichannel» setup for a total of four (two by two) musicians.\n\nHaving some control over the sound, the next obstacle was musical rather than technical. As many of us know, it can be hard playing together for the first time. Especially when not physically together and having some audience. Not being that used to just jamming around, we quickly figured out that picking a song to play that all of us knew was probably a good idea. And it worked in a way. Here’s a snippet of All of Us, playing «All of Me».\n\n\n   \n   All of us playing \"All Of Me\"\n\n\nStill a little hard to keep track of tempo and navigate who’s playing what, but then again we all have a song to stick to. We kept on choosing songs and also did a kind of successful version of «It Ain’t Over» with Anders, Pedro, and Henrik in Oslo playing with Willie in Trondheim.\n\nProcessing a Player\nAnother technique we wanted to try was processing a player playing in Trondheim from Oslo and having a processing-based musical feedback between the locations. For this I, Henrik in Oslo, played with Lindsay in Trondheim. He played the piano and I processed it with the «COW» effect on the OP-1, which is a delay/modulation based effect. This was a jam based approach to playing and our experience with it was very good actually. Only being two performers with distinct differences in sound made the navigation quite easy as we both easily heard what the other one was doing. And only having one sound producing musician makes it easier to switch around keys and tempos as the processing will eventually follow.\n\n\n   \n   Lindsay playing piano through OP-1 COW\n\n\nThe hardest part was doing the routing and figuring out the post/pre fader options on the mixer, but I will not consider this something to debate further as it’s simply a matter of figuring out the specifications and settings of the Midas PRO2.\n\nThoughts on Feedback\nLastly, these last weeks I wanted to fiddle around with EQ in an attempt to reduce feedback and resonant frequencies between NTNU and UiO. The mixer has the option to insert effects on the local master bus, so I wanted to put a graphical EQ on the sound on our side to experiment. Taking out narrow frequencies is a common way to reduce feedback between microphones and monitors using graphical EQs normally. Applying this on the Portal was, as are many things in this world, somewhat easier said than done. In the beginning it was quite easy getting rid of the highly resonant frequencies, but as they were lowered, other frequencies got the room to play and then they started resonating. Shortly the whole signal was EQ-ed to death, effectively just lowering the overall amplitude of the whole signal due to lowering almost all bands. So I started over with that in mind. The main issue was around 550 Hz, 800Hz and 260Hz, roughly, in addition to some hi mid and hi frequencies around 2,3kHz. It helped somewhat, but the received sound looses some body and feels a little thin when going hard on the EQ. This needs refining and tuning and a lot more testing. Maybe also measuring of the room and setting up all the EQs from scratch. It could be a great way to learn.\n",
        "url": "/networked-music/2021/03/03/henrikhs-first-weeks.html"
      },
    
      {
        "title": " Tele-A-Jammin: the recipe",
        "author": "\n",
        "excerpt": "Our portal experience mapped on a delicious Jam recipe\n",
        "content": "\n    Setting up portal\n\nTele-A-Jammin\n\nAs the spring comes and fruits will be ready to harvest in no time, have you thought of making some jam? After several hiccups, the 2020 SMC generation is finally allowed to access the portal, so this is team A(wesome)’s first attempt at a jam in the NTNU-UiO portals.\n\nHow to (make) jam\n\nStep 1: Pick the berries aka Pick a song!\n\nBelieve it or not, this step took us the most time. To continue with the metaphor, we could say that  the berries were either not ripe enough, too sweet or too sour. After going back and forth to try to decide which instruments each of us is going to play and if we’ll sing, we finally decided on the Intro of the album ..?.. by The xx.\n\nStep 2: Prepare the equipment aka Setup the connection\n\nThe same way you’d prepare the jars and the pot to make jam, we had to prepare the connection between the two portals. We intended to start with Zoom, but the tablet in NTNU was out of power (despite having been plugged in), so we had to change the plan and go on with the low latency and high quality connections.\n\nConnecting LoLa went good - after we managed to figure out how the mixer channels were routed. We had some problems with connecting the Tico machines. After trying some basic troubleshooting, we realised it was because the converter from SDI to HTML didn’t work properly, so we replaced it and it all worked. Even the colors worked - a wonder since we had problems with that before and we didn’t know how to fix it. Alena was also unsure, in the beginning, where does the in and out from the Tico machine go - which one goes into the camera and which one in the TV screen - problem fixed using a bit of logic and experimenting.\n\nAfter the tablet in Trondheim was ready to go, the Zoom connection was also set pretty quickly. However, later than they after rehearsals we tried to open the connection again with the Dungeon - a wannabe second portal in the same building as the UiO portal, and it unexpectedly crashed a few times. We fixed it by restarting the Zoom computer - not a super smart troubleshooting but effective.\n\nStep 4: Crush the berries aka  Mike up the instruments\n\nThe instrumental distribution went as follows: Choubey got the guitar part, Leigh got the drums and Alena got the piano. Since none of these were acoustic instruments, it was a relatively straightforward decision to connect them to the mixer and send them over LoLa. We needed cables, but were they long enough? Nope! So, instead of using the Jack to Jack cable for the drums we used it for the piano and we used a Jack to XLR cable for the drums (yay, stereo drums sound!). In Trondheim, Choubey also used a Jack to Jack for his guitar.\n\nStep 5: Add the sugar aka Setup the routing and debug the feedback issues\n\nWe struggled a bit to understand the routing of the other microphones in the room, but in the end we only let the podium mic (which we moved on the piano) be unmuted. In Trondheim, … This setup allowed us to minimalism the feedback we kept hearing over LoLa. In the UiO portal we weren’t able to hear our own sound (the specific channel was muted and had the fader all the way down) so we took care of routing it back to our speakers. Not having any acoustic instruments sure came in handy!\n\nStep 6: Boil everything aka Jam and record\n\nAfter all the preparations were ready we put on the song and started jamming on it.\nAlena is not good at playing after the ear and her piano sheet was only on her laptop, so it was difficult to keep track of where in the song we were. She quit trying to follow the sheet at some point and played the melody notes together with the guitar. You can hear the delay between them in the recording. We kept on jamming for half an hour and at the end we were definitely better, so that was a success.\n\nStep 7: Enjoy the jam!\n\nHere is our recordings. We can only say that we hope you’ll see progress soon! Leigh is taking care of transcribing the notes for Alena, so she can print them and not be lost in the song anymore, and that’ll surely improve the performance. We also plan to take a higher quality recording next time.\n\n\n  \n    \n    Alternate Text\n  \n  Audio recording of the jam\n\n\n\n  \n    \n  \n  Playing in the portal\n\n",
        "url": "/networked-music/2021/03/05/abhishec-tele-a-jamming.html"
      },
    
      {
        "title": "Pixel by pixel",
        "author": "\n",
        "excerpt": "The pixel sequencer is an interactive web app for sonification of images. Get online, upload your favorite picture, sit back and listen to it.\n",
        "content": "\n    \n\n\nThe pixel sequencer is an interactive web app for sonification of images, written in Javascript, CSS and HTML. The user can upload an image, decide on parameters like size (up to 16 pixels horizontally and vertically) and sensitivity. The image is then pixelated, and colour parameters from the image is the basis for the sonification of the image.\n\nThe idea\nMaking music based on a picture is not a new thing. Recent artists, like youtuber Andrew Huang has experimented with the use of MIDI notes on a piano roll to create a playable picture. The different MIDI notes on the piano roll appear to be pixels that form a picture, and what you get is a composition that sounds like a musical piece. Andrew Huang’s Glorious Midi Unicorn is a neat example.\n\nAnother example of sonification of images is the Pillars of Creation made by  System sounds. System sounds have among other things translated space images to music for NASA.\nThe Pillars of Creation image is converted to sound using a version of the inverse spectrogram method. The horizontal axis from left to right is the dimension of time, and the position of light sources on the vertical axis indicates pitch. The brightness and structure of the nebula controls the volume of the composition.\n\nChance composition\nInspired by the beforementioned, as well as compositional ideas from the 60s fluxus-movement (John Cage and others), using i.e. chance and indeterminacy as a tool for composing, we decided to build an app for automatic sonification of images.\n\nThe pixel sequencer is a toy and a tool, it’s a way of mixing senses, a way of composing – leaving some decisions to chance. It’s interactivity, it’s making sonification and composition accessible for everyone.\n\nHow does it work?\nWhen uploading a picture, the picture is scaled down to max 16x16 pixels, making a square with big pixels in different colours. When you press play, each column (of up to 16 pixels) is played sequentially, and what you hear is determined by different values from each of these pixels.\n\nBefore loading a picture, you should select how many pixels you want to convert it to, in both directions, as well as adjust the sensitivity. The sensitivity determines how easily a note is triggered. Darker pixels are triggered as notes (fixed for each row), and brighter are not triggered, and the higher the sensitivity-value you set, the lower the threshold for triggering is.\n\n\n    The code shows how data values from the picture are iterated over, and different formulas that outputs values like brightness, warmness and coldness.\n\n\nAccording to the number of rows (vertical parameter) you end up with, the more complex synthesizer you’ll have. As stated before, when hitting play, the whole column will determine what you hear. If you have only one row, you’ll only hear one note when triggered. With two rows you’ll hear two different notes triggered, not necessarily at the same time; that depends on how bright/dark pixels in the same column are.\n\nAs the complexity grows with more rows, it’ll be harder to audibly understand everything that’s going on, as effects and envelopes come into play as well.\n\nHere is a short demonstration of the sonification of the same picture with the same settings, only added one row for each loop:\n\n\n\n\nGrumpy Cat, row by row.\n\n\nThe mapping\n\nJust like System Sound’s Pillars of Creation Sonification, the pitch values are mapped to the vertical axis, and the time values are mapped to the horizontal axis. In the development of the app, we have been trying out several different ways of mappings, but in the end, we figured that since most people are used to perceive the horizontal axis as time in visual representation of music, we’d do the same here.\n\nAs the brightness value is deciding on and off of a note (dark colours indicate that the note is on and bright colours indicate that a note is off), it would not make much sense to use parameters from the same note to change other parameters, as the note would be off anyway if it was light. Therefore we explored how we could use the colour values from pixels next to or below the triggered note to change other parameters (like effects and harmonicity of the synth).\n\n\n    \n\n\nAlong the way when developing the application, we have been forced to take decisions on how the different parameters from the image should be mapped to different audio parameters. With up to potentially 256 pixels, and 16 synths with several effects, the possibilities are close to infinity. We have been editing and testing along the way and tried out many different mappings, and after a while, and also due to time, we came to the conclusion that we should keep it as simple as possible. We have built a framework that can be expanded upon in the future.\n\nWe ended up using 16 different synths, and four of these were pre-programmed and designed by Sebastian with some fine-tuned parameters. The remaining synths are very simple AM Synths from the Tone.js library. In addition to this, we have used four extra effects: autofilter, feedback delay, reverb and phasor.\n\nDrum machine\nThe four rows on top of the pixelated image-sequencer, works like a simple and easy-to-use step-sequenced drum machine, where you activate each step and instrument by pushing the corresponding button.\nWith bassdrum on the lower row, snare over that, and closed and open hihat on the the two upper rows, it’s very easy and fast to make a simple beat to play along with your image.\nAll drum sounds where made on a Moog Voyager and a Polyend/Dreadbox Medusa, recorded and processed (some EQ, compression + reverb on snare) in Logic and exported as short wave-samples.\nThe plan was to add more drumsets for users to choose, but in the end the drums are just an addition to the pixel sequencer, rather than a fully working drum machine.\nStill, the drums has a nice synthetic 80s vibe, and is useful for complementing the synth, and it specially can be a nice addition when you choose an odd horizontal dimension on your image, creating polyrhythmic between the synth and the 16 steps drum machine.\n\n\n\n\n\nGrumpy improvising with drum machine and Bernie.\n\n\nBack to the future\nThe aesthetics is very inspired by early late 80s/early 90s video-art, and the MTV-aesthetics of the time, with bright/vivid colours, low quality (large pixels), monospace fonts and a somewhat noisy look.\n\nCode &amp; pseudo\nSebastian and Anders had very little experience with Javascript, CSS and HTML, so the threshold was a little high for us. The boys’ initial thought was to use Csound and Python for the task, and the team talked about using HTML and Javascript for the interfacing, since Mari had experience with that. But we soon understood that it wouldn’t simplify anything, just add more complexity. And when Mari already had a quite nice prototype for the basic idea running, it seemed like a good idea to go with that.\nSo after some learning from Mari, we started working on each our ends, Anders drafting a pseudo-code, trying to imagine how tone.js could work, and how different values from pixels could be mapped to sound. Mari and Sebastian tried their best to understand the pseudo, and the complexity of the code and the produced sounds grew quite fast.\n\nIn the end-result, Mari took care of the main coding, Sebastian worked with synthezisers in tone.js, and Anders recorded synth drums and got the drum machine playing drums, as well as tweaking the design to fit his youth. In addition we all tried to get the grasp of both Javascript, HTML and CSS.\n\nThe MIDI rigidness vs. the RGB randomness\n\nWhen using RGB data of an image as a starting point for sonification, one could wonder what the purpose is. An image could basically be anything, so why not just use a random generator instead? It indeed depends on how the system is used. The element of chance is key. The system affords an infinite number of possible sonifications, according to how the user decides to use it. Also one image could “sound” in so many different ways, based on the number of different dimensions that are decided upon, as well as the sensitivity value.\n\nThere is nothing scientific behind the choice of mappings between parameters; everything has been aesthetic decisions based on what we think sounds intuitive and pleasant. The way MIDI values are mapped to vertical axis and the time is mapped to the horizontal axis is not a groundbreaking way of thinking of visualisation of music. So there is a contrast here between the rigidness of MIDI and the randomness of RGB. We believe that it works together and kind of makes some system in the chaos. The idea is that the user should be able to see how there is a link between the image and the sound, and we believe that we have been able to achieve this with our mapping.\n\nWe have made more of a system for sonification than just a sonification piece. While sonification of images is not a new thing, we have yet to see an online application that allows the user to upload a picture and sonify it in real time. So we hope you enjoy our pixel sequencer!\n",
        "url": "/sonification/2021/03/25/anderlid-sonification.html"
      },
    
      {
        "title": "Valkyrie: Aurora Sonified",
        "author": "\n",
        "excerpt": "The sound of Aurora sonified in a synth\n",
        "content": "\n    \n\n\nValkyrie:\n\nWho are Valkyries? Are they those fierce warriors who serves the god Odin and is sent by him to the battlefields to choose the souls who are worthy of a place in Valhalla? Are the sounds that we made coming from her lustrous shield, maybe? Maybe not?\nPerhaps, the relevant question is what is Valkyrie? It’s a synth we made that sonifies northern lights, the Aurora Borealis, for our sonification course SMC 4046.\nThe Inspiration: A strong sighting of the majestic aurora, here in Trondheim, the science behind how they are formed, and the interesting Nordic mythology were the fuels for it, and so began our quest to chase Northern Lights.\n\nAurora and its quantification\n\nOur research took us from Nordic mythology to the star of the show, the life giving sun and then to quantifying aurora i.e. how we measure it?\n\nMythologically aurora’s were considered to be the reflection or the glow from the shields of Valkyries, at some points it was also believed to be the Bifrost bridge that connects all the nine realms of the Norse cosmos. But it was  Kristian Birkeland who first explained the science behind aurora borealis\n\nSo how are they made?\nTo understand how they are made we need to go to the centre of the sun. There, the strong magnetic field ejects the hot plasma out which contains highly charged electrons and protons, these particles reach the outers of the solar system as the solar winds, the solar winds constantly vary in speed and density of the charged particles it carries.\n\nWhen the solar winds encounter earth’s magnetosphere most of the particles are reflected so that they don’t damage our atmosphere. But some of the particles come through in earth’s atmosphere where the magnetic field is weak, which is around earth’s poles. This particle collides with oxygen and nitrogen atoms and excites them, and in the process of cooling down and coming to their normal state they emit photons and produce lights which we call aurora.\n\nWhat aurora depends on, its measurement and how to read aurora charts?\n\nThere are multiple factors on which aurora depends:\n\n  KP Index\n  Strength of IMF\n  Polarity of IMF\n  Density if solar wind\n  Speed of solar wind\n  Weather\n\n\nchart\n\nKP Index\n\nThis is arguably the most important one, KP Index  is an index for geomagnetic activity on earth and is rated on a scale of 0 to 9, where 0 means no activity 5 means minor storm and 9 means severe geomagnetic storm. The number required to see the aurora depends on the altitude you are at, you can check the relation here. Real time kp index can be seen here\n\nStrength of IMF\n\nIn the first graph the black line Bt shows the strength of IMF. The higher this value, the better it is for enhanced geomagnetic conditions. We speak of a moderately strong total interplanetary magnetic field when the Bt exceeds 10nT. Strong values start at 20nT and we speak of a very strong total interplanetary magnetic field when values exceed 30nT. The units are in nano-Tesla (nT).\n\nPolarity of IMF\n\nThe Phi angle is the angle of the interplanetary magnetic field that is being carried out by the solar wind. In the first graph the orange line Bz shows the polarity. When we see the Phi angle go southward (into the negative) This shows the increase of the transport of solar winds &amp; mass into the Earth’s magnetosphere. When it goes into the positive, this shows a decrease in the transport of solar winds and mass into Earth’s magnetosphere.\nA better representation of this can be seen in the Phi GSM graph (second graph) as the dotted lines goes up and down (or in the towards and away region)\n\nDensity\n\nThe third graph shows the density of the solar winds, measured in particles per cubic cm. Normally a number 10 or higher is ideal but this doesn’t have to be very accurate.\n\nSpeed\n\nThe fourth graph shows the speed of the solar winds, measured in km/s. It varies from 300km/s to 1000km/s, a higher number means a stronger impact but geomagnetic storms can appear at lower speeds as well\n\nWeather\n\nThe weather should be clear with no clouds present and no moon is ideal. Also, the light pollution affects the chances so the less it is the better.\n\nImplementation\n\nThe project follows a relatively simple structure of implementation, which is as follows:\n\n  Download data from the web link\n  Sorting and cleaning data acquired.\n  Extracting the data and sending it to csound via python.\n  Mapping the data to parameters in the synth and effects\n  exporting it to a functional plugin\n\n\n\n      \n  \n\nThe system\n\nThe project was an ambitious one and we decided to divide the system and work on the components individually. Joel worked on download script and Synthesizer, Lindsay worked on Data sorting and Csv generation and Abhishek worked on Python - Csound interfacing and data extraction. All these components are briefly explained below:\n\nData Download\n\nThe data is downloaded by Python using the Selenium module. The text file is generated on the NOAA page using Javascript, so we needed to create a web crawler that hosts an instance of Firefox and navigate to the correct element of the web page to download the file. Because of this the downloading of data is relatively slow and the construction of the site makes the downloading unstable. This is a point of improvement in the project, to have a data source or a way of acquiring data that is faster and more stable. This would make it possible to play on constantly updated data, which unfortunately is not possible at the moment.\n\nData Sorting and csv file\n\nThe downloaded file is a text file so we remove the extra lines we don’t need, and some errors in data. After this we convert the text file into a csv file for easy handling of data in python. Once the csv file is generated, data is extracted from it by using pandas dataframe.\n\nCommunication between python and csound via osc\n\nCommunication between csound and python is done using Open Sound Control - OSC. By hosting an OSC server python can receive messages from csound “asking” for a specific time. This message is interpreted as an index which is used to select the appropriate set of data, which is then sent back to csound. To run the vst the python script has to be running independently listening for messages and responding back with data.\n\nSound synthesis in csound\n\nThe sound is generated in csound using two sound generators: granular synthesis of sinus waves and filtered noise. These two sounds are mixed together and pass through multiple effects before output to the DAW. By using granular synthesis as the main sound generation we are offered a lot of flexibility in characteristics of the sound. The different parameters that control the sound are mapped to different parts of the data in a way that reflects how the aurora is affected by the same data. A low chance of aurora is reflected in a dull sound without much motion, and as individual grains are masked the synth seems to struggle just as the magnetic storm is struggling to penetrate the magnetosphere. Similarly when the speed of the solar wind is high the sound and movement of the generated noise is increased, as you would expect the sound of wind on earth.\n\nMapping of variables to sound\n\nThe data set consists of measurements every 5 minutes for seven days. For every variable and measurement we get three variables of the maximum, the minimum and the medium measurement for that variable that 5 minutes period. For instance Bt (strength of the magnetic field in the solar wind), on March 20:th 2021 at 01:00 am the medium, minimum and maximum are respectively: 15.99, 15.88 and 16.03 nT. By mapping these three different values and/or the difference between them we interpret the connection between data and the aurora to create a sound that represents the conditions for aurora.\nHere follows a brief presentation how different parameters of the sound are affected by values of the data. All values are scaled and transformed into ranges that make sense for each application. The details of the scaling are not discussed here, but can be viewed in the code placed on github.\n\nParameters\n\n\n  \n    Relative duration of grains: Base duration is controlled by medium density. This is modulated by an oscillator where the speed and the amplitude is controlled maximum and minimum density.\n  \n  \n    Random masking of grains, and grain envelope: Both are controlled by medium Bz.\n  \n  \n    Noise generation and filtering: Medium speed controls the amount of noise in the mix. The noise is filtered by 8 band pass filters where the steepness of the filter is controlled by the minimum speed, and modulated by an oscillator with a speed determined by the difference between maximum and minimum speed. The frequency of the lowers filter is one octave below the played note, and the following filters are placed harmonically brighter. The brighter filters have a lower volume by a factor controlled by the maximum speed.\n  \n  \n    Chorus: The volume of the chorus is determined by maximum Bt. The minimum delay time and the rate of change is controlled by the medium Bt. The amount of modulation is controlled by the minimum and medium Bt.\n  \n  \n    ADSR: All the components of the ADSR-envelope are controlled by both the medium density and medium Bt.\n  \n  \n    Phaser: The volume of the phaser is controlled by the maximum Bz. The speed and direction is determined by medium Bz and the sharpness of the filter is controlled by minimum Bz.\n  \n  \n    Reverb:The volume of the reverb is controlled by maximum Bt. The cutoff frequency is controlled by the minimum Bt and the medium Bt controls the length of the reverb.\n  \n\n\nDemonstration\n\n\n   \n   Aurora_sonified_by_valkyrie\n\n\nEndnotes and Future Developments\n\nIn this quest of chasing the aurora in the given timeframe, we are happy and feel satisfied with the result we have achieved. Even so, we all agree that the synth can be improved a lot in different aspects.\n\n  \n    Make a one packaged program: We would be working on the project, trying to make it more stable and possibly make it a one package deal, in which users don’t have to run python separately, just running the vst plugin does the job.\n  \n  \n    A sound installation: One other cool future aspect of this project could be implementing it as a sound installation where the synth receives high resolution data in real time and plays the music accordingly.\n  \n  \n    An interactive webpage: A dynamic web page where users can interact with the sound without thinking about the nuances of DAW and plugin is another area where the project can be expanded.\n  \n\n\nThe code and files can be accessed via github repository here\n\nUseful links\n\nReal time solar wind data:\nhttps://www.swpc.noaa.gov/products/real-time-solar-wind\n\nSpace weather data:\nhttps://www.swpc.noaa.gov/communities/space-weather-enthusiasts\n\nAurora information:\nhttps://en.wikipedia.org/wiki/Aurora\nhttps://site.uit.no/spaceweather/what-are-the-northern-lights-aurora/\nhttps://www.space.com/15139-northern-lights-auroras-earth-facts-sdcmp.html\nhttps://www.nasa.gov/mission_pages/sunearth/aurora-news-stories/index.html\nhttps://www.swpc.noaa.gov/\nhttps://www.theaurorazone.com/about-the-aurora/aurora-legends\n",
        "url": "/sonification/2021/03/26/abhishec-aurora-sonified.html"
      },
    
      {
        "title": "Growing Monoliths Discovered On Mars",
        "author": "\n",
        "excerpt": "Mars is a hot topic these days, and weather seems to always be a hot topic too. So how about making a project with both? We ended up gamifying the weather on Mars by discovering the musical potential it may have.\n",
        "content": "\n    \n\n\nThe Idea\nA lot of things is sonificationable, which maybe is both the beauty and the beast when dealing with sonification. After a brainstorming session our group quickly landed on Mars as a topic for our sonification project. It has this spacious timeless mysteriousness to it and is highly relevant as research is being carried out more than ever before. So what to sonify? Weather of course. We humans love weather, whether it’s bad or good, doesn’t matter. We simply love it. But once we’ve talked about the weather here on earth, what more should we talk about? Yes, exactly, the weather on Mars. Mars has weather too and we were eager to work with the sonification of it.\n\nRoles and Contribution\nA team benefit we had is our individual strengths in programming. We quickly landed tasks for each member. Pedro has experience with game programming and development, so it made sense assigning that task to him. Stephen was chosen as backend developer due to his strong capabilities in computer programming, whereas Henrik was chosen to do the sound engine working with Max/MSP and sound synthesis. This distribution felt good and we were confident that this would provide us solid ground for executing the project. Finally, we all worked together on the sonification itself, deciding how the martian weather data would be interpreted and how the game mechanics would influence the audio and music.\n\nThe Process\nSo the first thing we needed was some weather data. Weather isn’t updated as frequent on Mars, so after going through some datasets we finally found what we were looking for. A daily weather datasheet. The data will always be one week old, as NASA wants to keep that as a secret buffer. So not much to do about that. The dataset has, among other values, temperature, pressure and radiation as some key factors, which we thought could be nice to work with.\nSo how to work them? With sonification projects we often find that you as a «user» find the sonification and you experience it in a passive setting. We felt like there was some potential here and started to talk about games as sonification tool. Our idea became to sonify the user’s interaction with the dataset via a game, adding another layer of sonification on top of the dataset. The weather on Mars being updated on a daily basis means that there is a limited number of values for each variable. Having the game, we then saw an opportunity in designing the game as changing the weather on Mars in different directions, which again will result in new music coming out of the data. A strange feedback loop with the user at centre.\n\nThe Data\nIt made sense to use Python to parse and manage the Martian weather data, as well as to handle communication between the Unity game engine and Max/MSP.\nCommunication was set up using Open Sound Control (OSC), with the script acting as both a OSC client and server. The script steps through the daily (or sol-y - the Martian day is called a Sol) weather data that has been compiled into a JSON file, adjusts it based on in-game actions, and then sends that data out to Max and Unity. Talking of in-game actions, what is this game actually about?\n\n\n    Dataflow via OSC between Python, Unity and Max/MSP\n\n\nThe Game\nThe game is a driving based game. The scenery is of course Mars. You’re driving the Preservance Rover on the planet, discovering it. Out of the blue monoliths are starting to pop up and you have to drive into them to explore and discover the secrets of the monoliths. Driving into the monolith breaks it, and based on the size it will have an effect on which way (better or worse) the weather is changed towards, the music playing based on weather and impact collision and also some damage to the vehicle. Greater monoliths earn you more damage, but obviously also more points. And don’t worry, as you drive on Mars you will discover health packs. Breaking small monoliths will improve weather while big monoliths make it worse. In nice weather the pressure is higher, so the monoliths will grow slower. This will give you many small slow growing monoliths that don’t give you much points, so it gets harder and harder to avoid them as you wanna keep them growing so you can hit the big ones for real point gathering. The other way around when the monoliths grow big they will become dangerous and you will have to navigate around them so you don’t destroy the vehicle in the chase for discovering smaller monoliths that will make the weather better and the monoliths smaller again. When the monoliths reach maximum height, they become so dangerous that they will knock you out in one hit, so you would want to knock them down before that happens. It’s a game of balancing the climate and not being greedy for points as you at the same time don’t want to go too safe only hitting the small ones. In a way a metaphor to what’s going on with the climate on planet Earth too.\n\nGame Development\n\nThe game was developed in the Unity game engine and uses a portion of code from the game Shrinking Planet, which was implemented in the Ludum Dare game jam. The referenced code is intended to move an object around an sphere. From that base, we implemented the mechanics related to the gameplay described before involving the interaction between the Python and the Max/MSP modules through OSC messages (an open source library for OSC in Unity can be found here). Besides these interactions, the game reacts visually to the influence of the weather by using three parameters: Weather, Pressure, and Radiation. The weather parameter allows to set the environment to a hot, moderate, or cold state, which changes the color for the space background and the planet surface; the pressure modify a ‘dizziness’ visual effect in the whole rendering; and the radiation changes the intensity of three directional lights that points towards the planet.\n\nThe game goal was set to earn score points and try to avoid losing life amount. This data is shown in an interface on top of the screen. When the player dies, an interface covering the whole screen is presented to inform the user that he or she has lost and require to start a new game. Also, the current weather state is shown all the time in the top-center of the screen.\n\nIn the next section we will explain the work behind the audio, which is partially reproduced in Unity for specific sound effects such as a monolith hit, health pack, game status (new game or game over) and the car engine. Some little audio manipulation and triggering logic was develop in this case. For the car engine, the sample is looped infinitely and changed in pitch when the player increases or decreases the speed, also the panning changes when the car turns left or right. The monolith hit is played according to the height of the object that is hit and the sample to play is chosen randomly from a pool of similar sounds with little variations in order to avoid a monotonous sound landscape. In a similar way, the health pack sound is slightly changed in pitch and volume for give variation to the gameplay.\n\nWe used third-party elements to enrich the visuals of the game which can be found in the following list:\n\n\n  Shrinking Planet: Source Code\n  UnityOSC\n  Mars Perseverance Rover, 3D Model\n  Mars Planet, 3D Model\n  Monolith, 3D Model\n  Unity Skyboxes\n\n\nThe Music\nAlmost all audio is synthesized using Max/MSP. Before making the actual music we started with synthesizing the sound of Mars. The week before we started working on the project NASA released the first recordings of sound on Mars. This «noise» was something that could be synthesized. The benefit of synthesizing it rather than using the recording is the general feeling over time, as the synthesized noise will never repeat itself like a looped noise recording. It’s also easier to synthesize it in stereo rather than using the mono recording from Mars, so in general it feels a little more immersive and nice.\nThe music is generated from weather data. There are three different modes for weather - cold, moderate and hot. This acts as a quantification controlling the range of different parameters in the patcher where sound is being generated. Cold weather has a +1 octave more «ice-bell-ish» sound to it and the reverb is also more lush and with higher frequencies. For moderate the synth sound is warmer as well as original octave and the reverb moves into hybrid delay fields while also getting lower damping. For the hot mode the synth sound is very warm, -1 octave and the reverb is more in a delay state with low dampening. For the modes vibrato is also affected, cold mode has a fast and firm vibrato, moderate slightly looser and warm mode has a slower and wobbly vibrato. There are different musical scales for the modes as well. Cold has a minor scale, moderate is pentatonic while warm goes to major scale. All of these variable changes for the modes are based on knowledge we’ve gotten through the studies of metaphors, entrainment and verticality during SMC. We wanted to solve the musical mapping in such a way to make it feel natural and intuitive to the user, so music makes sense and the sonification has a communicative purpose.\nWhen you discover a monolith there is a nudge happening in the delay time based on the size of the monolith. Bigger monoliths make a bigger impact than the smaller ones, and this is to create some direct interaction, with the impact of hitting something, within the music. It also creates some lucky musical accidents and makes the music less static.\nAll sounds in the game are synthesized, except the sound of the vehicle engine which is sampled from a recording of the real Preservance driving on Mars. We thought it was cool to include an actual recording from Mars in the game, and surprisingly the tone of the engine fits with the harmonic content in the music. The recording was released the same week as we were doing the project, so it felt like it was meant to be. Sound FX are synthesized so everything lives within the same soundscape. Synthesizing the monolith-impact we took inspiration from the introduction lecture in the coarse where when students were to describe a sound. We had to think of what an impact would sound like and how we could synthesize the smashed rocks falling from being smashed in the impact. The method we used was applying a lot of overdrive, amplitude modulation in stereo phase and filtering in different stages, all triggered by the same signal.\n\n\n  \n    \n    Should show a media player\n  \n  7 levels of monolith smashing\n\n\nThe sound for collecting health pack is inspired by classic retro games - a short very positive melody giving you a positive shot of energy when collecting something.\n\n\n  \n    \n    Should show a media player\n  \n  Health Pack\n\n\nThe «start game» and «game over» over melody lines are referring to 2001: A Space Odyssey by using An der schönen, blauen Donau by J. Strauss as the base for the melody. The whole game itself kind of refers to that movie with the mystical space thematics and the monoliths, or should we say monoliths, occurring on an undiscovered planet. When you start the game it has a major scale snippet playing, and a classic minor scale variation in a slower tempo for the game over version. The fast major one is used at the start to motivate you, and the slow minor one at the end shows compassion with the user loosing the game. They are all based on a simple sine wave synth to fit well with the music playing in the game.\n\n\n  \n    \n    Should show a media player\n  \n  New Game\n\n\n\n  \n    \n    Should show a media player\n  \n  Game Over\n\n\nDemonstration Video\n\n\n   \n   Gameplay Demonstration\n\n\nEnd Note\nWorking on a project like this has been slightly different than other technical tasks we’ve done in SMC. We ended up using a lot of time actually figuring out what the game should be like to make the most sense, and the same thing goes for figuring out what the audio was going to be like. These parts took a while probably because there are so many possibilities and going through many revisions of ideas and evaluating them is a tedious process. But we feel like we got there in the end. We were all able to use our own skills in the project. Pedro did some excellent game programming and sew it all together, Stephen treated datasets crushed the numbers and Henrik got to play around with synthesis and build a sound engine. Small details ended up being the hardest part, but in retrospect we’re happy we took time to fix them.\nPlaying the game is really fun, and the music adds excitement to the gameplay when it reacts on your treatment of the weather data. Things we have thought about is making the car speed up throughout the gameplay as you gain higher scores, that way the difficulty would increase. Another element we would want to include is another enemy, and being in the 2001: space odyssey universe - what better than gorillas chasing you for destroying their precious monoliths? That was also the feedback from Stephen’s son, he missed the element of being chased by robots (gorillas, or maybe even gorilla robots). So we all have something to look forward to here.\n",
        "url": "/sonification/2021/03/26/henrikhs-growing-monoliths-discovered-on-mars.html"
      },
    
      {
        "title": "Posture Guard",
        "author": "\n",
        "excerpt": "Back pains, neck pains, shoulder pains - what do they all have in common? They are caused by bad posture while working on a laptop. So I made a program that makes the laptop help out maintaining a good posture while working.\n",
        "content": "\n    \n\n\nThe Idea\nThe last 15 years of my life working on laptops has been more or less a daily routine, and I can’t see it becoming less relevant the older I get. 7 years ago I started to feel shoulder and upper back pains, and a physiotherapist didn’t take long to tell me that my posture while working on a laptop was the reason for my troubles. He showed me how to sit and what to do, but of course, following his advise turned out to be way harder than it should’ve been. That being completely my responsibility and not the physiotherapist’s. Lately these pains have been knocking on my shoulders’ doors again, and once again I’ll have to deal with it.\nDuring motion tracking class we’ve talked a lot about using motion tracking techniques to analyze how performers play instruments and use this data to improve the playing style of the performer. So I figured I wanted to use motion tracking to help myself, and possibly others, with my posture while working on my laptop. The idea is basically a system that acts as an alarm whenever the posture is getting worse.  My main problem with following the physiotherapist’s advise is that whenever I work I get very focused on what I’m working with and I simply forget them. So making the computer help me remember should be the least it could do for me, as it after all is the root of my problems.\n\nAnalyzing Posture\nTo analyze my posture I did an optical motion capture in the lab using OptiTrack Motive. I stationed myself at a desk with my laptop. I started sitting in a healthy upright position and slowly drifted downwards into the bad position bending over the laptop. At the same time I did a video capture with my computer webcam so I could align and compare the posture motion capture with the webcam video capture. I exported the CSV data and ran it through the analyzing notebooks we had been using. Tweaks had to be done to the code, which was a nice challenge for digging into the details, understanding them and getting the numbers I needed. I compared the «top back», «head front» and «head top» markers’ displacements to each other.\n\n\n    Accumulated displacement of markers for top back, top head and front head.\n\n\nWhat I found from the plot of these markers was that my «top head», or just head, was the most moving body part when slowly falling into a «bend-over-laptop» posture. The neck also moved quite a bit, but the head, being placed on top of the neck, still had the biggest displacement. From this I started thinking that if my head is in a good position, then the rest of my neck and back will naturally be placed in a good position giving a good overall posture. A good posture is usually when the ears align with the resting shoulders with an open chest (Morrison, 2018).\n\n\n  \n    \n    Should show a video player\n  \n  Video of optical motion capture done with OptiTrack Motive\n\n\nAdditionally I did a motion analysis at home with my computer webcam to detect movement as I’m falling closer to the screen. I did four similar videos and they all showed more or less the same pattern. Which I also expected, as the movement is fairly similar each time. The good thing about this was that it showed that there wasn’t much difference in what the computer «saw» each time I was transitioning my posture.\n\n\n    Video capture analysis of Quantum of Motion showing a similar capture for all four tests for transitioning from a healthy posture to a bad posture and back.\n\n\nCapturing Methods\nFor the task itself working with the webcam and video capture at the core made sense. It’s already integrated in the computer and doesn’t affect the user like other motion capture system may do. Using a side camera has been considered and it’s a good idea as the main motion to be captured is easiest to spot from the side. I have not been working with a side camera for now as I was eager to find a way to work with an already installed webcam. On the other hand there are limitations to using only cameras, such as lighting situations and some instability in data. So I figured using an additional different capture method would provide an addition dataset that could stabilize the treated data and performance of the program. Phones are simple and great motion trackers, so I used an OSC-connection sending the phones’ gyro (pitch, roll and yaw) and used the pitch data to track the downwards tilt of my neck. By attaching the phone to my head in an upright position the pitch allowed for tracking of head movement in the up and down movement - the nod. Additionally I used the roll for rotation tracking and the yaw for sideways tilt. This gave a good tracking of head position, which is good for making sure that the user’s posture is good.\n\nCapturing Motion\nFor the video capture the computer webcam ended up being sufficient for handling the whole tracking task. The projects is made in Max/MSP with Jitter, and having read about OpenCV I figured a well known package to use in Max/MSP is Jean-Marc Pelletier’s cv.jit - Computer Vision for Jitter. It offers many of the same things, but in the work environment of Max/MSP. To start with, working with color tracking seemed like a possible approach, selecting a color and tracking it and finding bounds of the colored area to draw a shape around it. This worked for the purpose of tracking colors, but for the purpose of tracking body posture it didn’t do a great job as face colors are organic and not strong like the colors of for instance post-it notes. So face colors were detected a lot of places. When looking through the cv.jit library I came across the cv.jit.faces object, which offers face tracking. Tracking faces made sense as that is mainly what the computer sees when someone is working on it. The function works in a way that it looks for the general basics of a face (two eyes, a nose in the middle and a mouth below) and returns the square bounds in pixels around the face. Working with it for a while revealed that this method was excellent for the task of tracking posture using the head/face as marker. The tracker draws a square around the faces it’s tracking and provides bounds given in pixels for each line. The benefit of this was that this square will increase in size when moving closer to the computer and get smaller the further away the user is. This is used as a Z coordinate in the video capture, making it possible for the computer to analyze proximity of the face at any time. This was a big step for the project to reach the end goal and carries out the main motion tracking. However, there are limitations to how this function tracks and that’s where the phones’ gyro data gets useful. For instance, there is no stable way to track via video if the head is tilted in the up and down position, nor does it track the sideways tilt. By using the phone, as an intermediate gyro-tracker solution, because of the inconvenience of having a phone attached to the back of your head while working, it’s way more detailed and easier to keep track of posture. This also opened up for notifying the user of what specifically was going wrong with the posture. Is it your head that is starting to tilt down, are you leaning over to the side or are you simply too close to the screen? Another great feature with the phone is the roll, which in this case tracks which way you’re looking. This has been used to switch off the alarms when the user is not looking at the screen, which is convenient if the user feels like looking out a window or talk to someone in the room without being alarmed about posture.\n\n\n  \n    \n    Should show a video player\n  \n  The basics of Posture Guard - tracking face and proximity via webcam\n\n\nSonification of Posture\nFor the video capture and tracking there are two different alarms, one subtle mode which is based on sine waves with sonic and rhythmic phasing i stereo, and there’s an «obnoxious» one which sounds more like a traditional siren alarm system. Mainly so you can choose a way to be alarmed. The alarms becomes more intense as the user approaches the computer, which serves as a sonification of the proximity between user and computer. When gyro tracking is connected, additional sounds are played. When pitch is detecting a forward tilted neck, it plays a rising sine wave based on how low the neck is, telling the user how much he or she needs to raise the neck back to an upright position. The yaw is tracking sideways tilt, and has a wind type effect blowing towards the left when the user tilts right, and vice versa, so the user is encouraged to tilt the neck back up in an upright position from the current position. This makes use of the bodily metaphors and verticality we know of in the relation between sound and motion and should serve as a logical sound mapping being intuitive for the user to understand. All sounds are synthesized real time inside the running patch for modulation control and flexibility.\n\n\n  \n    \n    Should show a media player\n  \n  Nice alarm mode moving very close to the screen and back.\n\n\n\n  \n    \n    Should show a media player\n  \n  Obnoxious alarm mode moving very close to the screen and back.\n\n\n\n  \n    \n    Should show a media player\n  \n  Neck tilt (pitch) mapped to pitch where tone height tells you according to verticality how much you need to correct your neck tilt to obtain good posture.\n\n\n\n  \n    \n    Should show a media player\n  \n  Sideways neck movement where sound tells you which direction you should move your neck on the left/right plane to obtain good posture. In the example the head is first leaning too much to the right, triggering a signal that signifies you should be moving more to the left, and the second is the other way around.\n\n\nUsage\nRegarding use it quickly became clear that parameters like sensitivity and reaction time had to be incorporated, as it’s pretty annoying having the alarm trigger at small movements or if you just want to have a closer look at the screen for a short period of time. This posture guard only reacts when you move closer, and not away from the computer due to many chairs having a «lean back» function which isn’t necessarily a bad thing for posture. Unless working from a very backwards heavy posture. The mode saying asking where you went was made so that if the program doesn’t detect a face, it stops the alarms so it doesn’t play an annoying alarm sound with no one present. Highly beneficial when sitting in an open office or if you’re wearing wireless headphones and go to grab a coffee.\n\nConclusion\nUsing the Posture Guard is quite interesting as I get to know how often I actually descend from a healthy posture while working on a computer. Which is quite often. At least that has been my experience. As a helper it’s both nice to have and annoying, even though I did my best to make it as gentle as possible. The system doesn’t require any external markers or gadgets, unless used with a phone in gyro mode, but still it feels like something that is quite responsive to what you do. Using the camera’s sense of face size for tracking proximity was an eye opener for working with a three dimensional space captured by one camera alone. This could also be applied for simple strong colored objects when tracking colors, keeping track of the size of the bounds found by the camera. For improvement finding a smaller and more user friendly gyrometer with a wireless connection is a god idea as the gyro mode gives a pretty detailed tracking of the whole neck and head parts. The face tracking itself is a pretty processor heavy task, and something to work on with. Scaling the video helps, and this could probably be scaled down further for lighter processing, even though using 240x160 was sufficient for use on my computer system, giving a pixelated video monitor. A solution for this would be to do the tracking on scaled video data and then scaling it back up when drawing the text and figures on the monitor. But then again, it’s only for visual monitoring purposes in a program where sound is used to notify the user. I did test the program while running video chat and an Ableton Live project, and my computer still works. Thank you.\n",
        "url": "/motion-capture/2021/04/16/henrikhs-postureGuard.html"
      },
    
      {
        "title": "An air guitar experiment with OpenCV",
        "author": "\n",
        "excerpt": "Get to know OpenCV by building an air guitar player. Shaggy perm not required.\n",
        "content": "Introduction\nFor my Motion Tracking assignment, I wanted to get to know OpenCV, an open source computer vision library. I had enjoyed going through a few tutorials, and liked the idea of diving a little deeper. Plus it meant I could code in Python, which I am warming to, despite it not being Ruby.\n\nSo the next question was - who’s motion am I going to track, and for what purpose? My dog is now too old and sleepy to film her jumping through hoops in the park, and anyway, this is supposed to be a masters in music. A MIDI controller maybe? Could I get my webcam to recognise the sliders I am moving on my MIDI controller and translate them to OSC messages? Then if I am ever at band practice and have forgotten my USB cable, I could just turn on my webcam and use my MIDI controller wirelessly! The only slight hitch in the plan was that I didn’t actually have a MIDI controller.\nI did have some guitars on my wall though - maybe I could make an air guitar player? I could stand in front of my webcam, wave my hands about in the air as if I was Brian May at Wembley Stadium and trigger some guitar samples in a (hopefully) semi-realistic way.\n\nMethodology\nMy initial panic was triggered by the thought “how on earth am I going to identify my hands waving about in a webcam”. It had me working through lots of tutorials on feature detection, learning about how to spot edges and corners, the panic increasing by the second as I realised how big a task I had taken on.\n\nThankfully, I stumbled on MediaPipe, an open source machine learning library for human pose detection and tracking (amongst many other things). I could feed each frame from the webcam into this, and it would spit out a series of landmarks signifying the location of hips, hands, shoulders etc. Sorted! The motion tracking showed my shoulders visibly relaxing.\n\nThe second stage of panic was born of the realisation that I still had to do some maths to get this to work. OpenCV makes it easy to step through a camera feed frame by frame, and MediaPipe makes it easy to get a list of landmarks that correspond to points on the body, but what then? \nThe landmarks need to be translated into x/y points relative to the frames dimensions. I decided that I only needed to know 4 points on the body - one for each of the hips and hands. The hands are obvious - we need to know if the strumming hand is moving over the strings or not, and where on the neck the fretting hand is.\n\nThe hip points could be used to position the guitar. Talking of the guitar - how are we going to draw it? I had already played around with OpenCVs various drawing features, and so quickly cobbled together an ellipse for the guitar body, and a thickened line for the neck. Realising that I didn’t want to spend the whole assignment using rudimentary drawing tools to create a guitar that would still look terrible, I looked into the other option which was finding an image of a guitar and superimposing that onto the frame. This worked much better, especially once I had figured out the surprisingly complicated process of displaying a PNG with a transparent background.\n\nAt this stage the guitar was the correct size when I was standing in front of my webcam, but turned into a giant guitar as I stepped further back in the frame. Hip points to the rescue! I used the distance between the hip points to scale the guitar image to the appropriate size as I moved forward and backwards in the frame.\n\n\n    Scaled guitar image in place!\n\n\nStrumming\nThe next step was to identify whether my hands were in the process of strumming or not. I used the hip points again to define a rectangle which would correspond to the strum-zone. If my right hand (I am right handed when I play guitar, and left handed for everything else) was within that rectangle, a guitar strum would be triggered. This isn’t really how guitars work however, and so it was time to apply some of the maths that I had learnt in class. I needed to know if the hand was moving up or down over the strings, which meant that I needed to have a record of the hands position from the previous frame to compare to. Once I had this, I could work out the direction and distance that the hand had travelled. The distance also allowed me to calculate velocity using the derivative. So I set about recording multiple guitar strums - up, down, fast and slow - and used the information I calculated to trigger the correct sample. The right hand was working!\n\n    # get distance right hand has moved in the y direction by\n    # comparing current hand position to previous values\n    distance = human.rhand_y - human.prev_rhand_y\n\n    # calculate derviative of distance to get our velocity\n    velocity = np.sqrt(distance ** 2) / 1\n\n    # define strum area\n    tlx = int(human.center_x - human.hip_width/1.5)\n    tly = int(human.center_y - human.hip_width/2)\n    brx = int(human.center_x + human.hip_width/1.5)\n    bry = int(human.center_y + human.hip_width/2)\n\n    strum = ''\n\n    # if right hand is over the strum area\n    if (tly &lt; human.rhand_y &lt; bry) and (tlx &lt; human.rhand_x &lt; brx):\n      if distance &gt; 0: # down strum\n        if 10 &lt; velocity &lt; 70: # slow strum\n          strum = 'down_slow'\n        elif velocity &gt; 70: # fast strum\n          strum = 'down_fast'\n      elif  distance &lt; 0: # up strum\n        if 10 &lt; velocity &lt; 70: # slow strum\n          strum = 'up_slow'\n        elif velocity &gt; 70: # fast strum\n          strum = 'up_fast'\n\n\nI had originally thought to use Csound for the audio, but as I was just triggering a sample it seemed a little like overkill. Instead I found a very simple Python library called Playsound which does just one thing - play an audio file you send to it.\nNow on to the left hand. Maths wasn’t my strong point the last time I actually did any, which was over 30 years ago at school, but I knew I was going to need it to work out my left hands position on the neck, so I rolled up my sleeves and took some deep breaths. To keep things simple, I decided to split the neck into 3 regions, each one triggering a different chord. The neck is at an angle, so I knew that to calculate the length of the neck, and the position of my hand along it, it was going to involve lots of triangles. Pythons math library has a nice simple hypot function, which allows me to throw in a couple of points and find the hypotenuse, so it’s an easy solution for finding the distance between 2 points.\n\nSo I had the length of the neck, but how do I find out whether the hand is near it? Numpy came to my rescue. I used Numpys linspace function to step along the neck at intervals, checking whether the hands co-ordinates were nearby. If they were, I could do the triangle thing again to calculate how far along the neck it was, and so work out which chord should be triggered.\n\n    # calculate neck length\n    neck_len = hypot(self.neck_start[0] - self.neck_end[0], \\\n                     self.neck_start[1] - self.neck_end[1])\n\n    # step along the neck at intervals, checking if lhand is near\n    for p in np.linspace(self.neck_start, self.neck_end, 20):\n\n      hand_distance_from_neck = hypot(human.lhand_x - p[0], \\\n                                      human.lhand_y - p[1])\n\n      if hand_distance_from_neck &lt; 40: # hand is touching the neck\n\n        # grab the coordinates of the left hand\n        x = int(p[0])\n        y = int(p[1])\n\n        # calculate how far along the neck the hand is\n        distance_from_end = hypot(x - self.neck_x, y - self.neck_y)\n        percent = int((distance_from_end / neck_len) * 100)\n\n        # split neck into 3 in order to grab the correct chord sample\n        if 0 &lt; percent &lt; 33:\n          note = '1_'\n        elif 33 &lt; percent &lt; 66:\n          note = '2_'\n        elif 66 &lt; percent &lt; 100:\n          note = '3_'\n\n        cv2.circle(frame, (x,y), 10, (0,255,0), -1)\n        break\n\n\nResults\nThe script was working - I could strum with my right hand, and determine the chord that would be played with my left. The guitar sample can be triggered multiple times when holding my hand over the strings, and I played around with various settings to try and alleviate this. I introduced a timestamp that is set once a chord has been played, and prevented any further triggering of samples within 0.2 seconds. This works, but it also slows down the rate at which you can strum. So I tried reducing the minimum distance required to move the strumming hand before a chord is triggered, which seems to be a better solution.\n\nAnother issue was that previous samples aren’t cut off when new samples are played, which doesn’t help with the realism! The Playsound library scores highly for simplicity, but this comes at the cost of tweak-ability. So I switched over to a library called Simpleaudio, which also allowed for stopping of audio. Cutting off the audio didn’t sound very natural either, and so I settled on a compromise, and configured things so that the previous audio sample is cut off when changing chords. This helps with the sustain when strumming a single chord, but makes sure that when you change chord, the previous chord doesn’t bleed over the currently sounding one.\n\nThe scaling of the superimposed guitar image was a little jumpy, but then the calculation for the scaling was pretty rudimentary and so to be expected. I found that I wasn’t always sure of my left hand placement, and so I added a marker to show the detected position of my hand on the fretboard - this gave useful visual feedback while I was playing.\n\nAnother thing I didn’t take into consideration was that I wrote the code on my 2019 iMac, but it needs to run on my older 2012 laptop for the presentation. I found that the scaling of the guitar image was causing various problems on my laptop that I didn’t come across on my iMac. And during the actual presentation, the scaling issues were exacerbated by the low lighting in the room. That my back jumper didn’t exactly stand out against the black curtains didn’t help either.\n\nConclusions\nThis was a fun project to work on. OpenCV and MediaPipe worked well for the most part, and when you consider this has been running on an old (2012) laptop using just its built in webcam, I think the tracking accuracy is surprisingly good.\n\nThere is lots of room for improvements of course. An improved scaling algorithm for the guitar image, using more landmarks from MediaPipe to get a better/smoother distance estimation. A larger number of guitar samples would be a big improvement - more chords, and more variations of strumming. Using Csound could give more control over the guitar samples as well, including the use of effects, and being able to use gestures to turn effects on and off could be lots of fun.\n\nUltimately, this project was an experiment to see what can be done with OpenCV. I think I would focus future efforts on building something that use gestures to control MIDI sequencers - this would be something I could see myself using in my own music making.\n",
        "url": "/motion-capture/2021/04/17/stephedg-airguitar.html"
      },
    
      {
        "title": "'Air' Instruments Based on Real-Time Motion Tracking",
        "author": "\n",
        "excerpt": "Let’s make music with movements in the air.\n",
        "content": "The Concept\n\nThe goal of this project can be established with this concise statement:\n\n‘Make music with movements in the air.’\n\nThis project, as well as applications based on the previous statement, are relevant to expand the range of possibilities for sonification and music performances so that it increases the expressivity through our bodies and enrich the experience of making and listening to music. There are some works related with this topic that can be explored. (e.g. Jensenius et al., Maes et al.)\n\nMy personal motivation for doing this project came from my previous experience in game development and my interest in the implementation of real-time systems as well as previous basic work on motion capture systems back in 2018. One of my biggest influences came from my participation at the NIME conference in 2019 in which I could appreciate some works and performances related with gestures and music making.\n\nAlthough the statement mentioned at the beginning covers a wide range of possible applications, I wanted to focus on a particular set of movements and gestures as well as specific instruments.\n\nBefore diving into more details regarding this project, I want to present the concept behind the idea, which is depicted in the figure below.\n\n\n   \n   The Concept\n\n\nAs you can see, the main idea is tracking movement trough a motion capture system, taking the data in real-time from those movements, and sending to a process to map motion into musical features. The result of the data mapping process is sent to a sound generator that can be a sampler and/or synthesizer to make sound and music out of it.\n\nUsing this concept, I implemented two “air” instruments for a live performance which are the following ones:\n\n\n  \n    Drums: The idea is that the player can do gestures that simulate a “hit” in the air with each hand by holding an object that represents a drumstick and produce drums sounds every time a “hit” is performed. For this case, this implementation is limited to play a kick drum with the right hand and a snare drum with the left hand. The inspiration for this instrument came from a previous work in the SMC master that can be found here.\n  \n  \n    Theremin: This is an instrument that is naturally played in the air, and my intention was to implement a digital version of it with some modifications. Particularly, I am using the distance between a hand and a point in space to map frequency and amplitude of a sine wave. The point is an object that can be tracked by the motion capture system to obtain its position, which is used to calculate the needed distance to the position of the hand. More details will be provided later.\n  \n\n\nBoth instruments coexist in the same environment to be used for a musical performance and are part of a networked architecture explained in the following section.\n\nMethodology\n\nThe System\n\n\n   \n   The System\n\n\nThe image above depicts the architecture of an interconnected system for transforming movement features into music. After some experimentations and testing regarding the equipment that was utilized, I ended up using two computers connected directly through an ethernet cable. In the first computer runs the software Motive which is used for managing the optical motion capture system (MoCap) OptiTrack. Motive has an internal module for streaming data through the network called NatNet. This data is sent to the NaNet Depacketization Client which is a Python module implemented by the developers of Motive to receive the data from the network and filter only the id, position, and rotation from rigid bodies (static objects identified by the MoCap) to be used in a Python environment. Despite NatNet client can be used in a different computer connected to the same network in which the MoCap computer is, I decided to run it in the same machine in order to improve the performance of the data processing by reducing the transmission time from Motive to the Python client. This configuration allows to take the data from this client and handle it directly in a real-time process in Python.\n\nContinuing with the data flow from the previous image, the real-time process in Python uses the information from the rigid bodies to map movement data into meaningful musical data relevant to the two instruments described earlier (Drums and Theremin). Then, the results are sent through the network using OSC messages to another computer in which there are VST Plugins that interpret the messages and reacts accordingly. Those plugins are running in the DAW Ableton Live. The advantage of having the sound engine in another machine allows to focus the hardware resources on the music making and sound processing.\n\nOne peculiarity regarding the communication between the computers is the use of a direct ethernet connection. It works through WIFI if both computers are connected to the same network, however, it is faster and more stable to send data through a direct physical connection.\n\nMore explanation about the modules in the system is presented below.\n\n‘OptiTrack’ Motion Capture System\n\nAn optical motion capture system (MoCap) is a platform commonly used in entertainment for animating characters, however it is widely used in other applications and one of them is the study of body movements. For this project I took advantage of the high reliability of this kind of systems and I decided to track the position of four objects illustrated below.\n\n\n   \n   Objects to Track (Rigid Bodies)\n\n\nA MoCap system detects reflective objects trough its infrared cameras and track their position as single points in a 3D space, but it can build more complex structures out of these points such as rigid bodies and skeletons. A rigid body is a set of points that has a configuration that is not intended to change and can be tracked as one only object that has a position and rotation in the 3D space. The skeleton is also a set of this markers, but it is shaped under a more complex structure that has static objects as bones that are interconnected and can be moved and rotated according to an object with articulations (generally a human being).\n\nAs you can see in the image above. I am using two static rigid bodies and two other ones taken from the bones on each hand since bones can be treated as rigid bodies. The data from these objects is sent to the Python module as described before.\n\nPython Module\n\nThis module uses NaNet for receiving data from the MoCap regarding the frames and rigid bodies in real-time. From this data, I am using only the timestamp from each frame and the position from the four rigid bodies mentioned above into the implementation of the following modules:\n\nHit Detection (Drums)\n\nThe idea is using the static rigid bodies 1 and 2 as “virtual drumsticks” for a kick drum and a snare drum. Both use the same strategy for hit detection depicted below.\n\n\n   \n   Hit Detection (Drums)\n\n\nThe first condition for the hit detection needs the calculation of Jerk which is the variation of acceleration in time. It was chosen because there is a high variation of movement when a hit occurs since it is related with a sudden and quick stop of a moving object, which means that the acceleration goes to zero in a very small amount of time. It required to calculate instant velocity, then instant acceleration, and finally instant jerk, all depending on the previous one. “Instant” in this discrete context means to consider the delta time and delta position between the current frame and the previous frame for making the calculations.\n\nSince the result from the previous process is a vector, I determined the magnitude of it and compared it against a threshold (10000 m/s^3). However, it detected the hit when the movement happened up and down. Thus, for limiting the hit when it goes down, I used the displacement vector (using the delta position between frames) to calculate its angle regarding the up vector of the coordinate system and check if this value is greater than 90° to ensure the right direction of the movement.\n\nFinally, the detection happens several times in a very small amount of time, but because I just needed one detection, I used a small delay (0.3 sec) to wait for the next one, which helps to skip other undesired detections in the moment of one hit.\n\nThe result of this strategy is one OSC message (/kick or /snare) per rigid body that is sent every time a hit is detected.\n\nDistance Calculation (Theremin)\n\nFor this instrument it is needed the interaction of two objects for each parameter (amplitude and frequency) that the sound generator requires (four in total). The figure below illustrates the strategy used in this module.\n\n\n   \n   Distance Calculation (Theremin)\n\n\nThis strategy calculates the distance between two objects, specifically from one static rigid body to a hand bone in a virtual skeleton. The computation of the distance is performed by using the displacement vector between both positions and obtaining its magnitude, which is the classical way to do it. After this calculation, the distance is normalized to a value between zero and one by considering a threshold between the objects (0.8 m).\n\nThis distance calculation is applied to two pairs of objects: Right hand – rigid body 1, and Left hand - rigid body 2. The firs one is mapped to a 3 octaves A minor scale considering the frequency of the musical notes at the end, and the second one to a normalized amplitude (0 - 1) considering the inverse of the distance. The mappings are depicted in the figure above.\n\nThe result is sent by using an OSC message (one address and two values) every 0.1 seconds in the pattern ‘/sine freq amp’ where freq corresponds to the frequency and amp to the amplitude of a sine wave that is constantly playing in the VST plugin that resides in the other computer.\n\nVST Plugins and Ableton Live\n\nI developed two simple VST plugins for this project to translate the results from the processes explained above into sound. I used CSound in Cabbage framework. The first VST is a sampler that take the /kick and /snare OSC massages to play a kick drum and a snare drum respectively. The second VST receives the ‘/sine freq amp’ OSC message to feed a sine wave with the frequency and amplitude provided by the message.\n\nThose plugins were loaded into audio tracks in Ableton Live and used in a looping session for a live performance executed together with MIDI controllers.\n\nResults\n\nBecause of the precision and reliability of the motion capture system, the response of the output according to the movement in real-time feels adequately and fluent for a live performance since there is an immediate response to the actions performed in the air.\n\nA demonstration of how it works is shown in the following video:\n\n\n   \n   'Air' Instruments - Live Performance\n\n\nConclusions\n\nOne of the challenges in this project was managing a real-time setup for the whole architecture presented here. I used the Portal room at Aalborg University and there were some considerations regarding network and computers to use in order to make it work. Thus, it is essential to know the equipment we have and the limitations to deal with when we work with interconnected systems.\n\nMost of the fixed values used (thresholds and delay) were obtained experimentally to fit my particular style to play such instruments, hence it is important to develop calibration strategies to change those values according to the performer and the environment, even apply smart strategies for automatic adjustments if needed.\n\nThere is a high potential in this project to expand it to more motion and sound possibilities as well as the inclusion of visual feedback to enrich the experience for the performer and the audience. Also, it could be extended to a collaborative environment in which more people interact with each other as well as other objects.\n\nFinally, I want to express my enjoinment in developing this project. I am looking forward to continuing the use of such technologies during this master program and my professional life, and I also hope that this work inspires others for looking into more innovative ways to increase expressivity in making music through our bodies.\n",
        "url": "/motion-capture/2021/04/17/pedropl-airIntruments.html"
      },
    
      {
        "title": "Kodaly EarTrainer-App",
        "author": "\n",
        "excerpt": "App for training your ears based on old Hungarian methodolgy\n",
        "content": "Introduction\n\nFor the final project in SMC4043 I decided to work on an iOS-application\nwhere i utilised computer vision and machine learning to extract hand\nposes from a live camera feed and map Kodaly hand signs to the different\nscale degrees of the C-major scale. The purpose of this project was to\ncreate a prototype of a possible ear training app which teaches the\nability to recognize each natural scale degree of a major scale and tie\neach degree of the scale to a specific hand sign. Although I didn’t\nmanage to complete the task, I have made the framework for the app\nreadily available once I have gathered enough data to train a usable\nmodel. The bulk of my time was spent getting the app up and running,\nsomething which turned out to be a quite intricate process. At least\nconsidering that I don’t have a background in Computer Science and have\ndone little programming previously. But thanks to the internet and good\ndocumentation I was able to get a bare bones prototype up and running.\nEven though I have not thought about or seen anyone combine ear training\nwith hand signs before I discovered the Kodaly method, I think it is an\ninteresting concept. For me as a guitar player my approach to ear\ntraining is quite visual. Since I imagine how the intervals looks on the\nguitar and I associate the different scale degrees and intervals with\nhow I would play them on my instrument. It makes the process of ear\ntraining a bit less abstract. Mapping the different intervals to these\nhand signs can potentially be useful as a pedagogical tool.\n\nMethodology\n\nAs I see the use of motion capture devices in musical applications, the\nmost important part is the mapping. Making meaningful connections\nbetween the musical output and the movement of the body. In my case, I\nhad to figure out how I could connect the different scale degrees to a\nspecific movement. In my first prototype I have drawn one point on each\nof the finger tips, as you can see in the illustration below.\n\n    Overlay on the TIP of each finger\n\n\nWhen you hit the “Play Sound”-button a random note gets produced in the\nrange of 1-7. The application counts the number of fingers it detects,\nand if the fingers are the same as the number of the scale degree an\nimage appears with the correct number displayed on the screen.\n\nstruct FingersOverlay: Shape {\n    let points: [CGPoint]\n    private let pointsPath = UIBezierPath()\n    \n    init(with points: [CGPoint]) {\n        self.points = points\n    }\n    \n    func path(in rect: CGRect) -&gt; Path {\n        for point in points {\n            pointsPath.move(to: point)\n            pointsPath.addArc(withCenter: point, radius: 5, startAngle: 0, endAngle: 2 * .pi, clockwise: true)\n        }\n        \n        return Path(pointsPath.cgPath)\n    }\n}\n\n\nIn the code snippet above you can see how each point has been drawn on\nthe fingertips of every finger recognized. The recognized finger tips\ncomes from another part of the project where I have used the Vision\nframework from Apple by using the function VNRecognizedPoints to detect\nonly the fingertips of the hand. Each point of the fingertips are stored\nin an array.\n\nif  overlayPoints.count == audioPlayer.soundPlayed {\n                Image(systemName: \"\\(audioPlayer.soundPlayed!).circle.fill\")\n                    .resizable()\n                    .imageScale(.large)\n                    .foregroundColor(.white)\n                    .frame(width: 200, height: 200)\n                    .shadow(radius: 5)\n                    .animation(\n                        Animation.easeInOut(duration: 5)\n                            .delay(3))\n            } else {\n                EmptyView()\n            }\n\n\nIn the ContentView.swift-file, which is the View that displays\neverything you can see on the iPhone-screen, I just count the number of\narrays that are being displayed and compare them to a number I get from\na seperate class I have defined in AudioPlayer.swift. If the number of\nfingerpoint arrays are the same as the number of the note that was\nplayed, an image appears on the screen. The content of AudioPlayer.swift\nlooks like this:\n\nimport AVFoundation\n\nclass AudioPlayer: ObservableObject {\n\n    var player: AVAudioPlayer?\n    var soundPlayed: Int?\n\n    func playSound() {\n            let sounds = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"]\n            guard let sound = sounds.randomElement() else { return }\n            let url = Bundle.main.url(forResource: sound, withExtension: \"wav\")\n            \n            self.soundPlayed = Int(sound)\n            print(soundPlayed!)\n        \n            guard url != nil else {\n                return\n            }\n            do {\n                player = try AVAudioPlayer(contentsOf: url!)\n                player?.play()\n            } catch {\n                print(\"Could not find file\")\n            }\n        }\n}\n\n\nAs you can see in the code snippet above, I have made a function called\nplaySound(). This function gets triggered by the “Play Sound”-button you\ncan see at the bottom of Figure 1. It picks a random sound from the\narray called “sounds” and stores the name of the file as an optional Int\nin the soundPlayed variable. The sounds I created myself in Ableton\nLive.\n\nHowever, even though this works perfectly fine, the mapping is not that\ninteresting or meaningful. It is not completely arbitrary as it\ncorresponds to the actual scale degree and the concept of thinking of\nnotes as numbers in relation to the tonic of the scale is a really\npowerful tool if you want to develop your ears. To take this project a\nstep further I therefore decided to implement the Kodaly hand signs in\nmy app.\n\nThe Kodaly Method\n\nThe Kodaly Method is an approach to music education developed in Hungary\nby the Hungarian ethnomusicologist, composer, pedagogue, linguist and\nphilosopher Zoltan Kodaly. His method was developed as a result of his\nview that the current music education system in the Hungarian school\nsystem was of poor quality. Among all of the methods he developed was\nthe method for solfege hand signs. The idea was borrowed from an English\neducator called John Curwen and the technique assigns to each scale\ndegree a hand sign that shows its particular tonal function.\n\n\n    Kodaly Hand Signs\n\n\nImplementing the hand signs\n\nThis approach seemed like a good fit for my project. And was helpful in\ncreating a meaningful connection between sound and action. To implement\nthe hand signs I chose to train a ML-model using CreateML, which is\nApple´s own plug-and-play software for creating custom machine learning\nmodels. The software is quiet straight forward to use. You just\ndrag-and-drop your dataset divided into predefined folders (classes),\nthe application automatically splits your data and starts the training.\nYou have some options for cropping, blurring, rotation etc, and you can\nchoose the maximum number of iterations. But apart from that there is no\nreal options for tweaking. The feature extractions are pre-defined and\nyou can´t really fine tune your model as you my like if the results you\nget are not up to the standards you would like. For some strange reason\nthe training always stopped after 10 iterations, and I was not able to\ntroubleshoot this issue. We have a saying in Norwegian that goes like\nthis “Det enkle er ofte det beste”. I don´t know if that was true in\nthis particular case. The good thing about this application is that the\noutput you get is a class object. Which makes it really easy to\nimplement into your application. I tried to collect as much data as I\ncould, but it turned out to not be sufficient for my purpose. I would\nguess that the hand signs might be to similar and it would require much\nmore data than I was able to assemble in the short time I had available.\nA better approach to this would probably be to drop the machine learning\nand take a more heuristic approach and just hard code in the values of\nthe different hand signs. I could f.ex measure the values of the\noverlaypoints when my hand makes the fist sign and map it to the\n“Do”-value. This would probably be sufficient, but I was not completely\nsure how to accomplish this, but for later development this can be an\napproach to explore.\n\nResults and conclusion\n\nI was able to make a prototype of an app which plays a randomly selected\nnote and compares the scale degree of the note with the amount of\nfingers displayed on the screen. Even though this implementation was not\nmy final goal it was quite satisfying and fun to play around with.\nHowever, for a final product where all of the possible intervals would\nhave been implemented this would not suffice. Unless you are one of the\nlucky persons born with 11 fingers. Therefore, the Kodaly-approach is\nmuch more suitable for this particular task.\n\n\n    Kodaly Hand Signs Chromatic\n\n\nBecause as you can see in figure 3 the whole chromatic scale has been\nimplemented and the system is usable in a context where all of the 11\nscale degrees are being used. Even though I did not fully complete the\nchallenges I had set up for myself, I think the idea and concept is\nquite interesting, and should be explored further.\n\nAs a final conclusion I would say that the course in itself has been a\ngreat learning experience. Although the course was more generic in its\napproach to motion capture and not particularly focused on the musical\napplications of motion capture, it was fascinating to see each students\ndifferent approach to the task we were given. An although I did not\nmanage to complete my project I got an opportunity to explore the world\nof computer vision, which I had not been exposed to earlier. The\napplication of computer vision in musical contexts are endless, and will\nbe explored further. I also find it more rewarding exploring easily\navailable tools. Rather than having to rely upon expensive closed off\nsystems, I find the cheap and open resources much more attractive.\n\n\n\nResources used in this project:\n\nhttps://www.raywenderlich.com/19454476-vision-tutorial-for-ios-detect-body-and-hand-pose\n\nhttps://developer.apple.com/documentation/vision/detecting_hand_poses_with_vision\n\nhttps://developer.apple.com/videos/play/wwdc2020/10043\n",
        "url": "/motion-capture/2021/04/18/thomasanda-kodalyeartrainer.html"
      },
    
      {
        "title": "Shim-Sham Motion Capture",
        "author": "\n",
        "excerpt": "We’ve learned about motion capture in a research environment. But what about motion capture in the entertainment field? In this project I attempted to make an animation in Blender based on motion captured in the lab using the Optitrack system. Beside this, I also analysed three takes of a Shim Sham dance. For more details and some sneak peaks read this blog post.\n",
        "content": "Music-related motion capture\n\n\n    \n\n\nMy background and inspiration\n\nAs a research assistant for RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion, I came in contact with several methods of tracking motion. I worked a bit with the Video Analysis toolbox for python, and I mentored the Music Moves: Why does music makes us move? online course for two years now, learning more and more about the world of motion tracking. Last semester I also started to work with motion-capture (mocap) data - labelling markers and gap-filling trajectories of long-ago aquired data in Qualsis - in preparation for an open source catalogue of data.\n\nI took this course because I wanted to cement the knowledge I already had in this field and to learnt what I didn’t have the chance to yet. The online course part of this class went smoothly, the Music Moves course is interesting and for the first time I also got the chance to take the tests and get the certificate (with a final score of 97% yay). The other part of this course consisted of several classes during which we were introduced to several techniques of motion tracking and finally, the project. One of the topics I enjoyed the most was the introduction to biomechanics class, where I finally got to understand some of the underlying physical and biological principles which puzzled me before - plus, it was sciency talk which I missed terribly! I’ve learned a lot during the class on accelerometers as well, even if I didn’t use this technology in my final project, and I got to do a short study of how much I moved over time while doing planks and listening to music - besides the great workout. The video analysis class was extremely relevant for a seminar that I’ll have to hold for WoNoMute, and that’s why I didn’t focus on this technology either. Instead, I chose to do a project based on mocap data, to finally learn about the whole timeline of a project that uses an optical infrared marker based system - in this case, Optitrack.\n\nHaving been exposed to mocap in research labs before, I started to wonder how is mocap used outside of this area and for what. That’s how I thought of the entertainment world and how mocap data is used to animate 3D characters. When our teacher for the course, Tejaswinee told me about Blender, I decided to give it a go and try to animate some characters using mocap data. I was eager to try it and curious if I will enjoy it as much as I thought.\n\nThe project\n\nMy goals for this project were to learn as much as possible about the process of capturing motion in the lab using the optitrack system and have as clean of a data as possible, as well as going as far as I could with animating some characters based on the captured data. Since I wanted some music-related motion, I chose a fragment of a swing routine called Shim Sham. I decided to record three takes and later use them to animate three different characters and to perform some analysis on them - inspecting the markers and their displacement and speed.\n\nMocap with Optitrack in the lab\n\nOptitrack and Qualsis are two different mocap systems, each with their ups and downs. I learned that Optitrack is much more friendly for creating skeletons and rigid bodies and this was a good plus. I couldn’t believe how straight forward was to create a predefined skeleton. We did it in class in half an hour! However, there are several factors that can influence the quality of the data - e.g., the amount of drop outs (marker trajectories) - so I was set of having as reliable of a data as possible.\n\nThe first thing I did was to spend some time preparing the room - I pushed the furniture to the walls, reangled the cameras and got rid of all reflections - so that I didn’t have to mask anything before calibration. Which was very good, no intentional blind spots. Then, when calibrating, I took extra care to calibrate the space in which the dancing was going to happen. Maximizing the capturing areas for cameras and calibrating very well, I ensured that at all times, as many markers would be seen by at least three cameras, preferably more, and it worked. Taking in consideration the sampling rate of the system, I avoided a quick song, with quick movements hard to decipher. All this extra care payed off and the recordings was really clean with very few, if any gaps!\n\nCreating a skeleton was way easier than in Qualsis. I used a predefined skeleton for entertainemnt (since I was later going to animate it) with the baseline + 13 markers, 50 markers in total. During the first tests, I realised how important the correct placement of markers was, and I had to adjust several of their positions to align with the biomechanical constrains of a human body.\n\nAfter this, recording the data was quite straightforward. Below you can see a short sneak peak from during one of the recordings. Exporting the data for anlaysis went smoothly as well.\n\n\n   \n   Sneak Peak from the lab during data aquisition\n\n\nAnalysis the mocap data\n\nAfter exporting the mocap data as .csv files, I analysed them with the Jupyter Notebook provided in class. However, my computer is quite old and down’t have a lot of computational power, so instead of using all the data, I only used the first 1000 rows of each of the takes, aproximately 8 seconds. In this clip you can see which dance moves were retained in the analysed data.\n\n\n   \n   Short exerpt of the Shim Sham dance used in analysis\n\n\nWhen I came up with this project idea, I was interested to see if the notion of “warming up to the groove” (as mentioned by Hans T. Zeiner-Henriksen during the 5th week on the Music Moves course, when talking about groove research) would be observed in the data. Translated to motion, I expected to see that in the second and third takes there would be more displacement than in the first one, so the dancer would move more.\n\nIn the graphs below, the distance travelled for all markers in the three takes are shown. Visually, this seems to be in line with my intuition - you can see how in the first take, the legs and feet are moving significantly more than the rest of the body, whereas in the other takes the rest of the body is starting to move more and more - especially in the graph for the first take, the legs and feet are having a significantly different curve: the curves in the upper part of the graph are for the knees, shins, ankles, and toes. I calculated the average amount of displacement along each axis for the three takes, and the values are also consistent with this idea. The dancer himself mentioned after the recording that in the second take started to focus on moving the upper body (and booty!) as well as the moves with the feet, and that in the third take he tried to focus on everything (and stumbled upon some steps). A more sistematic study (with more takes) is necessary to find a definitive answer, though.\n\n\n    All markers distance travelled\n\n\nAfter considering the previous graphs, I was interested to see which markers had the most movement that was relevant for the performed dance move - going with the right foot in front once, then the left, then twice with the right, like in the dancing clip. Logically, the tip of the toes marker had the most movement, and also the most relevant. Here we can see the displacement of the toe tip markers positioned along the Y (posterior-anterior) axis. We can also start to notice the exact moves of the feet (and where the dancer stumbled over his steps a bit).\n\n\n    Foot tip markers' displacement along Y axis\n\n\nWhen analysing the speed of the markers I checked the trajectories for the knee, shin and ankle markers as well as for the toes, and all showed a clear indication of the dance moves. However, the markers placed on the tip of the toes gave the biggest amount of information (as expected) - you can observe this in the graph below, where all the markers of the right leg and foot in the first try are plotted.\n\n\n    Right leg and foot speed\n\n\nWhen plotting the speed of the left and right tip of toes markers for all three takes, the dance move performed becomes entirely obvious. We see two peaks for each move (speed was used to move the foot in front and again to take the foot back), and we can observe how the speed for the right and left foot differ. The mistakes become quite obvious as well (when there are more peaks than necessary for the move).\n\n\n    Speed of both feet toe tips\n\n\n(Almost) animating mocap data in Blender\n\nBlender is a open source 3D creation suite. And it’s exceedingly complicated, a fact I didn’t know before coming up with this project. I spent half the time watching tutorials and trying to understand how to move around the space and between modes and generally, how to use the program. I spend almost all the remaining time troubleshooting the retargeting between the mocap armature and the armature of the characters I downloaded from Sketchfab. I still had to manually add constrains for almost all bones in the final animations because of this and that reason (I don’t understand enough about this yet to explain it). Weirdly enough, the biomechanical skeleton we did in class had better results when retargeted on a stormtrooper character, than the entertainment skeleton I used for this project. I’ve learned that it’s much more complicated to retarget mocap data on characters if the armatures are not constructed of more or less the same bones - e.g., the monkey had no neck bone, which messed up the position of the head quite often. Since the presentation we had to give in class focused more on the animation part of the project, I will not give too many details here. Sufice to say that I didn’t go very far, and instead of having three cute and funny characters dancing Shim Sham in the same time, this is as far as I got… I couldn’t even figure out how to export it in a nice animation and add sound..\n\n\n   \n   Stormtrooper and monkey dancing shim sham\n\n\nFinal thoughts\n\nThrough this project, I had two man goals and I believe I managed to reach them both. On one hand, I wanted to learn how to go through the entire timeline of a mocap based project - from data aquisition to data analysis. This step was entirely successful - I managed to have a very clean data aquisition, and the analysis showed proof of what I intuitively believed to be true before. Arguably, I could have done a more complex analysis, and in future projects I will focus on extracting more information from the mocap data than the basics and on having better roots on existing literature. The purpose of the analysis should have also been clearer, and less exploratory, for which I apologize.\n\nOn the other hand, I wanted to challenge myself and see if I can manage to animate a character with the mocap data from the lab, and have an animated “performance”. This step was not as successful as the first, considering that I barely managed to have a somewhat okay animation with two characters. However, even being unsuccessful, this step taught me something about myself - no matter what I believed before, I now know with some amount of certainty that I would not like to work with 3D animations; it’s a fascinating world and I deveoped a huge respect for anyone who works in this field, but it’s not for me.\n\nOverall, this course was very useful for me. There was almost no overlap with what I knew from before, but rather all my previous knowledge which was scattered around the field of motion tracking is now better cemented is theoretical knowledge and improved by the practical knowledge I gained. Thank you, Tejaswinee, for a great course and for your help and support along the way!\n",
        "url": "/motion-capture/2021/04/18/alenacl-shimsham-mocap.html"
      },
    
      {
        "title": "Motion (and emotion) in recording",
        "author": "\n",
        "excerpt": "The first time I went to a recording studio in the early nineties, my eagerness to (music)-world domination—as well as my fascination for the possibility to put my beautiful playing to a magnetic tape—totally over-shadowed that the result sounded crappy, at least for a while.\n",
        "content": "Back in the nineties I used to play in a band known for it’s energic live-shows and none of this came through on the record we’d just recorded. What happened, where did the energy go? We were a trio, and even overdubbing stuff in the studio, it still sounded thin and boring. How could it be that our audience went nuts, with their pogo-jumping and dancing, on our concerts if that was what we sounded like? Where they just being polite, or maybe very easy-pleasy? Or was it rather that we–in the recording process–lost something on the way?\nQuestions like these has followed me ever since, and even if I’ve started to find my own way of recording myself, I’m still searching for answers on how to capture the intended vibe of the music being performed and recorded.\nAnd for this project, I thought I could start to investigate matters like these a little closer.\nTAPES AND HARD DRIVES\nI will not go into historical details of music and sound recording, internet is full of that for whomever wants to read more about it (check this or that for instance). But in terms of early recording techniques in popular music, one could briefly say that placing a microphone in a room with musicians playing live (full songs, or even full concerts), and press rec on tape, was the main way to do recording. This technique leaves most of the end result up to the musicians performances and the quality of the equipment used, and leaves little to nothing for post-processing.\nAs studio equipment evolves over time, the introduction of multitrack recording and more advanced mixers, post processing becomes more important. During the 60’s the musicians could record each their part alone, one by one, and the studio technician could equalize and pan the individual tracks. And in the very end, when all tracks were recorded, a mixdown to a stereo track would occur.\nToday, when basically all sound recording eventually ends up on a harddrive, with laptops being able to record an (almost) infinite number of tracks, with endless possibilities when it comes to effects and mixing facilities, musicians can even play only parts of their songs, taking away a lot of the performance, leaving the end result to duplication, sampling and all other post processing one can imagine\n\nMOTION, EMOTION AND MUSIC\nSo, back to this project of mine. After struggling with how to use motion tracking in a project that also made sense to me, I thought it could be interesting to see if there were some correlation between motion and recorded sound. And I wanted to implement the issues of the different mindset of pre and post processing of the sound. These days people can record the dry signal of a guitar, and leave most everything to post processing. Friends of mine in a metal-band recorded guitar DI with only a generic distortion for monitoring when playing, and then did all processing post recording (in the mix). At the other end of the scale, other friends of mine prefer to do most processing pre recording; meaning using their own effects, their own amp and preferably play it at the correct volume, for the perfect break-up of the amp and so on. The post-processing of the guitar-sound will then be more some sort of adjusting or compensating, as opposed to actually creating the sound.\nIn the first situation, one could argue that for instance the dynamics of the playing could be heavily affected, when not using the gear you are used to. Overdrives and distortions are a quite personal thing for many guitar players, myself included, and they will all sound different with different equipment in the signal chain, and with different playing styles. So one question that comes to mind is: will this method of recording affect the playing of the guitar player, and if so, how?\nMy own preference lean more towards the latter, and I have a belief that the perceived sound while playing, also greatly affect ones playing. And it also may reflect in the players emotions, as well as the motions, while playing.\nTHE SETUP\nSo I connected my guitar and split the signal path as shown in fig. 1 into one direct input to my DAW (input 1), and the other path going guitar –&gt; overdrive –&gt; reverb –&gt; delay –&gt; tube amp (with line out signal) –&gt; DAW (input 2). With this setup, I was able to record both the wet and the dry signal in one take, and I could choose whether to monitor (using headphones) the wet or the dry signal.\n\n\n    Fig. 1: The setup.\n\n\nI decided to do the experiment with 16 bars of a slow song I play in a band, with a quite atmospheric sound (lots of reverb and delays) playing chords. Using my iPhone as camera, I first played it when monitoring the wet signal, which is the sound I’m trying to achieve as an end result. And then I played it again, when monitoring the dry sound.\n\nVIDEO ANALYSIS\nBefore importing the videos to the VideoAnalysis application, I cut both videos to 16 bars, so that the lenght of it was the same (well, they were 1107 and 1103 frames). Below you can see crops of the two videos (no sound).\n\n\n\n\n\n\nLeft is wet monitoring, right is dry monitoring.\n\n\nAs you can see, the playing and thus the body motion (and emotion as well, quite visible in my face and body language [I’d say—knowing myself]) in the two videos differs quite a lot. When recording them I felt more uncomfortable in the one I only could hear the dry signal, where as monitoring the wet signal, I felt a lot more familiar. I did one take with each monitoring options; I didn’t want to rehears too much for each monitoring, and as a result I adjusted my playing technique after what I felt was better for the perceived sound.\nBefore using VideoAnalysis to analyze, it’s interesting to se how the I in the left image of the video is standing quite still at the end, where as in the right image—when monitoring the dry signal—quickly bends towards the computer to stop the recording. In my headphones, the sound quickly died, due to no reverb and delays in the monitoring, and I also cannot wait to be done with the recording, which felt awkward and strange.\nI analysed the exported .csv-file from VideoAnalysis in python, and as one can see from the graph in fig. 2, the quantity of motion (QOM) peaked at the end of the dry-monitoring video.\n\n\n    Fig. 2: Normalized and filtered quantity of motion (maybe even with traces of emotion?).\n\n\nBut you also see that in general the QOM seems higher in the wet-monitoring video, as the cumulative QOM-graph (fig. 3) also shows (even though the dry-monitoring actually rises above at the very end).\nIt could be argued that if I pulled out eight bars or so in the middle of the musical phrase, the result would be close to equal, as the graphs in fig. 3 follows each other quite consistently. While this is true, I would argue that the first motions in the wet-monitoring video is very music-related, and thus cannot be left out of the analysis. I could only analyse the first eight bars instead, and then the cumulative difference would be greater than it is now, because of lacking the last not-so-musically related end of the dry-monitoring video. But leaving also that part in is interesting, because it might tell us something about the emotion of the player.\n\n\n    Fig. 3: Cumulative quantity of motion.\n\n\nAnother thing to point out, which might relate to the emotion of the player, is how the wet-monitoring graph in fig. 2 seems a little smoother than the dry-monitoring one, that could be described as more un-even and less musical, maybe. Could this also be related to the players emotion?\nPICTURES OF YOU\nIn some of the pictures exported from VideoAnalysis one can see similar trends. In fig. 4 I’ve placed the two average images (based on taking the average of all images in video stream), one from each video, and the fact that my head is more transparent on the wet-monitoring image, shows that there’s more motion going on. Maybe also the head-movements are more closely linked to the players emotions?\n\n\n    Fig. 4 and 5: Monitoring wet on the left, dry on the right.\n\n\n\n    \n\n\nIn fig. 5, the greyscaled motion average images (based on taking the average of all motion images in video stream) it’s also obvious that for instance my right hand is moving more along the string-directions (to the tremolo arm, and possibly also playing more with different points of pick attacks). The neck of the guitar also has a broader grey area, meaning it moves more in the wet-monitoring video.\nSOUND ANALYSIS\nThere are apparent differences in the motion in the two videos, but how does the wet signal sound in both the cases. I aske python to show me a spectrogram of both wet recordings, as that in both cases is what I want as the end result. Fig. 6 shows that there are also apparent differences in the the sound recorded as a planned end result. One thing, as is also easy to spot in the video; is that the rhythm differs. Again, that’s maybe due to me trying to make the dry monitored take sound interesting. Another thing is that the spikes are somewhat taller in the dry monitored take, meaning there is more happening in the very high frequencies. This I believe is caused because I don’t hear how the effects react to my playing, thus not affecting my playing.\n\n\n    Fig. 6: Spectrogram of the wet signal recorded monitoring wet signal (top) and dry signal (bottom).\n\n\nCONCLUSION\nI have found this very limited attempt to look into this very broad field very interesting. As stated earlier, questions regarding these matters have followed me quite some time, and having discussed it with many of my music colleagues, both performers and studio technicians, I have never thought of doing research on the field. But having learnt more about music related body motions, emotions, it has triggered me to look more into it.\nI believe that there is a lot more to discover in this filed, and numerous research designs to be made here. Even my limited try-outs has revealed that there is—at least for me personally—a connection between music related motions and emotions, and that the perceived sound affects ones playing and therefore also the audible end result. What would be interesting is to go deeper into different aspects of a research like this, and maybe narrow it down to different kind of effects to be used. For instance one could have blues players playing through their normal amps, with their preferred overdrive (if they use it), and then play the same part monitored directly (dry). That would maybe be a good starting point, before going to all noise guitarist taking away their monster pedalboards.\nIn essence this is trying to find out on a broader scale what makes good recordings, that also correlates with the musicians who recorded it. We all have experienced recording some part, and when you hear it again after post processing, your comment is: «Cool … who’s playing?», haven’t we.\n\n\n    Fig. 7 shows the beautiful waggling in the y-axis, again, wet-monitored on the left, dry-monitored on the right, showing the more wavy motions when listening to the actual end result.\n\n\n\n  \n    \n    Should show a media player\n  \n  Wet signal, dry monitored.\n\n\n\n  \n    \n    Should show a media player\n  \n  Wet signal, wet monitored.\n\n",
        "url": "/motion-capture/2021/04/18/anderlid-Wet-Dry.html"
      },
    
      {
        "title": "Exploring the influence of expressive body movement on audio parameters of piano performances",
        "author": "\n",
        "excerpt": "How expressive body movement influence music?\n",
        "content": "1.INTRODUCTION\n\nMusicians move in many ways when performing music, expressive body movements are an integral part of performing music. This attribute, which is shared by all musical cultures (Blacking, 1973), the role of body movement in music performance also has been discussed by researchers for many years. For instance, the musician’s body is described as an intermediary between the physical environment and the individual musical experience (Leman, 2008). From the perception perspective, researchers found that the body movement is important in the expression of musical performance, in addition to acoustic cues, audiences can also perceive musical expressions through body movement, it contributes to the audience understand the score and the performer’s expressive interpretation of music (Davidson 1993,1996; Vines et al., 2003; Dahl and Friberg, 2007; Weiss et al., 2018).\n\nIn previous empirical studies, researchers have mainly focused on the cross-modal perception between movement and sound. For example, visual and auditory information is found to be closely linked, influencing the listener’s judgment of the musical performance(Dahl and Friberg, 2007; Castellano et al., 2007). Early research showed that non-musicians mainly use visual cues to perceive expressiveness in violin and piano performances(Davidson, 1993), more recent research demonstrated that although both auditory and visual kinematic cues contribute greatly to the perception of musical expressivity, the effect of visual kinematic cues appears to be stronger(Vuoskoski et al., 2014, 2016). Moreover, Massie-Laberge stated that auditors can distinguish pianists’ different expressive performances in three modalities (audio only, visual-only, or audio-visual) but are better in the audio-visual modality, and the body movement may have a perceptual impact on expressive parameters (Juchniewicz, 2008; Massie-Laberge et al., 2016, 2018).\n\nHowever, how expressive body movement affects the audio parameters of the music performance remains relatively low progress. Therefore, this project aims to synthesize the method developed by Davidson(1993, 1994) and Wandely(2002) on expressive music performance in different states, to explore the differences in audio parameters and also the body movement features of piano solo performances under three body movement conditions: Deadpan, Normal, and Exaggerated. Normal is defined as the body movements required for normal performance, the performer does not deliberately change his body movements. Deadpan is defined as using only the body movements necessary for the performance and reducing all body movements as much as possible. Exaggerated is defined as an exaggerated expressive body movement, which increases the range of expressive body movement as much as possible when performing. The three different performance conditions all require the pianist to keep the artistic expression as similar as possible.\n\nMy main research questions include the following two points: 1) How audio parameters are affected by a modiﬁcation in expressive body movements? 2) How expressive body movement change when playing under different conditions? I hypothesize that changes in expressive body movement will affect tempo and performance dynamics, and head movement may change drastically under different conditions.\n\n2. METHOD\n\n2.1 Participants and material\nParticipants\n\nDue to the epidemic and time constraints, the current experiment will only have one piano performer for analysis. The piano performer (male, 23 years old) have more than 15 years professional training and stage performance experience on piano solo performance.\n\nMusic material\n\nFor the selection of excerpts, three tracks from three composers from the same period, a total of six excerpts were selected, according to the technical difficulty of the music score, all the excerpts were divided into three levels: difficult, medium and easy\n\n• Hungarian Rhapsody No.2, S.244/2-Franz Liszt, Marked as Liszt 1(easy), Liszt 2(difficult), Liszt 3(difficult).\n\n• Ballade No.1, Op.23-Frédéric Chopin, Marked as Chopin1(medium), Chopin2(easy).\n\n• 6 Klavierstücke, Op.118-Johannes Brahms, Marked as Brahms1(medium).\n\n2.2 Procedure\n\nEach excerpt was performed in order in three different conditions three times: Normal, Deadpan, Exaggerated, a total of 54 files were recorded.\n\nIn Motion Capture section, motion data were collected, at a 120 frames per second, with a 8-camera Opti-track Motion Capture in Aalborg University, SMC Portal. Using 25 passive reflective markers on the performer’s heads, wrists, shoulders, torso, elbows and arms, the placement of markers on performer’s body is listed in Figure 1.\n\n\n    Figure 1. Upper body skeleton on Opti-track\n\n\nIn audio recording section, audio and MIDI files were recorded by a pair of AKG C414 microphones in AB Stereo Format and MIDI output through SSL 2+ Audio Interface, sampled at 48 kHz, 24 bit.\n\n\n    Figure 2. Recording Setup\n\n\nMotion capture data will be analyzed through Qualisys and Python, and audio files will be analyzed through Matlab MIR Toolbox and Python Librosa Package.\n\n3. Pianists’ audio and Movement Data Analysis\n\n3.1 Audio analysis\n\nThree audio parameters were analyzed: Duration, RMS and Pulse Clarity. The Duration and RMS were analyzed by Python Librosa package, and the latter parameter was analyzed by MIR Toolbox. Because the last parameter changes too drastically between the performances of the same excerpts, so only Duration and RMS were used in this project.\n\nDuration\n\nIn this section, I directly compared the duration of the same excerpt in three conditions. After calculating the average duration and variance for the six excerpts in three conditions(sorted from easy to difficult), I found that the difficult excerpts become faster with restricted body movement. In addition, the modified body movements made the easy level excerpts like Brahms1/Chopin1 slower than the normal version. Moreover, the body movement modifications rarely affect the duration in the two excerpts with the lowest difficulty factors, which also the two slowest excerpts.\n\nFrom the perspective of variance, I found that restricting body movement can affect the performance’s duration more than exaggerating them. and it is also matches the performer’s subjective feeling: It is more difficult to control the performance by restricting body movement than exaggerating body movement.\n\n\n    Figure 3. The average duration and variance for the six excerpts\n\n\n\n    Figure 4. Duration variance for six excerpts\n\n\nDynamic\n\nThe RMS indicates the average energy of the audio file, as all the files recorded in the same amplifier level, mic position, and piano position, so comparing the average RMS for audio files between the different conditions gives an indication for the dynamic of the performance.\n\nAs the data stated in Figure 5, I found that in the two difficult excerpts, the dynamics were significantly reduced under the exaggerated condition. According to the performer’s feedback, this is likely because the difficulty and the higher speed of the notes made hands span a lot, and the exaggerated movements made the fingers touches shallower and the sound intensity becomes less, which is typical of a lack of control in piano pedagogy.\n\nMoreover, from the RMS Variance, it is clear that exaggerated condition could affect more on RMS rather than deadpan condition.\n\n\n    Figure 5. The average RMS and variance of the six excerpts\n\n\n\n    Figure 6. RMS variance of six excerpts\n\n\n3.2 Body movement analysis\n\nIn this section, motion capture data of three excerpts (Brahms1, Liszt3, Chopin2) from different levels of difficulty are selected for presentation. In data analysis, 21 of the 25 measurement points are used. According to the position of the human body, these are grouped into six parts: head, shoulders, elbows, wrists, arms, and torso.\n\nMovement Velocity\n\nBy calculating the average movement velocity of each part of the body, I found that under the three movement conditions of all six excerpts, the movement velocity of the head and wrist are the fastest, and the velocity of the shoulder and torso are the slowest. Moreover, the velocity of the wrist is positively correlated with the average tempo of the excerpt.\n\nWhen comparing the movement velocity variance of three different conditions, I found that the variance of the velocity of the head is the largest, but there is almost no change in the wrist. My assumption is that the movement velocity of the wrist is mainly related to the difficulty, tempo, and structure of the music content, and has less connection with expressive body movement during the performance.\n\n\n    Figure 7. Average movement velocity of Brahms op.118\n\n\n\n    Figure 8. Velocity variance of Liszt 3\n\n\nQuantity of Movement\n\nIn the QoM analysis, I found that the amount of head and shoulder movement is the highest, while the wrist is the lowest. This pattern also has been found in other excerpts, which is related to classic piano performance skill, that is, stability of the wrist is vital to maintaining the tone of a classic piano piece.\n\nAs for the variance of QoM in three conditions. The change in head movement was the most variable across the three conditions, followed closely by the shoulders and elbows. Meanwhile, through qualitative observation, I found that in the Deadpan condition, the amount of head movement is significantly reduced. This may be because, from a kinematics point of view, the direct contribution of head movement to sound-producing activity is not obvious enough.\n\n\n    Figure 9. QoM data of Liszt3 in three conditions\n\n\n\n    Figure 10. QoM Variance of Liszt3 in three conditions\n\n\n4. CONCLUSION\n\nThrough the analysis of audio, we can draw the following tentative conclusions:\n\n• Changes in expressive body movements affect the tempo and dynamics of the piano performance, which is related to the difficulty of the music, with the higher the difficulty factor the bigger the impact on the music.\n\n• Comparing with exaggerated expressive body movements, restricting expressive body movements will affect music performance more drastically.\n\n• Exaggerated expressive body movements may affect sound dynamic significantly.\n\nFrom the analysis of body movement, we can also have following initial conclusions:\n\n• The head, wrists move faster during the piano performance, while the arms and shoulders move the slowest.\n\n• The movement velocity of the head, shoulders and torso is greatly affected by playing under different conditions, while the wrist and elbows are hardly affected by restrictions or exaggerated conditions.\n\n• The head and shoulders have the largest QoM, while the wrist QoM is the smallest.\n\n• The QoM of head varies significantly in different conditions.\n\n• The expressive body movement are closely linked to piano pedagogy.\n\n5. Reflection and Future Work\n\nThrough this exploration and analysis of expressive body movements in piano performances, it is not difficult to find that body movement have a very obvious impact on the audio parameters of piano performance. This also matches the previous empirical research (Davidson 1993,1996; Vines et al., 2003; Dahl and Friberg, 2007; Massie-Laberge et al., 2016, 2018). However, in this experiment, there was only one participant, both audio and body movement data was not comprehensive enough. Moreover, the use of the electric piano during the recording affected the performer’s performance to some extent. Hence, the conclusions can only be used for reference.\n\nIn future work, in addition to study the variation of audio parameters from a more detailed, micro-structured level, exploring the subjective perception of the music produced under different expressive body movement conditions is also a worthwhile research direction. The combination of subjective and objective analysis method will certainly provide a more comprehensive and convincing way out for this research.\n\n6. REFERENCE\n\nBlacking, J. (1973). How musical is man? Seattle, WA: University of Washington Press.\n\nG. Castellano, S. D. Villalba, and A. Camurri, (2007). “Recognising human emotions from body movement and gesture dynamics,” in International Conference on Affective Computing and Intelligent Interaction, A. C. R. PaivaRui, R. Prada, and R. W. Picard, Eds., Berlin: Springer Verlag, 2007, pp. 71–82.\n\nS. Dahl and A. Friberg, (2007). “Visual perception of expressiveness in musicians’ body movements,” Music Perception: An Interdisciplinary Journal, vol. 24, no. 5, pp. 433– 454, 2007.\n\nMassie-Laberge, Catherine &amp; Cossette, Isabelle &amp; Wanderley, Marcelo. (2018). Kinematic Analysis of Pianists’ Expressive Performances of Romantic Excerpts: Applications for Enhanced Pedagogical Approaches. Frontiers in Psychology. 9. 10.3389/fpsyg.2018.02725.\n\nMassie-Laberge, Catherine &amp; Cossette, Isabelle &amp; Wanderley, Marcelo. (2016). Hearing the Gesture: Perception of Body Actions in Romantic Piano Performances.\n\nGoebl, W. (2017). Movement and touch in piano performance, In B. Muller &amp; S. I. Wolf (Eds.). ¨ Handbook of Human Motion (pp. 1–18), Berlin: Springer. doi:10.1007/978-3-319-30808-1 109-1.\n\nJuchniewicz, Jay. (2008). The influence of physical movement on the perception of musical performance. Psychology of Music - PSYCHOL MUSIC. 36. 417-427. 10.1177/0305735607086046.\n\nNusseck M., Wanderley M.M., Spahn C. (2018) Body Movements in Music Performances: The Example of Clarinet Players. In: Müller B., Wolf S. (eds) Handbook of Human Motion. Springer, Cham.\n\nSarasúa, A., Caramiaux, B., Tanaka, A., &amp; Ortiz, M. (2017, June). Datasets for the analysis of expressive musical gestures. In Proceedings of the 4th International Conference on Movement Computing (pp. 1-4).\n\nThompson, M. R., &amp; Luck, G. (2012). Exploring relationships between pianists’ body movements, their expressive intentions, and structural elements of the music. Musicae Scientiae, 16(1), 19-40.\n\nVuoskoski, J. K., Thompson, M. R., Clarke, E. F., &amp; Spence, C. (2014). Crossmodal interactions in the perception of expressivity in musical performance. Attention, Perception, &amp; Psychophysics, 76(2), 591-604.\n\nVuoskoski, J. K., Thompson, M. R., Spence, C., &amp; Clarke, E. F. (2016). Interaction of sight and sound in the perception and experience of musical performance. Music Perception: An Interdisciplinary Journal, 33(4), 457-471.\n\nVuoskoski, J. K., Gatti, E., Spence, C., &amp; Clarke, E. F. (2016). Do visual cues intensify the emotional responses evoked by musical performance? A psychophysiological investigation. Psychomusicology: Music, Mind, and Brain, 26(2), 179.\n\nJensenius, Alexander Refsum; Wanderley, Marcelo M.; Godøy, Rolf Inge &amp; Leman, Marc (2010). Musical Gestures: concepts and methods in research, In Rolf Inge Godøy &amp; Marc Leman (ed.),  Musical Gestures: Sound, Movement, and Meaning.  Routledge.  ISBN 978-0-415-99887-1.\n\nLeman, Marc &amp; Godøy, Rolf Inge (2010). Why Study Musical Gestures?, In Rolf Inge Godøy &amp; Marc Leman (ed.),  Musical Gestures: Sound, Movement, and Meaning.  Routledge.  ISBN 978-0-415-99887-1.\n",
        "url": "/motion-capture/2021/04/18/wenbo-motiontracking.html"
      },
    
      {
        "title": "Walking in Seasons",
        "author": "\n",
        "excerpt": "Sonification of motion\n",
        "content": "Walking in Seasons\n\n\n    \n\n\nThe project:\nWe wanted to explore the motion tracking system that we have at the Portal at NTNU, using Motive and IR cameras. This system can track reflective markers in a room and you can quite easily make a recording and get the data to make anything you want with it. We had the idea to create a virtual space of sound where the four quadrants of the room represented one season each, and when you move through the room you can hear sounds from each season. We call this project “Walking in Seasons”.\n\nPython:\nThe Python code takes the data from Motive as a csv (comma separated values). You don’t really need to use any fancy libraries, it’s just a list of numbers that you can make Python do stuff with. Since we couldn’t make the Motive software stream the tracking data in real time everything was already filtered and gaps were filled when Python started working, which made everything much easier.\n\nAll that needed doing was organizing the data in Python (in more lists and dictionaries) and trying to figure out how to find out what the tracked person was doing depending on the numbers.\n\nGetting the position of the person was pretty straight forward, just checking on what x (left/right) and z (forward/backward) coordinates the head was. We had calibrated Motive so that x=0, z=0 was in the middle of the room, so if x was positive and z was positive we knew that the person was to the forward and right. If x was negative but z was positive the person was back to the right, and so on. Some if-statements (and lists) later we had Python figuring out in what quadrant of the room the person was through the whole recording!\n\nWe could track when the person was walking by checking when the feet passed from above to below a threshold on the y coordinate (up/down). Now we had two lists of interesting stuff made from the (pretty boring) raw data! Before moving on to Max MSP we made another list of how high up each of the hands were (y again), scaled between the lowest and highest point in the recording. It’s just so much easier to work further in Max if you know you’re going to get values from 0 to 1!\n\nWe got Python talking to Max through osc, just sending the values at the same speed that they were recorded so we later easily could sync the sound with the video.\n\nMax MSP:\nMax for Live is a great addition to Ableton Live, making projects like this much easier. When you run Max as a plugin in Ableton you can make Max control some things in Ableton, like volume sliders and other things like that, check this out if you’re curious: Controlling Live using Max for Live – Ableton)\nWe figured if we put all the sounds of different seasons on different tracks we could just lower the volume of the ones we didn’t want to hear at the moment. So if the person was in “winter” we lowered the volume of the other three seasons. After making some routing to make sure we knew where the different types of messages from Python were going we made some functions that changed the volume of the tracks. Gradually, of course, to avoid clicking. And also, using squared scaling instead of linear so the volume didn’t seem to drop out when going from one season to another, confused? Check this out: Constant power panning using square root of intensity | Computer Music)\n\nPlaying the step sounds was just a matter of making a bang every time Python sent a “step” message. Python just sent “1” every time a step was taken, to make this easy. We sent this bang along to trigger the clips of the stepping sounds, each on its own track. We controlled the volume of all the stepping sounds the same way as with the ambient sounds of the seasons, so we only needed to hear the steps for the rights season at the right time.\n\nThe hand messages were used to control the amount of send to effects, the only thing we needed to do was some mathy scaling magic to make sure we got the right amount of effect from the right amount of waving of the hands.\n\n\n    \n\n\nAbleton Live:\nAbleton Live was used in two cases here, first is to produce sounds and as discussed above to manipulate the sound. We made four tracks related to four seasons (Autumn, Winter, Monsoon, and Summer) The autumn track expresses a happy and chill vibe as a metaphor for the colors of the autumn season. The winter track is trying to depict a cold and stormy day of the winter, Summer track depicts a dark night with cricket and bug sounds in the background. Various step sounds again as mentioned above were made pertaining to the seasonal sounds, i.e., the autumn sound of walking on leaves, winter walking on snow, summer walking on a dusty road, and monsoon walking in water.\n\nThe other function of Ableton is to host the Max MSP patch and control different parameters related to the sounds and apply the effects on it according to hand positions. The Max MSP patch plays track summer if the head is the quadrant linked to summer and the step sound is also triggered accordingly. This is done for all four seasons. The left-hand position controls the amount of tremolo effect applied to the season sounds and the right-hand controls the distortion effect amount.\n\nA final track is then recorded in Ableton by running Python and Max patch at the same time, which is then used in the video.\n\n\n    \n\n\nMATLAB\n\nTo bring out the perspective of the markers placed on the body, we used MATLAB to produce the animation which was added in the demo video, as stated above, we faced  difficulty in streaming real time data, so we decided to make a proof of concept video which enhances a person to understand what exactly is happening.\n\nThe Motion tracking data from Motive OptiTrack was exported in .c3d format and then used in MATLAB (using MoCap Tool Box) to produce an animation.The animation shows movement of various markers and lines connecting the markers with various colours for each part of the body from the front view.\n\nDemonstrative video:\n\nThe Video was shot from two different angles, front and top. The front was recorded on an android phone and the top was recorded by an action Camera, giving us a large field view.\nThe top view was then layered with different representations for seasons and border lines. Both the views, the animation layer and the audio were edited on Adobe Premiere Pro.\n\n\n   \n   Walking_in_Seasons_Demonstration\n\n\nEndnotes:\nWe managed to create a system that sonifies the motion, although not in real-time completely but can be expressed as a proof of concept for a real-time immersive and interactive sound and motion installation. The quality of the system is fairly accurate but has a lot of scope for improvement. The first and foremost is being able to stream and calculate the data in real-time and then integrate it with the sound system, either using more markers or using the predefined marker set preset in Motive for a better visual representation of the skeleton and body. Mapping more audio parameters to different motions also including the derivatives of the motion can improve the system immensely. Overall the installation provides valuable insight into how to use the MoCap system for the sonification of motion.\n\nYou can access the python script and max patch using this link Walking_in_Seasons Python&amp;Max\n",
        "url": "/motion-capture/2021/04/21/abhishec-walking-in-seasons.html"
      },
    
      {
        "title": "Spring Concert 2021: Team B's Reflections",
        "author": "\n",
        "excerpt": "We’ll do it live. Team B gets its groove Bach.\n",
        "content": "Introduction\nFor the grand finale of SMC4022 we, Team B, wanted to play something that should be slightly challenging and rewarding. Through the semester we have been doing improvisations in class, and when working in the Portal outside class, we’ve played things from real punk to Daft Punk. At one point in this semester’s class Henrik and Lindsay (he’s not Team B, but we like him anyway!) did an audio processing improvisation which was something we wanted to experiment with as a team. Anders, being a pedal virtuoso, was obviously given the main role of doing the processing. A role he was more happy with compared to being the lead singer of our Team B punk band. Willie, who presumably should have been born in the 16th century, suggested that we do a processing improvisation on J.S. Bach’s cello prelude in G.* And so we did.\n\n(*Willie would have preferred 16th century music, but chose something modern like Bach instead so that no one would call him old fashioned.)\n\nLive Performance\n\n\n\nTeam B - Concert Excerpt.\n\n\nAnders’ Instrumentation\n\nBeing a pedal freak, I connected three pedals to aux 1 and three to aux 2 on the mixer. In the aux 1 loop I used a Chase Bliss Audio Mood -&gt; Gamechanger Audio Plus, the latter with a Red Panda Particle in its own effect loop. Without going into too much detail, all three of them are able to sample and play back incoming signals (in very different ways), as well as having a blend function, making it possible to let signal through without being effected. In this loop I tapped signal from all of my teammates, one at a time.\n\n\n   \n   Anders' pedals, captured by Stefano's lens.\n\n\nIn the other loop, I used the DL4 in looper mode, capturing Willie’s playing at the start and actually playing that part back throughout the performance. But with overdubs, ocassional reversed playback and everything being effected by the Gamechanger Audio Plasma (distortion) and the eternal-reverberation-able reverb with added tremolo/flanger/filter, the Procession from Old Blood Noise Endeavours, even beautiful played Bach can sound like noise.\nI routed the returns from the aux to separate channels (instead of back to the aux returns), to be able to sidechain my signal with Henriks OP-1 as source, but having to get our 15 minutes rehearsals down to 7, we skipped most of the dark electronic hip-hop in the middle.\nAnd yeah, I also played a few notes on the bass.\n\nHenrik’s Instrumentation\n\nFor the performance I played the Teenage Engineering OP-1. We rehearsed with having both Anders and myself processing Willie’s guitar, but we quickly figured it got way too messy way too fast. So the solution was assigning the processing role to Anders and me using the OP-1 to play rhythmic patterns for the first part of the performance and switching to a piano for the second part. I then also used the onboard delay effect to create some ambient texture to the sound, blending it with the heavily processed signal from Anders. Speaking of Anders - he also processed my sound locally. Having one of the driest signals, my role was also to be a kind of clock in the performance. Especially for Willie whose listening situation was slightly different from ours.\n\n\n   \n   Henrik's Instrumentation\n\n\nPedro’s Instrumentation\n\nPedro used a PRS electric guitar connected directly to the MIDAS mixer through a set of pedals for giving a reverb effect and made the connection possible with enough gain. In addition to that, he connected a computer to the stereo input (reached through a local mixer) that was running a strings synthesizer played through a mini-MIDI keyboard.\n\n\n   \n   Pedro's Instrumentation\n\n\nWillie’s Instrumentation\n\nMy instrument of choice for the musical journey was a Blackstar “Carry-on” electric guitar tuned in renaissance lute tuning and sloppily routed for a neck pickup. Although in earlier improvisations I experimented with running the guitar through AmpliTube 5 and utilizing a chain of effects, I decided to keep things simpler instead and eliminate the small amount of extra latency AmpliTube 5 was adding. To achieve a decent sound with enough gain, I used a Marshall Origin 20 amplifier and connected the emulated DI directly to the MIDAS. Although the cabinet emulation on the DI wasn’t particularly good, we felt that the benefit of not needing to use a microphone outweighed the tonal disadvantage for our purposes. I also kept my tone quite clean and transparent because, with so many layers of effects involved in our sound, I felt that a simple sound would provide a helpful musical anchor. Particularly as I was the only member of the team on the Trondheim side of the latency chasm, I wanted an easily blended and absorbed sound in case my contributions ever clashed with simultaneous decisions in Oslo.\n\n\n   \n   Willie keeps it simple.\n\n\nFinal Team Reflection\n\nFrom the very beginning of the masters programme, Team B has shown a good flow in the activities that have been performed. This time we were challenged in operating the portal and playing together in a better setup than our first online semester. Although this setup had some issues to be solved, we managed to overcome those problems, play, and learn in every opportunity we had in classes, and hours outside them.\nStrange issues occurred now and then, where sound was received from NTNU, but not sent. After a good amount of head scratching and debugging, the simplest solution of all seemed to solve it - a good old restart of the LoLa computer. Earlier in the semester, a restart of the converter had given the same result.\n\nFor this concert, we committed to an improvisation and effect processing adventure; we realized it was the best way to play together according to our musical skills, and it also helped us to dedicate some time to technical issues while keeping a good enough performance that deals with latency.\n\nWe hope that in the upcoming fall semester we can keep our flow and look for ways to enrich our performance through the technologies explored in the portal.\n",
        "url": "/networked-music/2021/05/01/pedropl-team-b-concert.html"
      },
    
      {
        "title": "Dispatch from the Portal: Dueling EQs",
        "author": "\n",
        "excerpt": "How do I sound? Good? What does good mean? How do I sound? Sigh…\n",
        "content": "Making Our Musical Sounds\n\nTone. Tone tone tone tone tone. For many musicians, a perfect tone is the dimension of music-making that is most elusive. Whether playing an acoustic instrument or an electric one, tone is often what separates the good from the great.\n\nBut with our attention in the Portal so completely consumed by combating latency, setting up scenes, and often just successfully turning everything on, tone is understandably one of the last things that gets considered. Success has thus far been measured by audible sound, minimal feedback, and the fact that nothing has burst into flames. Those of us who are tonally concerned do of course have the opportunity to EQ at the mixer (or wherever else in the signal chain it might make sense), but here’s where things get tricky.\n\nSure, we’ve EQ-ed ourselves to a degree that we’re happy with what we’re hearing and sending out to the other Portal, but what about the EQ-ing that they might be doing? EQ-ing to make a sound sit nicely in the mix on either side of the Portal is obviously necessary, but without the ability to hear the other side’s re-EQ of your own EQ how can a musician be sure that the sound being heard and reacted to several hundred kilometers away has any resemblance to the sound that was intended? It doesn’t make any sense for either Portal to return sound to its original maker (complete with charming levels of latency), if there’s to be any chance of a musical experience. But it also would sure be nice to know what flavor of your own performance is actually being heard after the multiple layers of EQ-ing.\n\n\n  \n    \n    Alternate Text\n  \n  Should it sound like this?\n\n\n\n  \n    \n    Alternate Text\n  \n  Or this?\n\n\n\n  \n    \n    Alternate Text\n  \n  Or this???\n\n\nHere’s a nice visual analogy regarding the challenge of multiple rounds of EQ-ing.\n\n   \n   Should it look like this?\n\n\n\n   \n   Or this?\n\n\n\n   \n   Or this???\n\n\nA photographer would never release an image into the world knowing that it would be edited and released before they had a chance to approve the final version. Performing without knowing how your sound will be EQ-ed before being heard by listeners may be similarly problematic.\n\nJoyfully Loud or Pleasingly Controllable\n\nThe Portal is a room with lots of exciting equipment and more than a dozen speakers. We’re all music and technology people here. Of course the first inclination is to play loud. And for some people, that means really LOUD. But maybe a different approach might yield a more musical experience.\n\nSince much of the EQ-ing done in the Portal is driven by the fight against feedback, headphones are an obvious solution to the volume wars. Similarly, using all electric instruments and running everything DI would almost completely eliminate problems of feedback. For those musicians whose acoustic instruments require microphones,  headphones, careful EQ-ing, and more careful physical placement in the Portal would be beneficial.\n\nHaving removed feedback dangers, the important next step seems to be an increased focus on soundchecking for tone. Twenty seconds of music followed by engaged listening to the mixes from each end of the Portal continuum would definitely provide a chance for the necessary tweaks to be made. Unfortunately, so far we’ve usually been so happy just to have the Portal working correctly that we’ve continued to music making with no attention paid to the aural flavor of the music.\n\nIncreasing Awareness\n\nWith our recent telematic performance there was suddenly a third layer to consider. EQ-ing between the two Portals is problematic enough, but, with the audio also routed from Oslo out to our livestream, there was an audience as well. In addition to having functional audio for successful telematic collaboration, there needed to be an end-product that was enjoyable and matched the musical conceptions of the performers at each Portal. Whether we succeeded in achieving a viable musical or artistic experience for the individual musicians is a topic for another blog post, but listening back to the livestream definitely provided some surprises. For those of us on the Trondheim side of things, some things that sounded great during performance sounded extremely different in the final product. Likewise, some textures that sounded thin in the Portal sounded much richer in the livestream audio. Since the latency problem requires telematic improvisation to often skew heavily to the gestural, it is even more important that the musical gestures we produce are being heard as intended.\n\nFuture Solutions\n\nIn the upcoming semester, we’re optimistic that our newfound comfort with the Portal will lead to more nuanced experiments. It isn’t enough just to make sounds together, now we need to focus on making those sounds more musical.\n",
        "url": "/networked-music/2021/05/03/williakm-dueling-eqs.html"
      },
    
      {
        "title": "Cross the Streams",
        "author": "\n",
        "excerpt": "When performing in two locations we need to cross the streams\n",
        "content": "For the final musical presentation of the portal subject for this semester I was responsible for streaming the audio and video to the SMC YouTube channel.  There were many different ways of doing this so I’ll highlight the considerations and decisions for going with the setup we finally used.\n\nOverall considerations\n\nWhen designing this system I wanted to use as few different systems as possible to minimise the amount of things that could fail, especially considering this was my first time using most of the hardware and was not familiar with how reliable any of them were.\n\nStreaming software\n\nThe go-to for free streaming software is OBS.  It’s open source and I was already familier with the slightly more user friendly version OBS Streamlabs from some personal experience live-streaming video games, so this setup was relatively easy for me.  What was different in this situation was that all the video transitions would be handled using an external hardware device so we would only have one “performance” scene in OBS.\n\nVideo\n\nAs mentioned in the above section, the video transitions were handled using the hardware device Atem Mini by Black Magic Design.  The other option was to use the Black Magic web presenter, but the model we have is old and only supports 720p so we decided against it.\n\n\n\nWhen all video feeds are sent directly to the streaming PC it is easy to create one scene per video source and then in OBS’s studio mode, preview which scene you will be transitioning to. Handling all the streams directly on the PC would require a hardware device capable of handling multiple streams or a capture device per stream, resulting in many hardware devices and lots of load on the cpu/gpu of the machine handling the streams.  Instead this processing is offloaded to the ATEM mini but the ATEM only outputs the final livestream over the USB/webcam.  In order to preview the scenes we were transitioning to, a second monitor was set up connected to the HDMI output of the ATEM and using the ATEM’s setup software we were able to configure the HDMI to output the “cued” scene to the HDMI while sending the live output over USB.\n\nFor the cameras, I put in a SDI to HDMI adapter on our PTZ camera and fed the HDMI to the Atem Mini. I did the same for the TICO feed from NTNU but also fed the pass-through SDI to the front screen so our performers could still see NTNU performing.  NTNU relied on our Zoom video because I didn’t pass through our feed to TICO from the SDI-HDMI box (due to lack of cables). We added a second camera at UiO that was fed into the ATEM mini through another SDI-HDMI converter and this camera was used for close ups.\n\nSound\n\nThere were a few options for the audio as well.\n\n\n  Send the audio from the mixer to the Atem Audio input using a mini jack\n  Send the audio from the mixer to the Streaming PC’s onboard soundcard using a mini jack (which is already set up)\n  Get the audio stream from the mixer to the Streaming PC using the mixer as a USB soundcard\n\n\n\n\nI decided to go with the third option because I didn’t want to rely on the onboard soundcard of the streaming PC. I preferred not to use mini jack on the ATEM for the audio stream as well as it introduced an unnecessary piece of hardware for the audio to go through when I could just get the audio stream directly from the mixer via its usb interface.  The way to configure the soundcard was not exactly intuitive (which I think can be said for a lot of the Midas settings) but in the end we had to configure it as a 2 channel in/2 channel out soundcard.  Initially during rehearsals we mapped the Main Out L/R to the USB interface, but then the level of the stream was tied to the level of the audio output in the UiO room, which we had to turn down to avoid feedback (or may have been completely muted if we decided to go with headphones instead) so in the end the Monitor L/R was mapped to the USB interface.  This was done by assigning a “User” route bank to the Card device on the Midas 32 and setting channel 1 and 2 of the user bank to Monitor L/R.\n\nOn the day\n\nOn the day we had some issues with the streaming PC.  It was decided to delay the performance for covid reasons, so a scene was created and broadcast 2 hours prior to the performance informing viewers that the live-stream was being delayed.  The streaming PC was not particularly powerful and I’m not sure of the exact reason, but after two hours of this pre-stream the machine was running at 100% cpu and was not sending any stream data YouTube. After terminating the stream and restarting, we were still experiencing large frame drops at 1080p and in the end I had to drop down to 720p. I’m not sure whether the UiO internet was under high load at this time but I think UiO has more than enough capacity that our 1080p livestream should not have been impacted by that though.\n\nFinal Thoughts\n\nPlay it safe\n\nWe were aware that the streaming PC was not very good hardware wise, but it is not easy to get a new machine provisioned and set up on the UiO network so we had to use this machine.  With this knowledge I should have gone with a 720p stream from the beginning, as even though the initial test stream we did was okay at 1080p most of the time, there were periods of 100% cpu usage.\n\nDo a dress rehearsal\n\nWhilst we did test that live-streaming worked during class, we never did a full “dress rehearsal”, it was more similar to a sound check or tech rehearsal. Having a background in theatre and/or music I think most of us were familiar with having a full dress rehearsal before a performance but felt the time pressure of this particular performance and were more focused on making sure our individual tasks, eg. sound, video, presenting were organised.\n\nDon’t change things on the day\n\nDuring one of the performances I wanted to focus on a musician standing at the mixer and remembered I had set up an OBS scene for that during rehearsal but it wasn’t there anymore.  I recreated the scene on the fly and then transitioned to it a few times during the performance.  Depending on the setup, this could have been fine, but unfortunately I had added the main audio input to the primary performance scene and not the general project, so when I cut to the new scene I had created, the audio cut out.  In this case it would have been better to play it safe.\n\nMonitor the stream\n\nI had headphones connected to the streaming PC but I wasn’t wearing them during the live performance which is a pretty obvious mistake.  This would have immediately made apparent the problem highlighted above where I changed to a scene but there was not audio.\n\nHave another channel for communication\n\nWe had not established a way to communicate between UiO and NTNU off-stream (not using LOLA) so when we were live we have no way of communicating with NTNU without it being broadcast to the viewers.  In the few instances where we needed to contact NTNU, for example to establish a better camera angle, we posted on our discord channel, but we had not informed them to check this (to my knowledge) so it was just lucky that someone checked it and made the corrections.\n\nTake a look\n\nIf you’d like to see the performance you can watch it on YouTube below.\n\n\n\n",
        "url": "/networked-music/2021/05/03/leigh-cross-the-streams.html"
      },
    
      {
        "title": "End of Semester Concert - Spring 2021: Audio Setup",
        "author": "\n",
        "excerpt": "As part of the audio team for the end-of-semester telematic concert, Pedro and Anders spent several hours in the portal, exploring different ways to organize audio routing. They also found time to experiment with effect loops. Check out the nice musical collaboration between two different musical cultures.\n",
        "content": "Ok, you might not find too many Ecuadorian or Norwegian cultur-specific expressions in this experimentation, but Pedro still plays some nice things, and always has to follow Anders mixing after capturing the piano-playing into his effect loop connected to the Midas M32:\n\n\n\n\n\n\nDon't mind Anders' filming, his gestures might not be very easy to understand, but he tries to show some of the routing for the piano into the effect loops. But nevermind, just listen.\n\n\nStreaming a telematic performance is a challenging process that demands the effort from several disciplines to make it work properly, and one of the fundamental elements is the audio setup.\n\nHere we’ll quickly go through some of the audio management process behind the SMC-master students of 2020’s end-of-semester concert In Space We Make Music.\n\nInput Organization\n\nFor the concert we decided to use different Scenes (on the mixer) for each group, since each group had quite different setups with regards to instrumentation and such. That way we could easily switch scene between performances, instead of muting and unmuting different tracks, as well as gainstaging each track. Another advantage is of course that each team can practice and set up their scene with effects, dynamics and such as they please.\n\n\n   \n   Input Channels in MIDAS Mixer.\n\n\nSo the idea was to create a base scene called Concert, and each group using that for further developing thei unique scene. After having the full overview of each groups instrumentation, we started setting up the mixer as shown in the schematics below:\n\n\n   \n   Input Chart in MIDAS Mixer, also showing what inputs are used by each group. Note that we used a small mixer (shown later) for the first two channels on the Midas. Lack of inputs in the stagebox as well as shortage of cables made it very useful.\n\n\nWe made sure that the base scene worked properly, and copied these settings into each groups scenes, for them to explore further and set up as they wanted.\n\nIn theory this worked really good, but as each group practiced, they ran into different challenges, like LoLa issues (that sometimes just needed a reboot), monitoring issues with feedback and such and other normal challenges one face in an audio setup.\n\nThis lead to some hectic moments the day of the concert, so everything is as it should be before a concert.\n\n\n   \n   Local mixer for channel 1 and 2.\n\n\nFrom NTNU we received a stereo signal on the LoLa network, so they were responsible for sending a good mix from their side, which was no problem, as there were only one guy playing there at the time.\n\nOutput Organization\n\nIn terms of output, we needed to manage two sources to feed the streaming and the local monitoring, also take care in sending our mix to Trondheim trough LoLa. For streaming, the video team took the output from the USB port in the Midas mixer and the signal was routed from the Monitor output. The local monitoring was given through the Main Stereo output that is send to the front speakers. Note that Monitor and Main Stereo are two different set of signals that can be routed in several places inside the mixer configuration.\n\nWith one of the groups using acoustic instruments (flute and double bass), we came with the idea of using headphones for monitoring instead of the local speakers to reduce feedback. So, we configured the mixer to send the output from the Monitor to one pair of outputs in the AES—the stagebox—(mapped with inputs from 1 to 8) and connected a headphone amplifier with several outputs for every participant. The configuration panel to change this setting is shown below. Note that output 7 and 8 would be the physical points to connect any device, in this case it was the headphones amplifier.\n\n\n   \n   Monitor Output to AES Outputs 7 and 8.\n\n\n\n   \n   Headphone amplifier.\n\n\nAlthough we made this setup work, when we tested with the groups playing, some of them preferred to hear the sound from the speaker and found this solution invasive and not suitable for the performance flow. So we discarded it, and just used the main speakers for monitoring at a low enough volume to avoid feedback issues, but anyone who wants to replicate this setup could use this solution. It would of course have been a lot better if every player could have their own headphone mixer, to mix their own monitoring, but there is no such equipment available in the portal.\n\nTime to play\n\nWith every performance saved as a scene, the operation of the mixer is fairly easy throughout the concert. Since two of the groups played two songs each, and one group did an improvisational piece, we made five scenes in total, and just changed between each performance. Due to a full stagebox, we didn’t have a designated microphone for the presenter, so the flute-mic (or saxophone, as it was used for, due to stand-in for a sick flutist) also served as presenter-mic—meaning Anders had to unmute it after switching scenes.\nThe switching between scenes could be a lot more efficient, if there weren’t to many confirmation stages on the Midas, but in total, it worked fine.\n\n\n   \n   Control center with angry old man in the Portal.\n\n\nTeam responsibilities and final reflection\n\nAnders and Pedro participated in several portal sections in classes and outside classes to test, install, and configure the setup used in the concert, each team was responsible to adjust their settings in rehearsals. In some specific tasks, Pedro figured out the way to route the output to the headphones amplifier and tested the whole setup considering this new addition. Anders operated the concert for all teams in the presentation day and made specific adjustments in the mixer on every test.\n\nAnders’ last reflections\n\nHaving some experience with both live and studio mixing, this was a fun task for me. I’m not very fond of digital mixers, they offer possibilities that’s not available in analog mixers, so to explore some of those have been interesting. Luckily, when I get lost in the menus, I’ve had good people around me to lead me back on track. Both Pedro, Leigh and Henrik has been invaluable in different situations. The concert ran quite smoothly, even though Alena sometimes was muted when talking (due to slow scene shifting because of numeral confirmations required by stupid Midas).\nIt was also rewarding to me to use my analog (and digital) effect pedals and the mixer itself as my main instrument for this telematic performance, and I would definitely do something like this again … maybe already next semester.\n\nPedro’s last reflections\n\nParticipating as part of the audio team for the concert was challenging because I still was struggling with the mental model for the operation of the MIDAS mixer, but it helped me to get a better understanding of it as well as the portal setup since we had to modify some configurations that were in there from the very beginning of the course, mostly regarding microphones and input types. The organizational aspect of the audio in the concert was a good point to learn how to manage live concerts and specially in a telematic setting as well as the needs from performers locally and remotely. I would suggest to continuing exploring ways to reduce feedback issues and manage proper monitoring that satisfies the performer’s demands.\n",
        "url": "/networked-music/2021/05/04/pedropl-concert-audio-setup.html"
      },
    
      {
        "title": "The SMC Portal through the eyes of a noob",
        "author": "\n",
        "excerpt": "The experiences I had in the SMC Portal during the spring 2021 semester and my evolution from a complete beginner to an almost-average user of a technical and awesome room like the SMC Portal.\n",
        "content": "The SMC Portal in Oslo: a noob’s quick guide and experiences\n\n\n    Concert setup\n\n\nFirst steps in the SMC Portal\n\nDuring our first semester (fall 2020) as part of the SMC Master’s program, we had an online course that introduced us to several software that could be used for communication and performance purposes (e.g. Zoom and JamKazam). We also refreshed our knowledge (or, if we had no previous knowledge, like me, we learned) about basic acoustic principles and several audio and video devices used in the digital world and their working principles. All this helped build a theoretical foundation for the practical activities of the second semester (spring 2021).\n\nAs a complete noob in the area of audio-visual technologies, stepping into the SMC Portal was overwhelming, even after the semester I spent learning about it online. Seeing the mixer nook sent shivers down my spine, and I started wondering how was I going to do remember everything, I had so much to learn…\n\n\n    SMC Portal mixer nook\n\n\nBuilding up knowledge throughout the semester\n\nAfter the initial shock, things settled down. Slowly, I started to understand the language spoken around me and learn the abbreviations (seriously, in this field almost everything is referred to by their brand and model number or other technical details!). The first step for me was to understand the devices that make up the Portal, and I managed this through a combination of shadowing Guy (our teacher) and my more-knowledgeable classmates around, asking sometimes annoyingly basic questions, and researching online. Here are a few of the main things I learned that might be useful for all of you noobs like me who will step foot in the Portal and wonder where to start.\n\nA normal day in the Portal: communication channels\n\nIn the portal we have two main communication channels that we use “daily” (meaning, for every session we are in contact with our classmates located in Trondheim). This is part of the basic turning on/off routine. For more information about what we do in the Portal and what can it be used for, see all our blog posts regarding the Portal.\n\n\n  \n    We first establish a Zoom connection, which uses the Logitech PTZ Pro 2 Camera part of the Logitech GROUP video conferencing system. This is our backup plan, in case the low latency channels (see below) are not working immediately and need additional troubleshooting. We also have an “alternative Zoom” on another PC at the back of the Portal, paired with a Logitech MeetUp); this is especially useful when we need a to share a different camera angle (for example during classes when it’s necessary to show the mixer). It is not possible to use Zoom for telematic performances, the latency is simply too big.\n  \n  \n    After Zoom is up and running, we establish an audio connection using LoLa, our LOw LAtency and high quality software based transmission system. We pair this up with a video connection using TICO, our hardware based TIny COdec compression technology - tiny in complexity, latency and compression. For this we use a Minrray PTZ camera (Pan-Tilt-Zoom) which has an unbelievably awesome image quality. This camera is also linked to the Zoom Room PC, but this semester e only used it for TICO. For telematic performances, this LoLa-TICO solution is the only one worth trying, and even then it will not be possible to play completely on time. See a snippet of our rehearsals below:\n  \n\n\n\n   \n   Ensemble A(wesome) rehearsal: Choubey's drums\n\n\nTechnicalities\n\nTo facilitate the communication and performance in this special room there are a few other worth noting devices I had to familiarize myself with, or at least understand their purpose. And I’m not talking about the speakers or screens in the room, that’s quite intuitive. I’m talking about the mixer (I postponed talking about this one enough), the stage box and the HDMI matrix.\n\n\n  The Mixer: Even having had no experience with audio-visual technologies before (more than my laptop, phone, a semi-professional Sony camera my dad had and the Microlab speaker system I got as a present for my 18th birthday), I still knew how a cool mixer is supposed to look like - a relatively big box like item, with a lot of buttons, knobs and faders (yes, I learned the correct names just recently), and millions of cables sticking out of it, connected to a bunch of instruments, speakers and microphones. Pretty basic, really. Not! It took me at least two classes to be brave enough to even start discovering how to use our Midas M32 Mixer more than just look at what other people did. In time, however, I learned more and more about it and stopped intimidating me that much. I still haven’t used it to its entire capacity, probably more like half, but I’m proud of my progress. I went from not knowing what an XLR cable is and anything about input channels, preamplifiers, auxiliary channels, mix buses to actually being able to connect an instrument or microphone to the mixer and routing it to the output I wanted (for example, sending the sound only to LoLa and the room speakers, but no Zoom), controlling its gain and getting rid of the feedback (and avoiding it in the first place), and saving my setup preferences in a separate scene. I just had to give it time, ask a lot of questions and practice/try as much as possible and learn from my mistakes; it ends up being quite logical after a while.\n\n\n\n   \n   Mixer panic\n\n\n\n  \n    The Stagebox: This name is intuitive enough, but I still had to research its correct usage. It is an interface that simply connects microphones, speakers and instruments to the mixer, allowing for a longer distance between the “stage” and the sound desk. In the Portal, it’s useful because the room is quite long, with a lot of microphones and instruments and speakers places everywhere; so, it simplifies setup to have a stage box in front of the room and the mixer in the back and also helps with not having too long cables.\n  \n  \n    The HDMI matrix: For an embarrassingly long time I thought the HDMI matrix was the main screen in the Portal, which consists of four smaller screen = a matrix; it was frustrating to try to turn it on and not see any feedback. After this happened a few times, I searched what an HDMI matrix actually is, I identified it in front of the room and finally spent some time understanding how it works, feeling relieved I never voiced out loud my misconception (well, now you all now, so don’t laugh). Just to put it out there, an HDMI matrix, also called a switcher, is a device used for customizing high speed audio-visual signal routing from multiple sources to multiple destinations. It’s a very useful device since in the Portal we have the Zoom PC and TICO (at the very least) sending visual signals to two screens (and even a projector).\n  \n\n\nMy contributions\n\nDue to the considerable knowledge gap between myself and the rest of my classmates, I didn’t contribute with a lot. I was happy learning from everybody and consolidating what I was slowly learning by myself. Taking the responsibility to turn on/off everything in the room and establish the communication channels, with some simple troubleshooting, was enough for me to feel immense progress. During our rehearsals for the end of semester concert, I also learned how to connect instruments to the mixer, and process the sound a bit with some gain and equalization. At least for my group, I managed with some basic knowledge of the input channels and mix buses and a bit of logic to figure out and fix our feedback problems: instead of only sending our mics and/or instruments to LoLa (to Trondheim), we also routed them back to our own system, and sometimes we would even route the sound we got from Trondheim back to them. I also proposed that we make the TICO - Minrray PTZ camera setup a more permanent solution, to avoid having to connect the SDI cables every class and having them in the middle of the room, and the idea gain traction and we managed to improve the Portal user-friendliness by a lot.\n\nWhat I used the knowledge for: The accomplishments\n\nThe Mocap data capturing\n\nThis semester I also attended the Music-related motion tracking course (SMC 4043). For the final project, I chose to use the Optitrack motion capture system set up in the Portal to capture some dancing data. For this, it was extremely useful to know how to navigate the room. I was able to use the setup to my advantage and it felt like a great accomplishment to be able to get in the room by myself, set up everything, troubleshoot some weird audio routing with the mixer and the alternative Zoom machine, and answer my participant’s questions about what was this or that used for; it was empowering. Well, except the time I thought that the receiver for the conference mics was actually a LAN router of sorts, but hey, we learn something new every day. You can read more about my project for the Motion tracking course in this post.\n\nEnd of semester concert\n\nFor the end of semester concert, my responsibilities were a bit less technically inclined on the day. I was part of the promo and design team, and although we did a good job having everything ready on time, our Facebook posts were deleted by the curators and we didn’t get permission to post in time to promote the concert properly. During the concert day, I was a presenter and although I have some experience with this position, I have to say it was weirder than I thought to present in front of a camera instead of a room of people, and not be able to “feel the crowd”. I stumbled a bit over my words, but it ended up good - I managed to charm our extremely large audience and they didn’t mind. Here’s a muted video of my antics while waiting for sound - to prove that not everything is milk and honey even after several sound checks…\n\n\n   \n   \"Loading\": Funny Alena presenting while muted\n\n\nAs part of the A(wesome) ensemble, I played the piano first with some midi effects and then without any. It was a completely new experience for me from two points of view: a) I was only ever part of classical music ensamples, so I had no experience playing in a band in the rock-jazzy sense or an electric instrument for that matter; b) For me, experiencing the telematic performance and working with the latency and all other implications was new and frustrating at times, same as for everybody. The performance ended up being successful, and I did manage to feel part of a band, even if we were not all in the same room. For more details about our mixing and instrumentation, check out this other post), and for a recording of our live stream go here (you might want to skip the first several minutes while we worked out some sound issues).\n\n\n    Concert ensemble A(wesome)\n\n\nReflections\n\nI have to say that in the beginning of the semester I was extremely skeptical towards “telematic performances”, I couldn’t see how that would work in a nice way. At the end of the semester, I’m still skeptical, but only because as a classical musician it does not have enough warmth for me - I don’t feel the audience and the other musicians the same - and the entire electronic setup fills up my attention with unwanted background noise and unexpected issues that would never appear when you only have your (analog!) instrument and an audience (sure, the audience is sometimes noisy as well, but in a different way than dozens of devices around you). Also, I now certainly know how telematic performances work, their ups and downs and that they have the potential of working, providing you erase your previous musical experiences and take it as it is - a way for musicians across the world to connect and play music together from hundreds or thousands of kilometers away, which is in and of itself a huge accomplishment.\n\nBesides the telematic performance aspect, this course taught be a lot about audio visual technologies in general, which is something I’ve been meaning to understand more about. It is great to now know enough to not feel completely lost around a studio, for example, or concert setup. Even if I still have a long way to go to reach the level of some of my classmates, I believe I made the most out of this course. There were frustrating aspects of the course, but it was awesome to finally be on campus and do something practical, learn from my own mistakes, and have such a nice end of semester concert. Thank you for this course!\n\n\n    SMC Portal concert generation 2020\n\n",
        "url": "/networked-music/2021/05/04/alenacl-a-noob-in-the-mct-portal.html"
      },
    
      {
        "title": "The collision of Jazz and Classical",
        "author": "\n",
        "excerpt": "The rehearsal experience for our telematic performance.\n",
        "content": "Concert Rehearsal\n\nFor the final concert, we thought it would be more convenient to stick to the acoustic instruments setup. Therefore, the team C decided to play two songs, Salut d’amour and Autumn Leaves, a classical music, and a jazz music, hope this combination can collide with new sparks.\n\nWenbo, Joni and Stephen on piano, flute and bass as usual, and Lindsay on jazz drums for Autumn Leaves and harp for Salut d’amour.\n\nTrondheim Setup\n\nThe audio setup from Trondheim consisted of one audio output from the computer, a MIDI instrument played through the DAW.\n\nFor Salut D’amour, the VST Etheralwinds Harp 2 was used by Jordi Francis, the link can be found here:\n\nFor the Autumn Leaves song, the VST XLN Audio Addictive Drums 2 was used, the trial version can be found here:\n\nThe Yamaha electric Piano was used as the MIDI instrument, both the VSTs were used on the Ableton Live 10 DAW.\n\nThe Lola was setup to send two channels and receive two channels from Oslo. While practicing, we made sure to mute all talkback mics in Trondheim to avoid any feedback. The output speakers were routed to the main Left and Right speakers and routed slightly to the rear surround speakers in the room, due to the positioning of the person at the corner of the room for the video setup.\n\nOslo Setup\n\nIn Oslo, since we were the only group using acoustic instruments, and to avoid the large amount of feedback that could be generated between LOLA transmissions, after discussions with Anders-the concert audio engineer, our initial recording solution was to use headphones rather than Genelec speakers for monitoring whenever possible, and to route the electric piano to the mixing console directly. For the mic setup, we set up an AKG C414 mic for the bass and one for the flute.\n\nFor the mic setup, we set up an AKG C414 mic for the bass and one for the flute. Since all Streaming was done through Oslo Side, we mixed the Trondheim audio stream with the Oslo local sound. For the Salut d’amour, we put the piano on the left side, Linday’s Harp on the right side, and the bass and flute in the middle. For the second song Autumn Leaves, we put the jazz drum stereo signals to the extreme left and right, and the bass to the right of center.\n\nAutumn Leaves\n\n    Our Rehearsal!\n\n\n\n  \n    \n    Listen to our Autumn Leaves!\n  \n  Listen to our Autumn Leaves!\n\n\nSome comments\n\nLike for our previous rehearsals, getting everything set up and working probably took a lot longer than it should have, but once we had it figured out things went reasonably smoothly. The latency, while noticeable, wasn’t overly off putting, and we managed to have a productive session.\n\nAbout half way through the practice the connection with LOLA was abruptly lost, and it took us a while to work out what had happened. Restarting the LOLA computer on the Oslo side got things working again, allowing us to get back to rehearsals.\n\nSalut d’amour was sounding better with practice, and so we decided to have a more electronic focused jam with the time we had left. Lindsay and Wenbo connected up their laptops and MIDI controllers, while Stephen plugged in his Elektron Digitakt drum machine. Synthetic beats and some mean keyboard action ensued. With everything tied to the drum machine, the music felt tight and natural to play over LOLA. Despite this, we decided to push on with our acoustic songs rather than introduce any last minute changes to our set. Maybe we can take a more electronic approach next semester!\n",
        "url": "/networked-music/2021/05/04/wenbo-secondreh.html"
      },
    
      {
        "title": "End of semester reflections for the Portal experience",
        "author": "\n",
        "excerpt": "The first physical portal experience during the pandemic\n",
        "content": "1. Overview\n\nThe SMC4022 was our first physical course since we attended the SMC. The aim of this course is to learn to operate and maintain the Portal, the audio-visual communication platform connecting the two campuses in Trondheim and Oslo. Meanwhile, the preparation of a telematic concert between UiO and NTNU is one of the important outcomes of this course.\n\nIn this semester, we followed Guy and Eigil, learned about the operation and the set up of the hardware devices in Portal, also the low-latency communication tools such as LOLA, TICO, Zoom between Oslo and Trondheim campuses. In this blog post, we will summarise and reflect on our portal experiences and our telematic concert.\n\n2. Experience and Troubleshooting\n\nExperience\n\nAlthough we knew about the MIDAS M32 mixer and all its specifications and routing charts, we actually found it complex to operate it. With help from Eigil and Guy, practicing on the mixer on different scenarios by setting up various microphones and instruments helped us learn faster.\n\nFew routines which made the mixer setup much simpler and easier for us are:\n\n  Try to work in groups of 3 or more people.\n  Always communicate and be loud about the tweaks you are making, so that others can guide you, if something goes wrong.\n  Do not be afraid to try out new parameters and settings. More technical steps can be found here\n\n\nThe primary learnings, playing music and communicating with each other in the team are:\n\n  Everyone is always concerned about how their instrument sounds on either end. So, we had to make sure, the gain stage and mixes on both ends are perfectly done, giving us one master control of how much is sent over to the other end.\n  Being ‘Patient’ while one end is trying to solve issues.\n  Complain unfailingly (always give feedback), to get the best results on your side.\n  Take breaks to ensure having the day without headaches.\n\n\nTroubleshooting\n\nThe experience in Portal was a bit of a mixed bag. During Team C rehearsals, we encountered a variety of software and hardware problems pretty much continuously from the beginning of the course right to the end of the concert. Here we will discuss three LOLA related problems that we encountered during the semester.\n\nThe first seems to be a common one - if LOLA is connected for a longer period of time (usually more than two hours), the audio receive will suddenly disconnect at one end, while the other side remains completely normal. This happened to us pretty much every rehearsal we had, and it always happened on the Oslo side. We found that after restarting the whole computer and reconnecting LOLA, communication was restored and everything worked normally again.\n\nThe second problem occurred during a rehearsal in the middle of the semester when we couldn’t send the audio signal to Trondheim but we could hear from the other side. After asking for help from Stefano, we learned that in the audio input offset option in the LOLA Setup, we should select the same number as the sound card channel setting, which is usually 6 on the Oslo end. We are not sure whether this had reset itself automatically or whether it had been changed by someone, but this became something we would check at the beginning of each session.\n\n\n    LOLA interface for audio input offset\n\n\nOur third problem happened on one of our last rehearsals, where no matter what we tried, we couldn’t hear anything from Trondheim. Despite both the computer and LOLA showing that the signal was being received, the mixer remained silent and we heard nothing. We spent 3 hours trying to fix it before giving up and trying to make do using Zoom (not a good idea!). This problem seemed to magically fix itself after a few days. After consulting with Stefano, we suspect it was caused by a master clock conflict between audio devices. It turned out that the master clock can be set on both the computer running LOLA, as well as the hardware audio interface - we needed to ensure that only one of those devises was set as the master clock.\n\n3a. Wenbo’s Reflection\n\nIn summary, it was really a blessing to be able to participate in a physical class and meet people face-to-face during the pandemic. For preparation of the concert, we encountered many problems but we also learnt so much through solving them. It was a valuable experience for me.\n\nFrom a musical perspective, as a traditional classical music soloist, I am not sure about the concept of telematic performance at the beginning. But rehearsing with Team C did make me feel the joy of playing in an ensemble, and made me realise that it is POSSIBLE to play music successfully together even if we are 500 kilometers away. Besides that, I played jazz music for the first time. The jazz rhythm was really hard to feel at the beginning, but with the help of Stephen and Lindsay, also the lead from Guy in the concert, I think I comprehended some tips on how to “swing” with the Jazz.\n\nDuring the concert, although the digital piano was being constantly compressed for unknown reasons, and re-listening to the Youtube stream was a completely different experience from the live one. But I guess that’s probably one of the charms of being involved in telematic performance, you can never predict what strange problems you’ll have next time!\n\nThe SMC4022 was a good exploration of telematic performance and low-latency transmission techniques, and I hope to further refine my understanding of Portal technology in the next semester!\n\n3b. Stephen’s Reflection\n\nI was very happy to finally be on campus for the portal course this semester. The portal is an exciting place with so much to learn about - pretty much everything in that room was new to me. A myriad of strange black boxes connected to other equally strange boxes via a million leads. It could also be frustrating as there was so much that could wrong, so many points of failure just in that one room, and thats before you take into account the second portal in Trondheim that we were connecting to.\n\nI really enjoyed the rehearsals. It was an amazing experience to play with Lindsay, Joni and Wenbo - musicians infinitely more talented than myself, and at the same time relaxed and easy to play with. Being hundreds of kilometers apart meant that the latency could be an issue at times (I suspect Lindsay would have struggled with this the most, being on his own up in Trondheim), but I felt it was just about manageable for the most part. Playing the concert was fun, especially getting to play Autumn Leaves with Guy on saxophone. Things sounded fine as we played in the room, but as Wenbo noted above, the streamed sound wasn’t brilliant. Maybe monitoring through headphones while playing would have helped us pick up on this sooner.\n\n3c. Lindsay’s Reflection\n\nIn our second semester, we were excited and looking forward to finally meet our classmates and to work in the core feature of the program, the high-tech SMC Portal.\nStarting to understand the technologies and getting a hands-on experience on what we learned theoretically in the first semester and getting around with using the mixer, LOLA and TICO setup was fairly challenging at first. Speaking on behalf of all SMC students, the one main thing we all were experts on, was using ZOOM.\n\nThe course had much to offer, in terms of utilizing the time and technologies at our disposable. Even though we were only 3 in Trondheim, I could reach a good amount of skill using the Midas M32, be quick to grasp ques from sound feedback while able to sort out the equalizing and spend some quality human time in these COVID times.\n\nThe practice and the rehearsals for the concert went quite well. I liked playing the drums for the jazz piece as the whole team enjoyed the song. The things we may want to improve for the concert would be to do a full stream sound check, because we could not hear the bass as well as the saxophone.\n\nReflecting on the concert, it was fun and engaging to play spontaneously with Guy on saxophone, who we never practiced with.\n\n4. Our Concert!\n\nCome and hear our concert, and huge thanks to Guy and his magnificent saxophone skills!\n\n\n",
        "url": "/networked-music/2021/05/04/wenbo-portalreflection.html"
      },
    
      {
        "title": "Preparing NTNU portal for the spring 2021 Concert.",
        "author": "\n",
        "excerpt": "How the NTNU Portal was setup for the spring concert.\n",
        "content": "Overview:\n\nExperiencing portal concert last semester in fall 2020 over Jamulus and a bit of JamKazam, It was time to organize and play music physically in the portal using the best of the low latency audio-video communications we have to learn and improve upon in the SMC: LoLa, Tico and Zoom. The uneven distribution of people, 3 folks at NTNU and 8 at UiO, meant, we at the NTNU side had a lot of roles to play, but this also meant most of the processing was done on the UiO side, and we just had to send the best-polished signals towards UiO. The result was a fun and enriching experience for all of us. Here’s a picture of how portal looked before we started reorganizing everything for the concert.\n\n\n    NTNU Portal in 360 clicked by Eigil\n\n\nTasks at Hand:\n\nNTNU Concert Setup\n\n\n    NTNU Portal setup\n\n\nSince most of the processing was done at the Oslo portal, there were three main tasks at hand in Trondheim - Audio stream, Video stream, and Communications. Our numbers matched perfectly with tasks in hand and we divided them as Lindsay, responsible for Audio, Abhishek for Video, and Willie responsible for communications between the portals.\n\nAudio Setup:\nOur audio setup was fairly straightforward, contrary to the Oslo side where they had scenes set up for all the teams. We instead had 3 channels for all our instruments, One channel for Abhishek’s Guitar, one for Willie’s Luitar, and one combined for routing Ableton signal for Lindsay’s Harp and midi Drums, also for Abhishek’s midi Drums. We had basic EQing and soft compression on the Guitar and Luitar channels. For the USB input (Ableton) we didn’t have any effects on the mixer, instead, I and Lindsay processed the audio in Ableton, which gave us more flexibility and scope for manipulation.\n\nVideo Setup:\nSince Oslo was the streaming point for the live stream on YouTube and had already set up the OBS, we just had to send the video stream directly via TICO. At first, we had two cameras set up, one focusing on the performer and one capturing a wide-angle view of the room. But then we decided to go only with one, as it’s just too much data to send over the internet with LoLa working in the background. It could work but at the cost of latency, which we at the core, aim to minimize at all levels. We usually use the PTZ camera to capture video and send it to the Oslo portal, but this time we framed up a big Sony camera and connected it using SDI cable and a mapper to connect it to the Tico box. We could then easily zoom in and out and also follow the performer moving around performing.\n\nCommunication:\nWillie was the main person responsible for all the communication and the equipment we used. To let the other side pay attention to what we want to communicate can be a huge task at times, as this is a problem that we Trondheim folks face, that sometimes we feel neglected if there’s a lot of things happening in the Oslo side, especially if we all 11 people are present at the portals. The main solution is then to efficiently and at times communicate loudly.\n\nFinal Reflection:\n\n\n    Portal Spring 2021 Cohort. \n\n\nAfter doing the portal course digitally for the first semester, I was pretty excited to have hands-on experience with all the advanced technology we had in portals, physically meeting and doing some work together again felt good for a change. My main goal was to learn more about the do’s and don’ts of being in this virtual-physical space, and also to learn the MIDAS M32 mixer which can get complicated and can feel like rocket science at times. After Organizing a “Telematic Concert” and playing also listening to my classmates jam together, I can safely say that I have to a certain extent achieved my goals. The routing within the MIDAS M32 is still a complicated problem I would like to solve and dig deep into, but overall it was a fruitful experience. Thank you Eigil, Guy, and everyone for the course.\n",
        "url": "/networked-music/2021/05/05/abhishec-preparing-NTNU-portal.html"
      },
    
      {
        "title": "Concert preparations: exploring the Portal",
        "author": "\n",
        "excerpt": "What we learned about the Portal and telematic performances in general while preparing our musical pieces for the end of semester concert. Details about our instrumentation and effects.\n",
        "content": "Concert preparations: expanding our knowledge of the portal\n\nAs a group of three people with considerably different musical backgrounds, one thing we struggled with was finding musical pieces that aligned with our instrumentation.\n\nOur possible choices were:\n    Leigh: Drums, bass, basic piano or guitar\n    Alena: Drums or piano\n    Choubey: Guitar or Midi Instruments\n\nWith these options in mind, we chose one slightly more precise indie/pop song and one more alternative/indie ambiental song. For each piece we had different challenges to overcome and we explored a different aspect of the SMC Portal.\n\nAdding Effects\n\nWhile practicing the song ‘Intro’ by ‘The xx’ we realised it sounded quite plain when we performed without the backing track, and the main thing that added atmosphere to the original track was some reverb on the guitar.\n\nOn the Midas\n\nOur first attempt was to take the guitar signal from NTNU and apply the in-built reverb effects provided by the Midas M32 mixer. Again, this was not very intuitive and it was more luck in the end that made it work rather than understanding what we were doing initially. As shown in the screenshot below, the effects are usually applied to buses.  Instead of this setup, we applied the effect directly to the NTNU track. The problem with this implementation was that Choubey at NTNU couldn’t hear the guitar with the reverb since it was only applied at UiO after we received his signal. At the NTNU side we only had a bit of EQ and a soft compression on the output signal.\n\n\n    Midas mixer hall effect\n\n\n\n    Midas mixer hall effect controls\n\n\nUsing an Amplifier\n\nDuring our first few sessions Choubey was micing up the Guitar amplifier using AKG C214, then routing it through the mixer, and sending that channel to UiO portal via LoLa. This setup worked fine but was slightly rigid in terms of removing feedback or adding audio effects to the dry sound. He had to place soundproofing walls around the amplifier so that it doesn’t feed back into the other mics we were using to communicate with each other or pick up unwanted noise. To remove this obstacle and make it more dynamic we then directly routed the amplifier output via a line out and connected it to the mixer, and then sent it to UiO side for their reverb processing. Directly sending the output from the amplifier gave us the flexibility of muting all the mics during performance, mitigating any feedback issues we had and the unwanted noise.\n\nWe also thought of routing the guitar signal through Ableton and then send it to UiO, but since reverb was added directly on the signal at the UiO side, and there was basic EQing and compression at the NTNU side, we didn’t implement this approach, as it felt redundant. But it could have made the setup even more flexible”\n\n\n  \n    \n    Intro by The xx with mixer reverb\n  \n  Intro with mixer reverb\n\n\nHere you can watch our final performance of this song:\n\n\n   \n   Ensemble A(wesome): \"Intro\" by The XX \n\n\nInstrumentation\n\nAfter using the instrumentation of drums, piano and guitar for our first song we wanted to try something different with the second. We decided to play “Near Light” by Ólafur Arnalds. We had discussed the possibility of Choubey using Ableton to integrate some kind of percussion or mixing which resulted in the choice to go with a drum kit.\n\nWe also wanted to integrate the lead synthesiser sound using Ableton from a laptop on the UiO side but due to the latency we couldn’t think of a good way of implementing it.  There needs to be some concession for elasticity in the tempo when the performers play together since the latency is not always predictable, so trying to trigger an instrument to begin at a specified tempo in the middle of the performance, and continue at that tempo until the end was problematic.  By the time we trigger the Ableton track we could be at a completely different tempo, and the synthesiser does not provide enough of a steady tempo or sound for us to follow it as the lead.  We could have implemented a click track that Alena could play along to from the start of the song, but since the song starts with a rubato feel it would have ruined the expressiveness. This is not a problem that’s caused by the telematic performance but it is exacerbated by it, since it may have been possible to synchronise with the instrument to a certain degree if it was in one location but also synchronising over a delayed connection was too difficult.\n\n\n  \n    \n    Near Light with two pianos and midi drums\n  \n  Near light with two pianos and midi drums\n\n\nWe tried to play the violins part on the piano (you can listen to an exerpt of this version from practice above), but it was harder to distinguish the melody later in the piece. In the end, we settled on having Alena play the piano part and Leigh the parts for the first and second violin on a Midi keyboard at UiO, together with Choubey playing some ambient drums on a Midi at NTNU. Even not playing the cello, viola or the lead synthesiser, the song still had a nice and sweet flow, so that was enough. Watch the video below for the final performance of this piece.\n\n\n   \n   Ensemble A(wesome): \"Near Light\" by Ólafur Arnalds\n\n\nLovely Latency\n\nFor the first song we had Choubey leading at NTNU and Alena and Leigh played along to what they heard as the signal arrived.  This is fine for the live-stream as we are sending it from UiO so everything is synchronised but at NTNU the performance sounded out of sync.  For the second song, it was decided Choubey would be playing Midi drums but they do not start for several bars into the song, so it was necessary for UiO to lead the performance.  This was the first time Alena and Leigh at UiO got a taste of what it was like being on the receiving end of the latency.  They would be playing together and the beat provided by Choubey at NTNU was always behind the beat so we would always be slowing down.  This resulted in the use of “ambient” drum beats for our final performance rather than a steady rock beat.\n\n\n   \n   Ensemble A(wesome) rehearsal: Choubey's drums\n\n\nFinal thoughts\n\nHaving a concert to prepare for gave us some proper incentive and motivated us to practice and experiment with effects and different instrumentations to get the best sound possible under the circumstances. Although we experienced some frustrating moments, in the end our work was repaid during the concert. It was a great and new experience!\n",
        "url": "/networked-music/2021/05/05/alenacl-concert-preparations-ensemble-a.html"
      },
    
      {
        "title": "SMC Portal II - The Dungeon",
        "author": "\n",
        "excerpt": "First time we entered the videolab, it was basically a storage room, full of outdated audio equipment and also hardware we would use. The ceiling lights didn’t work, and the cleaning personal hadn’t been there for quite a while.\n",
        "content": "One of the main objectives of the SMC Master’s program is to equip its students with personal and technical skills that are highly relevant for real-life challenges. The Applied Project courses are two opportunities for students to apply their music-technological knowledge in creative ways by completing complex projects in real-life settings and interdisciplinary teams.\nThis semester, the Musicology Department (IMV) decided to use available resources (software and hardware) to create another portal, where students can experiment and explore audio-visual technologies in search for high quality, high speed and low latency transmissions.\n\nThe Dungeon\n\nFor our applied project we were assigned to make a second portal that would be based on, and expand beyond, the functionalities of the existing portal. The SMC Portal is a room at IMV that is able to connect to other rooms in the world with very low latency. Usually through a software called LoLa.\n\nFor setting up another one of these rooms we analyzed in depth how the Portal treats audio regarding receiving and sending. The room we were given to repurpose was the Videolab, previously used for recording and editing audio and video. Our proposal kept many of the functionalities of the Videolab and we expanded it to also work as a portal.\nFlexibility and ease of use has been two important factors for us while working and developing what we named the Dungeon. The name relates to the Portal upstairs in a mythical fashion and is very describing for the room itself – located in the basement, with no windows and low ceiling, and dungeon-y vibes.\n\nExplanation and evaluation\n\nThe proposed solution was designed to implement and expand on the functionality of the existing Portal at UiO, whilst trying to reduce complexity. To this end, a second LoLa PC was set up in the Dungeon and connected to a 32 channel analogue mixer, a Mackie 32-8. Channels 1-20 are used for microphones and inputs while channels 21-32 are used for receiving signals from Zoom, LoLa and JackTrip through Zoom-computer, RME Fireface 800 interface and Mac Mini. The mixer is set up so audio can be passed through the faders to whatever submix you wish to use, and the B-mix knobs can be applied for a separate monitor mix in the Dungeon. This is fairly similar to the way the Midas M32 mixer is set up in the SMC Portal, which should make it easier to use the Dungeon if you already know how to operate the portal upstairs. In total there are 480 knobs plus 512 buttons just for the mixer channels, so making sure they are all reset can be a little tiring after a hard day’s work in the Dungeon. The mixer and the audio setup is documented in this diagram and mixer chart explaining all channels and how they should be reset after use.\n\n\n   \n   Dungeon Audio Diagram.\n\n\n\n   \n   Dungeon Mixer Chart.\n\n\nTo expand on the functionality of the room, we had three different networks patched to the ethernet ports in the room; the audio/visual network which allows access and control of the SDI cameras and displays (a PC is supplied for this but not set up), the UiO network which is connected to the Zoom and Mac Mini allowing allowing connections to JackTrip, and the LoLa network for the LoLa PC.  This provides for several different audio/visual configurations that include onsite and external connections, something previously unachievable with the SMC Portal’s setup.\n\nTwo DAWs (Ableton and Reaper) have been installed on the Mac Mini which allows people to use a large variety of virtual instruments as well as multi-track recordings, that previously was only able to be done if users provided their own laptop and software.  With the secondary speakers set up and connected to the McONE audio switcher, users are able to preserve the stereo field and audio experience while working directly on the PC and continue to use the room as a “video room”.\n\nTwo PAR lights can be found in the Dungeon. They are controllable by the Mac Mini using a program called LightKey which adds more atmosphere during telematic performances. The new fadable LED room lighting also provides an even white light for optimal use of the green screen.\n\nAn external Linux server has been set up to run JackTrip, which can become part of the SMC project for future students and be expanded for testing and production use of software that is otherwise difficult to have run on UiO networks due to network and permission constraints.\n\nDemonstration and Achieved Objective\n\nFor our demo we used a connection between the Portal and the Dungeon using JackTrip for streaming audio. The result was something one could consider a telematic performance. Upstairs, Anders played the piano and then drums, while downstairs, Henrik played drums on his OP-1 and Leigh contributed with some synth bass. Everything was routed to Zoom through the Mackie mixer. It was done on the fly as the break before our presentation got halved from ten to five minutes, so we weren’t able check our sound and levels properly. And yes, it’s fair to say that we maybe should have done this way before, but we tried and this ended up being our backup solution. What we meant to do was to use LoLa between the portals, but for some reason we weren’t able to hear the sound that was being sent from the portals. Luckily, we managed to get JackTrip up and running at the last minute and here is what we played.\n\n\n   \n   Live Demo over JackTrip.\n\n\nDeliverables and Documentation\n\nThe main and most obvious deliverable for this project was the room itself, evolving from something a bit more than a storage space to an aesthetically pleasant room with high quality audio-visual technologies ready to be used for extended classroom, telematic performance and other use cases.\n\nBefore …\n\n   \n   Our first glance at the videolab.\n\n\nConceptualization …\n\n\n   \n   Conceptualization of the Dungeon.\n\n\nAfter …\n\n\n   \n   The Dungeon today.\n\n\nAfter considering which of the available equipment would fit with the functionalities we had in mind, we took some time to design the room in 2D and 3D sketches. We presented the final conceptualisation in our proposal and after receiving the go from our external partner we started the implementation of IMV’s second portal - by installing software applications, network services and configurations, and setting up communication channels through Zoom, LoLa and JackTrip.\n\nAll through the project we tested and troubleshooted the functionalities of the room to maximize the benefits of the room and its use cases. We decided to focus on an extended classroom setup, with a main and alternative Zoom connections, and a telematic performance setup using audio connection through LoLa and/or Jacktrip. In Subsection 3.5.B of the user manual you can find some details and diagrams about three possible telematic connections.\n\nIn the Dungeon there are three quick setup guides for using Zoom and LoLa printed and displayed on the wall so that anyone going in can immediately start using the available facilities. To facilitate a quicker understanding of how to operate the mixer, the audio diagram and mixer chart can be found displayed on the wall near it. For anyone interested in more details about the functionalities of the room, check out our user manual - that’s where you can find instructions for using the audio-visual technologies, audio, video and network diagrams, quick setup guides, and some tips and tricks.\n\nEnjoy the Dungeon!\n",
        "url": "/applied-project/2021/05/10/anderlid-applied-dungeon.html"
      },
    
      {
        "title": "Designing a hybrid conference",
        "author": "\n",
        "excerpt": "How do you design conference that brings together both virtual and physical participants? This was the problem we explored for our Applied Project.\n",
        "content": "Rhythm Production and Perception Workshop (RPPW) is a biannual international event that brings researchers from a range of disciplines together to engage in discussions about the scientific study of rhythm. Our SMC4031 project was to design and develop a hybrid conference solution for the RITMO/RPPW conference scheduled for 22nd-25th June 2021.\n\nOnline conferences have been becoming more and more popular, allowing people from all over the world to attend events that might not have previously been possible due to time and cost restraints. Then COVID hit, and suddenly they were the only way to hold a conference. This means that there is now a lot of technology out there to support virtual conferences. What has been less explored however is the concept of the hybrid conference. As COVID (hopefully!) starts to recede, conferences will once again start to open up physically, but the demand for online access to these conferences is likely to remain high. So we need to find new solutions that can combine the online and offline elements of a conference into an engaging, unified whole.\n\nResearch\n\nWe started off by exploring virtual socialisation tools such as Attendify, High Fidelity and Gathertown. This is because we saw solving the hybrid social situation as the trickiest part of the problem. Upon further discussion with the RITMO team and interviews with participants from other online conferences, we learned that such high-tech tools would not make sense - the age group of the conference participants will range from 18 up to 80, and such solutions might prove too complex for our less tech savvy participants. Our interviews showed a general preference for tried and tested tools, and so our suggestions for the conference include Zoom, YouTube streaming and Slack. It could be that Slack doesn’t comply fully with the GDPR, and if this is the case, then we propose the open source Rocket Chat as a replacement.\n\nThe virtual conference\n\n\n   \n\n\nFor the online participants, the “Zoom Webinar” option can be used for keynotes, paper presentations, poster blitz, concerts, and symposiums. It also offers functionality for Q&amp;A sessions. Additionally, using the university’s Wowza server, these sessions can be streamed through YouTube as a secondary medium which would also mean that the talk would be available after it finishes.\n\nThe social chat tool Slack/Rocket Chat would be organised by pre-defined conference topics (Entrainment, Music Performance, SMS, Medical Intervention, and Speech), and each paper would be a thread inside it’s topic.\n\nThe physical conference\n\n\n   \n\n\nThe conference will take place physically in two locations: RITMO and ZEB building. Most of the conference will happen at RITMO, while concerts in the ZEB building.\n\nForsamlingssalen at RITMO will be the main room for talks and presentations, and it is already set up and configured for streaming.\n\nFor poster sessions, corridors at RITMO building would be used to place printed posters. Each poster would have QR codes, giving the physical attendants access to digital resources such as pre-recorded video, downloadable documents and links to Zoom &amp; chat channels.\nFor this conference all presenters will be virtual, but such a setup would allow a physical presenter to show their work to both online and in-person attendees at the same time using Zoom.\n\nFinally, concerts will be held in  Salen in the ZEB building. This room has now been configured for streaming with a similar setup to Forsamlingssalen. As the quality is slighter better than Zoom, YouTube would be the suggested primary platform for streaming.\n\nSocialising\n\n\n   \n\n\nAttendants will have the opportunity to socialise through several Zoom rooms and Rocket Chat channels organised by topic. To give virtual attendees the feeling of being in a shared physical place, we suggest introducing an audio/video setup into the RITMO cafe. This can be one of the social Zoom rooms available to participants, offering an opportunity for people to meet up in a more casual way.\n\nBackup plan\n\nEven though we have chosen fairly tried and tested solutions, when relying on complex combinations of technology, things can go wrong. And so a backup plan is needed. Here is ours:\n\n\n   \n\n\nSocialising - alternative solutions\n\nWe explored several alternatives for socialising between physical and virtual attendees, some of them more complex than others. As we mentioned at the beginning of this post, due to the wide range of people expected to attend this conference, the above recommendation uses the simplest and most familiar solutions. This section will briefly discuss some of the alternatives we explored, for those looking for more adventurous solutions.\n\nGather Town allows the building of maps in a 2D world in which people can connect. It uses spatial audio to great effect, and the cartoony, 2D space keeps things approachable and friendly. Several spaces can be created for supporting a conference which include parallel rooms and individual presentations, which could be useful for concurrent talks and poster sessions. Mozilla Hubs has a similar approach using an impressive 3D world. In addition, is also open source and self-host-able. While it offers a lot of power, our experience was that the controls felt a little awkward at times, and it might not be so intuitive for the less tech-savvy among us.\n\nIn the context of a hybrid environment, we need to merge physical and virtual participants in a single place. The solutions mentioned above work well in situations where everyone is online (or at least on a computer). At our conference however, we will have physical and virtual attendees moving between different interest points (talks, posters, social spaces etc.), so the question became: how can we bring the virtual world into the physical space, and the physical space into the virtual world? So we started looking at non-intrusive solutions using mobile phones or detection systems in the physical space. The following is a diagram of one possible solution.\n\n\n   \n\n\nIn the image above we have a physical and a virtual space. In the case of the physical space, the participant is tracked through an “Indoor Positioning System (IPS)”. Attendees in the virtual space can move with an avatar using a web application that maps the physical space. The participants can interact with each other or with pre-configured “interest points”, for example a poster or social area. When either participant approaches an interest point, it could trigger a set of actions depending on the type of interest point - for example, offering links to download information or join a Zoom room.\n\nThe hub\n\nBack to our proposal. We now have lots of Zoom rooms, information and chat channels, but how are we going to organise all of this? How is the attendee going to find their way around?\n\nAfter lots of user testing, we developed a website to help with this. A website that can be used by both physical and virtual participants to help them attend the talks they are interested in, find the information they need and meet up and socialise with others at the conference.\n\n\n   \n\n\nConclusion\n\nThis was an interesting project with lots of moving parts, and there were many different directions in which we could have gone. Hopefully we have met the brief while at the same time provided some food for thought for further explorations into hybrid events.\nSee you at the conference!\n",
        "url": "/applied-project/2021/05/11/stephedg-ritmo-conference.html"
      },
    
      {
        "title": "HEARING NOMONO: Our Journey into Audio Branding and Feedback Sounds",
        "author": "\n",
        "excerpt": "Audio branding and audio feedback are everywhere. This semester, we tried our hand at designing some for a young, Trondheim-based audio technology company.\n",
        "content": "Regardless of whether you’re reading this blog post on a phone, tablet, or computer, your device is capable of producing audio feedback sounds. And if you’ve watched any advertisement or media today, then you’ve undoubtedly heard a sound logo. Almost all human interaction with modern technology is predicated on responses to audible prompts and memorable branding, so the development of these sonic materials is an important part of any design process. We worked with NOMONO, a young Trondheim-based company, on the development of audio branding and audio feedback materials for their first hardware and software products.\n\nA Bit About NOMONO\n\nFounded in Trondheim in 2019, NOMONO’s mission is the development of innovative audio solutions for podcasters and storytellers in a variety of audio mediums. Currently the company is deep in the development stage for its initial hardware and software offerings (although if you’re reading this then it’s likely that the products have been released and our NDA has been lifted!). These products are designed for the recording and processing of ambisonic audio via a standalone ambisonic hardware platform (four lavalier microphones and a central ambisonic microphone all housed in a sturdy charging container) and an accompanying app and cloud storage platform for the management and processing of the audio created via the hardware platform.\n\n\n   \n   NOMONO's ambisonic recording hardware\n\n\nAt the time we entered the project, NOMONO was still settling on the final aesthetic character of their hardware and software, so our simultaneous development of its audio feedback aesthetics and the sound logo that would define NOMONO’s initial audio branding required ongoing discussion.\n\nOur Solutions &amp; Deliverables\n\nAlthough our initial plan was to design audio feedback sounds for mapping to specific user actions and processes, the NOMONO timeline grew delayed to the point where this wasn’t feasible. Thankfully, the discussions regarding NOMONO’s aesthetics and culture led us to a generalised approach that was feasible given the development limitations while allowing us to capture NOMONO’s aesthetic in our sonic materials. Our solution was to develop three sound libraries, each consisting of a set of 12 feedback sounds (four positive, four negative, and four processing). These libraries, earthBound, skySphere, and interStellar, depict the gradient between the Earth and outer space. They were inspired by the NOMONO team’s emphasis on merging dichotomies such as old and new, simple and complex, and intuitive yet innovative. From the documentation we provided to NOMONO:\n\n\n   \n   Our three sound libraries\n\n\n“The earthBound, skySphere, and interStellar libraries, whether deployed as complete units for selection by users or in smaller subsets as the NOMONO team finalises its vision, offer a diverse array of options for eventual mapping. The general functional families of Positive, Negative, and Processing ensure the inclusion of sounds that will map to any function or action.”\n\nSound Logo(s): Terrestrial -&gt; Celestial\n\n\n   \n   The motif at the heart of the NOMONO sound logo\n\n\nHaving developed the interpretation of NOMONO’s aesthetic/design/cultural character, we then designed iterations of the sound logo to match each of the sound libraries, giving NOMONO flexibility in future use of the materials. As can be seen above, the melodic motif that grounds each sound logo iteration is simply shaped and harmonically simple. Lacking harmonic information regarding its major or minor modality, the motif can be deployed in many different settings to achieve different effects.\n\nThe earthBound logo employs tactile, natural sounds and acoustic instruments (or midi depictions of acoustic instruments) alongside textural materials drawn from field recordings and Earth sounds that imply place within Norway and the Nordic region (ex. skis, trains, Icelandic volcanoes, etc.)\n\n\n  \n    \n    Alternate Text\n  \n  earthBound sound logo\n\n\nOur skySphere logo expands the musical materials vertically through arpeggiation, increased reverberation, and synthesised instruments alongside the still-present orchestral timbres. The field recordings begin to emphasise the atmospheric gradient and increased scale, including terrestrial sounds but increasingly focusing on the aerial and flowing. (ex. birds, whales, Arctic Ocean, rivers, etc.)\n\n\n  \n    \n    Alternate Text\n  \n  skySphere sound logo\n\n\ninterStellar leaves tangible natural sounds behind. The melody is set in an effects-laden texture and surrounded by a harmonisation that includes pitch materials of increasing dissonance. An underlying pedal tone twists downward against the architecture of the melody, adding an additional layer of unraveling physical space. Non-musical textures include airplane engines and the sound of black holes (acquired by NASA. We’re good, but we’re not that good!).\n\n\n  \n    \n    Alternate Text\n  \n  interStellar sound logo\n\n\nAudio Feedback Sounds\nOur development of the sound logos was guided by two main considerations: first, they should fit in the aesthetic of our three soundscapes and second, they should not be harsh and overwhelming. They should not create annoyance since these sounds will be heard numerous times by the user while operating the machine.\n\nearthBound:\nThis library accentuates the idea of earthiness by employing a tangible and tactile approach towards developing the sounds. The feedback sounds in this library are percussive at the core, are inspired by physical things in nature, and therefore are more natural sounding (even tending occasionally toward skeuomorphism). These sounds are mostly dry (meaning they don’t have any time-based audio effects like reverb, delay or echo) and they typically lie in the most sensitive hearing range of a human ear, which is from 500Hz - 4000Hz.\n\n  \n    \n    Alternate Text\n  \n  Six examples from the earthBound library\n\n\nskySphere:\nThis library is a bridge between earthBound and interStellar, with natural-sounding, tangible elements. The sonic elements are more spaced out, reverberant, and warm, evoking the atmosphere. The sounds are a mix of percussive elements with a short attack and pads with a longer attack and release. These sounds tend are wetter (effects such as reverb, echo, and delay).\n\n\n  \n    \n    Alternate Text\n  \n  Six examples from the skySphere library\n\n\ninterStellar:\nThe interStellar library was designed to bring the user/listener to an intangible realm where he/she subconsciously feels the futuristic and novel characteristics of the products and processes NOMONO’s new platform allows. This was achieved by crafting expressive ambient and ‘spacey’ materials using ROLI’s ‘Seaboard’ and its sound engine ‘Equator’. Compositionally, these sounds have an inner melodic structure somewhat reminiscent of the sound logo. A subset of the sounds in the processing category have a distinct pitch bending element, adding a new texture to the feedback sound palette to which the user has grown accustomed. These sounds also contain arpeggiated textures, and, at their cores, they contain enough character to serve as contextual cues to the listener.\n\n\n  \n    \n    Alternate Text\n  \n  Six examples from the interStellar library\n\n\nExplaining Ambisonics\nAmbisonics is a full-sphere surround sound format that enables users to experience an immersive audio sensation through various monitoring devices. Because Ambisonics is one of the most important features of NOMONO products and a key element of the company’s intended future direction, we used Ambisonics technology to produce the sound logo.\n\nFor the mixing session, we rendered the Mono, Stereo and 5.1 surround tracks into 64 tracks of 7th order Ambisonics via IEM Encoder, combined with 1st order ambisonics ambient tracks from Trondheim field recording.\n\n\n   \n   How it all works\n\n\nIn discussions with NOMONO, they emphasised that the ambisonics should not be about sound flying everywhere, but about making all sound elements more clear. That was also the principle behind our production. Instead of overly automating excessive panning, we used the features of Ambisonics to shape an immersive, highly characteristic sound space. We assigned the low frequency part of the ambient sound to the bottom of the Ambisonics sound field, while distributing the mid and high frequency sound effects higher in the spherical space. In contrast, all melodic instruments are arranged horizontally, or at a smaller elevation angle, at the front of the audience.\n\n\n   \n   Distributing audio in space\n\n\nSince most NOMONO users will connect their products via headphones, we rendered the sound logos to two versions: Ambisonics Binaural and 7th order Ambisonics. Ambisonics Binaural can be played through headphones directly, while the high order Ambisonics can be used in exhibitions, product launches, and other events where multiple speakers can be set up. Compared with the binaural version, 7th order ambisonics provide a more complete Ambisonics immersive experience. This setting also gives NOMONO more options when implementing these materials in the future.\n\nAmbisonics Field Recordings for Testing and Inclusion in Sound Logo(s)\n\nOne of the products which NOMONO is working on is a web-based software that performs signal processing, noise reduction, and sibilance detection algorithms in a dialogue-based setting on ambisonic recordings. To test out their software, the SMC team were tasked with the additional deliverable of ambisonic field/test recordings.\n\n\n   \n   Exploring Trondheim in the hunt for sounds\n\n\nAs you can see in the above images, we used an ambisonic microphone, the Zoom H3-VR, to record sounds from all over Trondheim to use in the sound logos. We recorded ski and river sounds in Bymarka. Sea water, seagulls and train sounds by the fjord. Church bells and conversations at Nidarosdomen. We also recorded conversations in a large tunnel with an enormous natural reverb as seen in the bottom right image. Abhishek and Lindsay recorded group conversations at Bumerang, an organisation that lends sports equipment, and Resykkell, a bicycle workshop. They also recorded multilingual conversations which included Hindi, Malwi Kannada, Odia, Bengali, and Persian, as well as differently accented English, among their friends. These sounds were delivered in raw format without processing to NOMONO, who will use them for testing and improving their noise-cancellation technologies.\n\nConclusion\n\nTo reiterate, we produced a whole lot of useful audio this semester. Our deliverables included three sound logo iterations (versions in both binaural and 7th order ambisonics), 36 audio feedback sounds (12 for each sound library), and ambisonic field recordings in at least five different languages and many different accents. Hopefully these materials will prove useful to NOMONO, and sometime soon we’ll be reading about NOMONO’s new products as the next big thing in audio.\n\n(We also want to extend a large “THANK YOU!” to Sigurd Saue, Brad Swanson, Viktor Rydal, and the rest of the NOMONO team for the great experience and all their help this semester. It has been a great experience.)\n",
        "url": "/applied-project/2021/05/11/williakm-hearing-nomono.html"
      },
    
      {
        "title": "Exploring Hardanger Fiddle Performance Patterns Through Interactive Computational Tools",
        "author": "\n",
        "excerpt": "This thesis presents the development and evaluation of two software applications that integrate contemporary research perspectives on the complex rhythmical structuring of Hardanger fiddle performances.\n",
        "content": "Problem Space &amp; Contribution\nAnalyzing musical performances is a challenging and emergent field of computational music research, aiming to reveal performance patterns and link them to musical contexts. The MIRAGE research project is contributing to this scientific body, currently addressing the modest amount of computational research on traditional Scandinavian folk music, utilizing a transdisciplinary approach that builds on contemporary musicological perspectives. However, collaborating across disciplines can introduce a number of challenges on its own. One way to mitigate challenges inherent in transdisciplinary research approaches is to develop tailored technologies that seek to increase the availability of expertise and knowledge across disciplines. Recent studies have also shown that most tailored technologies for music performance assessment are unreliable in educational settings, suggesting that further research is needed to evaluate the usability of computational tools that explore musical performances. Inspired by these contemporary conundrums, and my ability to acquire detailed Hardanger fiddle performance data from MIRAGE, my research question became:\n\n\n  How can we design interactive computational tools to explore performance patterns in Hardanger fiddle music?\n\n\nTherefore, the aim of the thesis was to increase the availability of computational music research in the field of musicology, offering small contributions to HCI (human-computer interaction), CMA (computational music analysis), Scandinavian folk music studies, and to the preservation and modernization of valuable cultural heritage.\n\nApproach\nThe thesis presents the development and evaluation of two software prototypes that integrate contemporary research perspectives on the complex rhythmical structuring of traditional Hardanger fiddle springar performances. The goal was that this procedure would pinpoint important design-related concepts and generate an evidence-based discussion that would ultimately address the research question.\n\nSystem Description\nThe toolkit I developed consisted of three modules, implemented in MaxMSP using the Bach library with additional JavaScript code, all designed based on detailed Hardanger fiddle performance data acquired from the MIRAGE. Although the tools harbored an array of interactive and analytical features, with several import and export options, each module had a primary functionality inspired by contemporary research on Hardanger fiddle performance patterns. The first module featured an interactive score representation that enabled users to manipulate the beat-level rhythmical structuring of a performance. Modules 2 and 3 were more analytical in nature, featuring custom-built plotting interfaces (created with [jsui] objects in Max). The second module enabled users to explore how timing patterns are related to motivic structuring while module 3 harbored the ability to investigate if regions with similar rhythmical properties share other musical parameters, such as pitch, velocity and metric position, incorporating a basic pattern-finding mechanism. To learn more about the core functionalities of these tools, see the video demonstration below:\n\n\n  \n    \n  This brief video demonstration goes through the very basic functionalities of the proposed toolkit.\n\n\n\nThe source code for the project is available on GitHub.\n\nEvaluation\nThe evaluation procedure featured a combination of methods. First, two operational tests were designed to assess the technical accuracy and general usability of modules 2 and 3, in particular, measuring whether the module results were consistent with contemporary performance studies and symbolic representations of the music (musical notation). Second, a questionnaire was administered directly following an online presentation, held on March 4th, 2021. The questionnaire featured fourteen questions designed to collect systematic feedback on more aesthetic and conceptual aspects of the prototypes.\n\nFindings &amp; Future Work\nWhen reflecting on the evaluation results in the context of the primary thesis objective, a number of key findings were identified. Perhaps the most essential finding was that there is a demand for more tailored technologies in the field of traditional Scandinavian folk music studies, and that the proposed toolkit can accommodate some of these demands. More specifically, the response showed a preference for tools capable of revealing the complex relationships between musical dimensions through interactive and visual means, much like module 3. However, module 3 was only partly successful in this regard as most of the feedback advocated for more advanced features, such as the ability to investigate how musical practices (such as bowing patterns or foot-stomping) are related to structural components (such as motifs) and other musical parameters. The results also revealed that it would be worthwhile to integrate existing computational analysis frameworks and retrieval systems into the toolkit implementation. This upgrade would improve the compatibility of the software and increase the overall prospects of the technologies becoming research instruments that can contribute to our contemporary understanding of Hardanger fiddle performance patterns.\n\n\n  \n    \n      \n    \n    \n      \n    \n    \n      \n    \n  \n   These images show the current multi-dimensional analysis capabilities of module 3. Displayed is the metric position (left), velocity (middle) and pitch (right) of two three-note regions in one Hardanger fiddle performance that share similar timing patterns.\n\n\nAdditional findings illuminated other design considerations for tools aiming to increase the availability of computational music research in the field of musicology, such as integrated features that actively facilitate nuanced interpretation processes. When linking performance parameters to various structural and symbolic layers, the number of variables in our analysis increases, rendering it more complex to make precise interpretations. Therefore, in order to tailor to an audience with varying degrees of musical or technical expertise, tools with the proposed levels of complexity should, ideally, include features that can guide the interpretation process.\n\nFuture Work\nOptimally, future development should see a software migration from Max to a web application. A web application is easier to use, more maintainable, has better support for interactive and UI design elements, and facilitates more complex functionalities. More importantly, a web solution would render the application to be more compatible with other software. Finally, having more carefully designed features in a web-based solution could also enable a better evaluation procedure with the ability to collect more varied data and reach more people in shorter periods of time.\n",
        "url": "/masters-thesis/2021/05/14/aleksati-hardanger-fiddle-performance-analysis.html"
      },
    
      {
        "title": "Reinforcement learning for use in cross-adaptive audio processing",
        "author": "\n",
        "excerpt": "This thesis is a study of reinforcement learning as a possible method for finding mappings in cross-adaptive audio effects\n",
        "content": "\n\nReinforcement learning for use in cross-adaptive audio processing\n\nThis blog post is a brief summary of my thesis work. The style is meant to be informal, and as such, I’ve left out references and other formalities. The full thesis is available here\n\nØyvind Brandtsegg was my supervisor.\n\nAbstract\n\n\n  This thesis is a study of reinforcement learning as a possible method for finding mappings in cross-adaptive audio effects. The context of a cross-adaptive performance session is framed as a reinforcement learning environment, in which the idea is to modify one audio signal by the features of another. The results show that reinforcement learning is a feasible way to generate such mappings. Still, there are affordances of using reinforcement learning for this purpose that are yet to be explored. The thesis also proposes a system architecture design that allows for real-time performance with the machine learning agent but is yet to be adequately implemented and tested. Future work could look into how to integrate the system in musical practice and to extend the cross-adaptive reinforcement learning environment to incorporate human feedback in the learning process.\n\n\nBackground\n\nThis thesis builds upon the research project titled “Cross-adaptive processing as musical intervention,” led by the Music Technology Group at the Department of Music at NTNU from 2016 to 2018. The research project studied musical performance sessions where cross-adaptive techniques for audio processing were utilized and evaluated. An adaptive audio effect is a combination of a sound transformation with a time-varying adaptive control. Cross-adaptive audio effects are a specialized form of adaptive audio effects where the effect parameters that operate on one signal are modulated by one or more audio features of an external audio signal. This creates new forms of musical interactions between the performers. An example of a cross-adaptive processing session that took place as part of the research project was when the vocalist Maja S. Ratkje was performing with the drummer Siv Kjenstad. The amplitude of the vocals reduced the reverb size of Kjenstad’s drums, and the amplitude of the drums reduced the delay feedback on Ratkje’s vocals. This example was recorded and can be listened to in the first video (from 08:00 minutes) on the webpage from one of the seminars that were held as part of the research project. As you can imagine, the resulting effect is that whenever either of the musicians stops performing, the other performers’ audio output takes up the whole sonic image on stage, either by a large reverb or long feedback.\n\nOne challenge in designing cross-adaptive processing scenarios is finding out how audio features should be mapped to effect parameters, since there are so many possibilities. Øyvind Brandtsegg (my supervisor) addressed this issue as part of the research project, and proposed that machine learning potentially could be a way of creating mappings at a higher level than manually connecting features to effect parameters. This challenge is what motivated my thesis project, i.e., how we can find interesting mappings in cross-adaptive audio effects without having to manually search through them all. In parallel, I had been interested in reinforcement learning, and how it could be applied in music technology. Reinforcement learning is an area of machine learning that deals with how intelligent agents can learn from interacting with an environment. The reinforcement learning loop consists of an agent that observes an environment, takes actions, and gets a reward based on how good or bad the action taken was. The reward is distributed from a reward function, which is a function of the state of the environment. This reward function is defined by the creator of the environment. The figure below illustrates the reinforcement learning cycle.\n\n\n  \n  The reinforcement learning cycle\n\n\nAfter some early discussions with my supervisor about what the objective for my thesis should be, we ended up with trying to use reinforcement learning to find interesting mappings in cross-adaptive audio effects. In essence, this meant that I had to develop a software that let a reinforcement learning agent learn mappings from the features of one audio signal to the effect parameters of another audio signal. Because the goal of cross-adaptive processing is to use it in a live music context, a secondary goal was to make the system useable in real-time scenarios with audio streams.\n\nMethods and implementation\n\nThe central part of the work in this thesis was to develop the reinforcement learning environment that was needed to bridge reinforcement learning and cross-adaptive processing. The reinforcement learning part was implemented in Python, and the digital signal processing was done in Csound. Some of the Csound code was reused from previous projects in the cross-adaptive research project, such as Øyvind Brandtsegg’s featexmod. The figure below illustrates the system architecture for training and live performance with the model, where the main difference is how the machine learning module communicates with the Csound rendering processes. The source code is available in a GitHub repository for anyone who wants to have a look.\n\n\n  \n  System architecture\n\n\nTo be able to measure degrees of success in the project, a quantifiable objective was set: to process a source audio sample in such a way that it has the greatest possible similarity with the target audio sample. A sample of white noise was used as source, and the famous “Amen break” was used as target. The effect that was used to produce the results was a low-pass filter with additional distortion control. Even though that effect is quite versatile as a sound-shaping tool, it has obvious constraints as to how much it is capable of modifying a given sound, especially in terms of harmonic components.\n\nThe evaluation of the project consisted of evaluating results that the prototype produced according to several conditions, such as which features were used in the analysis during training, or the degree of randomness in the exploration of possible actions.\n\nResults\n\nMore background material would have to be provided to describe the various results produced in the thesis properly. The interested reader can look into the complete thesis linked to at the beginning of this post. However, some of the more notable results were how feature selection impacted the training of the machine learning model, and how the model generalized to new sounds.\n\nSource and target audio samples\n\nThe following samples were used as source and target.\n\nSource: noise.wav\n\n\n\n\n\nTarget: amen_drum_break.wav\n\n\n\n\n\nFeature selection\n\nFeatures: [RMS]\n\nOne model was trained only on the RMS values of the source and target sounds. The result sounds like this:\n\n\n\n\n\nFeatures: [RMS, pitch, spectral centroid, spectral spread, spectral flatness, spectral flux]\n\nAnother model was trained with additional features. The resulting model did a lot better at representing the frequency content in the target audio sample than the prior model did.\n\n\n\n\n\nGeneralizability to new sounds\n\nAn unheard sample was used as source (instead of white noise) to test how a model trained on shaping white noise into the Amen break generalizes to new sounds.\n\nThe new source\n\n\n\nThe result\n\n\n\nReal-time\n\nAs mentioned in the abstract, designing a system that would work in live settings was a secondary goal of this project. For “live mode”, the two audio streams were rendered in separate processes, communicating with the machine learning module over OSC. The overall system was built in a way that enabled sending and receiving audio features and effect parameter mappings over OSC, but I didn’t find enough time to implement a proper way of synchronizing the streams. Thus, there is work that remains in synchronizing the OSC streams to be able to utilize the software in live performances.\n\nThe current implementation of live mode suffers from packets arriving at different times at the machine learning module, thus creating a sort of “jitter” in the parameter updates. The resulting output can be heard in the following audio sample (NB! this sample is very distorted, so it is advisable to reduce the volume before pressing play):\n\n\n\n\n\nConclusion and future work\n\nThis thesis investigated how a simplified version of a cross-adaptive performance scenario can be modeled as a reinforcement learning environment to create mappings in cross-adaptive audio effects. The contributions of this thesis are the research and development of an open-source system capable of creating mappings in cross-adaptive audio effects with techniques from reinforcement learning. Even though the implemented system should be considered a prototype, this thesis demonstrates that reinforcement learning is a viable technique for creating mappings in cross-adaptive audio effects. The crude implementation of asynchronous real-time inference shows that it should be feasible to use the system in real-time over OSC.\n\nThere is a lot left to be done in figuring out how the cross-adaptive environment best is brought into a performance and production context. This work would require systematic user testing. To further explore the affordances of reinforcement learning, the cross-adaptive reinforcement learning environment could be extended to incorporate human feedback in the learning process. Future work should also look into the generalizability of the results by combining other effects and feature extractors.\n\n\n\n\n\n\n\n\n\n\n\n",
        "url": "/masters-thesis/2021/05/15/ulrikah-rave.html"
      },
    
      {
        "title": "The Portable Portal: An Ecological Approach to Technology-Enhanced Learning in Bangladesh",
        "author": "\n",
        "excerpt": "Working toward a cogent ecological framework for technological-aid development, or: We are our technologies, they are Us\n",
        "content": "\n\n\n\n\n\n\nThe Portable Portal\n\nThe objective of this study was to research and design a distance-teaching system to be used by the girls and young women of Speak Up For The Poor’s Girl’s Education Program (GEP). Speak Up is a nonprofit organization working in Bangladesh, with the primary area of its focus being on the rural Dalit community. As girls in these communities are very vulnerable to child marriage, sex trafficking, and other exploitation and violence, keeping them in school is an essential goal of the GEP as this is the best way to ensure they have a chance of reaching their full potential. Many of the girls in this program have gone on to higher education and professional training, returning to their communities as advocates for and inspiration to future generations of girls. The video below gives a brief introduction to the Portable Portal and Speak Up:\n\n\n  \n    \n  Introduction to the Portable Portal\n\n\n\n\n\nBackground\n\nEarly in 2020 I re-connected with Troy Anderson, an old high-school friend of mine and the founder of Speak Up for the Poor. I was curious to know if there was something I could do for Speak Up that was related to SMC and my not-yet-determined master’s thesis. We talked about a variety of approaches but what really stuck was Troy’s wish for some way to help his students, 1400 girls in 30 different villages, with their English homework. Passing the national English exam is essential to being allowed to progress to higher education, but there is a lack of good English tutors in Bangladesh, particularly in rural areas. Troy wanted to be able to conduct a weekly lecture, based on the current chapter in the national English textbook, from his office in Khulna and broadcast it out to Speak Up’s “learning centers” in all 30 villages.\n\nSome type of videoconferencing solution is perhaps the obvious answer to this problem, but there are many factors inhibiting the success of information and communication technologies (ICT) in rural Bangladesh. Thus, the idea of the Portable Portal (PP) was born. Initial parameters we discussed for the device were as follows:\n\n\n  Rugged, durable, and portable construction to be used in rural villages\n  Flexible power options to deal with power grid unreliability\n  Ability to operate satisfactorily on a 3g/4g cellular network\n  Simple to operate for those with low technological competence\n\n\nAs I read further into the literature surrounding technological-aid interventions in the global South, it became obvious that these were not the only barriers to ICT success that I would encounter when designing this intervention. The world is littered with aid projects that have failed their recipients for one reason or another, and I felt I needed to know why. Thus, this thesis became a kind of scoping study, to explore these barriers and try to design the system to mitigate their impacts.\n\n\n\nRelevance to SMC\n\nIn SMC we focus on video-communication technology primarily as it relates to music. However, the applications go well beyond music, as we are all quite aware of from the last year and a half that we’ve spend on Zoom. While the focus of this thesis is not explicitly on telematic music, networked musical collaboration, or music education, the Portable Portal could certainly be used for these ends, particularly in rural and remote areas that suffer from poor power and internet infrastructure. The reason that it is not the focus of this thesis is simply that online musical collaboration is not a major area of concern for Speak Up For The Poor, whereas passing national English language exams is. It’s felt that the importance of this mission overshadows the potential musical uses of the PP, though any musical applications are, of course, a welcome side-effect.\n\n\n\nDesign Features\n\nThe initial parameters (listed above) that were discussed for the PP continued to be important in the development. As more was learned about the ecology of its proposed setting, other parameters were added:\n\nEco-design: The problem of e-waste in Bangladesh is growing rapidly, as it is in the world in general. Much of the world’s cast-off electronics come to rest in poor countries, where lax regulations and cheap labor have caused informal recycling and disposal industries to proliferate. This has extremely deleterious effects on the environment and health. The PP has been designed to be as durable, repairable, re-purposeable, and recyclable as possible, with an accessible, simple, and open-source design. Figure 1 illustrates points in the life-cycle of an ICT that can be influenced through careful design.\n\n\n\nFigure 1: Life Cycle\n\n\n\n\nLocal construction: By thoroughly documenting the assembly of the PP with diagrams, parts lists, and assembly videos, it is intended that the device be built in Bangladesh by local technicians. This aids both the local economy and cuts down on environmental impacts from shipping.\n\nSafety: Because of its setting, it was important to consider the safety of its users from impacts such as revenge porn, which had already affected several young women in the Speak Up GEP. In some conservative cultures, this can be nothing more than the sharing of a photo of a woman without her head scarf. An act so seemingly harmless to many observers can be quite traumatic for the women targeted, affecting their self-image, social standing, and potentially their physical safety as well. Designing the PP to access only the system website helps to prevent this, but of course only side-steps the issue, passing it on to other ICTs such as smartphones, which these young women will no doubt also encounter in their lives.\n\n\n\nSystem Description\n\nThe Portable Portal System consists of both a bespoke website containing embedded video conferencing tools, archived lecture videos, a calendar for scheduling sessions, and other necessary information, and a hardware device (the Portable Portal, or “PP”) purpose-built for accessing this website. Together, these elements constitute a system for distance teaching that is accessible to non-technically adept users.\n\nWebsite\n\nThe website contains three main pages, as follows:\n\n\n  Landing page: This page displays the calendar of all upcoming lectures. In addition, a countdown timer displays the time remaining until the next lecture begins. When logging on with the PP, this page will display first, unless a lecture is imminent. If lecture is imminent (within 60 seconds), PP automatically navigates to the Videoconference page.\n\n  Videoconference page: This page displays the embedded videoconferencing portal.\n\n  Lecture Archive page: This page contains all past video lectures that have been archived. Lectures are recorded automatically. The instructor can review and edit (if necessary) the lecture after its conclusion, and then fill out a form to categorize the lecture by date, subject, and corresponding textbook chapter before posting it to the web page.\n\n\nStudent users logging on to the website with a PP device will be automatically logged on with access to the basic functions described on the Student workflow diagram (see figure 2).\n\n\n\nFigure 2: Student Workflow\n\n\nInstructors logging on with instructor credentials allow additional functions, mainly the ability to initiate sessions and edit/post video via the instructor dashboard, as shown on the Instructor workflow diagram (see figure 3).\n\n\n\n\n\nFigure 3: Instructor Workflow\n\n\n\n\nPhysical Device\n\nThe PP consists of a heavy-duty road case containing a monitor screen, webcam, microphones, cellular modem, audio amplifier and speakers, a battery-backup UPS (uninterruptible power supply), and a micro-computer.\n\n\n\nFigure 4: Front Panel\n\n\nFigure 4 shows the front panel of the PP with the cover removed, revealing the monitor screen (c), webcam (a), mics (b), speakers (f), a volume knob (e) for the audio system, the “raise hand” button (g), and a 5-button keypad (d) for navigating the website. Controls are kept very simple by design, making the PP friendly to users with low technological expertise.\n\n\n\nFigure 5: Rear\n\n\nBehind the rear cover of the device (figure 5) is revealed the options for powering the device (i); a standard wall plug for when grid power is available, and a pair of screw terminals for attaching to a 12 volt DC source such as a standard car battery for use when grid power is unavailable or unreliable. The device will prioritize grid power if connected to both. A green led lights to indicate that the unit is properly connected, while red indicates a problem such as insufficient battery power or a reversed polarity connection (not visible in figure 5). Powering the device on is a simple matter of plugging it in or attaching to a battery, switching the unit on, and affirming that the led has lit green. To power down, the user has to simply turn off and unplug the unit. An internal UPS assures that the micro-computer shuts down properly.\n\nAll other components of the system are protected by a shroud that attaches via screws to the inside of the case. The shroud can be easily removed for service.\n\n\n\nResults of study\n\nThere are many factors that impact and are impacted by the use of ICT in Bangladesh, and more specifically within the Speak Up GEP. These range from quantifiable impacts such as infrastructure, environment, and economy to less-quantifiable impacts such as cultural values and relational norms.\n\nThe main take-away from this study is that our technologies are highly context-dependent. What works in the North does not necessarily work in the South. Because of this, we have to approach technological development as an act of co-creation, involving our users in the process. Furthermore, we are obligated to monitor our creations with a willingness to redesign or even retire them if their negative impacts overwhelm the positive. In this way we ensure that the recipients of our aid remain front-and-center, and that our interventions actually work to provide value in their lives.\n\n\n\n\n  Anatomy of the Portable Portal (older version)\n\n",
        "url": "/masters-thesis/2021/05/15/paulgk-portable-portal.html"
      },
    
      {
        "title": "Toward a Telepresence of Sound: Video Conferencing in Spatial Audio",
        "author": "\n",
        "excerpt": "Teleconferencing in spatial audio with the help of Jitsi Meet and Web Audio\n",
        "content": "Meet in Space\nVideo Conferencing in Spatial Audio\n\nAbstract\n\nDigital communications technologies have developed at an increasingly rapid pace, with the COVID-19 pandemic accelerating its recent adoption. This shift over the last few decades has seen a mass migration online, where utilities like video conferencing software have become essential to entire industries and institutions. This thesis proposes the integration of binaural spatialized audio within a web-based video conferencing platform for distributed conversations. The proposed system builds upon findings on the benefits of spatial audio in video conferencing platforms and is guided by the tenets of telepresence. The developed implementation is based on Jitsi Meet, a robust open-source conferencing system. It localizes participant’s voices through sound spatialization methods provided by the Web Audio API.\n\nThis project treads new ground in exploring how localized audio can be conceptualized within an accessible telecommunications platform, proposing a novel integration of HRTF-based binaural spatialization within a standard video conferencing layout. System design and experimental questions used in a technical evaluation and user study are informed from a review of audio and video conference systems found in the literature and commercial market. The system evaluation suggests its viability from a compatibility and performance perspective. Perceptual metrics of cognitive load, social presence, and intelligibility are further investigated by a user study where four remote subjects were asked to engage in a short group discussion on a live deployment of the system. Results find support for improvements across all defined metrics as well as increased opinion scores regarding the preference of conferencing with a spatial audio system.\n\nTeleconferencing is Here to Stay\n\nBut is often a tiring affair…\n\n  “Zoom” fatigue\n  Reduced dimensionality\n  Poor and inconsistent quality\n\n\nTeleconferencing has become apart of many of our daily rituals, either a result of the COVID epidemic or the rapid digitization of communication. However, most of us have experienced some symptom of fatigue as a result of our extended use of the system. Latency, network reliability, visual and audio fidelity can all contribute to a fatiguing experience, but software can also play a major role.\n\nIssues in Teleconferencing\n\nMy take:\n\n  Don’t reinvent the wheel - instead focus on one critical component\n  How can the treatment of audio bring us to the goal of telepresence, and closer to realistic conversations\n\n\nThere is a striking lack of realism in our video meetings, especially in how our voices are conveyed through digital exchange. This project is guided by the principles of telepresence, or the goal of conveying our sensory interactions in high fidelity, as we would in a face-to-face interaction. Every acoustic interaction we experience is spatial so aren’t our digital interactions?\n\nReplicating Face-to-face Interactions\n\n\n  Spatial model, spatial audio\n    \n      Visual-aural coherency\n      Binaural audio\n    \n  \n  Benefits from the literature\n    \n      Lateralizing audio can improve intelligibility\n      Disentangle double-talk\n      And more: reduce cognitive load, improve comprehension, and is generally more favorable\n    \n  \n\n\nThere are many issues that ought to be addressed in the field from a software and user experience perspective. Given that audio appears the most critical medium in task oriented communication, I chose to work on integrating spatial audio within a teleconferencing system. Spatial audio within telecommunication has a number of cool benefits and has not yet been integrated within a standard video conferencing platform.\n\nJitsi Meet and Web Audio\n\nJitsi Meet is one of the most popular FOSS video conferencing applications that requires no sign up or installation. It works cross browser and has a great support community.\n\nTo achieve spatial audio, the only current possibility for high-fidelity reproduction of sound is through binaural production of sound via headphones. Fortunately, WebAudio, a standardized method supported on all browsers, is able to faciliate full HRTF based binaural audio.\n\nImplementation\n\n\n  Capturing participants’ audio streams (WebRTC)\n  Head-related transfer functions via PannerNode\n  Dynamic processing of participants\n\n\nWith this, I integrated dynamic, toggle-able, spatial audio for each participant audio stream in a meeting. The voices of the participants appear coherent with the video streams as participants appear and disappear.\n\n\n\n\nDemo of Meet in Space\n\nValidating with a User Study\n\nAfter a technical evaluation of the system suggested its ability to scale at least to 5 concurrent participants, I ran a user study consisting of students from SMC as well as other’s who were experienced with conferencing on a daily basis. The experiment consisted of a brief conversation among 4 users, with and without spatial audio. The results, while preliminary, were promising and appear to support previous findings in the literature.\n\nThere was support for four hypothesis of perceived metrics:\n\n  Decreased cognitive effort\n  Increased social presence\n  Increased vocal intelligibility\n  Increase in opinion score\n\n\nAnd the future?\n\nSpatial audio is being adopted at a rapid pace, but has still yet to be introduced in small group interactions like the ones presented here. Hopefully, this thesis can provide a valid proof of concept for the benefits of spatial audio in video conferencing platforms.\n",
        "url": "/masters-thesis/2021/05/18/jacksong-meet-in-space.html"
      },
    
      {
        "title": "Theory Controller: A Silent IMS",
        "author": "\n",
        "excerpt": "Interactive system to control music theory\n",
        "content": "Abstract\n\nThis paper explores music theory real time controllability through the design of an interactive musical interface. More precisely, the instrument described here lets the user select any standard heptatonic scale - including all twelve keys and seven modes of the four main scale classes, diatonic, acoustic, harmonic minor and harmonic major - using a joystick-like gesture input device. The program presents a complex visual interface, and multiple use cases are explored. The system as it is does not produce any sound, it is thus not playable in performance settings on its own. It may however be used in conjunction with other compatible custom apps, such as an arpeggiator or a dynamic MIDI scale mapper, as well as virtual instruments.\n\nThe Theory Controller opens a discussion about chord progression awareness as well as inter-plugin scale data synchronisation. The typical user is portrayed, and possible enhancements for the system are envisioned. The latter is also compared with other existing music theory enabled interfaces.\n\nDesign\n\nThe interface offers one-to-one control of three main parameters: scale class, key and degree. The scale class parameter has four states (diatonic, acoustic, harmonic minor and hamronic major), the kay may be one of the twelve pitch classes, and the degree represents one of the seven heptatonic modes of the selected scale. That gives us a total of 336 scales.\n\nIn additions, these scales may be accessed using other navigation means: chord type, mode in type, semitone and minor third. To control these parameters, either a SpaceMouse or mouse and keyboards may be used. You can have an overview of the mapping in the following video. You may also test the system itself in the web app down below. Use the mouse to change the three main parameters on the bottom right corner and keyboard arrows to navigate using types.\n\n\n    \n    \n    Mapping\n\n\nThe visual interface consists of a third of the circle of fifths at the bottom, a smaller circle of modes on top of it, containing small circles for the accessible scales. Moreover, you can see mouse navigation buttons on the bottom right corner, the six chord types on top and finally a list of the available modes in type on the right.\n\n\n\n\nApplications\n\nThe instrument may be used in different manners. In order to make music, it has to be used in conjunction with a compatible app. For example, you can watch below a demonstration of the Theory Controller used with a dynamic MIDI scale mapper. The selected scale is communicated using OSC, and a Pure Data patch makes the drum pad trigger a MIDI tetrad chord according to the selected mode. That way, both musicians are always harmonically in synch, while only one has control of it using the Theory Controller. All generated MIDI data are routed to virtual instruments.\n\n\n    \n    \n    Demonstration\n\n",
        "url": "/masters-thesis/2021/06/03/theory-controller.html"
      },
    
      {
        "title": "Designing Gesture-based Interactive Museum Exhibit",
        "author": "\n",
        "excerpt": "A short summary of my Masters Thesis on Gesture-based interaction for museum exhibits.\n",
        "content": "\n\nAbstract\n\n\n  This thesis looks at research literature and identified principles useful for gesture-based interaction Museum Exhibit design.\nIt also reports on the design of a prototype exhibit. It showcases the use of dynamic gestures and spatial positioning as a means of interactively controlling the exhibit, made possible using the Azure Kinect within the Unity game engine.\n\n\nBackground\n\nMuseum exhibit design has seen a paradigm shift towards more technological approaches. They have been forced to renew themselves to approach a wider and younger audience and find themselves competing with other cultural institutions such as sporting events, cinemas etc.\nHaving worked at Rockheim, the national museum of popular music (Trondheim, Norway), I wanted to investigate this aspect and how gestures can be utilized to control exhibits. Hands-off designs have been especially relevant since the Covid-19 outbreak.\nGestures used for interactivity are defined as either discrete or continuous.\nA discrete gesture will elicit an interaction on completion. E.g., A swipe. A Continuous gesture provides continuous data that iteratively affect the output. E.g., Moving an object.\nDirect manipulation is one of the most common methods of interaction in VR. Another form of gestural interaction used is pointing with the use of raycasting. Some other ways make use of intermediary tools such as instrumentals (Holding different objects change the interaction), marker menus, widgets, or magic lenses.\n\n\n  \n  A selection of various interaction methods\n\n\nDiscoverability of gestures is a key challenge in interactive gesture design.\nProviding explicit instructions are unfavorable as it can be disruptive of factors such as goal making, sense of achievement, curiosity, and sense of autonomy, which are found to be important motivating factors. It is found preferrable to find gestures that are easily guessable, and only provide essential feedback and guidance for successful interaction.\nGesture Elicitation studies is the primary methodology adopted for identifying gestures that are suitable for a specific application. These studies ask users to guess the gesture they find most suitable for a referent - the outcome of the interaction. These studies have found commonalities in what gestures are performed and how they perform them.\nMany studies have pointed towards a case of media convergence, where gestures used for mobile touch-based interaction are adopted in larger variants, such as swiping, zooming etc.\nMany participants in such studies have also underlined that they would like to vary in how gestures are performed. Studies have also found the size of gestures to be dependent on the interactive object, often borrowing from how we normally physically manipulate objects. The size on how they perform gestures are also found to be dependent on the size of the display.\nDue to a high level of idiosyncrasy in how gestures are performed, it is a challenging task to for designers and algorithms to find effective ways of recognizing all the different ways gestures can be performed.\n\nOne of the main longstanding goals has been to achieve Natural Interaction, an aim to let interaction happen using gestures and movement people use in everyday life through gesticulation and physical manipulation. Making use of a gesture set and interactivity that builds upon everyday situations, is beneficial in being intuitive and allow for immediate interaction without introduction or guidance.\nA second goal is making Natural User Interfaces, based upon the same principles of intuitiveness. Secondly, it aims at minimizing, hiding, or finding alternative ways to traditional UI elements such as menus and windows. Virtual Environments are one of the approaches that allow for designing a NUI.\nUsing avatars that mirror user movements is found to have effect on social acceptance of using movement, which some might find to be socially awkward. It is also effective in prompting user movement and serves as an effective way of giving users continuous feedback.\n\nDesign\n\nThe thesis included the design of a prototype exhibit made with Unity using the Azure Kinect DK sensor array to retrieve gesture information. By making use of the Body Tracking API and the “Azure Kinect Examples” asset provided for Unity\nThe Azure Kinect Examples asset for Unity provides a way of retrieving data from the Body Tracking API directly into unity. The data provided is a skeletal joint array, from which we gestures are recognized by a hard-coded approach of relative positioning to determine states of Start positions and end positions of a discrete gesture.\nWhen a user or multiple users enter the sensors field of view, an avatar is created onto the scene. For each person that enters the scene the avatar will have a different color.\nBy raising your hand you will gain control of the exhibit.\nSwiping gestures allows for browsing various artists featured, and a pushing gesture will play a music video of the selected artist. The prototype also makes use of proxemic interaction where users can walk onto platforms in the virtual environment to change the decade on display.\n\n\n    \n    Demo of the exhibit\n\n\nFuture work\n\nPossible future work would look into how continuous gestures could be utilized in the exhibit, and how subtle guidance could be provided for a rapid successful interaction.\n\n\n",
        "url": "/masters-thesis/2021/06/20/simonrs-gestures.html"
      },
    
      {
        "title": "ImSoTra",
        "author": "\n",
        "excerpt": "Footfall induced noise in buildings is traditionally assessed with Impact Sound Transmission (IST) measurements following diffuse field model of the receiving room which is not valid below Schröder frequency neither it facilitates auralization. This master thesis aims to create a method to estimate low frequency (LF) sound pressure in the receiving room below Schröder frequency based on modal sum theory in room acoustics followed by measurement of IST, impulse response of the receiving room and acceleration of the main floor at two fixed position in two vertically adjacent laboratories.\n",
        "content": "\n    \n    Figure 1: IST from footfal in buildings\n\n\nThis blog post contains an overview of my master thesis. For more detail explanation, please check my final report. Digital files associated with the study including matlab scripts, original recording of sound pressure in the receiving room and the corresponding auralized sound pressure for the three sound sources can be found here. A video demonstrating measurement process is also included at the end of the blog post.\n\nAbstract\n\nFootfall induced noise in buildings is traditionally assessed with Impact Sound Transmission (IST) measurements following diffuse field model of the receiving room which is not valid below Schröder frequency neither it facilitates auralization. This master thesis aims to create a method to estimate low frequency (LF) sound pressure in the receiving room below Schröder frequency based on modal sum theory in room acoustics followed by measurement of IST, impulse response of the receiving room and acceleration of the main floor at two fixed position in two vertically adjacent laboratories. The study also aims to create monoaural auralization of the sound pressure with a hypothesis that the impulse response of the combined system of floor and receiving room can be used to synthesize the acoustic sound pressure at listening positions in the receiving room.\n\nThe method to create the LF-sound pressure in the receiving room was found to be working to some degree with the main floor showing comparatively higher modal density. More accurate results can be achieved by measuring acceleration of the floor at multiple points. Similarly, the auralization method worked between 20-200Hz with some errors and more accurate results can be accomplished by measuring force signals at higher sampling rate and a more precise calculation of impulse response of the coupled system.\n\nBackground &amp; Introduction\n\nSound can be a pleasing music or a disturbing noise which can also be hazardous to human health. Sound and noise are often used interchangeably. Although the average human hearing ability is very weak towards low frequencies, studies have found that Low Frequency (LF) noise can cause serious health effects even at low sound pressure levels. Sounds produced from engines, fans or traffic are few examples of Low Frequency (LF) sounds that are in abundance in urban environment. LF-noise are also common in apartments and multi-unit dwellings which are produced through Impact Sound Transmission (IST) from human footfall. Residents in apartments and multi-unit dwellings often complain such LF-noise from neighbouring apartments.\n\nThe measurement of IST in buildings is traditionally done while following the diffuse-field model of the receiving room. Sound field in rooms at low frequencies do not fall under the domain of diffuse-field. Therefore the sound pressure levels estimated by such models may not be accurate at very low frequencies. Moreover, the traditional approach do not facilitate auralization that allows to perceive/analyse sounds through listening.\n\nThe title of my master thesis is Study of Low Frequency Impact Sound Transmission For Auralization. The focus is on the study of Low Frequency (LF) noise induced by footsteps in buildings based on measurements of IST and acceleration of the floor followed by auralization of the measured sound pressure in the receiving room. The study estimates the LF-sound pressure/ sound pressure spectrum in the receiving room based on the modal sum theory in room acoustics. A hypothesis that the impulse response of the combined system of floor and receiving room can be used to synthesize the acoustic sound pressure at listening positions in the receiving room, is cosidered for the auralization.\n\nMethod and Implementation\n\n\n\nFigure 2: Measurement and Analysis Scheme\n\n\nA sketch of the measurement and analysis method opted in this study is presented in figure 2. The measurements of IST were followed by analytical estimation of the sound pressure level/sound pressure spectrum as well as auralization of the transmitted sound in the receiving room. Theoretical sound pressure and resonance frequency of the receiving room were estimated by applying the modal sum theory. The room resonance frequencies were validated by measuring the Impulse Response (IR) of the receiving room. Finally, subjective analysis of the auralization was performed by presenting author’s reflection on the percieived sounds through headphones while comparing the measured (original) and the corresponding auralized sounds.\n\nMeasurement of IST &amp; IR\n\nFor specification/ list of the equipment used in this study, please refer table 4.1 in the final report.\n\n\n  \n  Figure 3: Laboratories used for the IST measurment\n\n\nThe measurements of IST were conducted at two vertically adjacent laboratories at the Gløshaugen campus, NTNU in Trondheim namey Lydrom 2 and Lydrom 3 on upper and lower level respectively (figure 3). Three sound sources were selected which are; 1) medisin ball (a type of rubber ball), 2) Footstep with shoes (soft sole sport shoes) and 3)Footstep without shoes. For auralization, the ground force reaction of each of the sound sources were measured with a vernier force plate at a sampling rate of 1000 Hz as shown in figure 4.\n\n\n  \n  Figure 4: Ground force reaction measurement\n\n\nSimilalry, five excitation points and two accelerometer positions (figure 5a) were marked on the floor in the sending room (figure 5b). Each sound source was used to excite the floor at each excitation point. The acceleration of the floor was measured at the two fixed points while the induced sound pressure was measured at a corner position in the receiving room (figure 5c).\n\n\n  \n  Figure 5: Floor plan and positions of excitation points and accelerometers\n\n\nThe IST measurements were taken in three sessions, one for each of the three sound sources. Each session included ten sets of measurements of both sound pressure in the receiving room and acceleration of the floor in the sending room at each of the five excitation points for the corresponding sound source. One such set of measurement was taken by exciting the floor at an excitation point followed by a silence of 12-15 seconds. In the case of medisin ball, excitation was made by dropping the ball manually (figure 6a), on each point and catching it on the rebound. All three sessions (except measurement of acceleration) were repeated on a small section of floating floor that consisted of two layers of rockwool (20mm thick) and two layers of Expanded polystyrene (EPS) (one with 20mm and another with 25mm thickness).\n\n\n  \n  Figure 6: Measuring IST and IR\n\n\nThe IR was measured by placing a microphone at 5 different source positions on the ceiling and a homemade looudspeaker at a corner in the receiving room (figure 6b). The source and the listening positions were replicated from the corresponding positions in the IST measurements. The IR of the\nroom was measured/calculated by following the method of DFT-based deconvolution using a log-sweep as an excitation signal while using EASERA software.\n\nAuralization\n\n\n  \n  Figure 7: Creatiing Auralization signals\n\n\nThe auralized signals for each of the three sound sources were calculated by convolving the impulse response of the coupled system of floor and the receiving room with the measured ground force reaction of the corresponding sound source (figure 7). The impulse response of the coupled system was determined by convolving the measured sound pressure of the medisin ball with an equilization filter. For simplicity, a model of the force signal of medisin ball was created and the equilization filter was calculated based on the model of the force signal.\n\nResults &amp; Conclusion\n\nDynamic response, Sound pressure level &amp; Sound Pressure Spectrum\n\nThe simplified approach of studying the dynamic response of the main floor by measuring the acceleration of the floor at two fixed positions for all excitations did not seem to be enough. The method did not reveal many modes of vibration. It did not provide enough information on amplitude of vibration which did not allow to perform an accurate estimation of sound pressure level in the receiving room. Therefore measurements of acceleration of floor at many points are required to get more accurate estimation of the sound pressure level by using the modal sum method. On the other hand, the estimated sound pressure spectrum was very close to the measured values.\n\nAuralization\nFor the three sound sources, 6 audiofiles are presented below that contain the measured sound pressure and the corresponding auralized sound pressure in the receiving room. The sounds play in an ascending order from excitation point P1 through P5. All the soundfiles can be downloaded from here, if for any reasons the sounds are not playable below.\n\n\n    \n      \n      Audio 1: Measured sound pressure for excitation with medisin ball\n    \n    Audio 1: Measured sound pressure for excitation with medisin ball\n  \n\n\n  \n      \n   Your browser does not support the audio element.\n  \n  Audio 2: Auralized sound pressure for excitation with medisin ball\n  \n\n\n  \n      \n   Your browser does not support the audio element.\n  \n  Audio 3: Measured sound pressure for excitation with footstep with shoes\n  \n\n\n  \n      \n   Your browser does not support the audio element.\n  \n  Audio 4: Auralized sound pressure for excitation with footstep with shoes\n  \n\n\n  \n      \n   Your browser does not support the audio element.\n  \n  Audio 5: Measured sound pressure for excitation with footstep without shoes\n  \n\n\n  \n      \n   Your browser does not support the audio element.\n  \n  Audio 6: Auralized sound pressure for excitation with footstep without shoes\n  \n\n\n\n  Figure 8: Sound Pressure Spectrum at excitation point P1: Auralised Vs Original\n\n\nThe formulated hypothesis to create auralization signals was found to be affirmative based on the precision of the auralization of medisin ball excitation from 20-200Hz in 1/3 octave band. The auralization for the other two sound sources\nwere found to be relatively close to the corresponding original sounds. Besides, the subjective analysis of the aurlalized signals revealed that all of the auralized signals were more boomier, more louder and contained more ringing sounds compared to their corresponding original signals. The auralized signals introduced broad band noise different from the original signals. Force signals and estimation of impulse response of the coupled system have been identified as two major influencing factor for such inaccuracies. Quality recordings of the force signals with high sampling rate, sound pressure signals with high signal to noise ratio and precise inverser filter can contribute in calculating a more accurate impulse response of the coupled system to get more accurate auralization results while applying the current method.\n\nFuture works\n\nOne of the future works could be to study the vibration pattern of the floor more in\ndetail and do a structural modal analysis to understand causes of the resonance\nfrequencies and the vibration patterns of the main floor and floating floor.\n\nSimilarly, acceleration of the floor can be measured at many points on the floor so that\nmaximum vibrating modes of the floor could be captured in the measurement. The impact sound transmission can be measured at several listening positions in the receiving room. The amplitude of the sounds generated in a room are affected by the\nroom modes and this could be compared/studied well if measurements are taken at several listening positions.\n\nSimilalry, a more accurate impulse response of the system can be calculated in two ways which are; 1) Designing a more precise inverse filter 2) Using a new equipment like shakers for example which can directly measure the\nimpulse response of the coupled system. This setup will work almost in the same way like the method used to calculate the IR of the receiving room in this study. Besides, auralization could also be performed at a higher level by addressing directionality of the sounds in the receiving room either working with binaural approach or multi-channel spatial audio techniques.\n\n\n  \n    \n   Demo of the Measurement process\n  \n  Demo of the Measurement process\n\n\n",
        "url": "/masters-thesis/2021/06/30/ImSoTra.html"
      },
    
      {
        "title": "Meet SMC 2021 Team B",
        "author": "\n",
        "excerpt": "This is the brand new, fresh from the oven, Team B from the SMC 2021 program. Come and say hi!\n",
        "content": "The Fab Four?\n\n\n   \n   Team B\n\n\nOn paper, Team B is a group of four SMC students assigned by random chance. But after their first few meetings, this group of three Norwegians and a lone American realized that it was truly destiny that brought them together. Bringing a wide range of musical and technical expertise to the table, Team B hopes to combine their passions into something harder, better, faster, and stronger than the sum of their parts. We’re also all pretty chill guys overall. 😄\n\nKristian Wentzel\n\n\n   \n   \n\n\nKristian is a keyboard player from a rural area in Norway called Toten. Here, he grew up to the Soul &amp; Rhythms of his relatives. After picking up the flute to do some 17th of May marching (Editor’s note: the Norwegian national day) in the school band, he settled for the piano. Although his beloved grandfather advised him to pursue an education in Computer Science, the attraction towards a career in music was too strong. This resulted in a Bachelor’s Degree in “Music Performance”, and add-ons including “Live Electronics: Music Technology for Performers” – before finally wrapping the pieces together in attending the SMC Master’s programme.\n\nThe last decade, he has been a freelance musician performing with acts ranging from soul and jazz via death metal to pop and hip hop. He has been a sideman for both unknown and nationally renowned artists at festivals, radio and television. The music he feels the most at home in, is the kind leaning more towards the improvised and groove-oriented.\n\nKristian is a father of a girl, and resides just outside of Oslo. When not performing music, he can not be found fly fishing in the local rivers, though he’d love to partake in an evening of board games. He is excited to get this opportunity to pursue his interests further in exploring synthesizers, music technology – and learn lots of new stuff in all things related.\n\n\n  Instagram\n\n\nJoseph Clemente\n\n\n   \n   Joseph Clemente\n\n\nJoseph is a Computer Science major from the suburbs outside of Detroit, Michigan. He became aware of music technology during his bachelor’s at the University of Michigan and, after two years of work in vehicle emission testing, decided to continue pursuing this field in his professional career through the SMC program. He’s excited to learn more about the field and to combine his technical knowledge with new musical skills learned in the program. He also likes to think he’s a pretty fun guy despite this stone cold, paint-by-numbers bio he just wrote.\n\nJakob Høydal\n\n\n   \n   \n\n\nJakob is a bass player and audio-engineer born in the small town of Langevåg on the west-coast of Norway. At a young age he started in the local marching-band. In his teens he figured that rock was a lot more fun. He couldn’t bring his trombone to the band, so he started to play bass. He also played the double bass for a few years, but sold it so he could travel.\n\nIn later years Jakob studied music production at Kristiania University College. He also freelance at the technical side of concert venues and in the broadcast-scene.\n\nJakob looks forward to learning a lot about how things connect, and to expand his knowledge about technology in the music- and audio-field.\n\n\n  Instagram\n  Website\n\n\nArvid Falch\n\n\n   \n   \n\n\nArvid Falch is a Norwegian music composer. He spent his formative years recording and touring with various acts and artists, until he got tired of carrying heavy amps into yet another basement club. Since then he has worked as a composer and music supervisor. He has composed music for a variety of films, TV series, apps, shorts and commercials.\nMusically Arvid has steered more towards the classical world in the later years, but he still hangs out with his old band mates jamming with synths and guitars whenever he’s able.\n\nArvid is a father of two children and resides just outside Oslo, Norway. When not making music he can be found fly fishing in the local rivers or playing tabletop roleplaying games with his friends.\n\nJoining the SMC program, Arvid is really looking forward to explore technology which could be used in post production environments and creative musical collaborations across borders.\n\n\n  Website\n\n\nCompetencies\n\n\n   \n   Team B Competencies\n\n\nTeam B seems to be a pretty homogeneous group regarding the personal characteristics. Taking turns and stating our characteristics oftentimes resulted in agreement and appreciative nods from fellow members. We might just be a curious bunch of empathic team players keeping it cool – luckily with one of us willing to be a leader in a pinch. 👌 In stating the practical skills, Team B realized that they’re actually already a band according to the instruments covered. The technological knowledge appears to be diverse, sporting a software developer alongside audio engineers and music theorists.\n\nConclusion\n\nAfter a week with a lot of new experiences, we are all looking forward to making, creating and working with music technology. As a group of people with different backgrounds, experiences and workflows, we can’t wait to share and learn from each other.\n",
        "url": "/people/2021/08/27/introduction-team-b.html"
      },
    
      {
        "title": "Meet SMC Team A 2021",
        "author": "\n",
        "excerpt": "Hello! We are the new A team in town! Click here to get to know us better in our introduction post where we talk about our backgrounds, our motivations and goals.\n",
        "content": "\n   \n   An astonishing team.\n\n\nA-Team\n\nIntroduction\n\nHi, we are the A team! Not as in the Ed Sheeran song but as cool as the TV show! The A-team is a diverse team exploring various aspects of sound such as music, acoustic or soundscape analysis. We’re all from different parts of Europe and have come together from Norway, France, Scotland/Germany and Spain. We are a passionate and funny team (to varying degrees of success) who are looking forward to taking on the challenges at the SMC.\n\nRead our individual sections to get an idea of what to expect from us in the forthcoming months!\n\nTeam members\n\nSofía González\n\n\nHi!\nMy name is Sofía and I come all the way from Spain because I was given the opportunity to dive into the musical and technological field, two areas in which I have always been interested. I’m a singer who’s never had a band and I had to find my way around sound editing programs in order to make my own songs and covers. After that, I became very interested in learning more of this side of music, so I think I found my place at the SMC.\nAs for my formation, I have been in the conservatory for four years playing violin (but that was a looong time ago), I have a degree on English Studies and two years of singing lessons. That’s why I hope to learn as much as I’m allowed here in the SMC but also as much as my brain is able to take in without exploding…\nBesides the academic details, in my free time I enjoy just about anything that’s creative: watching movies/TV shows, playing videogames, creative writing, drawing, baking (and eating the baked goods), singing, writing songs…you name it.\n\n\n  YouTube\n  Instagram\n\n\nJoachim Poutaraud\n\n\n\nJoachim studied a Bachelor in Archaeology &amp; History of Art and worked as an assistant engineer for the soundscape archaeology research project Bretez : sound heritage enhancement directed by musicologist Mylène Pardoen. Furthermore, he was engaged as a member of the Acoustic Task Force, directed by acoustician Brian F. G. Katz and musicologist Mylène Pardoen in order to propose several acoustic models for the reconstruction of the cathedral Notre-Dame de Paris. His interests are related to soundscape studies, acoustic ecology and ecoacoutics and he hopes to be able to develop an interesting project in relation with the technological contributions of the SMC Master program.\n\n\n  Digital portfolio\n  Press\n\n\nHugh Alexander von Arnim\n\n\n   \n   Recording the Landes Jugend Jazz Orchester Hessen\n\n\nHugh studied a Bachelor in Sound and Music Production in Germany. However, his interests lie anywhere and everywhere there is something to learn (even if this learning is a slow process). Nonetheless, the through line is how all of these different areas can be related to music, his biggest passion, with Hugh having played piano and bass in many different ensembles over the course of his life. It’s the multidisciplinary character and the cutting edge aspects of the SMC program that really appealed to Hugh, and he hopes to be able to develop as a musician, a technologist, and a person over the course of the next two years.\n\nSome examples of his work include:\n\n\n  Sound Installation ‘Extrude’ exhibited in Darmstadt, Germany\n  Recording and mix of the piece ‘Interlude for Trombone’ performed by the Landes Jugend Jazz Orchester Hessen\n  Composition of the piece ‘Lost Time’\n\n\nOliver Getz\n\n\n   \n   Conducting the Vertex Ensemble in Boston, USA.\n\n\nOliver Getz is a video game composer, active in the Norwegian game development scene since 2013. He is a board member of Game Audio in Norway (GAiN) and owns an audio production company called Nordic Scoring, which was started after he finished his studies in Electronic Production &amp; Design at Berklee College of Music.\n\nIn the past, Oliver has conducted music ensembles, organised scoring stage recordings for visual media, and developed video games. Now he is in the SMC programme to learn more about audio interactivity. Head to his website to learn more about interactive music for games or audio production for edutainment projects.\n\n\n  Website\n  Music\n\n",
        "url": "/people/2021/08/27/introduction-team-a.html"
      },
    
      {
        "title": "Ensemble algorithms and music classification",
        "author": "\n",
        "excerpt": "‘Why is shoegaze so hard to classify?’ and other pertinent questions for the technologically inclined indie-kid.\n",
        "content": "For this machine learning project, I focused on applying supervised machine learning techniques to music genre classification. In particular, I wanted to look at how ensemble approaches can be used, how they compare to their base models and to each other, and whether they can tell me that the song I just wrote is shoegaze or dreampop.\n\nAutomatic music classification is a fairly well explored topic, and its techniques been put to use in music recommender systems in products such as Apple Music and Spotify. Using deep learning neural networks, high levels of accuracy have been achieved. There are some gaps however - listeners interested in music beyond the mainstream rarely receive relevant recommendations, and it is a widely know problem that recommender systems are prone to popularity bias.\n\nSo while much of the literature I came across focuses on very high level genres such as rock, pop and jazz, I chose 4 lesser known genres - post-punk, ambient electronic, dub and shoegaze. I thought it could be interesting to see how the techniques worked with less well defined genres/sub-genres, and I concede there might have been a touch of self interest driving this as well.\n\nWhen it comes to the audio features used for genre classification, previous work in this field has found the most success using Mel Frequency Cepstral Coefficients (MFCCs). MFCCs are a great way of providing a compact view of the spectrum of an audio signal. The Mel scale emphasises the frequencies of the spectrum that correspond with what we hear as different pitches, so the machine learning model can get a better handle on whats going on in the music.\n\nSo I then fed all of these MFCCs into some ensemble algorithms and waited. Luckily, I didn’t have to wait too long, as these are traditional machine learning algorithms, and not those new fangled neural networks that take several days to tell you that you accidentally left the genre ID in the training data.\n\nBut what is an ensemble algorithm I hear you ask? Ensembles combine multiple weak machine learning algorithms into a single more effective model, and I used three different types: an ada-booster, a gradient booster and a random forest. The base model that these three ensembles use is the decision tree, so I also ran the data through one of those so we can see the effect the ensembles are having.\n\n\n   \n\n\nAs you can see from the first figure, all ensemble methods provide a significant boost to the basic decision tree model, with the random forest yielding the best results here.\n\nIf we look at the performance of the individual genres, it was dub reggae that was most consistently identified correctly, and shoegaze that performed the worst. This is interesting, as other studies have shown reggae to be one of the harder genres to classify, but then these other studies have also managed to get 80-90% accuracy, and the best I managed here was 55%. Shoegaze is another interesting example - it can be seen that the ada-booster provided very little benefit over using the decision tree alone. Ada-boosters are know to be susceptible to noise however, and seeing as shoegaze tends to be pretty much noise with a lot of reverb added on top, this could explain the findings.\n\n\n   \n\n\nSo my results weren’t massively impressive, but it was interesting and fun to learn about and play with the different ensemble algorithms.\n\nWhat would I do different next time? I would experiment more with different features, rather than just sticking with MFCCs. MFCCs have obviously given some great results with convolution neural networks, but maybe the ensemble approach would benefit from a bit more exploration here.\n\nRegarding AI driven music classification in general, I think its important to note that while state of the art approaches using deep learning have managed some impressive results, most music recommender systems still put more emphasis on more human centred approaches such as collaborative filtering, which analyses the listeners behaviour in comparison to other listeners, as well as simple human curation. Genre is a complex subject, and a concept that is not entirely embedded in the music.\n\n",
        "url": "/machine-learning/2021/09/17/stephedg-ml.html"
      },
    
      {
        "title": "Estimation of Direction of Arrival (DOA) for First Order Ambisonic (FOA) Audio Files",
        "author": "\n",
        "excerpt": "Where is that sound coming from? Let’s explore how a machine could answer this question.\n",
        "content": "Note: The source code and technical report are available on GitHub\n\nThe aim of this project is to estimate the Direction of Arrival (DOA) from a sound event encoded as a First Order Ambisonic (FOA) audio file (4-channel). Basically, we want to know where a sound is coming from just by looking at a raw audio file for 3D sound, which can be solved by humans but still with some imprecision.\n\nDOA is usually presented as two angles: elevation ϕ, and azimuth θ as shown in the figure below. The sound event is the blue sound source as depicted in the illustration, and the estimated direction is represented as the red source. The goal is to reduce the error between both.\n\n\n   \n   DOA estimation illustration.\n\n\nThere are methods to calculate this estimation when we have a clean sound source. However, when noise, reverberance, and other artifacts are included in the sound, the task starts to become a difficult problem, which demands more robust methods. In that sense, machine learning techniques are suitable for this challenge.\n\nThe Solution\n\nMachine Learning (ML) algorithms need data to learn from. For this project, the journey started with finding a suitable data set to work with. I used the one provided by the Sound Event Localization and Detection Challenge (2019). A full description is provided on the website, but the important aspects regarding this data is that files are combined with noise, reverberance, and half of the set for training and testing purposes are overlapped with up to 2 sounds, which is more challenging for a potential solution. In summary, from this data set we can extract 15798 short FOA sounds for training and 3974 for testing, each file is associated with a localization in space (elevation, azimuth, distance).\n\nThe chosen machine learning technique was an Artificial Neural network (ANN) for regression, since the problem require to estimate multiple outputs (elevation, azimuth) that are numerical values in the real domain. The most challenging part before training this ANN was to find the right features for its input.\n\nMy first tries for building the features were based on the fact that each channel on the FOA file is different from each other in terms of delay and power. If we think about how humans can detect the direction from a sound source, we can tell that the sound comes first in one ear and then hits the other one with lower volume, additionally the shapes of our hears acts as filters. However, I obtained a really poor accuracy in the prediction.\n\nThe second set of attempts were inspired by successful solutions from previous works in which researchers used the spectrogram arranged as features, nevertheless, I did not get good results since the ANN that they employed were Convolutional Neural Networks(CNN) and/or Recurrent Neural Networks(RNN) with custom modifications, I was limited to use a full-connected feed-forward ANN.\n\nDiving in the literature, I tried the cross-spectral density (CSD) as the key solution for the right features. After a methodic search for the best hyper-parameters for ANN architecture, I ended up with the solution shown below.\n\n\n   \n   Architecture for DOA estimation.\n\n\nAs the picture above illustrates, the process for feature extraction was:\n\n\n  Normalize the audio signals.\n  Calculate CSD for three pairs of channels (W, X), (W, Y), (W, Z).\n  Extract the phase per each pair.\n  Interpolate the arrays to a set of 256 elements each.\n  Normalize each interpolated set.\n  Join the three arrays in one only feature set.\n\n\nAfter this process, a Whitening transformation (standard scaling) was applied to the whole set, then the number of features were reduced to 40 by applying Principal Component Analysis (PCA) since keeping 40 components was suitable for the problem (details in the technical report). Finally, it was provided to the ANN described in the picture above.\n\nNote that the ANN was trained with a label set converted from spherical coordinates to cartesian coordinates, which means that the task became a 3-output regression problem.\n\nResults\n\nThe performance metrics for the solution by calculating the error between the actual and predicted value regarding the training set, cross-validation, and testing set are shown in the table below.\n\n\n   \n   Performance metrics.\n\n\nFor each component (x,y,z), the  Coefficient of Determination (R2) is depicted in the next table. It indicates that the algorithm performed better for x and y compared to z.\n\n\n   \n   Coefficient of Determination (R2) for the DOA unit vector.\n\n\nThese results shows that we have an error about 30° between the actual direction and the estimated one. The worst (178.34°), average (30.24°), and best (0.49°) case from the testing set are illustrated in the following picture. The yellow vector is the reference (front), the blue is the actual DOA, and the red vector the estimated one.\n\n\n   \n   Three cases of interest for DOA estimation.\n\n\nReflection\n\nThe estimation of DOA is a complex task that have been solved with Deep Learning strategies, which were out of the scope for the requirements of this project. In those cases, for the same data set the best solution has an angle error of approximately 4.75° for the training set. The solution presented here has around 30°, which is acceptable for applications that does not require high precision, for example, to control visual feedback for 3D sound files.\n\nThe most challenging part was to figure out the right features. It is important in machine learning (ML) applications to have a handle of expertise regarding the characteristics of the data for achieving the highest accuracy possible. Then, some experimentation regarding the architecture for ML algorithms is required to improve the results, which can sometimes have some support, but in others could be hard to explain.\n\nI hope this work gives lights to build up an understanding of the process for DOA from a ML perspective.\n\nThe source code and technical report are available on GitHub\n",
        "url": "/machine-learning/2021/09/18/pedropl-ml-doa-estimation.html"
      },
    
      {
        "title": "Classifying Classical Piano Music Based On Composer’s Native Language Using Machine Learning",
        "author": "\n",
        "excerpt": "How does the language we speak help computers to classify classical music?\n",
        "content": "Introduction\n\nMusic classification is a category of music information retrieval (Kassler, 1966), involves categorizing musical works into various classes, such as genre, composer, mood, and it can be present in the music recommendation system for listeners (Tzanetakis &amp; Cook, 2002), or research purposes such as its cognitive effects (Subramaniam, Verma, Chandrasekhar, NarendraK., &amp; George, 2018).\n\nMusic classification usually involves extracting musical features from the audio files or scores to help the machine learning algorithm perform the categorization. However, in recent years linguists have discovered that the features of different musicians’ native languages can influence the music they compose. For example, Patel and Daniele (2003) compared 16 composers’ music work and found English and British music themes have a significantly higher nPVI (nPVI represents the durational contrast of continuous sequence) compared to French and French music themes.\n\nTherefore, this project aims to build a Machine Learning model to classify classical music based on the composer’s native language background, and also investigate whether linguistic features can contribute to the development of machine learning music classification algorithms.\n\nDataset\nGiantMIDI-Piano dataset (Kong, Li, Chen, &amp; Wang, 2020) was chosen based on the need to expand the number of composers and repertoire. GiantMIDI-Piano’s metadata is collected from the International Music Score Library Project (IMSLP) and audio files retrieved from Youtube, Piano solos recordings are transcribed to MIDI files using a high-resolution piano transcription system (Kong, Li, Song, Wan, &amp; Wang, 2020).\n\nAfter sorting through the original database, we have 5729 MIDI files composed by 877 composers from 12 countries speaking 6 languages for training.\n\n\n   \n   Data distribution\n\n\nFeatures\nFor linguistic features, we extracted the nPVI and CV from the melody line.\n\n\n   \n   nPVI\n\n\n   \n   CV\n\n\nFor Music features, we extracted 27 features through music 21(Cuthbert &amp; Ariza, 2010) and jSymbolic (McKay &amp; Fujinaga, 2006).\n\n\n   \n   Music features\n\n\nResult\nwe chose to use three supervised Machine Learning algorithms Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and Gaussian Naive Bayes (GNB) as classifiers and Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA) as dimensionality reduction techniques in Scikit-learn through Python for comparison. The numerical results of the experiments are below.\n\n\n   \n   Classification accruacy\n\n\nThrough the accuracy, contrary to our expectations, we found that KNN applied to linguistic features alone had the highest accuracy of any combination at 57.4%, while both SVM and Naive Bayes were much less accurate at just over 30% accuracy when using only linguistic features. From an overall perspective, the KNN algorithm has the highest accuracy, followed by SVM and the worst is Naive Bayes.\n\nFrom the perspective of features combination, it appears that using only linguistic features was the least accurate under both SVM and Naive Bayes classifiers. However, using only linguistic features had the highest accuracy of all combinations under the KNN classifier, and the accuracy obviously improved when music features were combined with linguistic features compared to using music features alone.\n\nLast but not least, in terms of the choice of dimensionality reduction techniques, we found dimensionality reduction techniques played a degrading role on both KNN and SVM in our case. Nevertheless, for the Naive Bayes, both PCA and LDA provide significant improvements to accuracy, and LDA, as a supervised dimensionality reduction technique, provides the largest improvement when we applied more complex features (Music + Linguistic).\n\nIn summary, the combination of applying linguistic features to the KNN classifier and not using dimensionality reduction techniques is the best combination for this comparison, such a combination also provided a good computational efficiency compared to other combinations.\n\nReflection\n\nNevertheless, the accuracy obtained in this experiment leaves us room for improvement. We suppose that the accuracy deficiency could be attributed mainly to two aspects: the imbalance of the dataset and the accruacy of melody line extraction.\n\nFirstly, the MIDI quantization and melody extraction used to extract the melodic lines lost a certain degree of rhythmic features from the original score, which had an impact on the calculation of nPVI.\n\nSecondly, only the English and German classes have approximately the same amount of samples and the other categories are only a third of their quantity in our dataset. Moreover, it is worth mentioning that in a less rigorous trial, we removed the smaller classes and left only the three largest classes, obtained a distinct increase in overall accuracy.\n\nAlthough we have used 27 musical features and 4 linguistic features, it is apparent that there are many more features also Machine Learning algorithms applicable to music classification that have not yet been addressed, which opens up the opportunity for working in the future.\n\nReference\n\nKassler, M. (1966). Toward musical information retrieval. Perspectives of New Music, 59–67.\n\nTzanetakis, G., &amp; Cook, P. (2002, July). Musical genre classification of audio signals. IEEE Transactions on Speech\nand Audio Processing, 10(5),\n\nSubramaniam, G., Verma, J., Chandrasekhar, N., NarendraK., C., &amp; George, K. (2018). Generating playlists on the\nbasis of emotion. 2018 IEEE Symposium Series on Computational Intelligence (SSCI), 366-373.\n\nPatel, A. D., &amp; Daniele, J. R. (2003). An empirical comparison of rhythm in language and music. Cognition, 87(1),\nB35-B45. Retrieved from https://www.sciencedirect.com/science/article/pii/ S0010027702001877 doi:\nhttps://doi.org/10.1016/S0010-0277(02)00187\n\nKong, Q., Choi, K., &amp; Wang, Y. (2020). Large-scale MIDI-based composer classification. ArXiv, abs/2010.14805.\n\nKong, Q., Li, B., Chen, J., &amp; Wang, Y. (2020). GiantMIDI-piano: A large-scale MIDI dataset for classical piano music.\narXiv preprint arXiv:2010.07061.\n\nKong, Q., Li, B., Song, X., Wan, Y., &amp; Wang, Y. (2020). High-resolution piano transcription with pedals by\nregressing onsets and offsets times. arXiv preprint arXiv:2010.01815.\n\nCuthbert, M. S., &amp; Ariza, C. (2010). music21: A toolkit for computer-aided musicology and symbolic music data.\n\nMcKay, C., &amp; Fujinaga, I. (2006). jsymbolic: A feature extractor for MIDI files. In Icmc.\n",
        "url": "/machine-learning/2021/09/19/wenbo-ml-nl.html"
      },
    
      {
        "title": "Internet time delay: a new musical language for a new time basis",
        "author": "\n",
        "excerpt": "Logistical considerations for large-scale telematic performances involving geographically displaced contributors still remain strongly present. Therefore, if networked performers are still to the vagaries of speed and bandwidth of multiple networks and if latency  problems remain a significant issue for audio-visual streaming of live Network Music Performance (NMP), one can rather reflect on trying  to find a new musical language for a new time basis.\n",
        "content": "\n  \n\n\nIntroduction\nThere are several challenges that need to be taken into consideration in Network Music Performance (NMP) with geographically displaced contributors. As discussed in the paper of Jonas Braasch [1], these challenges are mostly technical and they involve  physical-distance and latency issues. However, acceptance of these issues has been contributing to major innovations with, for instance, the development of a metronomic pulse system contributing to a new kind of time basis of this media as mentioned by Chafe Cáceres et al. in the paper of Roger Mills [2]. Nevertheless, logistical considerations for NMP occurring simultaneously in different time zones still remain strongly present. If networked performers are still to the vagaries of speed and bandwidth of multiple networks, and differences in audio-visual streaming applications [2], and if latency problems remain a significant issue for audio-visual streaming of live NMP, one can rather reflect on trying to find a new musical language working on this new time basis [3].\n\nDiscussion\n\nPHYSICAL-DISTANCE\n\nFirst of all, Braash argues that human communication language can be correctly established within the environment where it is practised [1]. Consequently, the only reliable solution to optimize the signals of our human interactions depends on a given acoustic environment. According to him, the acoustic environment seems to be decisive in spreading sounds efficiently. For instance, he mentions that the actual record for real acoustic long-distance communication is known as being held by sperm whales (Clark &amp; Clapham, 2004) which songs can carry hundreds of miles across the ocean using the acoustic properties of the Sound Fixing and Ranging channels (SOFAR channels; Ewing &amp; Worzel, 1948). This being mainly due to their water environment in which sperm whales spread signals at a speed reaching approximately 1480 m/s (5 336,435 km/h) at a temperature of 20°C, whereas sounds in air environment are limited to a speed of approximately 340 m/s (1224 km/h). This implies that while much has been achieved to reduce telematic music systems latency, the physical-distance between two collaborators still remains what determines the achievable minimal propagation delay. As a result, transmission delay induced by the telematic systems prevents us from reaching a virtual environment in “real-time”.\n\nLATENCY\n\nTo perceive musical interaction as natural in NMP, sounds coming to the human ear should not be displaced in time more than 20 milliseconds [4]. This means that for mutual awareness to be supported in a NMP, the maximum threshold should be around 40 milliseconds (the time it would take a performer to perceive a second performer’s reaction to his or her action) [5]. Moreover, even though electric signals could transfer audio data at the speed of light (approximately 300,000 km per seconds) with an unlimited bandwidth, latency would still reach approximately 133.4 milliseconds, which is much higher than the tolerable threshold [5]. Thereby, one of the biggest problems NMP is confronted with is that latency is an integral part of sound because it is generated by the performer’s local system and sent through the network system. If there is too much latency in the network system, it becomes thus difficult to create collective playability, especially if the musicians wish to adjust their playing or coordinate according to the sounds they hear or receive [2]. Therefore, acceptance of these technical challenges seems to be unavoidable.\n\nA NEW TIME BASIS FOR A NEW MUSICAL LANGUAGE\n\nThe character, nature and structure of the music played, and the types of instruments and systems used, determines the acceptance of the technical challenges linked to the latency. Consequently, synchronization elements can be used in NMP to find a new musical language that could work on a new time basis [3]. Because latency induces some artificial and fluctuating artefacts linked to the variations in data transfers on the network, one could rather accept transmission delays, time-based errors, de-sequencing or even partial loss of content [5], by considering their aesthetical properties. That is of revealing a materiality (or granularity) linked to the technique of the audiovisual streams, which could be accepted as a new sound material. Furthermore, this new kind of media could create a creative online community which will not be oriented towards time-limited event scenarios but which could manipulate and transform this new sound material while listening to music created collectively [5].\n\nReferences\n\n[1]  J. Braasch, (2009) The Telematic Music System: Affordances for a New Instrument to Shape the Music of Tomorrow, Contemporary Music Review, 28(4), 421-432. See https://doi.org/10.1080/07494460903422404\n\n[2]  R. Mills, (2019) Tele-Improvisation: Intercultural Interaction in the Online Global Music Jam Session, Springer Series on Cultural Computing.\n\n[3]  G. Föllmer, (2001) Crossfade - Sound Travels on the Web - Soft Music, a joint project of San Francisco Museum of Modern Art, San Francisco, California; ZKM-Center for Art and Media, Karlsruhe, Germany; Walker Art Center, Minneapolis, Minnesota; Goethe Forum, Munich, Germany. See http://crossfade.walkerart.org\n\n[4]  I. Hirsh, (1959) Auditory Perception of Temporal Order, Journal of the Acoustical Society of America 31, No. 6, 759–767.\n\n[5]  A. Barbosa, (2003) Displaced Soundscapes: A Survey of Network Systems for Music and Sonic Art Creation, Leonardo Music Journal, 13(11), 53–59. See https://doi.org/10.1162/096112104322750791\n.\n",
        "url": "/networked-music/2021/09/20/joachipo-internet-time-delay.html"
      },
    
      {
        "title": "Ensemble algorithms and music classification",
        "author": "\n",
        "excerpt": "Playing around with some supervised machine learning - genre classification is hard!\n",
        "content": "After a long sunny summer, we started our autumn semester with a very compact and interesting course - Machine Learning (ML)! The course was no joke, it’s great that we are done with a course by the 20th of September, but these past weeks were interse after the lethargy of the hot season. We’ve learnt a lot during this time, thought, and parts of that is the fact that ML is not the answer to anything. It’s a very good technique for some questions but not all…\n\nLuckily, music information retrieval (MIR) is a problem for which ML has been proven to help. (Automatic) genre classification is a subproblem of MIR. It is a widely studies application of ML and the topic of my project. The state of the art results of genre classification are using by (music) tech giants such as Apple (iTunes) of Spotify and I didn’t believe I had a chance against their performances. That is why I decided to focus half of my time on some literature search and find out more about the problem at hand before writing a program that would attempt to classify music.\n\nFrom the literature search I learned about two relevant phenomena:\n\n\n  \n    Genre taxonomy is criticized: Some researchers are debating whethear genre classification is a task worth investing so much resources into, as it has to be based on a genre taxonomy - that is, genres are labels created and decided by humans who are prone to subjectivism. The classification is based on people’s opinions of similarities in harmony and or rhythm or a cultural context but it is not an universal truth. That is why the usefulness of working toward an automatic genre classification based on human defined lables is debated.\n  \n  \n    Artist and album effect: Through trial and error it has been discovered that the performance of a ML algorithm is inaccurately high if songs from the same artist are in both the training and the test set. The reason for that is simple - the model learns to recognize the songs of that artist rather than generalize to other songs from the same genre.\n  \n\n\nAfter the literature search I went on to creating some code. I decided to use the Free Music Archive (FMA) dataset. It is a great resource for a multitude of ML tasks; it provides the full audio, it provides a large subset with all songs but only 30 seconds taken from the middle, a medium subset with only the songs labeled with only one genre (16 root genres), and a small subset with a balanced sub-subset (?!) of the medium set (8 root genres). It also provides very comprehensie metadata abouut the track, the artist and the album, as well as audio features extracted with librosa from all the songs, all separately in .csv files, which facilitates a quick start on any task. They also published a comprehensive manifest together with the data, where they tested the performance of several ML algorithms on several combinations of features extracted from their data - the medium subset.\n\nDue to limited computational power, I chose to work on the small subset. It onsists of 1000 tracks of the top represented root genres: Hip-Hop, Pop, Folk, Experimental, Rock, International, Electronic, Instrumental. In total there were 8000 fragments of 30 seconds. To split the data into training and test sets I used their defined splits, because theymade sure the artist and album effect would not occur; however, I combined the training and validation sets, with a resulting distribution of 90%-10%. I also decided to work directly from the csv file with the extracted features.\n\nIn total, I tested 3 ML algorithms with one variation:\n\n\n  Logistic Regression\n  K-Nearest Neighbor (k = 120)\n  Support Vector Machine (SVM)\n  SVM with additional Linear Discriminant Analysis (LDA) dimensionality reduction.\n\n\nI tested the performance of each of these algorithms on the following (combination of) features:\n\n\n  \n    Mel-Frequency Cepstral Coefficients (MFCC): of the spectral shape features, this seems to convey the most meaning with the least dimensionality; instead of the usual 15 or 20 coefficients, only the first 10 were chosen here;\n  \n  \n    MFCC + Tempo (manually assigned tempo);\n  \n  \n    MFCC + Spectral Centroid (indicator of “brighter” musical textures) + Spectral Contrast (the difference between the peaks and valleys of the audio spectrum).\n  \n\n\nThe models were evaluated based on their accuracy and the results of the analysis can be seen in the Table below. As you can notice, not great performances! In line with previous research, the SVM model perfrmed best. Interestingly, the simpler Logistic Regression Classifier performed as “good”.\n\n\n    Test set accuracies\n\n\nHow badly did it actually perform, though? Below is the confusion matrix of the performance of the SVM model trained on MFCC + spectral contrast + spectral centroid. Aside from Hip-Hop, Rock, and Electronic, the rest of the genres are mostly misclassified. For example, Folk songs have been classified as Pop, Folk, International and Instrumental almost equally. Same with Pop, it was classified mostly as Folk, Hip-Hop, and equally between Pop and Electronic. So you see, the existing genre txonomy is very subjective (what is “internationl” as a genre anyways?!).\n\n\n    Confusion matrix for SVM\n\n\nWhy did it perform so badly? Well… there’s several reasons that can be the case. Firstly, I used a small dataset. Secondly, the features I chose might not have been the best representations of the data; I could have also used the 30 seconds of audio. And thirdly, look below how the data is almost one big cluster! I used a 2D representation of the spectral contrast and centroid and it looks like each genre is just a layer on top of other layers in almost the same place! Possibly these two were not the best features to use for a visual representation, but it’s giving a rought idea of the complexity of the problem.\n\n\n    Scatter plot of centroid vs contrast\n\n\nIn conclusion, this project was a complex, but fun idea! I never thought how complex it is to classify genres, but I should have known given that I’m very bad at telling the genre of a song myself! There’s so much subjectivity and it’s all based on our own opinions afterall. If I were to continue with this project, I would test other features as well, and I would work with the large if not full dataset. A classification of subgenres might also be interesting to look into. If there are fewer outliners it might also be cool to look into which song specifically or genre got mislabelled often - as my classmate Stephen was saying, why is shoegaze so hard to classify? I am personally amazed at the fact that there is a genre called “international”, huh (and it has a subgenre called Juju!)\n\nIf you would like to read the whole technical report or take a look at the code, email me at alenacl@uio.no!\n",
        "url": "/machine-learning/2021/09/20/alena-genre-classification-ml.html"
      },
    
      {
        "title": "Can Machine learning classify audio effects, a dry to wet sound ?",
        "author": "\n",
        "excerpt": "Distortion or No Distortion - Machine learning magic\n",
        "content": "After the intense weeks of machine learning, what did I learn? Machine learning is hard and it’s a journey. But it is not as mysterious as it used to be anymore! Will Artificial Intelligence and Machine learning take over the world? Not anytime soon and not forever, well that’s debatable. But the world of machine learning is a fascinating one, is it capable of solving all the problems of the world? Definitely not, but for the right challenges applied in the right way, it leads to the astonishing result, and that’s why it is so famous. After days of fiddling around on the internet trying to find inspiration for a project that I wanted to do, I decided to look into if machine learning can distinguish audio effect(s) applied on the audio, essentially classification of a dry audio signal to a wet audio signal.\n\nHow did I start?\nFirst I decided on what machine learning technique will I use, supervised or unsupervised? And which techniques among these two categories? After a bit of research, I decided to go with supervised machine learning techniques and apply a couple of them to see which one performs the best. I ended up using Support Vector Machine (SVM), Multilayer Perceptron (MLP), and K-Nearest Neighbour (KNN) for the problem. The next task was to find an appropriate database for training and testing the model I wanted to create. I ended up using the IDMT-SMT-Audio-Effects database, as it had a variety of audio effects and multiple settings on each of them, applied to different types of guitars. It was perfect for me because then I could train my model to not just classify between a dry and wet sound but also classify basically into different positions of dry/wet knob, or different amount/type of same audio effect.\n\nApproach\nI took two approaches while dealing with this problem, the first was to just randomly split the data into training and training parts and use them to train and test the models. The other was to optimize all the processes before we actually train the model, so optimal split of data, Dimensionality Reduction, scaling and so on. The motivation behind this approach was to check how badly the system will perform without the optimization and how much the performance will enhance after applying these optimizing techniques.\n\nDataset\nThe data set as mentioned above I used was the IDMT-SMT-Audio-Effects, it is a dataset consisting of 55044 audio files recorded a 44.1 kHz, 16 bit, and in mono. This is divided into monophonic guitar and bass notes, and polyphonic guitar sounds, with an overall of 11 different types of audio effects applied to these recordings, you can get the dataset with detailed information here []. I ended up using a small chunk of the data. I used the monophonic guitar notes with distortion applied to them and the corresponding no-effect audio files. There are three different distortion settings in the audio effects, so it gave me enough bracket of audio files to play with.\n\nSystem Architecture\nAs discussed above, the system here uses supervised machine learning techniques to classify between audio samples with three variations of distortion audio effect and audio samples without any audio effect, essentially trying to classify between dry and wet audio samples. First, I extracted the features using the python library Librosa, plotted them for visual representation using matplotlib as shown in figure 1 and figure 2, and understanding their distribution.\n\n\n    Figure 1: Features Scatter Plot in 2D\n\n\n\n    Figure 1: Features Scatter Plot in 3D\n\n\nAfter that I divided the dataset into a 70/30 split using Scikit learns [train_test_split] method Scikit learn is also the library that I have extensively used in this project for employing all the machine learning techniques. Subsequently, I used SVM, MLP, and KNN and got the results. A brief introduction to these machine learning models is given below:\n\nSupport Vector Machine (SVM) – SVM has shown excellent results in binary and multi-class classification tasks, this technique tries to divide the dataset using a hyperplane separating negative and positive points with maximum distance.\n\nMultilayer perceptron (MLP) neural network classifier – a multi-layered neural network technique MLP has been used in various tasks ranging from image to audio classification. When there are high complexities in the features of the dataset, MLP is found to be successful in classification problems.\n\nK-Nearest Neighbor (KNN) – Being applied in various musical and audio analysis applications, the basic idea behind KNN is to allow a small number of neighbors to influence the decision of the organization of the dataset. It assumes that similar things exist in close proximity.\n\nAfter modeling the system using these techniques, parameter tuning, scaling, and Dimensionality reduction techniques are employed to increase the efficiency. To increase the performance of the system firstly a different cross-validation approach using Repeated Stratified KFold (RKF) to split the data for training and testing is applied. In Repeated Stratified KFold, the dataset is divided into ‘K’ number of folds and the cross-validation procedure is repeated multiple times, and the mean of all the runs across all the folds are taken. After Splitting the data using RKF, it is scaled using the standard scaler from the scikit learn library, we need scaling so that the machine learning model interprets the data on the same scale.\nAfter the scaling Dimensionality Reduction (DR) technique is used for reducing the feature dimension, also to find the best optimum features we can use to classify the data. In the proposed system, Principal Component Analysis (PCA) technique is used. PCA is a highly used DR technique that essentially transforms a large set of variables into a smaller one by trying to maximize the sum of the squared distance from the origin to the projected points.\nAfter tuning the dataset and features the three machine learning techniques stated above are implanted again to examine the increase in performance.\n\nResults\nAfter tuning the dataset and features I noticed a significant amount of improvement in all three techniques, with Multilayer perceptron performing the best among all giving an accuracy of 97.6% jumping from 83% before optimization, Support Vector machine produced an accuracy of 96.6% while it was % before, and K-Nearest Neighbor being the last still saw a significant improvement from 66% to 95%.\n\nConclusions\nIn this project I tried classification of audio samples with audio effect – Distortion applied at three different settings to the audio signals without any effect. Three different supervised machine learning techniques have been applied in order to determine the best-performing technique. Considering that the task was the classification of similar types of audio effects particularly distortion to a dry signal the acquired accuracy of the system, ranging from ~82% to jumping to 96% with optimization can be taken as a decent sub-par result to the problem. Although the accuracy can be improved it was noticed that most of the wrongly classified samples came from the no-effect category and were misplaced due to the affinity of the sound to the ones for which the effect was applied. Finally, in order to make the system more robust, Hyperparameter tuning, noise addition, and other such techniques can be taken into consideration for future prospects.\n",
        "url": "/machine-learning/2021/09/20/abhishec-audio-effect-classification-ml.html"
      },
    
      {
        "title": "Fight latency with latency",
        "author": "\n",
        "excerpt": "Alternative design approaches for telematic music systems.\n",
        "content": "Introduction\n\nIn his paper The Telematic Music System: Affordances for a New Instrument to Shape the Music of Tomorrow (Braasch 2009), the author argues that we should view the telematic music system as a new instrument, one that can change the music of tomorrow. I will discuss how the author outlines his reasons behind this perspective. The idea of the system as an instrument will then be compared with the design choices and musical outcomes of The Online Orchestra, a system described in the paper Telematic Performance and the challenge of latency (Rofe et al 2017). I will discuss how and if the creators of The Online Orchestra fulfills Braasch’ idea of designing a telematic music system as a new instrument capable of creating the music of tomorrow.\n\nDiscussion\n\nBy treating it as a new class of instrument Braasch urges telematic music system creators to set new design goals which do not attempt to replicate the real-world sound environment. Performers and composers should consider a telematic music system as a virtual space on it’s own, and be aware of it’s conditions and limitations. This, the author claims,  is the only way a telematic music system can surpass what is already existing in traditional performance spaces. Just as the invention of the piano enabled new sounds and new compositional possibilities, a system designed as a new instrument will have the potential to reap similar benefits. Contrarily, if the only ideal of the system is to show no signal degradation over the transmission pathway, it could only aspire to be equal to the quality of a traditional performance.\n\nAll telematic music systems that attempt to emulate a traditional performance space will run into issues with latency. System latency can be overcome by improved hardware and software, but propagation latency (information transfer speed) has it’s purely physical limits which will be a challenge to any telematic music system for the unforeseeable future.\n\nWhen latency itself is such a fundamental issue in telematic music systems, designers should try to negotiate this while designing new systems by looking closer at what latency does with music. As hard as it is to perform with latency, as a musical phenomenon it is nothing more than temporal displacement.\n\nOne interesting design solution mirroring Braaschs framework is proposed by Rofe et.al. in their paper Telematic Performance and the challenge of latency (Rofe et al 2017). Through their research they discovered that even a very small values of latency poses huge problems, as attempts to clap in time at 120 bpm with latency exceeding 30 ms were deemed almost impossible by music students. Surprisingly,  much higher values of latency could be managed easily by controlling it to equal a musical temporal value.\nOn the basis of this research the authors designed a telematic music system called The Online Orchestra, which measures latency and then adds a controlled amount of latency so that the total latency is equal to the note value of a quarter note.  The system then designates a master node which starts playing, and the receiving nodes receive the signal a quarter note later. The sound created from the receiving nodes will then arrive at the master node on the next beat.\n\nWith one master node (A), and two separate receiving nodes (B and C), on beat three of the first bar they would then each hear the following combinations:\nNode A will hear it’s own third beat and node B and C’s first beat. Node B will hear node A’s second beat and it’s own second beat, while it will only hear the first beat of node C. The opposite of B would be true for node C.\n\nAccording to the authors, the musicians performing with this system found it easy to navigate, as the music appeared to be synchronized in tempo. On the other hand it is easy to envision that the three different versions in the example would have the potential to sound very different.\n\nOne aspect I find interesting about this approach is that the outcome is not just one musical event, but multiple musical events. I propose that these events should not be regarded as separate individual musical events, but rather temporally different versions of the same musical event. I wish the authors of Telematic Performance and the challenge of latency would have discussed this outcome of their methods in more detail. One question that is left unanswered by the paper is how the music of the Online Orchestra should be represented if it is recorded, or performed in a traditional setting. The composer would have to give some priority in regard to which node should be the master node. One can imagine a recording version to have as many versions of the track as the composition had nodes, although this is left undiscussed in the text.\n\nThe Online Orchestra methodology focuses on composed music rather than improvisational music. The design choice of equalling the controlled latency to one quarter note might be hard to negotiate from a musicians perspective when improvising. Musical motifs would be temporally displaced with different downbeats.   \nIn the online improvisational software NINJAM, the applied latency is usually set to even longer values equaling a chosen number of measures, which leads to even more temporally offset musical results, while still sounding as the tempo is synchronized. (Mills 2019) This enables longer musical motifs to be at least rhythmically coherent , which would have the effect of masking the temporal displacement.\n\nThe approach of both NINJAM and The Online Orchestra do to an extent meet Braaschs requirement of viewing the telematic music system as a class of instrument on its own, creating results that are impossible to create with ordinary instruments or models of compositions.\n\nConclusion\n\nOne can argue that true zero-latency is a utopian goal in telematic music systems, as it will continue to pose an uncertainty based on network instability, and will always be a factor in cross-continental networks due to the speed of light that electric signals cannot exceed. I would even argue that low latency telematic music systems are less inclusive in terms of availability as they will benefit the users with the best network infrastructures and access to the highest standard of computer hardware. With this in mind, alternative approaches that do not replicate traditional performance structures should be welcomed. Hopefully with the result of creating new ideas in music such as temporally different, yet connected musical events.\nA musical event that exists both in the past, present and the future at the same time could very well be the music of tomorrow.\n\nWorks Cited\n\n\nBraasch, Jonas. (2009). The Telematic Music System: Affordances for a New Instrument to Shape the Music of Tomorrow. Contemporary Music Review, 28:4-5, 421-432, DOI: 10.1080/07494460903422404.\n\nRofe, Michael and Reuben, Federico (2017). Telematic\nperformance and the challenge of latency. The Journal of Music, Technology and\nEducation. 167–183. ISSN 1752-7074 . https://doi.org/10.1386/jmte.10.2-3.167_1\n\nMills R. (2019)  Telematics, Art and the Evolution of Networked Music Performance. Springer Series on Cultural Computing. Springer, Cham. https://doi-org.ezproxy.uio.no/10.1007/978-3-319-71039-6_2\n\n",
        "url": "/networked-music/2021/09/22/arvidf-fight-latency-with-latency.html"
      },
    
      {
        "title": "Approaches Toward Algorithmic Interdependence in Musical Performance",
        "author": "\n",
        "excerpt": "Is it possible to program interdependent algorithms to perform with each other?\n",
        "content": "Introduction\n\nOne of the first lines of the paper “Interconnected Musical Networks: Toward a Theoretical Framework,” states that “music performance is an interdependent art form” (Weinberg, 2005, p. 23). As the paper goes on, it becomes clear that their definition of “interdependence” almost exclusively involves human to human interactions. However, this leaves out many new opportunities afforded by recent advances in music technologic, specifically those without any humans involved. In the article “The Aesthetics of Interactive Computer Music,” the author theorizes that the closest thing we have to music with “no subject, no human performer, nor even any composer, conveying anything” was algorithmically generated tape music (Garnett, 2001, p. 28). However, with the dawn of machine learning and more powerful algorithms, I believe that it is possible to create music with artistic value using only algorithms communicating interdependently. In this review, I will be refuting claims from “The Aesthetics of Interactive Computer Music” about the limits and artistic merit of algorithmically generated music, and use the approaches for musical collaboration over the internet from “Interconnected Musical Networks: Toward a Theoretical Framework” to theorize about ways algorithmically generated music could be created.\n\nDiscussion\n\nThroughout the sections of “The Aesthetics of Interactive Computer Music” that refer to algorithmically generated music, it seems that the author is generally negative toward the concept. After theorizing about algorithmically generated tape music, it states that “a counterpart to the extreme precision and elaborate formalisms that this genre enables is the tendency for it to become further and further removed from anything anyone else actually wants to endure in concert” (Garnett, 2001, p. 29). While this may have been true in 2001 when this article was written, machine learning has made it easier than ever to create coherent music through the use of algorithms. Prime examples of this can be found using Magenta, which is a Python library using TensorFlow that “includes utilities for manipulating source data (primarily music and images), using this data to train machine learning models, and finally generating new content from these models (Magenta, 2021). With this highly intelligent tool alone, Garnett’s claim that “it is relatively easy to create algorithms that generate sounds whose qualities as music are inscrutable, beyond the cognitive or perceptive abilities of listeners,” (Garnett, 2001, p. 26) can easily be disproven without even mentioning other open-source machine learning tools that exist online now. Of course, algorithmic approaches definitely lose some qualities compared to human performance that will be hard to overcome. After playing around with demos from Magenta, I agree with the claim that algorithmic music “can lead to an over-emphasis on precision, a mistaking of precision as an end rather than as a means to an end as it ought to be” (Garnett, 2001, p. 29). However, I can only hope in the decades to come after I write this, more sophisticated machine learning algorithms will be developed that will give greater nuance to algorithmic music.\n\nWeinberg offers four approaches for musical collaboration with other people through the internet: the server approach, the bridge approach, the shaper approach, and the construction kit approach. In this next section, I will be going through each approach and commenting on how algorithms could be used in theory. The shaper approach is by far the most straightforward of the bunch. The approach is described as “a means to send musical data to disconnected participants and does not take advantage of the opportunity to interconnect and communicate among players” (Weinberg, 2005, p. 26). This approach is quite simply file sharing, but there are plenty of ways to make this approach more interesting with algorithms. I could imagine a scenario where a machine learning algorithm could generate sheet music, which will then be given to players to perform. The next approach, the bridge approach, is used “to connect distanced players so that they can play and improvise as if they were in the same space” (Weinberg, 2005, p. 27). In a few ways, algorithms playing with each other actually seems easier than human to human interaction using the bridge approach. If you have two machine learning algorithms that are trained to react to MIDI input and output MIDI information as well, this eliminates the need for processing power to send the larger audio data, as opposed to the much smaller MIDI values. As for the next two approaches, I will be grouping them together because, using an algorithmic approach, they become essentially the same. The shaper approach “takes a more active musical role by algorithmically generating musical materials and allowing participants to collaboratively modify and shape these materials” (Weinberg, 2005, p.27), while the construction kit approach allows skilled musicians “to contribute their music to multiple-user composition sessions, manipulate and shape their own and other players’ music, and take part in a collective creation” (Weinberg, 2005, p.28). A purely algorithmic approach to both of these approaches would involve a more complicated machine learning algorithm than the one in the theoretical algorithmic bridge approach, but the results would be much more fruitful. Instead of just sending MIDI data, algorithms can control other algorithms in every aspect of music performance, making the end result much more dynamic.\n\nConclusion\n\nIn conclusion, I believe that interdependence among algorithms will be an interesting topic to keep investigating for years to come. In the past, people like Garnett may not have seen a way forward for this kind of music creation, but recent advances in music technology like Magenta make algorithmically generated music easier than ever to execute. While I acknowledge the theoretical approaches to online algorithmic interdependence is ambitious, and the end result may not be incredibly satisfying due to the loss of human interaction, there are many ways we can base algorithmic interdependence in human-to-human approaches to music creation. I believe that this is a worthwhile area to research further and could one day create  adventurous, unpredictable music performances.\n\nSources\n\nGarnett, G. E. (2001). The Aesthetics of Interactive Computer Music. Computer Music Journal, 25(1), 21–33. http://www.jstor.org/stable/3681632\n\nMagenta. (2021). What is Magenta? https://magenta.tensorflow.org\n\nWeinberg, G. (2005). Interconnected Musical Networks: Toward a Theoretical Framework. Computer Music Journal 29(2), 23-39. https://www.muse.jhu.edu/article/184257.\n",
        "url": "/networked-music/2021/09/22/josephcl-algorithmic-interdependence.html"
      },
    
      {
        "title": "Telematic performance and communication: tools to fight loneliness towards a future of connection.",
        "author": "\n",
        "excerpt": "With telematic interaction on the rise during a global pandemic, we should explore telematic performances to help prevent loneliness and feelings of isolation through art.\n",
        "content": "In the midst of a pandemic, we have suffered more than ever from the emotional response that we understand as loneliness. Spending time sequestered in our houses and not being able to see our loved ones, to hug them, is almost unnatural to social beings such as humans. When we refer to these times as ‘tough’ or ‘trying’, we are not only thinking of the virus itself, but of the psychological damage it has caused in that aspect. The article “Tapes from Greece, and the Building of Community through the Telematic Medium” by Jefferson Pitcher deals very appropriately with this topic, though it was written prior to the pandemic. He writes about the need for communication in a technological era in which we feel disconnected. In the article “Telematic connections: sensing, feeling, being in space together”, Naomi P. Bennet delves into the disparity of telematic versus face-to-face communication.\n\n\n[Artist: CDD20]\n\nTapes from Greece and Building a Community\nUpon reading Jefferson’s article, I realized the loneliness we have felt during lockdown is nothing new for us humans. Pitcher writes about his initial skepticism towards telematic music performances, and how he grew to appreciate them. In order to explain his change of heart, he tells a series of anecdotes. He describes the disconnection we have suffered as a society in the process of moving from rural areas and into cities: in small towns, everybody knows and interacts with each other; meanwhile, the city is so big that you become anonymous and isolated. Then, he shares how his cousin, Beth, committed suicide. He wonders if that event could have been prevented had she not felt “cut off from the world” (Pitcher, 5) enough to leave “before the earth” took her (Pitcher, 5) and hopes that his child never feels that way. He also recounts how he sent recorded tapes to feel closer to his best friend while they were apart during a time before the internet. Upon reminiscing and reflecting on those memories, Pitcher eventually changed his opinion towards telematic music. He realized the importance of being connected in times and situations where we physically cannot. Musicians finding a way to make music together, even when they are far away from each other, reminded him of small-town life.\n\n\n[Artist: CDD20]\n\nTelematic connections: being in different spaces together\nOn the other hand, Naomi P. Bennet’s article, found in the International Journal of Performance Arts and Digital Media, analyzes how human connection differs when it is telematic. To do so, she explores two telematic performances she developed that deal with themes of telepresence and sharing intimacy via digital connection. She first explains the term ‘virtual touch’, which she uses to “describe a visceral experience in which feelings of touch can be elicited through the visual senses, transcending the presence of physical touch” (Bennet, 247). The goal of that term is to understand that telematic interaction can never be carried out as one would face-to-face interaction. Instead, she thinks that we need to re-define “what it means to be present with one another” (Bennet, 266) and the conception of ‘space’ as we usually understand it to make the most of telematic communication and performances. Next, she elaborates on the development of her telematic performances, and stresses how important it was that the actors apply the concept of ‘virtual touch’ in their performances. Despite the lack of physical presence, the people involved made meaningful and genuine connections. This proves that, while different from their physical counterparts, telematic communication and performances can still serve their purpose if done right. To conclude, Bennet emphasizes the importance of exploring the digital medium as a tool of connection, since we crave intimacy, touch and interaction, now more than ever.\n\n\n[Artist: CDD20]\n\nA third party opinion\nBoth articles deal with the factor of ‘connection’ in telematic communication and performance. However, while Pitcher reflects on the relevance of reaching out through telematic music performances, among other tools, Bennet proposes that telematic connection requires us to re-learn our conception of touch and intimacy, and how that affects telematic performance.\nPitcher’s article was enlightening and moving —however, it can come across as too focused on telematic music performance as a communication method. In other words, he writes about a very humanistic side of music, failing to consider music as a tool to experiment and challenge technology, and not just to connect and communicate. Additionally, the article’s perspective is personal and subjective, which makes for an enjoyable read, but not an informative one and not very useful from an investigative standpoint. Still, it urges us not to forget about the human factor in telematic performance.\nBennet’s article also focuses on the humanistic aspect of communication, but she shows both sides of the coin: she examines her own shows, which she created to investigate the ‘virtual touch’ phenomenon, thus providing a more experimental approach. This way, she offers technical information that might be useful for future reference. Aside from that, I find that her emphasis on shifting our conception of human touch and connection makes an original and essential point if we want to further explore and expand on telematic connection in the future.\n\n\nConclusion\nAfter experiencing a global pandemic and the isolation that came with it, exploring telematic interaction (be it communication, music creation, theatrical performances etc.) feels more relevant than ever. Pitcher’s article brings attention to our disconnection as a society and the importance of connecting through any means possible. Bennet’s article highlights how telematic and physical interactions are different but relevant in their own way. Both of them share common concerns but bring different perspectives to the table.\nFinally, I would like to add that I especially enjoyed reading Pitcher’s piece, since it was relatable enough to make me emotional. On top of that, both of the authors resonated with me in their search for creation and connection by any means possible. Art is meant to be shared, and nowadays we get to do it without distance as an obstacle.\n\n\nBibliography\n\n  \n    Pitcher, Jefferson (2009), Online supplement to LMJ 19, MIT Press Journals, URL: http://mitpressjournals.org\n  \n  \n    Naomi P. Bennett (2020) Telematic connections: sensing, feeling, being in space together, International Journal of Performance Arts and Digital Media, 16:3, 245-268, DOI: 10.1080/14794713.2020.1827531\n  \n\n",
        "url": "/networked-music/2021/09/22/sofia-telematic-performance-and-communication.html"
      },
    
      {
        "title": "Applying Actor-Network Methodology to Telematic Network Topologies",
        "author": "\n",
        "excerpt": "An inquiry into the application of an actor-network theory methodology to Gil Weinberg’s telematic musical network topologies.\n",
        "content": "\nIn his paper “Interconnected Musical Networks: Toward a Theoretical Framework”, Gil Weinberg proposes several topologies for the classification of telematic musical networks in a fourfold system, based upon a through line of musical-technological historic developments. These topologies are placed along two axes (decentralized to centralized, and synchronous to non-synchronous), and consist of connections between nodes, and in the case of centralized networks, the connection of these nodes to a hub (Weinberg 2005 p. 35). In Weinberg’s topologies, these “input nodes” are specified as “players” (Weinberg 2005 p. 34), and the centralized hub (if present) is noted to be “computerized” (Weinberg 2005 p. 33). In this way, Weinberg draws a clear distinction between the human and the technological; that is that the player input nodes create and manipulate the musical material, with the technological being the tools that facilitate this creation and manipulation. However, we can view these topologies from another perspective, that of actor-network theory, to examine the way in which the technological objects in these networks might also possess an agency of their own. Using the description of actor-network theory found in Robert Strachan’s book “Sonic Technologies: Popular music, Digital culture and the Creative Process” as a starting point, I will go on to refer to Benjamin Piekut’s paper “Actor-Networks in Music History: Clarifications and Critiques” to examine how Weinbergs topologies could be considered too generalised, and that the technologies themselves employed in the network could play a role in its definition.\n\nStrachan describes how, “digital technologies are more than mere tools; they are active and enmeshed within the creative process in a central way[, … that] they are a key part within the socio-technological-human networks that go towards making music in the post-digitization environment” (Strachan 2017 p. 8).1  In this way, “both [humans and objects] have agency [… and that] there is a dispersal of agency across differing biological and non-biological sites” (Strachan 2017 p. 8). He refers to Benjamin Piekut, stating that, “[s]eeing technological/human relationships in this way accounts for the ‘manner in which relationships in the real world multiply, overlap, and change [calling] attention to the motile web of relations that define and enable any actor’s role. The network affords an actor certain ways to work; change the network, and you change the actor’ (Piekut 2014 p. 194)” (Strachan 2017 p. 9, referring to Piekut 2014 p. 194).2 \n\nIt is worth noting here that in Piekut’s description of actor-network theory, he states that the theory, “decouples agency from intention and will. It is an action or an event – not an intention – that manifests an agency. If something makes a difference, then it is an actor” (Piekut 2014 p. 194).3\n\nPiekut argues from a music history perspective that actor-network theory should not be considered a theory, but rather, “is a methodology, [and] not a topology; it does not go looking for network-shaped things, but rather attempts to register the effects of anything that acts in a given situation, regardless of whether that actor is human, technological, discursive, or material” (Piekut 2014 p. 193). The aim of employing such a methodology, he argues, is, “[t]o provide an empirically justified description of historical events, one that highlights the controversies, trials, and contingencies of the truth, instead of reporting it as coherent, self-evident, and available for discovery[... and] is above all a methodology that helps us to attenuate normative assumptions about our object of inquiry, to put aside vague or reified concepts such as ‘music’, ‘society’ or even ‘network’” (Piekut 2014 p. 193).\n\n\n  \n  Figure 1\n\n\nIt could then be argued that when applying an ANT methodology to the examples named by Weinberg in his paper, the topologies he derives take too broad a view. In attempting to categorize historic performances simply through the creation of a series of technological connecting lines between human input nodes, we are ignoring the agencies of the non-human actors in these networks, which could lead us to overlook cultural and societal elements of such networks when, for example, attempting to recreate them. From this perspective, although John Cage’s Imaginary Landscape No. 4 and the League of Automatic Composer’s Network Computer Music would both tend towards the decentralized-synchronous quadrant of Weinberg’s axes, and would be described with an identical topology when reduced to the player input nodes (See Figure 1), this does not enable an analysis of the distinction in the networks of these performances, namely that of the differing agencies displayed by the most prominent technologies employed in the two works, the transistor radio and the computer. The agencies of these two devices differ greatly, requiring, to name but two examples, different levels of technical knowledge and financial output on behalf of the human actors. As Piekut concisely writes, “[f]or ANT, an actor need not realize, understand, or intend the difference, but it nonetheless should be accounted for in the analysis” (Piekut 2014 p. 196).\n\nFootnotes\n\n1   Strachan’s study is on digital technologies, but his description could equally apply to analogue technologies.\n\n2  It is important to keep clear the distinction between musical networks in a wider sense as referred to by Strachan and Piekut, and telematic networks as referred to by Weinberg. However, Weinberg’s telematic networks do form a subset of wider musical networks, as well as possessing their own relationships to wider cultural and historical contexts. A similar distinction is required when applying the term topology to each of these networks.\n\n3  Piekut also offers a critique of this definition under ANT, stating that, “[i]n fact, despite its provocative stance and the shrill responses it has occasioned, ANT puts forward a weak claim about agency. Something makes a difference. What is it? This minimal notion of agency fosters uncertainty about what an actor might be. From this position of uncertainty, one investigates empirically in order to specify the nature of this agency, somewhere along the ‘many metaphysical shades between full causality and sheer inexistence’.[…] So ANT does not throw out intention or consciousness altogether, but does suggest that differences get made in other ways, too” (Piekut 2014 p. 196, referring to Latour 2007 p. 72).\n\nWorks Cited\n\n\nLatour, B. (2007). Reassembling the social: an introduction to Actor-Network-Theory. Clarendon lectures in management studies, 1. publ. in pbk. Oxford: Oxford Univ. Press.\n\nPiekut, B. (2014). ‘Actor-Networks in Music History: Clarifications and Critiques’, Twentieth-Century Music, 11: 191–215. DOI: 10.1017/S147857221400005X\n\nStrachan, R. (2017). Sonic technologies: popular music, digital culture and the creative process. New York, NY: Bloomsbury Academic.\n\nWeinberg, G. (2005). ‘Interconnected Musical Networks: Toward a Theoretical Framework’, Computer Music Journal, 29/2: 23–39. DOI: 10.1162/0148926054094350\n\n",
        "url": "/networked-music/2021/09/22/hughav-ant-topology.html"
      },
    
      {
        "title": "Telematic Reality Check: An Evaluation of Design Principles for Telematic Music Applications in VR Environments",
        "author": "\n",
        "excerpt": "7 steps for better virtual reality music applications!\n",
        "content": "\n    \n\nIntroduction\nIn The Telematic Music System: Affordances for a New Instrument to Shape the Music of Tomorrow, Braasch proposes a pragmatic way of thinking about designing telematic music applications for VR, and by extension Virtual Reality Musical Instruments (VRMIs). Here I will present this perspective along with another, discuss their usefulness, and highlight the most important principles. As you will learn, some of the design principles presented are outdated given the current pace of innovations in the field. To conclude this evaluation, I present an alternative design solution based on these two papers and my own experience in virtual reality. I am proposing a modern set of actionable design choices for creating VRMIs that appeal to the average user and invites experienced musicians to innovate musically in a novel environment.\n\nThe Affordances of VR\nWhen designing a virtual reality telematic application, the only way to make it useful is to focus on the affordances of the virtual environment, the properties that determine how it most intuitively should be used (Braasch, 2009). This applies whether or not there are similarities between the telematic environment and the physical world. The current standard to which we measure the quality of a virtual reality project is most often how closely it mimics the real world, which does not translate well to musical instruments for two main reasons: signal degradation and latency. VRMIs, and virtual telematic environments, will for these reasons not be able to compete with a traditional venue. It is therefore proposed that the design process starts by answering the following questions:\n\n  What does a telematic environment provide that traditional onsite music venues cannot offer?\n  What can be communicated in a co-located (distributed) music performance that other forms of telecommunication, such as speech, cannot provide at the same level?\n\n\nThis perspective is difficult to argue against. Designing anything by utilising the weaknesses of its environment is swimming against the current. It’s difficult to do, takes much longer, and rarely produces a desired result. One thing this paper lacks, however, is specific design principles that can be measured and tested. This is where our next paper provides some insight.\n\nVirtual Reality Musical Instruments\nAs opposed to the former perspective, Virtual Reality Musical Instruments: State of the Art, Design Principles, and Future Directions (Serafin, 2016) outlines 9 actionable design principles that can be used for developing telematic music applications in virtual reality. Some of these are based on avoiding common technological obstacles rather than making decisions from the affordances of the system. Paraphrased for clarity, the complete list of design principles are:\n\n  Design for all types of feedback: auditory, visual, touch, and motion, as this helps the user learn how to use the VRMI.\n  Prevent latency.\n  Prevent cybersickness.\n  Extend physical instruments to translate skillsets.\n  Do not dismiss non-realistic interactions.\n  Prevent strain and discomfort caused by hardware.\n  Create a feeling of presence.\n  Represent the player’s body.\n  Include social interactions.\n\n\n\n    \n\n\nEvaluation\nSince the writing of Telematic Music System Affordances we are seeing more projects that play on the strengths of VR to great success, albeit perhaps not within the field of telematic music. Projects that come to mind are Job Simulator by Owlchemy Labs (2016) and Beat Saber by Beat Games (2018), both in which the user is statically positioned, relying on turning as a means for orientation and using virtual tools and objects to interact with other objects out of arm’s reach. Additionally, there have been developments in tracking movement as with redirected walking (RDW), which applies gains when the user’s path is curved (Langbehn et al. 2017). This allows for great flexibility when developing large virtual environments.\n\nHardware Limitations\nWhile preventing latency, cybersickness, and discomfort are important, I am unsure if they hold up as design principles specifically for telematic music applications and VRMIs as they are necessary considerations for any virtual reality application. Furthermore, with the release of Oculus Quest and other wireless HMDs we have greater freedom today with regards to how we design for movement without causing discomfort, and latency will always be an issue when more than one person is contributing in a virtual environment due to constraints in the physical world.\n\nFew instruments in the physical world are designed with ergonomics in focus, at least in regards to extended use. Musicians experience work related injuries every day. This is not to say preventing discomfort should be dismissed as a design principle, but rather that many instruments have had success regardless of the discomfort they provide. The question is, for how long can an instrument be played before causing discomfort, and is this timeframe acceptable for learning to play the instrument?\n\nInteraction\nWith regards to interaction, it is not recommended to represent any part of the user’s body that is not directly needed for running the application—the physical-virtual points—as this is where the user’s attention lies.\n\nOn the point of social interaction and collaborative performances, while growing, the VR audience is still comparatively small. If the instrument is too complicated, few artists would want to spend hours or days learning it to perform for a niche and select group of people with the exception of an occasional marketing stunt. I propose making VRMIs that are accessible to the average person as well as musicians, if the social aspect should truly be a principle of design. This would further allow integration with popular contemporary social VR environments without alienating their user base.\n\nDesign Principles\nWith these considerations in mind, along with the desire to provide actionable points, these are the design principles I propose for the development of telematic music applications in VR:\n\n  Design for all types of feedback.\n  Extend physical instruments to translate skillsets.\n  Do not dismiss non-realistic interactions and movement if they feel natural in the virtual environment and do not cause discomfort.\n  Do not disrupt the feeling of presence, necessary for accurate orientation and feedback.\n  Represent the user’s virtual-physical interaction points (eg. hands or feet).\n  Keep the system accessible, do not overcomplicate user input.\n  Constantly evaluate. Does user interaction feel natural or forced?\n\n\nSummary\nWhile both papers presented here have valuable and insightful perspectives, some of them are outdated. A handful of design principles covered here can and should be adjusted to better facilitate the affordances of VR and current developments. I have attempted to make such adjustments while maintaining the integrity of the ideas presented.\n\nReferences\n\n  S. Serafin, C. Erkut, J. Kojs, N. C. Nilsson and R. Nordahl, “Virtual Reality Musical Instruments: State of the Art, Design Principles, and Future Directions,” in Computer Music Journal, vol. 40, no. 3, pp. 22-40, Sept. 2016, doi: 10.1162/COMJ_a_00372.\n  J. Braasch (2009), “The Telematic Music System: Affordances for a New Instrument to Shape the Music of Tomorrow”, Contemporary Music Review, 28:4-5, 421-432, DOI: 10.1080/07494460903422404\n  E. Langbehn, P. Lubos, G. Bruder and F. Steinicke, “Bending the Curve: Sensitivity to Bending of Curved Paths and Application in Room-Scale VR,” in IEEE Transactions on Visualization and Computer Graphics, vol. 23, no. 4, pp. 1389-1398, April 2017, doi: 10.1109/TVCG.2017.2657220.\n\n",
        "url": "/networked-music/2021/09/22/olivegr-telematic-reality-check.html"
      },
    
      {
        "title": "Embodiment and Awareness in Telematic Music and Virtual Reality",
        "author": "\n",
        "excerpt": "A discisson about what Embodiment and Awareness means, and how its beeing used in Telematic Music and VR.\n",
        "content": "\n    \n\n\nIntroduction\n\nOur perception of what embodiment and awareness means in virtual reality and telematic communication is changing, leading to new ways of interpreting what a space is and how we perceive it. As communication in virtual spaces are advancing, we get more and more used to being in virtual or constructed spaces. But how do these perceptions hold up to literature and papers that came from a period where this wasn’t the norm?\nEmbodied spaces are according to anthropologists «The location where human experience and consciousness takes on material and spatial formx» [1]. Its multimodal, meaning a space can form in many ways. This can be physical, virtual, or static. Awareness is the ability of being conscious of something; to know, feel and be cognizant.\n\nDiscussion\nIn Braasch paper «The Telematic Music System: Affordances for a New Instrument to Shape the Music of Tomorrow» [2], he states that «We often tend to forget the important role of space in various music traditions….The relationship between and artist and the performing space must be interactive and include the awareness that the sound of the space, including cyberspace underlines certain boundary conditions». Braasch means that we must accept the limitations of compositions, performances, and experiences in a telematic or virtual reality. In one way you can’t do more with a system than what it’s designed to do. But Braasch also states that «Computer-generated worlds provide immense opportunities for embodiment». In Rebelo’s «Haptic Sensation and Instrumental Transgression» [3] he talks about the importance of how a space affects you. He states that space is multimodal, and it can’t be linked to any state. In a sense this is what embodiment and awareness is, a constant reference. This is different from virtual reality which can be constructed as a replica of the real world. But how could you embody a state of third or fourth dimension, and replicate states of randomness which is dependent on third party; like how much spit from a vocalist hits the audience?\nIn practice virtual reality is limited to what the end user can simulate, and what the system can provide of data. As of technology today, you can’t receive much more than auditory, visual and a few haptic feedbacks. You are also limited to the relative position you are and can’t move around as much as you want. This limits how effective you can embody replicas of actual events. But innovation is happening, with for example providing olfaction, a sense that can increase presence in virtual environments [4]. This links it to music, and the basis of creation. Spaces are to be used for exploration, creation, and innovation.\nBraasch claims that due to time and resources needed for telematic performances, it’s unlikely that musicians spend time and resources rehearsing in such a space. This brings us to today where tools for doing such is relatively easily available, and the public is used to using such spaces. In an essay by Naomi P. Bennet «Telematic connections: sensing, feeling, being in space together»[ 5], she explores how telematic connections can enlighten senses witch are lost when communicating in telematic space. She explores how «digital media has traditionally been thought as disembodied» and argues that it can lure out «intense feelings of embodied touch». She concludes that, while not perfect, virtual space has unique advantages for connection across continents, and our concept of space is evolving as we learn how this technology affect our daily lives. Bennet also tries out in practice «virtual touch», linked to Bahn’s (et al.) «Vibrobyte» [6]. Bennet wanted to link intimacy, touch, and connection to telematic communication. The Vibrobyte serves more as haptic feedback for co-located(telematic) performances. Both tries to do the same thing: linking sensation of touch over distances. Bennet’s Virtual Touch tries to make human connection over distances. The Vibrobyte angles it more towards a functional use in timing and contact with another musicians/conductor.\n\nConclusion\n\nIn conclusion, Braasch’s view of embodiment and awareness in technology is a bit outdated. He highlights the adaptation of technology as an issue to commence telematic communication. Of course, he could not have foreseen how technology would change our views of spaces. Embodiment and awareness are multimodal with different meanings for each state of conciseness. But how we traditionally thought physical embodiment was needed, is not as relevant today. We just must wait to see what is physically possible with virtual reality and telematic communication in the future.\nReferences\n\n[1] Setha M. Low (2003). Anthropological Theories of Body, Space, and Culture https://journals-sagepub-com.ezproxy.uio.no/doi/abs/10.1177/1206331202238959\n\n[2] Braash, Jonas, (2009). The Telematic Music System: Affordances for a New Instrument to Shape the Music of Tomorrow. https://www-tandfonline-com.ezproxy.uio.no/doi/full/10.1080/07494460903422404\n\n[3] Pedro Rebelo (2006). Haptic sensation and instrumental transgression https://www-tandfonline-com.ezproxy.uio.no/doi/full/10.1080/07494460600647402\n\n[4] B, Munyan, S. Neer, D. Beidel, F. Jentsch (2016). Olfactory Stimuli Increase Presence in Virtual Environments https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4910977/\n\n[5] Naomi P. Bennet (2020). Telematic connections: sensing, feeling, being in space together https://www-tandfonline-com.ezproxy.uio.no/doi/full/10.1080/14794713.2020.1827531\n\n[6] K. McDonald, D. Kouttron, C. Bahn, J. Braasch, P. Oliveros (2008) The vibrobyte: A Haptic Interface for Co-Located Performance https://asa-scitation-org.ezproxy.uio.no/doi/abs/10.1121/1.4782780\n",
        "url": "/networked-music/2021/09/23/jakobh-embodiness-in-vr.html"
      },
    
      {
        "title": "A Brief History of Improvisation Through Network Systems",
        "author": "\n",
        "excerpt": "A glimpse into the evolution of online improvisation and shared sonic environments.\n",
        "content": "\n    \n\n\nIntroduction\n\nThis post is fueled by a curiosity in exploring the evolution and history of improvising and jamming together online. As with networked music performances in general, there has been a significant progress in the field for quite some time already. Let’s dive into the topic by visiting a couple of papers on the subject. In “Displaced Soundscapes: A Survey of Network Systems for Music and Sonic Art Creation”, Barbosa makes a non-exhaustive survey of different technologies available for collaborating in music-making and creation of sonic art over different network systems (Barbosa, 2003). For this review, I will focus on his section of “on-line improvisation and shared sonic environments”. In “Dislocated Sound: A Survey of Improvisations in Networked Audio Platforms”, Mills goes even deeper into the improvisational part of networked music collaboration (Mills, 2010). In the proceeding section, I will discuss some of the early development in the field, as the sources dates back to 2003 and 2010 respectively.\n\nDiscussion\n\nBarbosa states that although a session of pure improvisation online would require the same kind of set-up as a non-improvisational one, these two approaches have major conceptual differences (Barbosa, 2003, p. 57). The improvisational approach leaves more space for individuality and spontaneity than it would when practicing and performing pre-composed music. I appreciate that he uses the term sonic environments, which to me sounds inclusive – where he places what he calls “computer network music” partly under the larger category of “sonic art”. Advancements in music technology and networking technology naturally lead to new possibilities for musicians to express themselves. Not limited to musicians, this could also welcome a much bigger part of the population not musically trained in the traditional sense. In fact, Barbosa states that “Since no musical knowledge or instrumental performance requirements can be demanded from an Internet user, a spontaneous improvisational approach is quite suitable, […]” This approach to online musicmaking will probably lead to new kinds of musical and sonical outcome, which could be thought-provoking for the traditionalists. I think sonic environments is a clever use of terminology in the sense that we don’t need to bother with any old-school definition of what music is. Let’s just get together online, make some noise, and improvise.\n\nOne of the examples Barbosa offers as platforms for improvising, is the “Public Sound Objects” (Barbosa &amp; Kaltenbrunner, 2002). This project brings together an actual physical space and a virtual presence on the internet. Multiple users can manipulate sound objects remotely, sent as control signals to the Public Sound Objects server when triggered. The server will handle all the synthesis and processing of audio. This will in turn be sent to the public installation site as well as streamed continuously back to the online users. It is worth mentioning that this software dates back to the early 2000’s.\n\nIn his paper, Mills presents an even earlier attempt at networked improvisatory performance dating as far back as 1991, when composer and performer Pauline Oliveros celebrated her 40th anniversary as a composer (Mills, 2010). This event was hosted on a video telephone transmission between six cities and sported twenty-minutes of broadcast from each city, ending in a grand improvisation between all participants. Even from the early days of networked improvisations, the technological conditions and latency was addressed and acknowledged as elements inevitably shaping the improvisation, exemplified in the following remembrance by Oliviero herself: “Since the telephone line would grab the loudest signal the improvisation was based on sensitivity to give and take. (Olivieros, 2009)” Mills continues his paper with an example of how online improvisation conceptually could attempt to be free-for-all with the “Cathedral” project by Duckworth and Farrell. Duckworth wanted his web-based multi-user environment to be as inclusive as possible, regarding the musical ability and culture of the participants. This was approached by implementing a real-time synthesizer who converted words or phrases in any language to musical sounds. Such low thresholds for being able to improvise with others isn’t necessarily always for the better, at least if we were to measure the musical quality of the output. But it sure contributes to democratizing the online improvisational scene, and I think it arguably will provide an entry-point for creative souls to discover the joys of music-creation and improvisation.\n\nMills goes through various platforms for online collaboration and improvisation, and the definition of improvisation is sometimes pretty broad. An improvisational interaction in a more traditional sense would suggest that the participants respond to each other by playing their instruments, while some of the mentioned platforms merely require users to upload sound files and mix them in response to others. This could be thought of as more of a collaborative interaction than actual improvisation. I will not attempt to define or limit what improvisation is, but I appreciate the inclusiveness of these platforms – not least from a music educational perspective. Mills finally arrives on real-time jam platforms tailored for musicians, with NINJAM from 2004 making it possible to plug in your microphone/instrument and jam with up to eight musicians synchronously. The technology behind this software will synchronize the collaborators to a “measured latency” after selecting the tempo (Mills, 2010, p. 188). JackTrip was soon to follow, relying on uncompressed audio, which would require high-speed internet access but also avoid the latency introduced by compression encode/decode algorithms.\n\nMuch have happened in this field since the advent of these papers, and some sections will expectedly feel outdated. Even more sophisticated low-latency technology has since appeared in LoLa1 – although JackTrip is still widely used today. The advancement of processing power in laptops and introduction of smartphones and tablets has been a game-changer, sky-rocketing the availability and mobility of collaborative musical interaction, with a vast library of cross-platform applications. Still, it seems we’ve had some great network platforms around for quite some time, making it possible for nearly each and everyone with a computer and internet access to engage in the joys of improvising. Meanwhile, the musicians among us are longing to log on with ever increasing speed and ever decreasing latencies.\n\n\n\n\n\n\n\nFootnotes\n[1] LoLa - Low Latency AV Streaming System\n\n\nReferences\n\nBarbosa, A. (2003). Displaced Soundscapes: A Survey of Network Systems for Music and Sonic Art Creation. Leonardo Music Journal, 53-59. https://doi.org/10.1162/096112104322750791\n\nBarbosa, Á., &amp; Kaltenbrunner, M. (2002). Public sound objects: A shared musical space on the web. 10.1109/WDM.2002.1176188, 9-16. https://doi.org/10.1109/WDM.2002.1176188\n\nMills, R. (2010). Dislocated Sound: A Survey of Improvisation in Networked Audio Platforms. NIME. https://www.nime.org/proceedings/2010/nime2010_186.pdf\n\nOliveros, P. (2009). From telephone to high speed Internet: A brief history of my tele-musical performances. Leonardo Music Journal Online Supplement to LMJ, 19, 2009.\n",
        "url": "/networked-music/2021/09/26/kriswent-network-improvisation.html"
      },
    
      {
        "title": " Don't stop the music please, but please do ",
        "author": "\n",
        "excerpt": "How we stopped the music, sliced, and restarted it to make a re-synthesized song using onset detection and spectral centroid.\n",
        "content": "\n  \n   \n\n\nIntroduction\n\nThis blog post will describe the process of taking two audio files and writing a Python program to slice and join the segments from both audio files based on the spectral centroid mean.\n\n\n“Don’t stop the music” was first recorded and released in 2007 by Rihanna. The song was written by Tawanna Dabney and Norwegian production team StarGate. Jamie Cullum recorded a cover of the song in 2009, and it is these two versions that our team decided to chop up and put together based on features of the audio.\n\nTo combine these files, we started by using librosa to separate the harmonic and percussive content of the songs. This allowed us to more accurately use onset detection on our audio files using the percussive tracks, which then let us slice up into segments based on the onsets. The harmonic versions of the segments were used to find the spectral centroid of each segment, and we then each segment for both songs in ascending order based on the average value of the spectral centroid.\n\nIn the end we added the segments together one by one based upon spectral centroid average, creating a new audio file with all the segments from the two original tracks, but rejoined in a completely new way. You can view a block diagram of our process below.\n\n\n   \n   Block diagram of our process\n\n\nIn our design, we decided to create a class for each song and stored a list of segment classes, which includes the audio array and the spectral centroid average value, in each song class. After creating a class for each segment, we sorted each segment based on the spectral centroid average value in a combined Pandas DataFrame, then used the indexes in the DataFrame to extract the correct audio segment from the class array.\n\n\n\nResults\n\nThe two songs chosen for our re-synthesis:\n\n\n    \n     Rihanna - Don’t Stop The Music  \n\n\n\n    \n    Jamie Cullum - Don’t Stop The Music (Cover)\n\n\nBy using two songs that are not really that similar but have the same foundation, we get some interesting artefacts in the re-synthesised audio. Since Rihanna’ original song is a dance-song, it emphasises the quarter notes. Cullum’s version uses sub-divisions to carry the song. This leads to the snare-drum from Cullum’s song appearing in some interesting places.\n\nSince the re-synthesized song is arranged in ascending order based on the spectral centroid, it leads to sounding a bit musical and rhythmic. It sounds like a duet between Rihanna and Cullum, fighting each other for the spotlight. The duet-fight ends with Rihanna naturally winning the battle by bringing a dance song to the fight.\n\n\n\n    \n      \n      Don’t Stop The Music, combined.\n    \n      A thirteen seconds audio sample of our combined audio, taken in the middle of the full audio where there is more of a balance between Rihanna and Cullum’s contributions.\n  \n\n\nChallenges\n\n\n  \n   Here you can see how each new segment also consists of the previous one(s). Rihanna's first segments shown in red, with opacity set to 5%. (Cullums segments in gush green are still on top of each other.) \n\n\n\nWe had an idea to plot the final audio as a traditional waveform, but with each segment color-coded to show it’s origin. This happened to be a much more complex task than anticipated. In the lack of managing to offset the coordinates along the X-axis so each new segment started where the previous left off, we approached it by filling in lots of zeros to offset them instead. Those zeros seemed so innocent, but oh my, how they teamed up against the CPU. It would often result in the code crashing. We later figured out that even if we could find a way to process and plot it all, which seemed close in sight several times, it would take too much time in the end.\n\n\n  \n   5 subplots with 100 segments each. The timer above shows that it processed each subplot in good time. But below the plot you can see it took a lot of time to actually show the final plot. \n\n\n\n\nWe tried different approaches. They were all based on using subplots with parts of the whole, to “reset” the ever-ascending pile of zeros lurking behind the scenes. In the figure above you can see how displaying 500 segments was doable, but this was almost as far as we could go without the code crashing. There were 987 segments in total, and we managed to plot 600 of them in about 3:30 minutes. We actually got confirmation – by printing a message for every 100 segments – that we were able to process them all. But there was no way it would show us the plot in the end. It took about 4:00 minutes to process, and small artifacts started to appear across the screen showing the struggle. Another approach was to make the subplots aligned horizontally without an axis and as close to each other as possible (with just a slight gap). This was the aesthetically most pleasing approach, and ended up being the way we visualized it in our code. Though for our final version, we thought it looked best with a “zoomed in” plot of only 100 segments. We found that different approaches had slight variations in time spent, but they all faced the same limits in the end. The process of finding a solution to make it work taught us a valuable lesson regarding CPU and time-consumption, and it forced us to think more cost-efficient overall.\n\n\n  \n   2 subplots of 100 segments each, with their axis removed. You can see a slight gap between the subplots. We thought this approach was the most aesthetically appealing.\n\n\nThe source code for this assignment can be viewed here. To use this program for different sound files, you must first place two new wav files in a folder called “Files” in the same directory as the code. Once this is done, change the strings “song1” and “song2” to accurately reflect the files you want to process.\n",
        "url": "/%E2%80%8B%E2%80%8Bsound-programming/2021/10/25/jakobhoy-TeamB-DontStopTheMusic.html"
      },
    
      {
        "title": "Audio Blending in Python",
        "author": "\n",
        "excerpt": "Blending audio tracks based on transients in Python.\n",
        "content": "Will it blend?\nOur most recent crazy experiment concluded with a choir of glitchy cicadas creepily singing in the middle of the night. Never heard glitchy cicadas? Well check this out:\nCicada Madness\n\n  \n    \n  \n\n\nThis audio file was created using these two field recordings:\nRecording 1\n\n  \n    \n    Your browser does not support the audio tag.\n  \n\nRecording 2\n\n  \n    \n    Your browser does not support the audio tag.\n    \n\n\nKeep reading to find out how!\n\n\nSlice &amp; Splice\nAs you heard, the audio we ended up with has an eerie vibe, worthy of being featured as a horror game ambience. Our program extracts slices from the field recordings and sequences them together based on audio data. We then export the resulting audio along with visualization and a spreadsheet to track each slice and determine if our program is fulfilling its destiny.\n\nAn overview of the process looks something like this:\n\n  Extract audio data.\n  Make cuts.\n  Compute weighted values, attach each slice to a value.\n  Re-order weighted values (largest value last), and slices along with them, to create a new audio file.\n\n\nWe approached the idea of slicing and splicing by cutting the audio file at its transients. In order to shuffle each slice of audio around we used weighted values computed from the RMS and spectral centroid mean and range of each audio file. There is a randomness to the way it sounds, but this method will actually result in the same order of slices each time the program is run, and each new pair of audio files fed into the it will shuffle the slices differently. We organized all slices in a spreadsheet in order to track down their origins at a later point. That way, we could figure out if the program did what we wanted it to do. Regarding the visualization, we chose to display the signal along with color coding to show where the slices originated. We also exported a visual representation of the data we extracted, the RMS and Spectral Centroid, for each recording. Here you can get an idea of the data we had to work with:\n\n\n\nWe extracted 275 chunks from the first audio file and 314 from the second file, for a total of 589 tiny moments of glitchy goodness. In the image below, you can see how the audio file was sliced, shuffled, and spliced back together. The orange bits are chunks that belong to the first input file, and the red bits belong to the second file:\n\n\n\nStay tuned for more wacky audio experiments!\n",
        "url": "/sound-programming/2021/10/27/team-a-python-3.html"
      },
    
      {
        "title": "Soundation Review: What to Expect",
        "author": "\n",
        "excerpt": "A concise review on the collaborative online DAW: Soundation.\n",
        "content": "Overview\nSoundation is a collaborative, online DAW which allows you to create music with people no matter the distance. We started a new Soundation project and created short segments of music to try out its different features. So without further ado, get ready for a preview of what the free version of Soundation can do, can’t do, and should do so you can decide if it’s for you before wasting your time or energy.\n\n\n\nThe Good\nThe first thing that you’ll notice when opening Soundation is it’s modern looking user interface. It is extremely easy on the eyes, we could work on it for hours with no issues. Interface control works as expected, with CTRL and SHIFT to toggle zooming and left-right scrolling with the mouse wheel respectively. Clicking and dragging along the header creates a looping region, but we haven’t found a shortcut to quickly loop around one or more regions yet.\n\nSoundation deals with long dropdown menus in a great way: the dropdown menus scroll in the opposite direction of your mouse. It works wonders with lengthy automation lists.\n\nSomething we were concerned about, this being an online collaboration tool, was how Soundation deals with one user removing things another is working on like deleting a MIDI clip while it’s being edited, or changing an instrument while it’s settings are being tweaked. This too works like you’d expect: the clip disappears, and the instrument is replaced.\n\nOn the topic of editing someone else’s work, the undo/redo buttons are local to each user, not stored in the session. This means if someone deletes a clip you are working on, only they can undo to bring it back. In our opinion, this is intuitive and expected behavior, but we understand that some might want a global undo/redo function.\n\nAs of writing this review, Soundation has 12 instruments to choose from along with a whole bunch of samples, which allows the user to create fairly intricate music. If this is not enough for you, you can of course record directly into an audio track from your mic or add audio files from elsewhere.\n\n\n  \n    \n    Your browser does not support the audio tag.\n  \n\n\nSoundation is also well documented, and there are plenty of tutorials online, both videos and articles made by the developers.\n\nTLDR;\n\n  Great and intuitive UI and interface control. Easy on the eyes.\n  Undo/Redo are local.\n  Lots of instruments and sounds to work with.\n\n\nThe Bad\nThere is one UI behaviour which doesn’t work well, and that is moving tracks up the stack. The window does not automatically scroll upward, and so you have to drag and drop the track you want to move multiple times to get to the top. Clunky.\n\nSomething to note is that there is no way to rename instrument tracks yet. They are given the name of their instrument, which can get confusing when there are many instruments of the same type getting the same name. This is especially true since there is no track color coding either, though regions/clips can be colored. Tracks can also not be collapsed.\n\nThe piano roll opens in a new window, which we know is standard in some DAWs but the option to dock it would be nice to avoid clutter.\n\n\n\nWhen inputting notes in a piano roll, we have a few concerns:\n\n  Firstly, when adjusting a note’s position, the note will keep sounding until you stop clicking on it. It’s nice with a preview, but it gets old very quickly.\n  And secondly, you can only enlarge or shorten a note from the ending of the note. To shorten a note from its beginning, you have to drag it back with your mouse, and then shorten it from the right.\n\n\nNow on to music making: a critical flaw that we found when trying out Soundation is that, when recording with a microphone, the input is recorded with a fair amount of delay, even on the lowest buffer settings.\n\n\n  \n    \n    Your browser does not support the audio tag.\n  \n\n\nDespite what you might believe, we were not off beat. This issue forces you to manually adjust the recorded clip to match with other tracks, far from ideal.\n\nAn aspect in which we found Soundation to be very lacking is in the variety of filters, which is excessively limited compared to other offline DAWs of similar characteristics. Not to mention that they can only be applied to tracks, and not to individual clips, as it happens with automation. This was a bit disappointing since it really restricts your sound design options.\n\nSome audio processing features we’re missing is the ability to synchronize sounds with the global tempo track (eg. LFOs) and pitch shifting, features we were looking for even early on. It doesn’t seem like Soundation supports CC messages at all, really.\n\nTLDR;\n\n  Limited color coding and track organization.\n  No input signal monitoring.\n  Input delay.\n  No LFO sync.\n  No CC.\n\n\nFinal Words\nSome features are locked for free users, which is to be expected. The limitations for free users include:\n\n\n  Only 3 projects.\n  Flexible export options and high quality audio.\n\n\nStill, here are some examples of what you can do with despite the limitations:\n\n\n  \n    \n    Your browser does not support the audio tag.\n  \n\n\n\n  \n    \n    Your browser does not support the audio tag.\n  \n\n\nOverall, Soundation is a solid and fun collaboration tool for beginners and veterans alike, but it could do with a couple of quality-of-life improvements and deeper features.\n",
        "url": "/networked-music/2021/11/01/sofiagon-soundation-review.html"
      },
    
      {
        "title": "Ethrio",
        "author": "\n",
        "excerpt": "Ethereal sounds from the three dimensions of music: Melody, Harmony, and Rhythm.\n",
        "content": "As a musician I’ve always been interested in having control of multiple layers of sounds happening simultaneously in a live performance framed in an environmental perspective. Also, I personally think that the sound itself, in terms of timbral properties, is a source of inspiration for composing music and sonic exploration, especially if it has to do with sounds that are considered “Ethreal” or “not from this world”, which is something that we can achieve with the use of sound synthesis.\n\nIn that sense, I took the three elemental dimensions of Western music: Melody, harmony, and rhythm, as well as audio effects, to implement a 3 in 1 instrument which can be interesting to play in terms of gestural performance. I named it ETHRIO as a reference to “Elthereal” for the sound characteristics and “Trio” for the three music dimensions mentioned above.\n\nOverview of Music Capabilities\n\nThe capabilities of this instrument can be described in the three dimensions:\n\nMelody is performed through a slider that is mapped with an A minor scale.\n\nHarmony is played by leaning the top-body towards the instrument to navigate in a simple 3-chord sequence (Am, Em, Dm).\n\nRhythm is achieved by playing Euclidian Rhythms in which the performer can change the configuration and speed of the patterns with just one button and a knob.\n\nThere are other features that will be presented later about details for sound and mapping.\n\nDesign and Implementation\n\nPhysical Interface\n\nThis solution uses the Bela platform as processor. The inputs are collected through the following sensors:\n\n\n  Soft Potentiometer.\n  Photoresistor.\n  Button.\n  Proximity sensor.\n  Rotatory potentiometer.\n\n\nAn RGB led is included as an output for visual feedback. The connection diagram for the electronics parts is depicted below.\n\n\n   \n   Ethrio - Electronic Components Diagram\n\n\nA physical prototype was developed in a way in which the system reflects stability and easy manipulation from the performer. The complete device is shown in the next picture.\n\n\n   \n   Ethrio - Physical Interface.\n\n\nMapping and Sound Generation\n\nThe following table explains the sound synthesis technique and mapping from the considered three music dimensions and effects such as pitch bend, LPFs, reverb, and delay. The implementation was performed in Csound.\n\n\n   \n   Ethrio - Sound Generation and Mapping.\n\n\nLive Performance\n\nBelow you will find a video showing the way this instrument works.\n\n\n   \n   Ethrio - Live Performance\n\n\nEvaluation\n\nThis instrument allows to achieve a good aesthetic easily since it contains predefined musical structures that can be explored by the performed through the sensors. However, there are challenges for having a good balance when interacting with the three dimensions. This balance has to do with the way in which the photoresistor is used since there are several parameters mapped to it.\n\nOne of this challenge is the pitch bend, since there are other sonic elements happening when the pitch is one tone above for the melody and can be disruptive when combining with the other instruments.\n\nAdditionally, the mapping of distance regarding the proximity sensor and the top-body to navigate in the chords should be balanced properly so that it feels more natural. Now it is difficult to find the right spot for a chord and keep playing it without an undesired change.\n\nRegarding the rhythmical patterns, the combination between the pattern and the delay effect is interesting to explore, however it is not clear enough when the pattern changes since the delay mask some of those changes. Perhaps an improvement of the visual feedback could help.\n\nAn interesting feeling, especially because of the rhythms, it feels like the musician is playing with someone else that accompany the performance. It is something that could be explorer further is terms of responses from the system for a human-machine improvisation session.\n\nFurther development of this instrument is considered for expanding the music capabilities (not just adjusted to a fixed scale or chord progression). Also, there is room for experiment with other sound features and timbral control to increase the “Ethereal” feeling in terms of sonic features.\n\nReferences\n\n\n  \n    Hunt A., Wanderley M. and Kirk R. (2000), Towards a model for instrumental mapping in expert musical interaction, in Proceedings of 2000 International Computer Music Conference\n  \n  \n    Hunt, A., Wanderley, M. M., and  Paradis, M. (2003), The importance of parameter mapping in electronic instrument design, in  Journal of New Music Research, 32(4).\n  \n  \n    Wanderley M. and Depalle M. (2004), Gestural control of sound synthesis, in Proceedings of the IEEE 92.\n  \n  \n    Birnbaum D., Fiebrink R., Malloch J., and Wanderley M. M. (2017), Towards a Dimension Space for Musical Devices by in Proceedings of the 5th international conference on New Interfaces for Musical Expression.\n  \n  \n    Toussaint G. (2005), The Euclidean Algorithm Generates Traditional Musical Rhythms, in BRIDGES: Mathematical Connections in Art, Music and Science.\n  \n  \n    Schereiber, I. (2009). Level 4 . 5 : Emergence. Game Design Concepts. Opgehaal van https://learn.canvas.net/courses/3/pages/level-4-dot-5-emergence\n  \n  \n    O’Modhrain, S (2011), A framework for the evaluation of digital musical instruments, by, in Computer Music Journal\n  \n\n",
        "url": "/interactive-music/2021/11/01/pedropl-ethrio.html"
      },
    
      {
        "title": "D'n'B",
        "author": "\n",
        "excerpt": "Exploration and design of the ‘Drum and Bass’ interactive music system with Csound\n",
        "content": "Introduction\n\nInspired by the raw nature of the low frequencies, I envisioned a powerful bass synth in combination with drums leading me to design “D’n’B”.\nD’n’B is FM based synth consisting of 3 oscillators and a sample triggering drums along with it. This system is a multi dexterous instrument with other features that control the parameters of the synth.\n\nMusicality\n\nThe bass synth is monophonic, has a fluid pitch control and the fundamental oscillator is limited to bass frequencies upto 220 Hz, but with 3 oscillators in series it is tuned to cover a larger spectrum.\nThe performer also has vibrato and timbre control, implemented for greater expression.\n\nFor the rhythm section, drum samples can be triggered on pressing the buttons. 5 different samples are mapped to each button.\n\nDesign and implementation\n\nThe system is dived into 3 main sections. Input/output section(BELA), the bass synth, the drums.\nI’ve arranged it such that the bass synth is on the left, BELA in the between and the drums on the right.\nThis is an ambidextrous system and can be interchanged according to the performer.\n\nBelow you will see the schematic diagram of the system.\n\n\n   \n   Ethrio - Electronic Components Diagram\n\n\nComponents used\n\n\n  Soft Potentiometer\n  Flex sensor\n  Rotatory potentiometer\n  slider potentiometer\n  Push buttons\n  LEDs\n  Resistors\n  BELA\n\n\nBass synthesizer\n\nThe main oscillator of the synth is an ‘foscil’ which is based on frequency modulation and The wave form used in the demonstration is a based on a distortion table. Other waves such as sine, saw-tooth and square waves can also be used.\nIt has amplitude envelope, a frequency sweep and a logic which attempts to separate frequency from resonance.\nThe soft potentiometer is mapped to the frequency of the 1st oscillator and the fundamentals are limited from 80Hz to 220Hz.\nThe second oscillator is a high pression ‘poscil’ which is used with an lfo of square wave(uni-polar) with an increasing rate mapped to the soft-pot. This gives the synth a weird low thumping noise which increases gradually or suddenly if you slide up on the soft-pot.\nThe Third oscillator is a ‘vco2’ which is multiplied by 0.7 times the frequency of the first oscillator. This gives it nice mids and high frequencies complementing the main oscillator.\n\nEffect Chain\n\nReverb and Low pass filter are in series respectively.\nThe Reverb mix is mapped to the the rotatory potentiometer\nThe Low pass fitler’s cut off frequency is mapped to the flex sensor, which is to be attached to either thumbs. Moving the thumb inward and outward controls the filter parameter.\n\nDrums\n\nThe drums section is a simple triggering mechanism mapped to push buttons and for visual presentation, each button is linked to a LED.\nThere are 5 buttons which are mapped to 5 different sounds(samples).\n\nDemonstration\n\nBelow you can watch the demo video.\n\n\n   \n   D'n'B Demonstration\n\n\nReflections\n\nBuilding this instrument in such a short time made me realize the effectiveness of time management and planning accurate details before hand. This instrument is interesting due to the fact that it has complete control of the notes the performer plays, it is very sensitive to horizontal movement, every small variation is noticeable sonically, it involves playing rhythm and melody and it is quite hard to master playing it with two hands.\n\nIf we had more time, I would work intensively on the sound synthesis and using it in various musical contexts to see how far it can go.\n",
        "url": "/interactive-music/2021/11/01/lindsay-D'n'B-ims.html"
      },
    
      {
        "title": "Sequencephere-Linesequencer",
        "author": "\n",
        "excerpt": "Exploration and design of a drum Sequencer and synth using Bela as an interactive music system with Csound\n",
        "content": "Inspiration\n\nWhile looking for inspiration for my system, I landed up on this instrument on the Bela blog called Opal rhythm computor, this basically is a drum machine &amp; sampler and then I found this device called Orba which is a synth, sampler and controller all in one. I always wanted to keep my instrument simple, straightforward and easy to use, so I decided to combine this two design aesthetics and try to build a sequencer. Although in the end I ended up making a sequencephere, but at the end, the classic moment of when you really want to something work and it doesn’t, happened, and my project was evolved into a drum sequencer that could record a pattern and play it in a loop, combined with a softpot synth. So with one hand you could play a melody on the softpot and with the other you could play the drums on top of it.\n\nKey Features and Functionalities\n\nThe sequencer has four buttons arranged in a sequence. Each of these button is mapped to an audio file, that is triggered when the button is pressed. These buttons when triggered plays the sample in a loop, and the user can keep updating the pattern, while pressing the button, the samples are quantified automatically to the closest beat. There are LEDs placed along with the buttons for providing a visual cue to understand what samples is being triggered and when is it triggered in the loop. In addition to this, a Softpot is used to create a digital saw wave synthesizer to play a melody on top of the drum sequencer.\n\nDesign and Implementation\n\nThe way I envisioned it to be and got pretty close to finish it is shown below in the figure.\n\n\n   \n   sequencephere\n\n\nIn the ideal version, there are 6 buttons arranged in a circular fashion, that are used to trigger samples. These a softpot is placed at the bottom to play melodies on top of the drum sequencer, a rotatory potentiometer is used to control the speed of the drum sequencer and a flex potentiometer and other sensors are used for audio effects. The user can program the sequence with one hand and play the melody with other hand on the softpot. The user can also control the audio effects and tempo the sequencer using the rotatory potentiometer. There are LEDs placed along with the button for visual feedback. A switch is placed in between to reset the pattern to scratch.\n\nBut in the working model shown in the diagram below I have four buttons which are triggering the samples as intended and the corresponding LEDs glow when they are triggered. This buttons are arranged sequentially and are looped. So if a user for example triggers a kick sample using the first button in the first bar, the kick will be played in the loop in every first bar, and user can then keep updating the pattern by pressing the button and adding more samples. There is also a softpot as intended to play a melody on top of the pattern, and a rotatory potentiometer to adjust the tempo of the pattern.\n\n\n   \n   Linesequencer\n\n\nThe list of Components I have used are as follows:\n       1. BELA board\n       2. 4 LEDs\n       3. 4 Push buttons\n       4. Softpot\n       5. Rotatory Potentiometer\n       6. Resistors\n\nThe system design is pretty simple and has one to one mapping as shown below:\n\n\n   \n   System design\n\n\nPerformance\n\nBelow you can watch the demo video.\n\n\n   \n   Demonstration\n\n\nFuture aspects and Reflections\n\nThis was one of most exciting course of the SMC and was really fun to design a system from scratch. Although the system that I was able to design was kind of basic but I feel I got to learn a lot in this course and is my main outcome of the course. Due to some circumstances out of my control I wasn’t able to finish the project as I intended to but this again taught me about time management and some very valuable moral lessons about living in Norway. But for the future prospects I would like to make it as a one cohesive system in which there are more sensors being used to control some audio effects, a switch to restart the system, and also increase/decrease the range of synth, change the generating wave of the synth and also have more control over the buttons, in the sense of velocity and other dynamics. Overall making the system more controllable and user friendly but trying to keep it simple and straightforward, so that anybody irrespective of there knowledge about music can play and have fun with it.\n\nReferences\n\nMapping strategies for musical performance, by Hunt A. and Kirk R. in Trends in Gestural Control of Music, 2000.\n\nDrummond, J. (2009). Understanding Interactive Systems. Organised Sound, 14(2), 124-133. doi:10.1017/S1355771809000235F\n\nTetsuro Kitahara, “Smart loop sequencer: An audio-based approach for ease of music creation”, The Journal of the Acoustical Society of America 140, 3090-3090 (2016)\n\nPrince, D. P., &amp; Perugini, S. (2018). An application of the Actor model of concurrency in Python: A Euclidean rhythm music sequencer. Journal of computing sciences in colleges, 34(1).\n",
        "url": "/interactive-music/2021/11/03/abhishec-cirquencer-ims.html"
      },
    
      {
        "title": "Satellite Sessions - Connecting DAWs",
        "author": "\n",
        "excerpt": "A review of Satellite Sessions, a plugin that connects digital audio workstations and creators.\n",
        "content": "\n  \n\n\n  { What?\nA plug-in that lets you collaborate over different\ndigital audio workstations. It works as a Google\nDrive, or dropbox companion that skips importing\nand exporting audio from your DAW.\n}\n\n  { Support?\nOfficial support for Ableton Live 10/11,\nLogic Pro, FL Studio, Cubase, Studio One,\nReason, Reaper, Bitwig, Digital Performer,\nGarageBand and Pro Tools.\n}\n\n  { Price?\nAs of now, Free! They may release a pro\nversion in addition to the free version.\n}\n\n\n\n\nWhat is Satellite Sessions?\nSatellite Sessions plugin-bundle by Mixed In Key is a free collaboration tool used between digital audio workstations. It works similarly to dropbox and other file-sharing platforms, but specialises it’s workflow to a DAW. Satellite Sessions opens as a plugin in your DAW, where you can share audio and MIDI files with other collaborators. You can easily set project attributes, to make sure you work in the same key and tempo so there will be no conflicts when working with the same project. We chose this as our tool for asynchronous music collaboration as we thought this could have real practical value. You won’t need to learn a new DAW in a browser or app, as you can stick to your own preferred DAW and workflow in music-making.\n\nThe plugin-bundle comes in three parts; Satellite Session, Satellite Audio and Satellite MIDI. Satellite Session works as the portal to transfer and receive files, added to an empty MIDI track. Satellite Audio and MIDI lets you transfer your audio or MIDI files respectively into your Satellite Session. Satellite Audio can either be added to an audio or MIDI track, to send audio files. Insert the plugin first in the insert chain to just send the dry signal, or place it later to capture any plugin up to the point that Satellite Audio is located. Satellite MIDI can be added to MIDI tracks to share MIDI files. When adding these plugins to your tracks, new tracks will automatically appear in your Satellite session. The plugin can also preview shared MIDI files by pressing the keyboard icon. This is a nifty feature for quick pre-listening before importing MIDI to your DAW and choosing a softsynth for MIDI playback. You arm the ‘upload cloud’  in the plugin of the tracks you want to share, press play in your DAW, and the Satellite Session will record and upload the armed tracks. The timeline and bars in the Satellite Session correspond to that of the DAW. Uploaded tracks will instantly show up in the other collaborators corresponding Satellite Session.\n\n\n\n  \n\nSatellite Sessions plugin window\n\n\n\n\n\nTo invite a collaborator to your Satellite Session, just add their email in the ‘Invite’ menu and they will get an email with a Session Invitation Code. You can easily sign up and log in with your Google account or preferred email. You can also choose to make your session public, and anyone with your Invite ID can view the project. With ‘Chaos Mode’ enabled, everyone can also contribute with Audio/MIDI tracks.\n\nInstallation is quite simple. As mentioned previously; it works just like any other plugin. On Mac/OS X you will need to be running 10.13.0 or higher. On Windows you need Win8 or higher. It’s compatible with most modern DAW’s that support plugins and timeline workflow.\n\n\n\nIn practical use\nTo collaborate with Satellite Session is a breeze compared to sharing files over Dropbox or Google Drive. Since both participants already are synced to the same time-measure, tempo and key, the sharing of tracks straight to the timeline could not be easier. The plugin already has a built in file-system that follows the DAW’s session. This makes it effortless to import audio to and from Satellite. But if you want to extract the tracks as audio files to your hard-drive, you have to import and bounce them through your DAW first. It would have been nice to have a root folder visible in the plugin, which makes it easier to extract without bouncing.\n\nWe tried out the software by creating a song together, in which Jacob was using Logic Pro X and Kristian was using Ableton Live 11 Suite. This worked surprisingly smoothly, and we uploaded both recorded instruments and MIDI-instruments as well as FX busses. The upload of the collaborators files seemed to happen instantaneously, with the quality set to ‘Ultimate’. What the difference between ‘Ultimate’ and ‘Fastest’ in Satellite Sessions actually means, and if it compresses the audio somehow, still remains a mystery to us. We found no documentation or mention of this topic on their website, but audio uploaded appeared as .wav-files with bit depth and sample rate similar to the project settings when imported into our collaborators DAW.\n\nAs an experiment, we also tried to join our Satellite session through BeSpoke, a new modular DAW freed from the traditional timeline layout. This was not a great success, as BeSpoke would fast-forward the Satellite plugins timeline by a ratio of about 100 bars per one beat. We managed to extract tracks from Satellite to a “sampleplayer” module in BeSpoke for playback, though.\n\n\n\n\n  \nCan you spot Yuri?\n\n\n\n\n  \n    \n\n  \n\nThe result: \"Yuris Satellite\", a tribute to Yuri Gagarin.\n\n\n\n\nOne of the few downsides we found was the email notification system, which seems not to be responding very rapidly. Kristian got an email at 19:29 saying Jakob had made three updates, with a “screenshot” of a generic session. This was actually 3 updates made at 13:23 when we tested the plugin together at SMC. This could lead to minor confusion, but is no big deal if you’re aware of it.\nAnother minor issue would happen when you upload tracks with Audio/MIDI Satellite plugins to your session, but forget to save the project locally in your DAW. Those tracks in the Satellite can not be re-assigned to your local tracks anymore, even if you give tracks similar names in a new project. No biggie, but you’ll lose the flexibility to replace/update the content of those specific tracks, which you’re able to do when opening a saved project with your Satellite session.\n\nIn addition to sharing individual tracks, there is nothing stopping you from adding Satellite Audio plugins to buses (drum bus, reverb bus, etc.) or even the master track as well. This makes the plugin really flexible, and it’s easy to share stems from your projects. When mixing the song, we used a reverb from Jakobs plugins and a delay from Kristian.\n\nAutomatic naming of tracks added to Satellite Session, a live audio feed between the DAW’s, as well as a proper file-system with dedicated folders for each session would have been nice to see added. When mixing, or talking about details in the song over video calls, we missed having a live audio feed like audiomovers built into the software. The files could not be transferred without bouncing the audio as well. A professional always wants to know where the files are located, so they can be sure everything works when reopening the session on a different computer.\n\nAll in all this is a great tool for audio file sharing on-the-go. It skips the tedious process of   bouncing stems, uploading to a file-sharer, downloading the files on your companions computer and importing them to your DAW. This enhances the creative process, and gives more freedom to its users.\n\n\n\nIf you want to listen to the full song, you can download it here.\n",
        "url": "/networked-music/2021/11/03/jakobhoy-SatelliteSession.html"
      },
    
      {
        "title": "The algorithmic note stack juggler",
        "author": "\n",
        "excerpt": "Interactive composition with the Algorithmic Note Stack Juggler.\n",
        "content": "When I sat down to start this project, I was staring blankly at the collection of wires, buttons and various un-identified electronic components I had been given, and my 8 year old son came over to ask what I was doing.\n\n“Well, my project is to build some sort of musical instrument. Something that will make beepy noises I expect” I said.\n\n“Ah, so like note-blocks in Minecraft?”, he replied.\n\nIt was at this point that the idea was born. Or, well… stolen?\n\nFor those of you unfamiliar with Minecraft and its note-blocks, each tap on a note block steps through the notes of a scale. Emil and I realised that if we lined up several note blocks and each tapped on different blocks in a random order, it started to sound quite interesting. With some practice, we could generate some lovely melodies. But you wouldn’t exactly call our blocks an expressive musical instrument… or would you?\n\nConduits vs Collaborators\n\nOne view on musical instruments is that they are conduits through which ideas and meanings can be passed. The idea is that expressivity is paramount, and there should be as little as possible in between the musician and the expression of their ideas. Our Minecraft blocks probably wouldn’t rate that highly as a musical instrument from this viewpoint - you tap them, a note comes out. We don’t even get much choice in which note comes out - they simply run through a scale.\n\nThere is an alternative way to view musical instruments however. What if an instrument is looked at as more of a musical collaborator, a tool the musician works together with to create musical ideas? This idea of shared control between a musician and an instrument was first explored by Joel Chadabe in the 1960s and 70s. He developed an approach he would later call ‘interactive composition’, a “performance process wherein a performer shares control of the music by interacting with a musical instrument” (Chadabe, 1984). Drummond (2009) wrote that these interactive systems have the potential for variation and unpredictability in their response, and depending on the context may well be considered more in terms of a composition or structured improvisation rather than an instrument. From this perspective, our Minecraft blocks could be considered an interactive system, and I quickly set about building my own version with a Bela, breadboard and a ton of wires, buttons and LEDs.\n\nThe build\n\nThe main idea was to have 3 stacks of notes which could be triggered independently using clicky buttons. Each note stack would consist of different chord tones - 1st and 5th for the first stack, 1st, 3rd, 5th and 7th for the second, and 1st, 3rd and 7th for the third. I also added controls to manipulate the note stacks, including a button to reverse the note order, and a pot that would allow for transposition of the chord tones. For tonal variation, I added a filter cutoff knob, and a knob that brought in a second oscillator.\n\n\n\nThe sound generation part was written in csound, and consisted of 2 oscillators - a square wave and a triangle. The square wave would be the main oscillator, with the triangle adding the bass notes, following its own related note stacks of firsts and fifths. The oscillators are then run through a Moog ladder filter, a delay and a reverb. There is limited processing power in those wee Belas, especially when you start throwing around ladder filters. To keep things under control, three csound instruments were used. The first handled the controls and managed the note stacks, a second handled the sound generation, and a third master instrument added the filter and effects.\n\nPerformance\n\nHere is the Algorithmic Note Stack Juggler in action (and yes, I know its a terrible name, but no-one was forthcoming when I asked for suggestions at the beginning of my presentation).\n\n\n   \n\n\nReferences\n\nDrummond, J. (2009). Understanding Interactive Systems, Organised Sound, Cambridge Core. https://www-cambridge-org.ezproxy.uio.no/core/journals/organised-sound/article/understanding-interactive-systems/BF81A560B5C9D96355BC400065C7A1DF\n\nMudd, T. (2019). Material-Oriented Musical Interactions. In S. Holland, T. Mudd, K. Wilkie-McKenna, A. McPherson, &amp; M. M. Wanderley (Eds.), New Directions in Music and Human-Computer Interaction (pp. 123–133). Springer International Publishing. https://doi.org/10.1007/978-3-319-92069-6_8\n\nChadabe, J. (1984). Interactive Composing: An Overview. Computer Music Journal, 8(1), 22–27. https://doi.org/10.2307/3679894\n\nDahlstedt, P. (2018, February 22). Action and Perception. The Oxford Handbook of Algorithmic Music. https://doi.org/10.1093/oxfordhb/9780190226992.013.4\n",
        "url": "/interactive-music/2021/11/03/stephedg-notestack.html"
      },
    
      {
        "title": "The Triadmin",
        "author": "\n",
        "excerpt": "An instrument without any tangible interface.\n",
        "content": "The Triadmin\n\nMotivation\n\nAt the start of this project, my goal was to design an instrument that do not have any button or tangible interface on which pressure has to be applied to generate sonic the output. The main inspiration for this idea is the instrument invented by Leon Theremin in 1928.\n\nFor the musical output itself, what I wanted was for the system to play a base pitch while playing harmonized triads on top, that would accompany the base pitch. These accompanying notes would be in the triad an octave above the base pitch, and would create a pleasant sound in accompaniment.\n\nSensors\n\nUltrasonic sensor\n\nThe ultrasonic sensors measures the distance from the sensor to the nearest obstacle, by using the sound waves it shoots out. In my system, the way this works is that the distance the sensor detects is translated into what pitch the base pitch will be playing at. This type of sensor has been used before, for example in (Torre, Andersen and Baldé, 2016), they used the distance picked up by the sensor to determine the velocity of the note being played.\n\nLight Dependent Resistor\n\nThe light dependent resistor measures the light level in the room, and in my system that is translated into how often the the ultrasonic sensor shoots out a signal, and therefore how fast or short a note played by the system would be. To cover the sensor, either the natural lighting can be used, or you can partially cover it with something like a black foam. Light dependent resistors have also been previously used by (Mandoux and Wohlthat, 2004), in an attempt to replicate the scratching of turntables, where the light sensors turned the scratching on or off.\n\nElectret Microphone\n\nThe electret microphone is a microphone that records the soundwaves it receives, and in the system the amplitude of these soundwaves are what determines what volume the accompanying sounds are playing at. This will make it so the louder the audio the system hears, the less volume the accompanying sounds are playing at.\n\nSpeaker\n\nThe speaker will output the audio the system creates, and will therefore produce the audio the microphone hears. In addition, the speaker will be moved in front of the ultrasonic sensor and therefore be what determines the base pitch of the audio output.\n\nHow to use the system\n\nThe intended way of using the system is to hold the speaker in the left had, meanwhile holding a black foam in the right hand. The hand holding the speaker will move towards and away from the ultrasonic sensor, which will also affect the volume the microphone hears. The arm can also twist the microphone in different directions to adjust what the microphone hears. The right hand, holding the black foam, will cover and uncover the light dependent resistor and therefore the tempo of the system.\n\n\n\nThis is how the system has been wired up:\n\n\n\nPerformance\n\n\n   \n   Performance\n\n\nEvaluation\n\nThis evaluation is done in accordance with (O’modhrain, 2011)\n\nHaving presented this system to audiences, the audience has described the system as intuitive and easy to understandable, and have also wanted to try it out themselves and play with it.\n\nAs a player, I found the system to be fun and intuitive, but there are some frustrations in regards to the sensors. The light dependent resistor is very intuitive and works well, however the ultrasonic sensor can be a bit finnicky and miss your signals, partially just because the speaker is so small. Lastly, the microphone does not appear to have any effect at all, however that may be due to coding errors and not so much the microphone.\n\nFrom a designer perspective, the system does appear to have hidden affordances, or alternate ways to play the instrument. For example, by covering the light sensor completely, the system changes pitch very fast, including the supporting notes, leading to them creating this space-noise like sound which I ended up using in the performance.\n\nAccording to the system classification by (Wanderley and Depalle, 2004), the system consists mainly of secondary active feedback, which means that the feedback comes from the audio being produced by the system, and not much else. The main problem with this is that it makes it hard to learn the system and properly use it, so if you need a specific pitch you need to work it out by feel as opposed to getting exact feedback.\n\nAnother consideration is that while my initial motivation was to create a system where you have no physical interaction, however as it became designed the system ended up getting quite physical as you’re both holding the speaker and the black foam.\n\nFuture Work\n\nFor future work, I would like to have some sensors that fulfill the same role, but are able to work better in the case of the ultrasonic sensor, or to get the microphone finished coding. Another thing I’d like to do is make more feedback options to make the system more intuitive in a setting with multiple instruments.\n\nReferences\n\nWanderley, M.M. and Depalle, P., 2004. Gestural control of sound synthesis. Proceedings of the IEEE, 92(4), pp.632-644.\n\nO’modhrain, S., 2011. A framework for the evaluation of digital musical instruments. Computer music journal, 35(1), pp.28-42.\n\nTorre, G., Andersen, K. and Baldé, F., 2016. The Hands: The making of a digital musical instrument. Computer Music Journal, 40(2), pp.22-34.\n\nMandoux, F. and Wohlthat, S., 2004. 2F1213 Musical Communication and Music Technology Design of a HCI for Skipproof.\n",
        "url": "/interactive-music/2021/11/03/halvorsh-triadmin.html"
      },
    
      {
        "title": "SkyTracks: What’s the Use?",
        "author": "\n",
        "excerpt": "A review and critique of the online DAW SkyTracks for asynchronous collaboration\n",
        "content": "\n   \n\n\nIntroduction\n\nThis article will review and critique the online DAW SkyTracks, and should have ended with a collaborative track made using the software. SkyTracks is a DAW with an emphasis on asynchronous collaboration. Since it all takes place in web browsers, there is no need for any installation. There are three price tiers, and for this review we have decided to use the free tier. We chose this software due to its seemingly simple design, as well as the option to use it without paying.\n\nExperiment\n\nDue to complications with SkyTracks, which we will describe in more detail below, our use of the software was relatively simple. One group member started out adding a few tracks, and then the other group member added onto those tracks asynchronously to create a final track. Both contributors worked on Mac, iPad and iPhone for this collaboration.\n\nResults\n\nA brief rundown of SkyTracks’ features\n\nThe user interface looks like a simplified DAW, and promises to be intuitive. The recommended browsers are Opera and Chrome, and if you open it up in any other browser you will be warned that there are features not available. Trying different browsers, we were not able to identify any of these features.\n\nAdding a new track is simple, and you can choose between an audio, MIDI, and drum track. The MIDI track comes with only one sampled instrument: the acoustic grand piano. SkyTracks has its own built-in drum machine that is intuitive and gives you a dedicated 8 step sequencer for each drum sample you wish to program, with three kits included. It works like a charm until you realize that the processing breaks down after about three drum samples and plays back completely, uselessly glitchy.\n\n\n  \n    \n  \n  Example video of a drum track with three drums playing at 153 bpm; the tempo drifts as SkyTracks struggle to play back and the sounds clip like crazy\n\n\nThe tempo speeds up and down trying due to processing, and the samples either clip or don’t play back at all.\n\nImporting or recording an audio file is also fairly simple, until you try editing it. FX can be added to all the tracks separately, but just one instance of reverb on our audio track made the project impossible to play back. You can save different versions as you work, so it should be simple to edit your friend’s track, save it as a new version, and let them review your changes. SkyTracks even promises the ability to import PDFs and video into the project, but as three MIDI tracks crashes the project, these promises seems utopian.\n\nEditing\n\nEditing anything is a nightmare, until you realize that you’re actually not able to edit anything at all. Slicing and resizing audio is supposed to be simple, but no matter what web browser or computer we tried, it didn’t work satisfactory. Editing MIDI was not possible either and few to no results came out of endless attempts at moving even just a single note.\n\nIssues\n\nDid we mention that SkyTracks barely manages to play the four tracks we ended up successfully laying down? Our final song has two MIDI tracks, an audio track and a drum track. Opening up the project in any browser and hitting play usually resulted in complete silence. Soloing a track and playing it back was sometimes the only solution to hear anything at all.\nThere’s also a SkyTracks app in the app store, and after going through hours of frustration we hoped this would end our struggles. But it turns out the app only gives you playback options, and no editing is allowed whatsoever. We can still not grasp the use of this app, and all it did was add to the list of frustrations.\n\nSkyTracks also lacks the ability to troubleshoot the glitchy aspects of playback. While we were not able to test SkyTracks with a wired internet connection, we believe that our wifi speed was sufficiently fast enough for our purposes. The SkyTracks blog features an article explaining sample rate and bit depth, yet there are no options to change the bit rate, sample rate, or buffer size. Instead, an “HQ” button on the top menu bar is included that does not change anything. Adding the ability to change the aforementioned audio parameters could vastly improve the SkyTracks program by not overloading your computer’s CPU during playback.\n\nSkyTracks promises a lot if you are willing to pay the subscription, but without a functionable free tier, there is little incentive to sign up for a subscription. We hope they somehow increase the processing power and editing abilities on the higher tiers, but as a free async software it was completely useless. The different tiers of subscription are Basic (Free), Pro (6.95$/month) and Studio (15.95$/month). The paid tiers offers more storage and a VST plugin which allows you to work in your native DAW and add tracks to SkyTracks from your DAW.\n\nSubscriptions and tiers seems to be the trend in both async collaboration software and DAWs these days, and long gone are the days where you could get a full product for free. We absolutely do not recommend SkyTracks for any musical collaboration use.\n\nIn order to have any music to show for at the end of this review, we would have had to start from scratch by download the sample audio track, open a project in a DAW, and reprogram the drums and MIDI piano track. Trying to export our track from SkyTracks led to  minutes of watching the export bar hanging half-way, until we were forced to quit the browser. Instead, we give you this video of how SkyTracks attempts to play back our seemingly humble project:\n\n\n  \n    \n  \n\n\nConclusion\n\nThe idea of an intuitive, browser-based DAW for async music collaboration is not without its merits. But clearly SkyTracks was not the company to successfully implement this idea. \nIf this was what a browser-only, free async music collaboration software could deliver in 2021, we would highly recommend musicians to return to shared folders in cloud services and work in their native DAWs. Thankfully some of SkyTracks’ competitors delivers more actual functionality.\n",
        "url": "/networked-music/2021/11/03/josephcl-skytracks-review.html"
      },
    
      {
        "title": "EarSketch (Or How to Get More Python in Your Life)",
        "author": "\n",
        "excerpt": "A review of the asynchronous music production software ‘EarSketch’\n",
        "content": "Introduction\n\nEarsketch is a browser based compositional tool developed by the Georgia Institute of Technology (Georgia Tech). The aim of its development was to create an accessible environment in which high school and undergraduate students would be able to gain skill in the area of music technology and computational composition, as there are often high financial and practical barriers when it comes to providing students access to music software within educational institutions.\n\n\n  \n\nHugh and Joachim EarSketch session\n\n\n\n\nEarsketch takes the form of a familiar DAW-style interface with a commercial digital audio workstation program (Cockos’ Reaper) that can be opened in a web-browser. Users can assemble songs from a library of nearly 4000 samples, add effects and transitions, and perform some basic audio analysis. However, there is a catch! Instead of a drag and drop, menu-based system found in most DAWs, Earsketch requires the user to program their track using either the JavaScript or Python programming language. Joachim and Hugh decided to test their Python skills and see what they could come up with.\n\nExperiment\n\nJoachim started the session by initializing the Python script and set a 100 BPM tempo using the following code:\n\nfrom earsketch import *\n\ninit()\nsetTempo(100)\n\n\nHe then accessed the sound collection to filter some FUNK and NEW_FUNK samples and started arranging the chosen instrumental samples using the fitMedia() function which has arguments to specify the file name, track number, start location, and end location.\n\nfitMedia(fileName , trackNumber , startLocation , endLocation)\n\nHe then laid down some vocals by making a quick record using the in-built sound recording feature. Earsketch also allow the users to upload sound and use sounds of the collaborative database of Creative Commons Licensed sounds Freesound.\n\nJoachim then shared the script with Hugh by sending an url shortcut file.\n\nHugh then could added some effects such as compression, delay, phasors, and reverb as well as setting some volume automation, using the setEffect() function.\n# Effects\nsetEffect(2, COMPRESSOR, COMPRESSOR_THRESHOLD, -4.0)\nsetEffect(2, VOLUME,GAIN, -20, 1, 6, 5)\nsetEffect(7, DELAY, DELAY_TIME, 500.0)\nsetEffect(8, VOLUME,GAIN, 12)\nsetEffect(8, PHASER, PHASER_FEEDBACK, -1.0)\nsetEffect(8, REVERB, REVERB_TIME, 1000.0)\n\n\nHe then sent the script back to Joachim, so that Joachim could programmed some rhythmical fills using the following code:\nfillA = \"0---0-0-00--0000\"\nfillB = \"0--0--0--0--0-0-\"\nmakeBeat(OS_SNARE03, 3, 4, fillA)\nmakeBeat(COMMON_LOVE_DRUMBEAT_1, 3, 8, fillB)\n\nIn an innovative way of working, the beats in the fill can be programmed as variables using a string of zeros 0 and dashes -, and the sound can then be specified using the makeBeat() function which takes these strings as an argument.\n\nAt the end of the script, Joachim added the line finish() which ensures that the music is prepared for playback in the digital audio workstation. Hugh and Joachim then hit the run button to compile the code, and then listened back to their awesome song! After agreeing that it really was awesome, they decided to upload it directly to Soundcloud using the inbuilt Soundcloud integration, so that the whole world could listen to their awesome baby.\n\n\nUser 230930804 · Awesome Tune\n\nResults\nHugh and Joachim found that there were some positive and negative aspects to working in Earsketch.\n\nPros\nThe first positive aspect that they found was that sharing the music back and forth was very simple and fast. Because they were mainly working with the in-built sound collection, they just had to send the Python script to each other and could continue working instantaneously. This was a lot quicker and more efficient than having to send DAW-files and bounced tracks. They also found that the DAW-style display was very well laid out, and appreciated that the documentation was included in the interface. They also appreciated ability to record sounds or integrate user samples.\n\nCons\nOn the other hand, they found that the included samples were a little limited, with Joachim in particular bemoaning the lack of Jazz samples. They also found that the disconnect between making a change and being able to directly play it back stymied their workflow. The major sticking point here was that after each change the code had to be recompiled, which took a non-negligible amount of time. There was also no teletype style functionality or ability to track changes, which made collaboration a little more difficult, especially if the project were to grow in size and scope.\n\nBottom Line\nOn a more general level, however, they agreed that the program could offer a good introduction to programming and computational music to students with little or no background in these areas. Finally, they both agreed that, for the time being, more traditional methods of working might be a little more suitable when it comes to the practicality of creating professional work.\n\n\n\nReferences\n\n[1]  McCoid, S., Freeman, J., Magerko, B., Michaud, C., Jenkins, T., Mcklin, T., &amp; Kan, H. (2013). EarSketch: An integrated approach to teaching introductory computer music. Organised Sound, 18(2), 146-160. https://doi.org/10.1017/S135577181300006X\n\n[2]  Siva, S., Im, T., Freeman, J., Magerko , B., &amp; Hendler, G. (n.d.). Introduction to Programming with Python and EarSketch. Retrieved November 2, 2021 , from. https://ggc-itec2120.github.io/EarSketchBook/#_introduction_to_earsketch\n\n\n\n",
        "url": "/networked-music/2021/11/05/joachipo-earsketch.html"
      },
    
      {
        "title": "Telematic Conducting: Modelling real-world orchestral tendencies via video latency",
        "author": "\n",
        "excerpt": "A conductor in one portal. An orchestra in another. What could go wrong?\n",
        "content": "During our collective time in the portal(s), the musical pursuits of the SMC 2020 class have all shared a fairly specific layout. Groups of musicians assemble on each side of the latency chasm and then attempt to find some form of musical understanding while navigating telematic performance. Sometimes these efforts work out; sometimes they don’t. For this assignment, I decided to consider a different approach to telematic music-making, an approach that I actually think might hold substantial promise for satisfying group performance.\n\nI propose that a telematic set-up in which one portal is inhabited by a lone conductor while the other portal contains a complete orchestra would actually provide a situation for music-making with more in common with non-telematic orchestral performance than one might expect.\n\nNote: I encourage anyone reading this to watch the far more detailed 10-minute lecture I have included at the bottom of this page. If you like thoughts on telematic performance and gifs of strange conducting techniques, it will definitely be worth your while.\n\nConductors… What in the world are they doing?\n\nThe basic principles of conducting are simple. A conductor provides the tempo and other important musical information to an ensemble via arm, hand, and sometimes bodily movements. Specific patterns correspond to different time signatures, and within these patterns are specific spots that indicate each beat within a measure.  The conductor’s motions allow far larger ensembles to play together than could ever coordinate their musical contributions without some kind of leader or musical traffic cop.\n\n\n   \n   Image source: https://music.stackexchange.com/questions/73007/what-are-the-baton-movements-for-different-time-signatures\n\n\nWhile this is what theoretically happens, reality in high-level ensembles can look a whole lot different.\n\n\n   \n   Arturo Toscanini gets real.\n   Image source: https://italianacademy.columbia.edu/event/toscanini-mini-festival\n\n\nProfessional conductors typically have highly personal styles that might minimise or ignore the traditional ‘beating of time’ completely, instead emphasising stranger motions that only ensembles familiar with their idiosyncrasies can interpret. Most important for our purposes here, many conductors intentionally conduct ahead of their ensembles. This anticipatory conducting means that many ensembles are consistently playing many milliseconds behind their conductors. In short, they’re intentionally producing real-world latency!\n\n“It’s very mysterious … I’ve seen European orchestras respond so far after the beat, that I have no idea how they know exactly where to place that.”\nJoAnn Falletta, Music Director of the Buffalo Philharmonic Orchestra\nhttps://www.wqxr.org/story/why-do-orchestras-play-behind-beat/\n\n(Ignoring for a moment the question of anticipatory conducting, the acoustic profile of an orchestra in a large hall can be extremely complicated. Many ensembles and stages are large enough that sounds initiated simultaneously by players on different parts of the stage might reach the conductor dozens of milliseconds apart. And this isn’t even taking into account the reverberations, instrumental directionality, and individual player attempts to compensate by playing slightly ahead of or behind the greater ensemble. In short, the illusion of a large ensemble playing with perfect synchronicity is only achieved by a combination of savvy players and the reverberant blur of a large hall that smooths musical edges.)\n\nVideo latency\n\nVideo latency can be defined (in a simplistic sense) as the ‘glass-to-glass’ delay between the initial capture of video and its arrival at the intended destination (the first glass being the initial camera lens and the final glass being the screen on which the video is being displayed). In its journey from start to finish, the video must traverse the chain of camera → encoder → network → decoder → final destination.\n\n\n   \n   The journey of a Zoom meeting.\n\n\nWhile we have a large amount of control over the beginning and end stages of the video’s route, the ‘network’ period is something that is harder to control. This is the stage in which packet loss and degradation of the video information can occur (particularly if we are trying to minimize latency).\n\nThis latency (and the equally complicated audio latency that comes with its own challenges) is responsible for much of the challenge of telematic performance. The visual and audio cues on which performers rely to play together are not only delayed, but delayed unequal amounts. For performing between the portals this has often equaled general dissatisfaction for many of us who are more accustomed to live ensemble musicking.\n\nThe Conductor ←→ orchestra portal set-up\n\nWhen one considers the general acoustic chaos that characterises orchestral performance, the concept of placing it within the telematic setting in this specific layout doesn’t seem overly impossible. Conductors and ensembles are accustomed to (and in many aspects have developed by choice) a lattice of differing latencies in their non-telematic practice that somehow don’t seem to negatively impact their musical performances. Moving into a telematic setting would require some negotiation of new latency details, but not details unlike those the participants are already familiar with.\n\nIn recent tests, the glass-to-glass video latency between the Trondheim and Oslo portals averaged 45 milliseconds when using Zoom. Assuming at least those speeds, and potentially much better ones should Teico or LoLa be used, our simple Zoom set-up would be adequate for performers who are used to playing with an intentional latency.\n\nThe conductor’s perspective\n\n\n   \n   Leonard Bernstein likes the portal.\n\n\nFor the conductor, working alone in her or his own portal will primarily mean being accustomed to degrees of both audio and video latency. As I’ve explained above, for many conductors this won’t be at all outside the norm. Conductors might choose to have their orchestras play closer to or further from their beat if the latency feels different to that to which they are accustomed. But the potential for mic-ing orchestral sections or even individual instruments would actually lead to a more unified degree of latency across the ensemble than when playing in an oversized hall.\n\nThe players’ perspectives\nFor players, little would likely change. There would of course likely be a learning curve as they determined the new distance between their played beats and those given by the conductor. But this would be potentially outweighed by the fact that multiple screens could provide a better view of the conductor than available in a non-telematic situation (as long as there is not significant latency between the screens in the local set-up).\n\nConclusions\nUntil we are lucky enough to find an orchestra willing to experiment in our portals, this is all theoretical. However, I believe that there is real potential for telematic musicking in the format that I have described here. Orchestras have been successfully combating the challenges of latencies and convoluted reverberations in large concert halls for centuries now. Why wouldn’t they be capable of applying those same approaches to an arguably more controllable, telematic setting?\n\n\n  \n    \n    Telematic Conducting Lecture\n  \n  Telematic Conducting Lecture\n\n",
        "url": "/networked-music/2021/11/14/telematic-conducting.html"
      },
    
      {
        "title": "A short post about feedback",
        "author": "\n",
        "excerpt": "Feeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeedback\n",
        "content": "This tutorial is going to be about feedback - we will be looking at what feedback is, why it happens and what we can do about it.\n\nWhat is feedback\n\nAcoustic feedback is that delightful squeal we are all familiar with that happens when we turn things up just a little too much. It is when a microphone picks up sound from a loudspeaker and sends it back to the amplifier to be reamplified. If the gain is greater than the acoustical loss, the signal continues to build up and the system starts to act as an oscillator.\n\n\n\nEven if the gain isn’t quite high enough for the system to go into self oscillation, feedback can still result in unpleasant ringing tones during speech due to long delay times at particular frequencies.\n\nOf course, anyone that has listened to Sonic Youth knows that feedback can be your friend, but it tends to be less welcome when working on telematic performances or otherwise collaborating in the portal.\n\nThere are several elements that can contribute towards unwanted feedback in a system, including the types of microphones used, where they are placed in the room, the rooms shape and size and last but certainly not least, the volume you are trying to achieve.\n\nWe will start by looking at microphones, and what role they play.\n\nMicrophones\n\nA microphone is a transducer that changes one form of energy - sound waves - into another - electrical signals.\n\nThe directionality of a microphone refers to its ability to record from various directions.\n\nUnidirectional microphones will record sounds from a single direction, bidirectional from 2 opposite directions, and omnidirectional records from all directions with equal sensitivity.\n\nMost microphones are unidirectional, and use what’s called the cardioid pattern. This is a heart shaped pattern (hence its name) that allows the microphone to record everything in front of it (and a little to the sides), while rejecting sound that comes from behind.\n\nOther patterns include:\n\n  Supercardioid, hypercardioid\n    \n      these offer a narrower front pickup angle than a standard cardioid, with the hyper cardioid offering the narrowest. block more of the sides out, but can let a little of the behind signal in. Good for noisy environments. The AKG podium mic is an example of a supercardioid.\n    \n  \n  Figure of eight\n    \n      A bi-directional pattern that records from the front and back, and rejects sound from the side.\n    \n  \n\n\nSome microphones offer the ability to switch between different patterns, in order to change their directionality. The AKG ceiling microphone we have in the portal is an example of a multi-pattern microphone.\n\nWhen dealing with feedback, a more directional microphone is preferred - the cardioid and super-cardioid patterns tend to work the best here.\n\nMicrophone placement\n\nWhen it comes to microphone placement, as near to the source, and as far away from the speakers as possible is the general rule here.\n\nFor situations like meetings and classes in the Portal, the ideal would be individual, mute-able mics for each participant, although of course that isn’t most convenient. Ceiling mics are certainly more convenient in this regard, and can often work well, provided the microphones are facing away from the speakers.\n\nSpeaker placement is also important. It seems obvious to keep the microphones out of the direct line of a loudspeaker, but speaker dispersion angles can vary, and the sound can end up bouncing off walls back into the microphones. This brings us to the properties of the room itself, so it’s time to dip into the loveliness that is room acoustics.\n\nRoom acoustics\n\nEvery room has a particular acoustic profile. This is a fancy way of saying that sound bounces off the walls in different ways, depending on a number of factors, including\n\n\n  the size and shape of the room\n  the materials the sound is bouncing off - so a metal box is going to have a different profile to a lushly carpeted and curtained living room for example.\n\n\nFeedback tends to occur at the frequencies of prominent room resonances - in order words, at the peaks in the reverberant sound field. If a room has a natural tendency to resonate at 800hz for example, we will find the frequency of any feedback will correspond with this resonant frequency. These frequency peaks are called the room modes. For more of a deep dive into the theory behind room modes, check out Lindsays tutorial - he goes into all the gory details.\n\nAnother important aspect of a rooms response is the delay time of reverberant sound. The combination of the frequency peaks - the modes - and a long delay time can result in modal ringing - one of the types of feedback we are wanting to eliminate.\n\nWhen it comes to room acoustics, the good news is that the situation is very often treatable. Two approaches are generally taken here - the first is to treat the room so that the peaks are reduced. This can be done with bass traps in the corners, mid/high frequency absorbers on the side walls, and diffusers to spread and scatter the sound waves. The second is by using equalisation, which I will talk more about in the next section.\n\nThe portal\n\nSo how can we apply all this newly acquired knowledge to our dear portal, I hear you ask?\n\nWell, let’s start by taking a look at the size and shape of the portal, and see what we can gleam from this.\n\nThese are the various attributes of the room\n\n\n  Height: 2.932\n  Width: 5.003\n  Length: 10.410\n  Volume: 152.51 m3\n  Surface: 194.4 m2\n\n\nUsing these measurements (and a handy website that does all the work for us), we can calculate the reverberation time. A useful measure of reverberation time is called the RT60 value, which is the time in seconds it takes for a sound to decrease by 60dB. The recommended RT60 value for a meeting or lecture room is 0.6 seconds. As you can see, the portal could use some room treatment - especially in the low end - to get there, but it’s not too far off.\n\n\n\nLets now look at the microphones.\n\nWe have 3 microphones mounted on the ceiling in the portal\n\n\n  two DPA 2011 twin diaphragm cardioid microphones\n  a single AKG C414 XLS condenser microphone\n\n\nWe also have a single podium microphone for the main speaker - an AKG CK80, a hyper-cardioid shotgun condenser.\n\nI carried out a process called ringing out the room to find which pesky frequencies are causing our feedback issues. This process involves putting in earplugs, and turning up the volume of each microphone until feedback starts to occur. Then I used the RTA display on the Midas mixer to identify the frequency of the feedback. I would then use equalisation to narrowly cut that frequency, and then repeat the process.\n\nThis helped me identify 3 frequencies that were causing issues.\n\n  3 kHz\n  1.5 kHz\n  800 Hz\n\n\nInterestingly, while the podium microphone was very easy to drive into feedback, I struggled getting any feedback from the ceiling microphones. And after cutting the 3 frequencies identified above, I struggled to get even the podium microphone to feedback. So that worked!\n\nSummary and general tips\n\nSo what are a few takeaways from this if you are struggling with feedback in your own space?\n\n\n  Choose a suitable microphone - this means a more directional microphone like a carioid that will pick up most of its signal from the front, and reject sound coming from behind.\n  Place the microphones as near to the source as possible, and out of the path of the loudspeakers.\n  Find out how your room behaves. Take some simple measurements, find out whether some room treatment could help tame things.\n  Ring out your room. Pop in those earplugs, turn up the volume and find out exactly which frequencies are causing the problems.\n\n\nI hope this tutorial has been helpful, and that you now have a better understanding of how feedback works, and how we can begin to tame it.\n",
        "url": "/networked-music/2021/11/14/stephedg-feedback.html"
      },
    
      {
        "title": "Touchpoint that can potentially improve the audio latency in a communication system",
        "author": "\n",
        "excerpt": "Quick tips for future SMC-ers or external partners who will be using the SMC Portal for the first time: this article gives you practical information to start with improving audio latency.\n",
        "content": "Quick tips for future SMC-ers or external partners who will be using the SMC Portal for the first time: this article gives you practical information to start with improving audio latency.\n\nIntroduction\n\nWhenever we start working with a new communication system, finding the touchpoint affecting the audio latency can be useful. In human communication, a touchpoint is that part of the system we interact with  connecting  relevant stakeholders within a particular business or an organisation. In a more technical sense, I am borrowing the term: a touchpoint is the element on an electronic device that you can touch in order to carry out a particular action. This blog post will be looking into the touchpoints that can potentially improve the audio latency in a communication system.\n\nCommunication Models\n\nBefore diving into the technical setup, I want to look into two particular communication models. Communication models refer to the conceptual frameworks or theories that explain the way of humans exchange information or signals, from “hello” to advertising to morse code.  It also represents the entire process of communication between the sender and the receiver. The purpose of a communication model is to describe the underlying mechanism of a communication system: What happens during the information transmission process between sender and receiver? How will the message eventually be delivered?  A communication model takes into account the many relationships that a system involves.  It explains the elements of a communication process, for example, context, encoding, decoding, channel, message, feedback, noise, sender and receiver. Shannon and Weaver’s linear model of communication can serve as a basic description of one-way communication. However, their model does not directly support feedback. Schramm’s two-way circular communication model between sender and receiver emphasises the encoding and decoding of messages. A message is only sent when encoded by the sender and decoded after has been received. This model allows for unpredictable human complexity and that makes it relevant to the Human-Machine-Interaction (HMI). For HCI and/or HMI designers, asking “what if..?” in every stage of the process can enhance overall experience flow. Shannon and Weaver’s model remains however a starting point of communication novice. The useful aspect of it is the concept of noise. From Schramm’s model we get the idea of the communication between two-way.\n\n\n\n\n\nPortal experiment\n\nCase 1: UiO local testing\n\nCase 1 was a local latency test at UiO. Two microphones (Input 1 and Input 2) were set up, in which Input 1 received an acoustic signal, in this case, the beat from a drum stick. The signal was then converted to digital form and arrived at a Mac. Input 2 registered an electronic signal after a roundtrip of “time travel”. (watch the video for more information).\n\nAudio latency testing:\n\nThe MIDAS mixer has two sampling rates (SR), they are either 44.1 kHz or 48 kHz. With these limited options at hand, I have tested both SRs from the local UiO Portal. There was a 1ms delay between both SRs.  But overall, the local latency is approximately between 5-6ms.\n\n\n\nCase 2:  UiO Zoom and Lola to NTNU Lola\n\nCase 2 was a latency test between UiO and NTNU. Four microphones (Input 1, Input 2, Input 3 and Input 4) were set up, in which Input 1 received an acoustic signal and served as an Analog-Digital Convertor (ADC). Input 2 received an electronic signal after a roundtrip of “time travel”. The Input 3 microphone connected to the mixer from UiO and sent the signal through either Zoom or Lola to NTNU. The output from UiO played in NTNU’s speaker and the signal was then sent back to UiO via another microphone (Input 4).\n\n\n\nAudio latency testing:\n\nIn alignment with case 1, both the 44.1 kHz and 48 kHz sampling frequencies were tested. What made this session different was that it was a circular roundtrip communication. Meaning that both UiO and NTNU equally encode and decode messages through “time travel”. While in UiO, both Zoom and Lola were tested, in NTNU, only the Lola was tested due to some technical issues with the Zoom.\n\n\n\n\n\nDAW used:\n\nIn this session, I used Reaper, which is a free digital audio workstation. I took two samples from two dynamic microphones (Input 1 and Input 2). Then, the time delay between two signals was observed based on both the nearest zero-crossing points before each attack of the trigger sound.\n\nCurrent limitations:\n\nNTNU was not able to connect with Zoom. Both 48 kHz samples were not collected during the testing sessions due to technical errors. Therefore, only 6 pairs of roundtrip and a pair of local testing at UiO were carried out.  However, in the future, if there is also a higher sampling rates option, such as 96 kHz in the MIDAS mixer it could be used instead to decrease audio latency. Additionally, the buffer size in Lola software at the moment cannot go lower than 64 samples. It could be an interesting problem to look at for future SMC students, namely how to increase the buffering.\n\nTouchpoint to consider while dealing with audio latency in a communication system:\n\nA take-away message for the new SMC students: the most useful things to consider when you need to improve audio latency are whether there are some touchpoints you could start with. In the the Lola software, it’s where you can change the buffer size, for instance. And then, you can also alter the MIDAS mixer sample rates. Another touchpoint may depend on other particulars of the setup, for example, the setting in a DAW and the capacity of a sound card.\n\nSpecial thanks:\nPedro and Jarle for being supportive learning buddies.\nAbhishek for being in the NTNU Portal.\n\n\n    \n    \n\n",
        "url": "/networked-music/2021/11/14/joni-touchpoint-audio-latency.html"
      },
    
      {
        "title": "Audio-Video Synchronization for Streaming",
        "author": "\n",
        "excerpt": "This approach considers a streaming solution from multiple sources and different locations (Salen, Video Room, Portal)\n",
        "content": "In order to provide the best perceptual quality in a multimedia transmission, the synchronization between audio and video (AV-Sync) has to be preserved. If that does not happen, theses inaccuracies can be perceived, affecting the visual and auditory experience (Yang et. al. 2007).\n\nThere is research that suggests that errors in Audio and Video synchronization can be identified in around 185 ms with a variance of approximately 42 ms (Younkin and Corrieau 2008). Additionally, we have other studies in which errors can be perceived near to 200 ms for audio ahead of video and 320 ms for audio delayed with respect to video (Staelens et. al. 2012). The goal is to reduce these inaccuracies as much as possible to improve the multimedia experience.\n\nStreaming Scenarios\n\nIn the case of Audio and Video synchronization, I considered three of the main locations that we usually use on the SMC Program: Salen, the Video Room, and the Portal. The objectives are:\n\n\n  Measure Audio and Video synchronization errors for these three places.\n  Fix the errors by introducing latency, which will happen mostly regarding the audio.\n  Stream on YouTube sources from these three places at the same time.\n\n\nThe idea is to setup a streaming system within the Portal and receive the audio and video from a source placed on each location. These sources will be received via internet from Salen and the video room to the streaming system in the portal, and the local signals in the portal itself will be sent to this system. Finally, all these sources are streamed to a service, that in this case, will be YouTube. The picture below illustrates this setup.\n\n\n   \n   Complete Streaming Scenario.\n\n\nNext you will find a description for every location.\n\nSalen\n\nIn Salen, we have a source which is an Audio and Video synchronization test that is used on every location. The audio is capture by a microphone connected to a MIDAS mixer which sends a stereo analog signal to a PC in which we have OBS ninja running. We create in this PC an instance of OBS ninja for sending audio and video. For video, we point a camera to the source, which is connected to a video mixer. The video feed is sent to the PC through a capture card, and finally this video feeds the OBS ninja instance which is transmitted to the internet considering both, audio and video.\n\n\n   \n   Salen Diagram.\n\n\nVideo Room\n\nIn the video room we have a similar source. We capture the audio through a microphone connected to the analog mixer and we sent this signal to the LOLA PC and then to internet. We have a camera that is connected directly to the Zoom PC through USB. In this PC we are running an instance of OBS ninja, but in this case, we are just sending the video. As you can notice, we have two different sources for audio and video going to the network.\n\n\n   \n   Video Room Diagram.\n\n\nPortal\n\nIn the portal we use the same video source. The audio is capture by a microphone that goes to the MIDAS mixer. The video is capture through a camera. Both, audio and video are sent to the ATEM mini. It allows to feed the Streaming PC as one source that represents the local setup. The Streaming PC collects all the source in OBS Studio, in which the synchronization corrections happen.\nThe audio from the Video room is received through the LOLA PC which sends the signal to the MIDAS mixer. The audio signal is capture by the streaming PC through the sound card from the mixer so that we can have a separated source.\nThe video feed from the video room is received from OBS ninja. We get both, audio and video from Salen through a different OBS ninja source. Once we have the three sources in terms of audio and video we can perform measurements and corrections regarding AV-Sync and stream to YouTube.\n\n\n   \n   Portal Diagram.\n\n\nMeasurement Technique\n\nFor performing the measurements, we record the multiple video feed in OBS Studio but we mute all the sources except the one that we are interested in, so that we can measure the difference between audio and video in a video editing software. OBS looks like the next image when all sources are integrated.\n\n\n   \n   All sources in OBS Studio.\n\n\nOnce we have the recording, we identify the closest frame to the sound, and we check the approximate number that is given in the video test as it is shown in the picture below. We do that with all the three sources several times and we average the value.\n\n\n   \n   Measuring in video editor.\n\n\nResults\n\nThe results are individual per each location. For Salen we have 250 ms, for the video room 200 ms, and for the portal 50 ms. Note that in the Portal, according to previous research we can say that the synchronization error will not be perceived. But for the other ones is necessary to do a correction.\n\n\n   \n   AV-Sync errors per location.\n\n\nCorrection\n\nOBS Studio allows us to introduce latency for the audio sources through a parameter called “Sync offset”. We just have to write the measured numbers and we will get every individual source synchronized in terms of audio and video. The interface that OBS offers is shown in the next picture.\n\n\n   \n   OBS audio correction.\n\n\nComparisons between before and after correction.\n\nIn the following video you will find the presentation of this topic. From minute 5:30 there is a set of videos showing the test source and the comparisons when the audio latency is introduced for synchronization.\n\n\n   \n   Presentation\n\n\nConclusions\n\nValues can be affected regarding AV-Sync errors by changing quality settings. It can be focused on audio and/or video in terms of resolution, bitrate, sample rate, network and others. In that case is recommendable to measure every time we need to assemble this kind of application.\nErrors can be introduced locally. It means that we can have local latency and we could fix locally and then send, but still, we probably will need to fix in the other end. In that sense, we must preserve the solution we choose, that is, fixing synchronization errors locally and in the streaming system or just in the streaming system. The goal is to reduce these errors in the final end.\nA Correction is not needed if the latency is not perceived. In this case we could skip the correction for the portal because it was within an acceptable value for human perception.\n\nMeasurement and fixing techniques can be extended outside SMC, as well as external sources. For example, Salen can be considered an external source in the SMC portals, so we can add as many sources as we want by using, for example, OBS ninja to transmit audio and video and apply the same techniques.\n\nFinally, future work considers synchronization of the sources after the individual correction of each one. It can be managed by having a way to centralize a source or deal with an actual telematic performance in real time so that we can try to run events synchronized among all the sources.\n\nReferences\n\n\n  \n    Yang, M., Bourbakis, N., Chen, Z., &amp; Trifas, M. (2007). An efficient audio-video synchronization methodology. Proceedings of the 2007 IEEE International Conference on Multimedia and Expo, ICME 2007, 767–770. https://doi.org/10.1109/icme.2007.4284763\n  \n  \n    Younkin, A. C., &amp; Corriveau, P. J. (2008). Determining the amount of audio-video synchronization errors perceptible to the average end-user. IEEE Transactions on Broadcasting, 54(3), 623–627. https://doi.org/10.1109/TBC.2008.2002102\n  \n  \n    Staelens, N., De Meulenaere, J., Bleumers, L., Van Wallendael, G., De Cock, J., Geeraert, K., … Demeester, P. (2012). Assessing the importance of audio/video synchronization for simultaneous translation of video sequences. Multimedia Systems, 18(6), 445–457. https://doi.org/10.1007/s00530-012-0262-4\n  \n\n",
        "url": "/networked-music/2021/11/15/pedropl-avsync-streaming.html"
      },
    
      {
        "title": "Room acoustics: what are room modes and how do they influence the physical space?",
        "author": "\n",
        "excerpt": "This blog post explains what room modes are, how they affect the physical space and what can be done about it. It was made together with a video lecture.\n",
        "content": "Room acoustics\nWhat are room modes and how do they influence the physical space?\n\nIntroducing key concepts\n\nOne of the most discussed topics in room acoustics is that of room modes. But what exactly are they and why are they important? Let’s clear out the confusion and misunderstandings together.\n\nRoom modes represent the resonant frequencies that occur within a room.\n\n\n    Organs\n\n\nJust like organ pipes, rooms resonate; similarly, just like the different pipe dimensions produce different resonant frequencies, room dimensions also correspond to certain frequencies.\n\nBased on this, the goal when designing a good room for music, conferences, or home theatre systems is to control the decay time and minimize this coloration – strongest at bass frequencies between 20-200 Hz.\n\nWhen talking about soundwaves, we often think of their frequency first. However, they are also characterized by their wavelength (size) – the distance between two consecutive peaks in a wave. To exemplify, a 1kHz soundwave would have a wavelength of about 0.35m. Lower frequencies have much longer wavelengths – a wave of 150 Hz has a wavelength of 2.28m (almost the height of a normal room!)\n\nResonance chambers\n\nIf you work with sound or play an instrument, then you’re probably already familiar with the way the physical volume affects sound. To produce different frequencies, instruments use strings and chambers of varying sizes – e.g. the difference between a bass and a treble instrument.\n\n\n    Double bass\n\n\nSimilarly, any room will act as a resonance chamber at specific frequencies depending on its size. For example, if you consider a room with the longest length of 6m, this would correspond to about 56 Hz. The first room mode corresponds with the longest dimension of the room. In the case of our example, 56 Hz would be the first and strongest room mode.\n\nThis sounds quite abstract. How can you test it?\nSimply choose a tone based on your room dimensions and play it over your speakers (it’s important that it has a subwoofer). Now move around the room and observe how in some spots of the room the tone is very strong, whereas in others it suddenly disappears. Crazy, right?!\n\nThis is an example of an axial room mode, the strongest type of room modes and the main focus in a rectangular room. The frequencies and wavelengths of such a room correspond with the three axes of the Cartesian coordinates of the room.\n\nWhen listening to audio in a room with strong room modes, unwanted characteristics are observed at particular frequencies, such as:\n\n  long decay time,\n  boominess, and\n  apparent pitch changes.\n\n\nThis happens because as a tone with one frequency excites and is then dominated by a strong resonance at a slightly different frequency, the low frequencies then affect the middle and high frequencies.\n\nNearly all systems, even those with extremely high-end equipment, suffer from the negative impact of modal resonances - e.g. a room with long decay times in the bass range can sound “muddy” or “boomy.”\n\nPractical demonstration\n\nListen to these recordings of two songs in the same room, each recorded before and after acoustic treatment. Notice how the long decay times affect the clarity of the low-, middle- and upper- frequencies.\n\n\n  \n    \n    Alternate Text\n  \n  Audio recordings of before and after acoustic treatment\n\n\nAddressing problems of resonance\n\nIn a room, the first room mode is not the only one to produce resonance. Each harmonic integer multiples of the fundamental modal resonance will do it too – e.g. a room has the first mode at 80 Hz; it will also resonate at 160/240/320/… Hz. These harmonic resonances are mostly observed as feedback issues in a microphone-speaker setting.\n\nIn broadcast and education, speech intelligibility is extremely important (lectures, interview, etc). Thus, controlling decay times is very important for clarity of dialogue, clearing up the low end to give your sub more impact, and alleviating localization problems common with sound systems. Low frequency resonances result in the smearing of vocal frequencies and need to be controlled for clear and concise speech.\n\nOptimizing the SMC portal and other rooms built for teleconferencing and telematic performances\n\n1. “Optimum” Room Dimensions\n\nIf a room has evenly divisible dimensions – e.g. a height of 3 m and a length of 6 m – then it’s going to have similar modes: similar frequencies will stack upon themselves and exasperate the problem even more.\n\nSo, room dimensions are one way we can control room modes. The standard modal approach for designing a room with good acoustics is to create as many different resonances as possible and to spread them as evenly as possible across the frequency spectrum (see the ‘Handbook for Sound Engineers’). Bigger rooms also reduce the spacing between resonances.\n\nIn general, the lower the better for the first resonant frequency, because this region is where the frequency response is most variable.\n2. Acoustic Materials\n\nBass traps and large broadband absorbers are absorbers of acoustic energy designed to damp low-frequency sound-energy. They are typically made from porous materials such as rockwool, mineral wool, basotect, or other types of acoustic foams, and incorporate elements such as membranes, airspaces and additional thicknesses of fiberglass (≥ 6″) – all needed to control low frequency bands.\n\nWhat is the best place to put them in a room? They are most effective if placed on:\n\n  A tri-corner (e.g. where 2 walls meet the ceiling or the floor);\n  A wall/wall corner (e.g. between the side and back wall);\n  The ceiling.\n\n\nA diffuser is any reflective structure that has an irregular surface capable of scattering the reflections. To be effective, it needs to have bumps and humps of at least several inches (≥ 1/4 of wavelengths).\n\n\n    Diffuser\n\n\nOther effective acoustic materials are soft furnishings that ‘accidentally’ create absorption such as curtains and carpets.\n\nAttention! Porous absorbers (most commonly sound absorbing panels) don’t work on low frequencies unless they are very thick, or placed far enough away from the wall.\n\nFor example, for a 200 Hz sound wave, the wavelength is 1125/200 = 5.6 feet; in a sound wave, the velocity is maximum (and the pressure is zero) at ¼ the wavelength from a boundary or the wall; such, a quarter of the example soundwave is 1.4 feet (0.43 meters). You read it right! You need a lot of space to absorb low frequencies using porous absorption!\n\n3. Optimal listener speaker locations\n\nModal theory tells us that a subwoofer in a room corner will excite all modes. In contrast, for a subwoofer perfectly centered along one wall, several modes will not be excited at all.\n\nSo, ideally, the loudspeaker should be out and away from the corners. The optimal placement can be calculated using some good-ol’ math, and then tested by ear and with instruments.\n\nAs any other resonance chamber, for a room to resonate, it needs containment. Thus, opening large windows and doors will help relieve the room modes, creating an “escape route”.\n\n4. Digital pre/post Equalization\n\nAn effective way of achieving better frequency response is to apply parametric EQ directly on the sound before it is sent out or on the microphone. When two rooms are virtually (and electronically) connected, such as the SMC Portals, this works like a charm – e.g. applying two filters on 35 and 63 Hz.\nCheck out Stephen’s video for more information of feedback equalization.\n\nI hope this blog post helped you gain a better understanding of room modes and the ways to use or misuse them!\n\nVideo Tutorial\n\n\n   \n   Video Presentation\n\n\nHere are some other resources and tools for calculation, check out these links:\n\nAcoustic tool for room mode calculation\n\nRoom acoustics\n\nRoom modes\n\nBass trap placement guide\n",
        "url": "/networked-music/2021/11/15/lindsay-room-modes.html"
      },
    
      {
        "title": " Audio Latency in the Telematic Setting",
        "author": "\n",
        "excerpt": "Latency and its fundamentals in the telematic performance context.\n",
        "content": "To begin with, it makes sense to define what is a telematic performance and what do we mean by latency especially in the context of telematic performances. So,\n\nWhat is a Telematic Performance and what is latency?\n\nTelematic performance is live performance via high-bandwidth internet by performers in different geographic locations\n\n\n   \n   Telematic performance between UiO and NTNU\n\n\nLatency: Broadly defined, latency is the time delay between the input and output of a system.\n\n\n   \n   \n\n\nBUT, how does this translates into the context of telematic performances?\n\nIn the context of telematic music-making, this can be understood as the delay between the moment a musician in one location makes a sound and the moment a second musician in a different location hears that sound.\n\n\n   \n   \n\n\nWhen does the latency becomes a problem?\n\nTo understand this, I have considered two situations:\n\n  Traditional Performances\n  Telematic Performances\n\n\nTraditional performances: The traditional performances are the ones in which the performers are present in the same physical space along with the listener. In this kind of performance, the latency comes into effect when there is a considerable distance between the performers themselves and the listener as well, for example in a big concert hall. Some technical aspects like the distance between loudspeakers and the performer and also listener can also add to the latency in this case. On the other, if the performers are playing a score and are following the conductor, they follow the cue of the conductor and if that cue is misjudged the performer starts going out of time because of the delay.\n\nTelematic Performances: Telematic performances as explained above are the ones in which the performers are in two different physical locations. Any attempt to perform music over a network requires a confrontation with the issue of latency. The latency becomes a major source of frustration while performing if it’s too much. Many technical aspects add to the latency as well, which are explained later below.\n\nTherefore, either latency needs to be reduced to the point where it is no longer noticeable or creative alternatives to working with latency need to be developed.\n\nThreshold of latency\n\nBut at what point does the latency becomes perceivably disturbing. Extensive research done by (Braasch - The Telematic Circle – a University-Based Collabor.Pdf, n.d.) shows that if the latency is somewhere between 25ms and 30ms, and above 30ms the delay between the signals produced at both the ends is so much that the latency takes over the synchronized playing.\n\nMajor Factors contributing to Latency:\nThe factors can be broadly classified into two elements: Transmission delay &amp; Signal processing delay, and are listed below:\n\n  Distance\n  Network Framework\n  Bandwidth\n  Network Traffic\n  Data Size\n  System latency\n  Computer Processing power\n\n\n\n     \n     \n  \n\nDistance: Distance is an inevitable cause of delay, considering the data is sent over an optical fibre network and it travels at the speed of light, there will be a delay in transmission, for instance, Trondheim to Oslo distance is ~400 km, so at the speed of light, the transmission time for a data packet to be sent between the two locations will be ~13ms.\n\nNetwork Framework: Data is not transmitted entirely on Fibre optics, at both ends some part of the transmission will be on copper cables/Wi-Fi/ mobile network, etc, also routers/firewalls add to latency.\n\nBandwidth: The speed of the internet - how much data can be transferred from one point in a network to another within a specific amount of time?\n\nNetwork Traffic: The number of users on the network at the given time.\n\nData Size: The size of data being sent affects the time taken for the transmission, large packets are broken into smaller packets - transmitted - and then recombined which adds to latency.\n\nSystem Latency: This consists of all the signal processing being done at both the ends before the transmission and after the transmission. So, capturing of sound – Mixing – AtoD conversion – transmission – DtoA conversion – decompression – output to the speakers, all these processes add to latency.\n\nComputer Processing Power: This is very obvious that higher speed computers will process the data quickly resulting in low latency values.\n\nOptimizing Latency in Telematic performances\nSo how can we optimize latency to be as minimum as possible? The first obvious choice is to optimize the system and the network connection we are using, by that I mean optimizing the determinant factors described above. The other option is to use an advanced system like LoLa, Jacktrip or similar, but we have to keep in mind that they have their requirements and are not easy to fulfil. Then comes the choice of embracing latency, by that I mean using latency in our favour, in the sense that performing music that does not require perfect time synchronization or learning to play with latency. We can also adjust some technical parameters like sampling rate, buffer size to achieve lower latencies. But even though we optimize all these values to their potentials, in principle, there will still be some latency that will be hard to eliminate.\nTesting and experiments\nWe in the portals at UiO and NTNU tried to do some experiments by changing the sampling rate on the Midas M32 mixer present at both sides and also changing the host system by which we are sending the audio back and forth to check the effect of it on audio latency. During the workshop in the portal lecture, we also measured latency between the portals, in three scenarios, sending the signal back to Oslo as we received it through the mixer, putting the microphone adjacent to the loudspeaker as shown in the image below and then placing the microphone where we normally sit in the Trondheim portal so that it also adds the latency due to the distance. I also did a test in audacity at my home, in which I created a loopback system, and then recorded the signal generated in another track. You can find all the results below.\n\n\n   \n   Setup Oslo\n\n\n\n   \n   ntnu setup without the mic hooked, I did hook it later. \n\n\n\n   \n   UiO-NTNU test with different sampling rate on mixer\n\n\n\n   \n   Worskhop test between the portals\n\n\n\n   \n   Home audacity test\n\n\nVideo Presentation and Slides\nIf you wish to hear about the topic in more detail you can watch the below video lecture and the presentation slides.\n\n\n  \n    \n    Telematic Conducting Lecture\n  \n  Audio latency lecture\n\n",
        "url": "/networked-music/2021/11/15/abhishec-audio-latency-telematic-context.html"
      },
    
      {
        "title": "Latency as an opportunity to embrace",
        "author": "\n",
        "excerpt": "How can we turn unavoidable latency into an integral part of telematic performance?\n",
        "content": "In the era of COVID pandemic, online collaborative performance became the only way we can afford the music, but this form of collaboration suffers from audio latency, is there any way we can use it as an advantage?\n\nIs latency always bad?\n\nLatency is everywhere on the earth, and actually, our auditory abilities are built on latency. The sound latency between two ears from the sound source is one of the bases for the human’s sound localization ability, and this is also the basis for the construction of stereo audio, which is particularly adapted to sounds below 1500 Hz and is the fundamental frequency range of the various musical instruments and also the human voice. According to research, People can perceive the directional difference of about 1 to 3 degrees, because of the latency for the sound to reach both ears is about 10 microseconds. Latency also forms our perception of sound space, the latency and proportion between direct and reflected sound allows us to perceive the size and texture of space.\n\nModern telematic audio systems have always strived for low latency, but in fact, latency has also always been present in traditional music performance. According to sensory-motor synchrony study, There is usually a 30-50ms delay between the musician’s prediction, and sound producing action. Delays are everywhere in music hall. Conductors often give instructions for beats in advance, and even in the best orchestra, musicians are not perfectly synchronised with each other. Actually, imperfection makes the music more human, and these humanities are so rooted in our auditory culture that we can become uncomfortable with perfectly synchronised orchestral music produced by MIDI.\n\nWhen does the delay become a problem?\n\nResearchers found that in a test involving two Mozart duets, that a latency of anything over 100ms was enough to render the experience either unmusical or non-interactive (Bartlette et al. 2006: 49). Others have observed when the latency around 11ms(Chafe et al. 2004: 1), each performer waits slightly for the other, creating a gradual deceleration in tempo. More extensive studies tend to agree that latencies of between 20 to 30ms are where notable disruptions to performers start to occur(Haas [1949] 1972: 145).\n\nBetween NTNU and UiO, we measured around 29ms latency for direct signals, and around 40ms included microphone and speakers.  As a result, we can still feel audible latency in the Portal. So, instead of trying to figure out how to reduce latency, we should embrace it and make it part of the music. The first solution is to develop an audio system with fixed latency, and the second solution is to use a special approach to compose telematic music.\n\nOnline Orchestra\n\nResearchers from the UK have used an idea to develop a system called Online Orchestra. In this system, the software will first check the latency of each part of the system and choose a value bigger than the latency of each part as the tempo of the music and lock the latency, e.g. setting a 500ms latency is equivalent to one beat at a 120BPM music piece, then the latency between each part is one beat. In the figure 1, When the conductor makes a command in the first beat, all the nodes, on the second beat, will receive this signal and make the sound, and these signals, delayed by another beat, return to the conductor side in the third beat and are sounded in unison. Although the sound of each node is inconsistent, at the end, the sound is perfectly harmonised in conductor’s perspective.\n\n\n   \n   Online Orchestra illustration\n\n\nComposing for latency\n\nTexture Distribution\nAs well as from a technical perspective, we can also compose music for latency using special arrangements. A simple solution is to distribute a texture across the ensemble. This score shows an example of this type of distributed texture. In the music score below, we can see the focus moves from the strings and vocal materials of Node 1 to flutes in Node 2, with the strings imitated in the flutes to smooth the transition. flutes have highly active materials, but this rhythmic activity does not spread to other parts of the ensemble. Latency therefore does not disrupt the overall coherence.\n\n   \n   Texture distribution\n\n\nPolyrhythm and Ostinato\n\nThe second method here is to arrange polyrhythm and ostinato for complex rhythm, A rhythmic ostinato is a short, constantly repeated rhythmic pattern, the pattern onset points with respect to each other are less significance, especially under the situation that the latency is locked to one beat. We can know more about Ostinato from the video below:\n\n\n   \n   Ostinato demonstration\n\n\nHarmony\n\nThe third option is to prioritize linear music organization, as opposed to vertical harmonic progression. In other words, the harmony determines the colour of the music, and if we blurs and reduces the harmonic transitions in telematic music, the impact of the delay is reduced. Like the score below, the harmony of the music is not affected even though the music is shifted due to the delay.\n\n\n   \n   Texture distribution\n\n\nConclusion\nIn summary, we may never get a telematic system with zero latency, but we can embrace latency through rigorous, delicate system design and musical arrangements. Finally, let’s listen an excerpt from In Sea Cold Lyonesse, a telematic performance applied the technical and compositional techniques we have just described. this work spread three locations in the UK and involving over 100 musicians. Hope you enjoy it:)\n\n\n   \n   In Sea Cold Lyonesse\n\n\nReferences\n\nBraasch, Jonas. (2009). The Telematic Music System: Affordances for a New Instrument to Shape the Music of Tomorrow. Contemporary Music Review, 28:4-5, 421-432, DOI: 10.1080/07494460903422404.\n\nRofe, Michael and Reuben, Federico (2017). Telematic performance and the challenge of latency. The Journal of Music, Technology and Education. 167–183. ISSN 1752-7074 . https://doi.org/10.1386/jmte.10.2-3.167_1\n\nRofe, M., &amp; Geelhoed, E. (2017). Composing for a latency-rich environment. Journal of Music, Technology &amp; Education, 10(2-3), 231-255.\n\nMills R. (2019) Telematics, Art and the Evolution of Networked Music Performance. Springer Series on Cultural Computing. Springer, Cham. https://doi-org.ezproxy.uio.no/10.1007/978-3-319-71039-6_2\n\nPalmer, C., &amp; Demos, A. (2021). Are we in time? How predictive coding and dynamical systems explain musical synchrony.\n",
        "url": "/networked-music/2021/11/15/wenboyi-audiolatencyy.html"
      },
    
      {
        "title": "Video latency: definition, key concepts, and examples",
        "author": "\n",
        "excerpt": "This blogpost is made after the video lecture on the same topic and it includes a definition of video latency and other related key concepts, as well as concrete examples from the SMC portals.\n",
        "content": "Introducing Video Latency\n\nThe aim of this blogpost is to explain and clarify concepts and processes rather than introduce new ideas or recommendations. Audio latency or the problem of audio-video synchronization is not discussed here.\n\nLet’s start with the beginning. Video Latency is the difference between the time of capturing and that of displaying whatever video was captured.\n\nThe total time difference between source and viewer is called glass-to-glass latency, or end-to-end. Other terms like “capture latency”, “encoding latency” only refer to the lag added at a specific step of the workflow.\n\nEach user case has its own latency requirements. One-way streams of live events to large audiences can have up to 45 seconds of delay without any bad consequences, whereas in the live stream of a football match, for example, so much delay would be problematic (think about social media spoiling an important goal you haven’t seen yet…)\n\nIt is even more important to have low video latency in a two-way conference, real-time device control, or, of course, telematics performances (think about playing together… and missing all the visual cues and feedback!)\n\n\n    Wowza Media Systems video latencies\n\n\nLive Streaming\nLet’s quickly discuss the most important components of the streaming chain with regards to video latency (aka the ones that usually add the most lag).\n\nVideo encoding is the process of compressing raw video for it to be later transported over the internet. At this step, the encoder needs to compress the content according to the available bandwidth of the network.\n\nThere are two types of video encoding, file-based and live. In the first case, encoders are used to compress and reduce the size of video content so that it uses less storage space and is easier to transfer. Since the video files are not live, the latency is rarely a key problem here.\n\n\n    Haivision representation of the streaming workflow\n\n\nLive video encoding is the process of compressing real-time video and audio content prior to streaming – significantly reducing bandwidth while maintaining picture quality. However, depending on the type of encoder used, compressing live video can add to the glass-to-glass latency, negatively impacting the overall experience quality.\n\nVideo decoding is the process opposite of encoding. It can output uncompressed video through SDI for further video processing or over HDMI for displaying directly on a screen.\n\nTo keep latency low in a video streaming workflow, it’s important to work on each step at a time – e.g., if the video encoder is adding latency, there won’t be a way to “catch up” on that delay later in the streaming process.\n\nCODECs\n\nAnother critical step that adds to the glass-to-glass latency is compressing and decompressing data into files or real-time streams. This process is done following video protocols known as the codecs. The term codec is a portmanteau of the words enCOding and DECoding.\n\nMost codecs use a “lossy” compression method – some redundant spatial and temporal information is lost. “Lossless” compression is used when the goal is to reduce file and stream sizes by only a slight amount in order to keep picture quality identical to the original source.\n\nCodecs for live video (mainly H.264/AVC or H.265/HEVC) can reduce raw content data by as much as a thousand times, saving much needed bandwidth – e.g., a typical uncompressed HD stream is about 1.5 gigabits, but compressed it gets to around 5 megabits for live broadcast television.\n\nNetwork transport protocols are also influencing the end-to-end latency. Different protocols will introduce different amounts of latency to the streaming workflow, so, for live applications a transport protocol with as low latency as possible should be chosen. Encrypting data for security purposes is another lag-adding parameter. “Packet loss” and following error correction methods also adds latency.\n\nDepending on each use case, the image quality or the low-latency will be more important. For applications where latency is critical, such as telematic performances, picture quality can often be exchanged in favor of minimizing latency. However, if the video quality is of importance one has to accept extra latency. Ultimately, the optimal combination of bitrate, picture quality, and latency settings will result in a great live experience over any network.\n\nPractical examples from the SMC Portals\n\nProtocols\n\nOn a daily basis we use Zoom to communicate between Trondheim and Oslo, at least visually. For a better performance, this technology combines the two codecs:\n\n  The Advanced Video Coding (AVC) codec has a fast encoding speed and is very efficient for HD videos. However, as the demand for 4K continues, it is slowly being replaced with the High Efficiency Video Coding (HEVC) protocol, which can deliver the same quality at half the bitrate (while using significant processing power).\n  FFMPEG (Fast Forward MPEG) is an open source, command-line based, multimedia project for encoding and decoding a variety of media formats (both audio and video) – essentially an upgrade of the grandpa MPEG.\n\n\nBoth the Trondheim and the UiO portal are equipped with two very good cameras: the Logitech Pro and Minrray PTZ, both supporting AVC. The Minrray PTZ camera even supports the better HEVC (too bad Zoom doesn’t support it).\n\nOnce a semester or so, for our awesome telematic performances, we need to set up a stream that feeds several camera perspectives (and audio!) to the Internet, mostly to Youtube. For this we use OBS Studio – which also uses AVC. For a transport protocol, it uses SRT (Secure Reliable Transport) – open source streaming protocol that enables encryption and utilizes packet recovery to maintain high quality over unreliable networks without compromising latency.\n\nVideo Latency in the SMC Portals\n\n\n    Testing 2-way connection over zoom\n\n\n    Testing 3-way connection over Google Meet\n\n\nUsing a fun method of filming a web clock from more locations and then calculating the time difference, we tested some actual latencies:\n\n\n    Video latencies tested in the UiO and NTNU portal\n\n\nWhat is this telling us? Firstly, the latency between Trondheim and Oslo is not that terrible! And secondly, even in the same room there’s latency to consider.\n\nClosing remarks\n\nRemember, when working with video latency, to:\n\n  choose hardware that is engineered to keep latency as low as possible even when using a standard internet connection;\n  choose equipment that supports high efficiency video codecs;\n  make sure the transport protocols are suitable for your task;\n  find the balance between latency, picture quality and bandwidth depending on the use case.\n\n\nIf you’re searching for inspiration for another latency test consider this use cases: during one of our SMC courses you need to use several camera perspectives: one for the lecturer, one for the class, one for the mixer view. Calculate the video latencies (and perhaps also video quality) between all cameras and compare them. Based on your findings decide which cameras should be placed where. Hint: perhaps it is more important to have a high quality stream of the mixer view rather than the other angles…\n\nAs a last thought… have you considered that latency can be a good thing? For example to prevent obscenities from airing, for live subtitling, or closed captioning.\n\nCongratulations to all SMC students and teacher that set up our portals – despite our constant complaining the “worst” latency was still under 50 ms! Great job!\n\nReferences and further reading\nEberlein, P. (n.d.). Understanding Video Latency. U.S. Tech. http://www.us-tech.com/RelId/1490479/ISvars/default/Understanding_Video_Latency.htm\nHaivision. (n.d.). The Essential Guide to Video Encoding: From Video Compression and Codecs, to Latency and Transport Protocols. Retrieved November 1, 2021 from https://www.haivision.com/resources/white-paper/the-essential-guide-to-low-latency-video-streaming/\nNikols, L. (2021). Video Encoding Basics: What is Latency and Why Does it Matter? Haivision. https://www.haivision.com/blog/all/video-encoding-basics-video-latency/\nUbik, S, &amp; Pospíšilík, J. (2021). Video Camera Latency Analysis and Measurement. IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 1, pp. 140-147, doi: 10.1109/TCSVT.2020.2978057.\nWowza Media Systems. (2021). What Is Low Latency and Who Needs It? (Update). https://www.wowza.com/blog/what-is-low-latency-and-who-needs-it\n",
        "url": "/networked-music/2021/11/15/alenacl-video-latency-clarifications.html"
      },
    
      {
        "title": "Audio-video sync",
        "author": "\n",
        "excerpt": "Due to the fact that sound travels a lot slower through air than the light, our brain is used to seeing before hearing.\n",
        "content": "The Advanced Television System Comittee, Inc, (ATSC) is an «is an international, non-profit organization developing voluntary standards for digital television.» According to the ATSC Implementation Subcommittee, “The sound program should never lead the video program by more than 15 milliseconds, and should never lag the video program by more than 45 milliseconds”. Because the speed of light is so much faster than the speed of sound (299 792 458 m/s [light] vs approx. 343 m/s [sound]) our tolerance for leading audio is less than that for lagging audio.\n\nPart 1: Inbetween the audio and video\nFor the trial-lecture movie, I wanted to explore some post-production syncronisation-tools. A tool for syncing recordings from multiple cameras and/or sound recorders is Syncalia which is free to use with all functionalities the first 20 days. So I decided to use my iPhone, my Sanyo Xacti camera and my Zoom H2N sound recorder for some testing of this. I also had to find another video-editor than the usual iMovie, one that is compatible with Syncalia. And since DaVince Resolve is free, and also a very powerful video-editing tool, I decided to go for this.\n\nTo sync or not to Syncalia\nAfter having downloaded the footage (with sound) from my two camera sources and one sound source, I was looking forward to have Syncalia doing all the syncing, making further editing a lot easier for me.\nTo use Syncalia, you basically put all footage from each camera on their own layer, same thing with the sounds; one layer containing sound from one (and the same) source. No matter where on the timeline you put it, it should be fine. Then you export an .xml or .fcpxml, which are small text-files containing metadata about duration, aperture, lenses, recording time and such. Syncalia opens these files in a swoosh, and short videos like this is synced in a couple of seconds. Then you export the synced xml-file, and import this back into your video-editor, now all synced. Super easy! You can also open these files in a text editor, to see what kind of data it’s using:\n\n\n    \n\n   Opening the .xml-files in a text editor can reveal what the file contains, and it should be possible to go in there and fix issues with the syncronisation. As you see, the two .MOV-files from my iPhone comes before the first Sanyo-movie, as explained in the text.\n\n\n\nWhat time is it?\nBut when you realize that your recording devices doesn’t share the same time you run into some problems. To me it seems like the hardware needs to be perfectly in sync regarding what time it is. I discovered (too late) that my Zoom thought it was 1. january 1970. The Sanyo-camera and iPhone agreed on the date and hour of the day, but there was a three minutes difference there as well, causing issues for the syncing. As you can see in the screenshot of the text editor, Syncalia has placed two iPhone-files (IMG_1438.MOV and IMG_1438.MOV) before the first Sanyo-file (SANY0002.MP4), simply because when the Sanyo-cam recorded it’s first recording, it registered the time to be 15:46, and the iPhone had the simultaneously recorded file registered at 15:43.\nIt seems that Syncalia would work better with a little more high-end equipment that get’s the date and time via satelite or whatever, so that all equipment actually is synced down to the millisecond. Otherwise you will still have to sync everything using other methods.\n\nIf Syncalia had lined up my clips perfectly, I would have had the recording from the Sanyo cam starting out with approx. 100 ms latency (due to me standing 33 meters away in 3° celsius) compared to the Zoom audio recorded in my hand. Of course the sound recorded that far away has a lot of other surrounding noises and sounds in it, making it impossible to hear my voice clearly, but it was still possible to hear the delay between the two sources. This delay dimished more and more the closer to the Sanyo-cam I got, and with synced recording devices—and using Syncalia—the sound would end up almost perfectly in sync at the very end, when I’m only about 1 meter from the camera.\n\nThis experiment would be interesting to try out in an ideal space, with high-end equipment, making it possible to have a good zoomed-in picture and a super cardoid shot-gun microphone, making it possible to experience the lip-sync issues we’d end up with. Speaking of lip-sync, a research by Younkin and Corriveau found the threshold to be a lot higher than the +15 to -45 ms that the ATSC is working with. They found that the mean threshold of 185,19 ms before people stated that the sync was off. This was a research based on lip-sync with one talking person.\n\nPart 2: What about musicking together out of sync?\n\nThe last part of this little journey of mine, took me to the Portal and the Dungeon at UiO. Because we have these fairly recently acquired Ximea-cameras, that connects to the LoLa machines, I wanted to to a simple test with a drum machine and compare Zoom and LoLa video.\nUntil now I’ve talked about lip-sync and AV-sync in a media to spectator sense, but what about when we are trying to communicate musically over long distances? Telematic music has been widely discussed in a lot of these blog posts, and there are numerous ways people tries to work around the latency issues. But that left aside, I wanted to see how much better a setup with Ximea-camera was compared to using Zoom.\n\nIn the Portal Pedro and I set up a Zoom-meeting and joined the meeting from the Dungeon downstairs. I set up a Ximea-cam and we connected through LoLa. The drum machine (Roland TR-8) was put on the floor, playing a beat over LoLa, and I pointed both the Zoom-camera and the Ximea-camera on the drum machine, and in the Portal upstairs, Pedro had already set up the two monitors behind the mixing desk to show Ximea-cam on the left monitor and Zoom-cam on the right, as well as having the sound coming over the speakers as usual in a LoLa-session.\nI filmed these two screens in one shot, and with a timer showing milliseconds on the Zoom-screen, I should be able to measure the visual difference between the Ximea- and Zoom-setups.\n\nIt has to be mentioned that when using Ximea on the oldest LoLa-machine (the one in the Dungeon), the framerate needed to be set to 25, with a fairly low resolution (640x480, RBG), and still we had quite some buffersize-glitches in the sound (as you can hear in the video). Not the best thing for playing music together.\n\nDuring one of the recording, Stefano and Aleksander entered the Dungeon with a new LoLa-machine, connected a Ximea-camera and ran full HD with 60 fps and colors, with less latency than my setup on the old machine. So I guess it will be possible to have higher quality video and sound in a very near future now.\n\nAs you can see; there’s quite a substancial lower latency on the Ximea than on the Zoom, about 128 ms less, also on the old LoLa machine, but glitching sounds might occur.\n\n\n    \n\n   In this illustration you see two pictures from the video, showing the Ximea-cam on the left and the Zoom-cam on the right monitor. The time difference between the first led being unlit is 128 ms, according to the timer over the Zoom-picture. \n\n\n\nAnd now, if you like to, you can watch the video I made, but beware, your English pronunciation might be worse after watching it.\n\n\n\n\n\n\n",
        "url": "/networked-music/2021/11/15/anderlid-portal.html"
      },
    
      {
        "title": "Mixing a multitrack project in Python",
        "author": "\n",
        "excerpt": "Attempting to mix a multitrack song with homemade FX in our very own mini PythonDAW\n",
        "content": "Introduction\n\nBeing students at the SMC program is now a condition we’ve gotten accustomed to as our first term is gradually coming to an end. Usually we get some sort of assignments to solve every week, and most likely we will spend way too much time solving it. So when the last assignment was announced we decided to get extra ambitious and spend even more time on it, making sure our personal lives and sanity would be completely ruined come Christmas.\n\nWe decided to use our recently acquired Python skills to build a program that could mix a multitrack recording. With homebrew FX and everything you’d need, basically making a tiny Python-DAW except for a nice GUI. That would have made it too slick, we wanted this to be pure code.\n\nThe code for this project is available here.\n\nThe Music\n\nIn order to mix anything, we needed some music. And as our sanity was already slipping, we skipped this part until we had a program up and running. When time came to create the music our minds was so entrenched in lines of code that we almost had to write a Python program which could write music for us.\n\nThe solution was to pick some files from an old unreleased project of Arvid’s band Misha Non Penguin, and then take away all processing and writing them out as mono files.\n\nWe ended up with five tracks:\n\n\n  two drum tracks\n  one bass synth\n  one synth brass stabs\n  one plucked lead synth.\n\n\nOn purpose we chose poorly sounding synth patches, completely unprocessed. We also had a midifile containing all the midi. The challenge was on.\n\n\n  \n    \n    Mixed\n  \n  The original unprocessed music\n\n\nThe Program\n\nSegmenting and rejoining segments of audio has somehow become the bread and butter of the SMC program, so the assignment required us to do this. But this was also going to enable us to apply different processing on each segment of any track, which turned out to be an interesting option in the end result.\n\nThe basic thing needed whenever things are sliced up and pasted back together are some short overlap and fading functions, in order to avoid clicks and artifacts in the sound. Our approach to achieve this was the following:\n\n\n  \n    Creating an empty array of zeros called results, with the same size as the audio we’d segment.\n  \n  \n    Running the audio file in a for loop\n  \n  \n    Based on MIDI which gave us the downbeat of every measure, we would slice out one measure of the audio.\n  \n  \n    The segment would be given a short fade in and fade out (equal to the length of our overlap, see further down)\n  \n  \n    This segment would then be processed with whatever effects we wanted.\n  \n  \n    The segment would be zero padded (adding zeros) in the front of the segment and in the back, making the segment the same size as the original audio and our result array.\n  \n  \n    Finally the segment would be cut in the beginning by an amount equal to our overlap (set to 1024 samples in this project) and zeros were added in the end to make up for the samples removed in the beginning.\n  \n  \n    The overlap value would then be updated for the next loop, in which it had to be equal to it’s former value + itself in order to work.\n  \n  \n    Eventually we would sum the result with the segment, and repeating this process until the whole track had been mixed.\n  \n\n\nWe’ll come back to this gloriously inefficient way of processing audio later, but whenever we refer to our for loop from now on, this is what we are talking about. As an example it could look something like this:\n\n# Creating the result array we will add all signals back into\nresult = np.zeros(s.size)\n\n# Temporary arrays used in the for loop\nsegment = np.array([])\nseg = np.array([])\n\n# Setting parameters for the loop\ncounter = 0\ncurrent_beat = 0\noverlap = 0\noverlap_length = 1024\n\n\nfor i in downbeats_sample_time:\n    # To check computational time and where it struggles, uncomment print(i).\n    #print(i)\n\n    counter += 1\n\n    # Slicing the segment from current_beat to the sample position i\n    seg = s[current_beat:i]    \n    # adding fade ins and outs to the segment\n    segment = fade(seg, overlap_length)\n\n    # Processing segments one by one with different FX and signal processing\n    if counter == 1:\n\n        segment = compressor(segment, 0.3, 1, 1)\n        seg_fx = IRDelay(segment, 8)\n        segment_padded = padder(segment, s, current_beat)\n        segment_fx = padder(seg_fx, s, current_beat)\n\n        segment_padded = (segment_padded + segment_fx)\n\n    elif counter == 2:\n        segment = compressor(segment, 0.3, 1, 1)\n        segment = softClipper(segment, 7)\n        segment_padded = padder(segment, s, current_beat)\n\n    elif counter == 3:\n        segment = compressor(segment, 0.3, 1, 1)\n        seg_fx = IIRReverb(segment, 20, 800, 0.78, 1700, 0.68)\n        segment_padded = padder(segment, s, current_beat)\n        segment_fx = padder(seg_fx, s, current_beat)\n\n        segment_padded = (segment_padded + segment_fx)\n\n    elif counter == 4:\n\n        segment = compressor(segment, 0.3, 1, 1)\n        segment = reverse(segment)\n        segment_padded = padder(segment, s, current_beat)\n\n    # updating the current beat for the next loop\n    current_beat = current_beat + seg.size    \n    segment_overlapped = segment_padded[overlap:segment_padded.size]\n\n    # getting the size right again by adding zeros to the end:\n    segment_final = np.pad(segment_overlapped, [0, overlap])\n\n    # Increasing overlap for next round of for loop\n    overlap = overlap + overlap_length\n\n    # Just making sure the sizes are allright\n    segment_final = sizecheck(segment_final, result)\n\n    # Resetting the counter when it reaches 4\n    if counter == 4:\n        counter = 0\n    # pasting the segments back in, one by one\n    result = result + segment_final\n\n\n# Plotting the original track compared to the result after mixing it\nplt.figure(figsize=(8, 4))\nplt.subplot(2,1,1)\nlibrosa.display.waveplot(s, sr=sr)\nplt.subplot(2,1,2)\nlibrosa.display.waveplot(result, sr=sr)\nplt.show()\nhihats = result\n\nipd.display(ipd.Audio(s, rate=sr))\nipd.display(ipd.Audio(hihats, rate=sr))\n\n\nOur Processing Tools\n\nOur tiny Python-DAW needed processing tools, and we set about to program the usual suspects in a DAW. We would need compressors, reverb, delay, saturation and filters for EQ-ing. Easy! One week until deadline? No stress.\n\n\n    \n\n   Challenge accepted.\n\n\n\nCompressors\n\nHow does a compressor really work? Not like our first attempt. Our first compressor went through every sample checking if it’s absolute value was above a set threshold. Then it would check how much the sample amplitude exceeded the threshold, and finally multiplying it with a hardcoded value below 1. This was done in a for loop which could be ran multiple times depending on an argument in the function. After the for loop it eventually multiplied the whole signal with a makeup gain, enabling us to first compress the peaks, then turning the whole signal up a bit. In theory this little thingy did everything a compressor should do, but it did it in it’s own very time consuming way.\n\nSo it had to be improved, and that’s when we made our framecompressor. It uses a hanning window turned upside down as it’s compression envelope and analyses and compresses frames of the audio instead of sample by sample. We used a window of 2048 samples for our frames and then jumped back 1024 samples before analyzing the next frame in order to have some overlap and avoid pumping in the compression. It’s not too slow and did the job when we needed compression.\n\n\n    \n\n   Upside down Hanning window with ratio set to -0.4, resulting in 0.4 compression at the most.\n\n\n\nReverbs and Delay\n\nWe made an IIR filter on the basis of Schroeder’s very first algorithm for creating an artificial reverberator.\n\nRead more about the Schroeder reverb here\n\nIt uses a set up of four parallel comb filters which then runs into two cascading allpass filters. Setting the parameters takes a little guessing and praying to the binary gods, and it’s not a reverb you would recommend to your favorite pop star, but it’s an actual reverb and we proudly used it in our mix.\n\n\n  \n    \n    Unprocessed sine tone\n  \n  Dry sound\n\n\n\n  \n    \n    Sine tone with our IIR Reverb\n  \n  Wet sound\n\n\nOur delay was a FIR filter impulse response generating function, which does the processing in the frequency domain and therefore is much quicker to use. By choosing a number of impulses the audio signal would then be repeated as many times as the number of impulses, with a decreasing gain. It is definitely not an all purpose delay, as it works best on small segments of audio.\n\n\n  \n    \n    Brass stabs\n  \n  Brass stabs with our FIR Delay\n\n\nFilters and saturation\n\nFor filters we chose to make a function that created a FIR filter which could be either lowpass or highpass, and which did the short time fourier transform (STFT) of the signal and applied it’s filtering in the frequency domain. This ensured it would be quick, and you could set parameters for passband, stopband and order of the filter in the arguments.\n\nThe saturation function (aptly named softClipper) uses numpy.arctan and some creative math to apply saturation (or distortion if turned up):\ndef softClipper(audio, drive, output=0.8):\n\n    \"\"\"audio = Source to be SoftClipped\n    drive = Amount of SoftClipping (Try between 10-40)\n    output = Output volume\n\n    The signal is normalized before output attenuation for better control.\n    \"\"\"\n\n    # Drive can not be set to 0\n    if drive == 0:\n        drive = 1\n\n    piDivisor = 2 / np.pi\n    driver = np.arctan(audio * drive)\n\n    softclip = piDivisor * driver\n\n    normalized = softclip/np.max(softclip)\n    softClipped = softclip * output\n\n    return softClipped\n\n\nAs the last of our fx we made a small function to reverse an audio segment. This maybe the only function that Python would do quicker and easier than it’s more user friendly counterparts. Just numpy.flip(audio) and the Beatles would have been sold.\n\nInstruments\n\nOur program also needed some instruments, or rather functions that could create sound. Why? Because DAWs have that. And we wanted to make a DAW in Python. Did we use any of them? Hardly. Just one of them. But it’s there and maybe someone else would one day feel the need to enter the abyss of doom and attempt to use our program to mix some music.\n\nWe made sine, square, sawtooth and triangle oscillators, and even a pulsewidth modulated oscillator with it’s own LFO. There’s also an ADS amplitude envelope.\n\nMixing the music\n\nWith our Python DAW up and running, we set about to actually process the audio files. At this point of time our deadline was a point in the distant past and sleep deprivation had kicked in, but the joy of mixing in Python kept us going through the night for a couple of days in a row.\n\nThe process would be something like this:\n\n\n  Load one track using librosa.\n  Applying some filtering, reverb and saturation in the for loop. Maybe set up some conditional processing or change parameters in a FX every time the for loop iterated.\n  Hit the run cell command in Jupyter notebook and wait for a couple of minutes before finally hearing the result.\n  Changing some parameters in the FX.\n  Repeat process from 3.\n\n\nIt might sound like a walk in the park but don’t be fooled. The computation would be deadly slow for some of our FX, and it could take hours of testing until we would achieve anything close to a pleasing result. Sometimes we’d get lucky and find some nice settings for our FX early on, while most of the time it would sound disastrously bad and require endless fiddling with the parameters and waiting for the code to run.\n\nBut at last you could end up with something interesting, and then say some prayers hoping it would work in the final mix.\n\n\n  \n    \n    Unprocessed\n  \n  Pluck synth unprocessed\n\n\n\n  \n    \n    Mixed\n  \n  Pluck Synth processed by our tiny Python DAW\n\n\nMusic visualization\n\nFinally, all we needed was a nice visualization of our waveforms freshly processed… Based on the most popular DAWs GUI, we could generate two waveforms style:\n\n\n  one similar to Audacity (with RMS &amp; waveform overlapped)\n  one similar to Traktor/Serato DJ software (with vertical lines of the waveform color-coded by spectral centroid)\n\n\nTo acheive this, we first had to design spectral features functions based on mathematical formulas in order to extract the features we were interested in.\n\nRoot mean square, defined as the square root of the mean square (the arithmetic mean of the squares of a set of numbers).\n\n\n\n\n\n\n\nSpectral Centroid, as the name suggests, a spectral centroid is the location of the center of mass of the spectrum. Since the audio files were digital signals and the spectral centroid is a measure, this appears useful in the characterization of the spectrum of our processed audio file signal.\n\n\n\n\n\n\n\n\n\nFurthermore, coloring a waveform with spectral centroid required that we\n\n\n  \n    Compute an onset-detection function in order to locate note onset events by picking peaks in an onset strength envelope\n  \n  \n    Detect peaks in the onset-detection function to get the position of note onsets\n  \n  \n    Compute the spectral centroid over all the time segments delimited by the detected onsets\n  \n  \n    And finally, reshape the segments and time line values together and use a color gradient based on the range of the minimum and maximum centroid values of each segment\n  \n\n\n# Reshape the values for the plot\npoints = np.array([times, segments]).T.reshape(-1, 1, 2)\nsegments_reshaped = np.concatenate([points[:-1], points[1:]], axis=1)\n\n# Plot multiple colored lines on the figure using line collection\nnorm = plt.Normalize(0, max(cents)/2) # normalize centroid frequency\nlc = LineCollection(segments_reshaped, cmap='jet_r', norm=norm)\nlc.set_array(centroid) # use centroid values to determine the lines\nlc.set_linewidth(1)\nline = ax.add_collection(lc)\ncbar = fig.colorbar(line,ax=ax, label=\"Frequency (Hz)\") # colorbar\ncbar.set_label(\"Frequency (Hz)\", color='#f4f4f4')\ncbar.ax.tick_params(colors='#f4f4f4')\n\nAt last we could end up with something quite nice and informative.\n\n\n  \n\nColoring a waveform with spectral centroid\n\n\n\nAfter a few nights of this ordeal we then had six tracks we could sum together. (We re-synthesized the lead synth with pretty midi’s built in synthesize() function, and applied our usual processing to the track in order to have two different lead synths which we could pan). We set different amplitude values for two tracks, the mixL and mixR, and then merged them together to form a stereo_file. And closed our eyes and felt asleep over our computers to the sound of our very own and very first Python DAW mix.\n\n\n  \n    \n    Mixed\n  \n  Our final mix\n\n",
        "url": "/sound-programming/2021/11/24/arvidf-mixing-in-python.html"
      },
    
      {
        "title": "Chorale Rearranger: Chopping Up Bach",
        "author": "\n",
        "excerpt": "Can we use Midi data to rearrange a recording of a Bach chorale?\n",
        "content": "\n   \n   Bach, chopped\n\n\nIntroduction\n\nIt’s interesting to think about the ways in which we store musical information digitally. Different media all provide different levels of access to different precepts of a piece of music. A PDF of the score provides us with information laid out in a logical manner in case we would like to perform it ourselves; however it’s difficult to obtain information about the overall structure at a cursory glance. In comparison, a waveform of a recording of the piece allows us to quickly see the overall structure of the piece’s dynamics, but good luck trying to perform it yourself with only the waveform to work with!\n\n\n   \n\n\n\n   \n    The score and the waveform of Metallica's Master of Puppets. One allows us to quickly see the dynamic range, and one allows us to perform it ourselves. \n\n\nNone of these digital representations provide a complete picture of what is happening in the musical work, in fact we might actually receive quite a different impression of a piece depending upon the medium in which it’s presented to us. So what happens if we take the information provided in one medium and start using it to alter the information provided in another? Can we access precepts that are more hidden in one of the types of media? And can we change it to make something completely new?These are the questions that fuelled the development of the chorale rearranger.\n\nIf you want to access our Python and Pure Data code it can be found here.\n\nI Hear Voices\n\nBach’s chorales are settings of Lutheran hymns, with polyphonic harmonisations split between Soprano, Alto, Tenor, and Bass (SATB) voices. When listening to a performance, these voices meld together, forming rich and complex harmony. (As an aside, the chorales were also the focus of a quite well-known computational music project, Deep Bach, where an algorithm was trained to compose music using solely the chorales as input data. Check it out here.)\n\nIn a digital representation of a chorale in the form of an audio file, information on the individual voices is inaccessible. We are presented with the complex harmonic and rhythmic interplay of the ensemble, but to pull out information on just one of the voices is an extremely difficult task. And we can completely forget about changing it.\n\nHowever, there is another digital representation where all of the information on the individual voices is freely available: MIDI. MIDI is simply a way of representing music as a series of note objects, with each note having assigned values on when it should start and end, how loud it should be, and what pitch it is. If you feed this information into a synthesiser capable of parsing MIDI, you will have the piece of music played back to you. However, all of the notes aren’t just jumbled together, but rather are split into separate ‘instrument’ objects, based upon what should be playing them too. So by looking at the MIDI data of a chorale, the information on what each of the voices is doing is immediately clear.\n\nSo we came up with a plan: We would take an audio recording and the MIDI representation of one of Bach’s chorales, and based upon the information provided in the MIDI data, attempt to cut into the audio and recreate the individual voices. However, there’s a catch! We would repeat the chorale four times, and in each repetition, we would switch around some of the information in the MIDI data between the voices. Through each repetition we would shift over the pitch of each note by one voice, so that, for example, in the first run through the chorale, the soprano notes would be assigned the soprano pitches, then in the second the alto pitches, then the tenor pitches, then the bass. We hoped that this would reframe the harmonic and rhythmic relationships, and offer a new perspective on the chosen chorale.\n\n\n   \n   Our plan for the chorale rearranger\n\n\nPiece and Reconciliation\nAs we were going to be repeating the chorale four times, we decided to use a relatively short one. We settled upon the 3rd movement from the Matthäus-Passion, which uses the text Herzliebster Jesu, was hast du verbrochen, written by Johann Heermann, set to a melody by Johann Crüger.\n\n\n   \n   The score of the 3rd movement from the Matthäus-Passion\n\n\nWe found an audio recording performed by the Collegium Vocale Gent which you can listen to here.\n\nWe also found a MIDI file here, which we’ve played back through a synthesiser so that you can listen to it:\n\n\n  \n    \n  \n  Synthesised version of the MIDI version\n\n\nThe first thing that jumps out is that, even though these are two representations of the same piece, they are actually quite different. They vary in several musical precepts, including tempi, phrasing, and even key! So before we could start cutting into the audio, we had to reconcile these differences.\n\nThe first precept we attempted to reconcile was the key. What’s interesting here is that we weren’t interested in the objective key of each of the representations, but rather in the respective difference between the two. And because the pitch information for each note is easily accessible in the MIDI data, if we found this difference then we could simply add it to or subtract it from all of this pitch information to put it into the key we want. We also realised that, as this difference in pitch is valid across the whole piece, we didn’t need to find it for the whole, but rather simply for the first chord. So, based upon the start and end times of the first note of the MIDI file, we cut out the first chord from the audio.\n\nAs we already had the pitch information of the MIDI data, we now needed to find it for the audio. We realised that if we looked at the audio in the frequency domain, there would be peaks at the fundamentals of the pitches being sung. So we took the periodogram of the audio, calculated the highest peak, then looked at which of the pitches in the MIDI file was closest, calculated the difference, and subtracted it from all of the pitches in the entire MIDI data.\n\n\n   \n   Periodogram of the first chord in the audio. The peak is marked by the red dotted line\n\n\nWe then re-synthesised the entire MIDI file and layered it on to the audio to check if our method worked. You can listen to this and hear the results yourself!\n\n\n  \n    \n  \n  The pitch altered MIDI file layered upon the audio\n\n\nThe next precept that we attempted to reconcile was the tempo. Again, the objective tempi were of no interest but rather the difference. So after applying tempo estimation functions to the audio and the MIDI and calculating the ratio between them, we then multiplied the duration of each note in the MIDI file by it and then once again layered the two to hear if our method was successful.\n\n\n  \n    \n  \n  The tempo altered MIDI file layered upon the audio\n\n\nThe final precept, and by far the most difficult, that we attempted to reconcile was the phrasing. If you listen to the audio, you can hear that between the four main phrases of the chorale, the choir takes long, expressive pauses. In contrast, the MIDI file just barrels on through, meaning that the alignment of the notes between the two drifts in and out. If we could also tell the MIDI file to make these pauses, they would theoretically better align.\n\nAfter trying and failing many times to come up with a method to do this, we settled on taking the amplitude envelope of the audio file and again attempting to find the peaks. However, as we only wanted four peaks, after setting starting parameters, we then iterated over the envelope slowly changing these parameters until only four peaks remained.\n\n\n   \n   The amplitude envelope of the audio data. It’s possible to see where pauses are at the end of each phrase. The peaks we calculated are the red dotted lines.\n\n\nAfter doing this, we found the MIDI note which had the nearest starting point to the peak, calculated the difference in time between them, and then added or subtracted this difference from all the subsequent MIDI notes’ start and end values. We repeated this step for each peak. Although this method wasn’t entirely successful, you can listen to the layered results below.\n\n\n  \n    \n  \n  The phrasing altered MIDI file layered upon the audio\n\n\nWith the audio and MIDI reconciled as best we could, we could then start with the rearranging!\n\nShifting Pitches\n\nThe method that we used to chop up the audio for each voice was as follows:\n\n\n  \n    Slice the audio between the start and end times of the MIDI note.\n  \n  \n    Apply a narrow band-pass filter to the audio at the pitch of the MIDI note.\n  \n  \n    Pass the audio segment through a saturation function. As the filtering removed a lot of the spectral content, we needed some way to add something back in, and this saturator did the job.\n  \n  \n    Apply a sigmoid window and pad the segment to ensure that there were no clicks when joining the segments back together.\n  \n\n\n\n   \n   Sigmoid window\n\n\n\n  Append the segment onto the previous segment.\n\n\n\n   \n   Our method for chopping the audio\n\n\nWe repeated this process for each of the repetitions, each time shifting the pitch values that we took for the filter by one voice. In addition, with each repetition, we widened the pass band of the band-pass filter to allow more of the original audio through over the course of the piece, hopefully providing the final piece with some more energy over its course.\n\nThere was a problem however. Each voice doesn’t contain the same number of notes, so when taking the pitches from the notes of one voice and applying them to the segments sliced using the note durations from another we had to decide how to assign them and what to do with the excess values.\n\nOur first idea was to relate the assignment to the timing of the original piece. This would mean that if there were subdivisions in the note duration of one voice in relation to a single note in the voice that the pitches were being taken from, both of these subdivided notes would be assigned the pitch of the single note. This would also be the case in reverse, so that if the voice from which the pitches were being taken had subdivisions, only the pitch from the first of these subdivided notes would be applied to the longer note in the voice in which the pitch was being assigned to.\n\n\n   \n   Our first idea on how to assign pitches\n\n\nHowever, we realised that if we did this, there wouldn’t be so much shifting in the harmonic contexts of our final piece. So we came up with another idea.We would just run through each of the pitches in turn, with each note being assigned the following pitch sequentially. If the voice being assigned the pitches had fewer notes than the voice from which the pitches were being taken from, the last of these pitch values would be discarded. In the opposite case, the final notes would just hold on the final pitch.\n\n\n   \n   How we ended up assigning pitches\n\n\nThis resulted in the harmonies shifting for each repetition, sometimes building dissonances and sometimes creating new consonances.You can listen to the result here:\n\n\n  \n    \n  \n  The result of our Chorale Rearrangement\n\n\nHowever, this still sounds a little raw. So we decided to apply some processing to make it sound a little more like we imagined.\n\nTrust the Process\n\nThe first step we took to process the audio was to underlay it with pure sine tones at a low volume. We did this by using the same steps as we did to process the audio, except instead of using the information provided by the MIDI file to chop the audio, we used it to generate the sine waves. This made the pitches become clearer and the consonances and dissonances easier to follow.\n\nAfter this, we passed the audio file through to Pure Data to apply some light reverb and parallel filters with shifting centre frequencies to create some subtle movement.\n\nAfter passing the audio back into Python, we convolved our piece with the impulse response (recorded by Nick Green) of the silo of a Maltese citadel in order to provide an ethereal, open sense of space.\n\n\n  \n    \n  \n  Impulse response of the Gozo Citadel Silo\n\n\nOur final, rearranged chorale sounds like this:\n\n\n  \n    \n  \n  The rearranged chorale\n\n\nThe first variation sounds similar to the original, but as the variations continue, the textural consonances and dissonances start shifting, recontextualising what we just heard. We were pretty happy with the results!\n\nWe then created some waveforms in different styles, allowing us another view on the musical information of our piece. We wanted to visualize the frequency content of the stereo waveform, so we created 6 new waveforms from the originals and discarded any frequencies above, between, or below two thresholds (one between high and mid, the other between mid and low). Low frequencies are coloured red, mids are green, and blue represents high frequencies relative to the frequency content of the file. There is little blue in this plot because there is little to no audio information in the highs of the audio input as defined by our program. The light green areas are overlaps between red and green. Three tracks for each channel were then layered on top of each other to show which parts were predominantly bright, mellow, or dark in relation to time.\n\nAnother way we visualized the audio was through layering the waveform with its own RMS, which gives a more accurate representation of loudness over time.\n\n\n   \n   The waveform with the RMS visualised within\n\n\nInformed by Information\nSo what do the many different ways of representing musical information digitally enable us to do? We hope that through our project we’ve shown that through synthesizing many sources, it is possible to get varying views and perspectives on a musical work, and to continue on to create something new, beyond what is possible when only considering a single source.\n",
        "url": "/sound-programming/2021/11/26/josephcl-chorale-rearranger.html"
      },
    
      {
        "title": "Setting Levels in Virtual Communication",
        "author": "\n",
        "excerpt": "Inspired by how virtual communication has been adopted and integrated in daily life around the globe, this post looks at how simple control messages might help prevent acoustic feedback.\n",
        "content": "\n\nAudio Feedback / LOLA / ZOOM / &amp; MIDI\n\n\n\n\n\nLooking Back - and Feedback\n\nAs 2021 comes to an end, a particular virus might hopefully find its way to follow. A look back on all of what two years of special measures has brought to change - and without wish or patience to circle round any darker corners - it is apparent that virtual communication has arrived. It certainly had to - and also: it works.\n\n\n\nSMC_Portal virtual communication persists in 2021 through lectures and seminars, group collaborations, and scattered non-academic pursuits. Well documented throughout this blog - the SMC_Portal is the audio/video link connecting students in Trondheim (NTNU) and Oslo (UiO) and part-takers from around the globe joining classes and workshops via ZOOM.\n\nThe particulars of the Portal audio-communication architecture engender some seemingly inherent artifacts. Seeking to squeeze every drop of intelligibility from the LOLA sound, anyone might be tempted to turn the volume up just a little too far. Resulting is the much dreaded Audio Feedback.\n\n\n  \n  fig. 1 - Portal feedback is not usual feedback -\n\n\nRules of audio thumb dictate that microphones should not be routed to nearby loudspeakers if avoidable. The amplified signal from the loudspeakers potentially bleeding back into the microphones, and if gained to hot, causing a self-amplifying loop.\n\nWhere multiple microphones in Trondheim and Oslo are live by default - receiving and transmitting once the power is turned on - the SMC_Portal has its inherent feedback structure. Although local loudspeakers do not amplify local microphones - Oslo mics are not sent to Oslo speakers, Trondheim mics are not sent to Trondheim speakers - the system itself constitutes an augmented audio loop. As fig.1 illustrates - if the gain staging done at both communication nodes is set too hot local microphones might very well receive their own amplified signal, only by the detour of the other nodes’ loudspeakers and microphones.\n\nComplicating things further is the transitory latency governing the Trondheim/Oslo communication.  Once acoustic feedback has occurred in the system, the initial response is naturally: mute everything! Just make that howling noise disappear! To later establish what instigated the problem has proven particularly tricky. The Portal audio moves so counterintuitively that traditional approaches to feedback-reduction (EQing, Gating) have fallen short.\n\nAnd yet, rules of thumb, again, dictate that if acoustic feedback occurs, it is because IT IS TOO LOUD! Gain the microphones DOWN by 6dB - or even 12dB - and that incessant ringing disappears.\n\n\n\n\n\nLooking forward\n\nCooperative communication in 2020 and 2021 has been exercised by speaking into a computer keyboard while looking at a small screen. Regardless of age, education,  ethnicity, gender - regardless of most any socio-cultural context or geographical divide - people have learned to adopt a new language allowing those communications to be good ones.\n\nZOOM, Teams, and Skype have some very impressive echo-canceling algorithms working to make the laptop meeting more pleasant. But more important still might be the unmute button. Simply turning the microphone off when not using it is an excellent idea. If everyone does their part - acoustic feedback becomes a non-issue.\n\nWhich has inspired this simple proposition:\n\n\n\n\n\nFighting Audio Feedback with MIDI\n\nThe quickSet_TotalMix.pd patch is directly inspired by the mentioned unmute buttons of ZOOM et al. With simple, customizable keystrokes or similarly settable MIDI ControlChange messages - or MIDI notes - the patch sends six presets to safeguard against acoustic feedback. [download_patch]\n\n\n  \n  fig. 2 - TotalMix submixes reflected in the quickSet_patch \n\n\nThis control is made possible by the technological backbones providing for the Portal virtual communication. LOLA: the “Low Latency AV Streaming System” requires one of six RME Audio Interfaces to operate. All of these sound cards utilize the MIDI programmable TotalMix software as the graphical user interface.\n\nSelecting a TotalMix output operates similar to a Sends On Fader function - as found in many digital mixing consoles. fig. 2 shows how the quickSet_patch has mapped the four input-faders directly corresponding to their output mix. This way, the quickSet_patch affords an intuitive hands-on control of any sub-mix fader. In turn, allowing for triggering six sub-mix presets affecting all outputs. Three of these presets are seen at the top of fig. 2\n\nThe “… is talking” presets give that when one of the LOLA nodes is talking, the other nodes’ microphone outputs are attenuated by -20dB. A particularly safe buffer against that looping signal from fig. 1. If “ZOOM is talking” or “MUSIC is playing” both LOLA nodes microphone outputs are attenuated. A “Default” preset sets all microphone outputs to an equal level at -6dB - again, a safety buffer. Finally, there is a “MUTE ALL” preset - because, well… Just in case.\n\n\n\n\n\n### MIDI setup\n\n\n  fig. 3  MIDI-setup in PureData and TotalMix  \n\n\n\nRouting MIDI from the quickSet-patch to TotalMix and LOLA is relatively straightforward. Although LOLA is Windows only, and internal MIDI-routing can be cumbersome, all LOLA-compatible RME sound cards have dedicated MIDI-ports. This makes possible for the quickSet_PureData-patch to send its information through any MIDI interface that utilizes standard 5-pin DIN-cables directly into the LOLA operating RME-card as illustrated in fig. 3. Here: the TotalMix MIDI input port is set to its own Fireface UCX input. Set the PureData MIDI input to what controller one wishes to use\n\n!NOTE! the PureData MIDI Output sending control messages to TotalMix must be set to the topmost of the Output Devices slots for the quickSet_patch to operate as intended straight out of the box (seen left in fig. 3).\n\n\n\nFor the Future…\n\nThere is much still wanting in the quickSet_patch. A preset storing system… OSC routing… Optimizing MIDI timing… As it is, it also “requires” another fixture of the laptop meeting: The Moderator. Perhaps a path forward is with automating these tasks? Assuming that with use in real-world application, there might be arise more questions than answers…\n\n… but hopefully less Audio Feedback\n\n\n\n\n\n\n\nPOST SCRIPT  — These somewhat hypnotizing silver sinewaves are the faders on a Midas M32 mixing console. Controlling the M32 - another fixture in the SMC_Portal setup - is another path to reducing feedback. Next year then…\nThough serving no sensible purpose for any audio - and disregarding the absolute racket emanating from twentyfive hardworking motorized faders - there is a particular satisfaction in sending some MIDI-messages from a MaxMSP_patch to set some things in motion.\n\n… so a huge thanks to Joni Mok for all help, reflections, inspiration, and drive!!\n\n\n\n\n\nFURTHER READING:\n\n\n  A fantastic resource on DSP to minimize acoustic feedback:\n    \n      https://www.ranecommercial.com/legacy/note158.html\n    \n  \n  The TotalMix MIDI-implementation chart:\n    \n      http://rmouneyres.free.fr/puredata/MIDI_Assignigation_TotalMix.pdf\n    \n  \n  A 5-minute video explaining and demonstrating how to maximize signal quality from any speaking-microphone:\n    \n      Sound Advice for Intelligible Audio\n    \n  \n\n\n\n\n\n\n\n",
        "url": "/networked-music/2021/11/26/jarlefst-portal-feedback.html"
      },
    
      {
        "title": "The Telematic Experience: Space in Sound in Space",
        "author": "\n",
        "excerpt": "A report on our telematic experience in Salen\n",
        "content": "\n   \n   Performers in Salen before the concert\n\n\nIntroduction\n\nThe term telematic performance refers to a live performance (art, dance, music, etc.) which makes use of telecommunications and information technology to distribute the performers between two or more locations. When it comes to organizing and running a large-scale public networking concert in the UiO Portals, this means that we had to be able to receive audio from the other performers, manipulate it in real time, and be able to communicate musically despite being in separate rooms.\n\nAs Barry Truax mentioned [1], composition and diffusion can be understood as two complementary and related processes: bringing sounds together, and spreading them out again in an organized fashion. In our point of view, a telematic performance can be understood in a similar way because it focuses on space in sound in space:\n\n\n  Space in sound i.e. variations and manipulations of sound using Pure Data patches.\n  Sound in space i.e. receiving and broadcasting sound through the UiO Portals\n\n\nFor this specific performance, two teams were split between two spaces in the Department of Musicology (Portal &amp; Salen). As members of the team Salen, Hugh and Jakob focused on technical configurations and routing while Joachim and Oliver focused on the performance.\n\nPerformer Reflections\n\nWe created our systems independently, but communicated about what each of us wanted to contribute through a few in-person meetings. We agreed on constraints due to the use of several acoustic instruments, along with a general outline of the music: an improvised sound texture.\n\nAccording with team Portal, we agreed on two different improvised sound textures:\n\n\n  Dry i.e. rhythmical and sharp musical variations or manipulations\n  Wet i.e. extensive use of reverb, delay and feedback with sustained sounds\n\n\nFurthermore, we didn’t have proper rehearsals with all the performers in one physical space, as the whole point was to create music over a network and to consider the latency introduced by the network.\n\n\n   \n   Performers reflecting on the performance\n\n\nTechnicians Reflections\n\nFor the rehearsals, we split the work mostly evenly, although Jakob mainly focused on the video and LOLA setup, and Hugh on the audio setup. For the concert Jakob handled the video stream mix and kept an eye on the LOLA machine, and Hugh did the audio mixes. Three mixes were created, the Salen FOH mix, the LOLA mix to send to the Portal, and the streaming mix. It was sometimes a challenge to keep the routing clear in our heads, especially in terms of the different mixes. There was a challenge posed in that the streaming mix send was also the master send to FOH, and therefore it was easy to accidentally clip the streaming mix. On the whole, however, we were satisfied with our technical setup and performance.\n\n\n   \n   Technicians configuring the routing\n\n\nTechnical Setup, Softwares, Settings and Configurations\n\nCommunication\nIn order to communicate during the rehearsals, we primarily used the software Discord for troubleshooting. We also weren’t beyond relying upon a good old phone call when we needed to talk effectively.\n\nMedia\n\n\n  Routing diagram for each location\n  Link to stream\n  Mixer routing spreadsheet\n  Poster\n\n\nPerformers equipments\n\nOliver’s equipment\n\n\n  Laptop\n  Zoom H6 Interface &amp; XY Microphone (1 device).\n  XLRs inputs from the mixer (other performers) into H6, USB to laptop.\n  Mini-jack from Laptop to DI box to mixer.\n  iPhone, mini-jack to 2 x DI box to mixer.\n\n\nPure Data patches:\n\n\n   \n   Ambient Generator: This patch is controlled from an iPhone 6 and controls which notes are played and the modulation of those notes\n\n\n\n   \n   Autosampler: This patch takes input from the performers in the Portal as well as a kalimba and snare drum. It then lets us sequence and change the speed of the audio we get\n\n\nJoachim’s equipment\n\n\n  Laptop Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz 1.80 GHz 8Go RAM\n  AudioBox Interface (Presonus), USB to laptop.\n  Stereo mini-jack from laptop audio output to 2 x DI box to mixer\n  Alto saxophone\n  Shure SM57 Microphone\n\n\nPure Data patch:\n\n   \n   Telematic performance patch: This patch takes the alto saxophone as input. Sound can be either pitched with the subpatch pitch shift, sequenced, modulated using the RM/AM modulator or filtered using the Parallel filter bank analyzer\n\n\n\n\nThe Music and Experience of Playing Telematically\n\nSeveral challenges needed to be taken into consideration regarding the telematic performance. As discussed by Jonas Braasch [2], these challenges remain mainly technical and involve physical-distance and latency issues. In our case, because we were still new to the sudden changes of speed and bandwidth of multiple networks, and because latency problems remain a significant issue for audio-visual streaming of live telematic performance, we tried to develop a new musical language working on this new time basis. With this in mind, considering our performance a structured sound texture rather than a song was more appropriate and manageable.\n\nMedia Content and Relevant Links\n\nEnd of semester concert for students enrolled in the Physical-Virtual Communication and Music (SMC4024) Autumn 2021\n\nTelematic performance\n\nTelematic concert guide\n\nFinal Thoughts\n\nWe were happy to have the opportunity to put the skills and theories that we had learned throughout the semester into practice, especially in a manner that allowed us to explore musical spaces that interested us. The concert also emphasized the way in which we work well as a team, with very quick division of work and no conflicts in the preparations. We are already looking forward to the next concert and to further explore aspects of space in sound and sound in space.\n\nReferences\n\n[1] Truax, B. (1998). Composition and diffusion: Space in sound in space. Organised Sound, 3(2), 141-146. doi:10.1017/S1355771898002076\n\n[2] Braasch, J. The Telematic Music System: Affordances for a New Instrument to Shape the Music of Tomorrow, Contemporary Music Review, Routledge, 2009\n",
        "url": "/networked-music/2021/12/10/joachipo-telematic.html"
      },
    
      {
        "title": "The Granjular Christmas Concert (Portal View)",
        "author": "\n",
        "excerpt": "A report on our telematic performance in the Portal.\n",
        "content": "Telematic Performance December 3rd, 2021\n\n\n   \n   Performers in the Portal during the concert\n\n\nIntroduction\n\nThe annual Christmas concert of the first year SMC students has become a tradition at the Oslo campus, and this year it was our time to mark the start of the holidays with a telematic performance. We decided on the theme “A Granjular Christmas Performance”, which for the non-Norwegian speaking audience is a pun on the words “gran” (spruce, aka christmas tree) and “jul” (Christmas). In this post we will try our best to describe how we went about it, both technically and musically.\n\nThe concert took place at two separate locations at IMV (Institute of Musicology) on December 3rd, 2021. Our team was based in our much beloved Portal, while the other team and audience were situated in Salen, an auditorium in the basement of IMV.\n\nWhat is telematic music and LoLa?\n\nIn order to better understand the concert that we carried out, it is necessary to understand two concepts: what telematic music is and what LoLa is.\n\nTelematic music can be defined as “music performed live and simultaneously across geographic locations via the internet.”  (Oliveros et al., 2009, p. 1).\n\nLoLa, as the name points out, is a “Low Latency audio visual streaming system” that provides the possibility for an accurate real-time connection between different places. It is the key piece of technology that allowed us to perform telematic music during our concert. LoLa webpage.\n\nMusical vision\n\nMusically, the four performers (two in the Portal, two in Salen) planned for an improvisation with lots of live sampling and granular synthesis. We used some eclectic selections of gear and homemade Pure data patches to create sounds and create interesting textures together, aiming for one part of dry textures (minimal use of reverb), and one part wet, with lots of reverb and delay. The aim was to listen to each other’s sounds and build upon the different sonic contributions, as well as ornamenting the melodic contributions of the saxophone being played live downstairs in Salen.\n\n\n   \n   The audio rig used by the performers\n\n\nTechnical setup\n\n\n   \n   Block diagram of technical setup\n\n\nThe LoLa rack is a recent creation by SMC, providing an all-in-one, portable setup for low latency telematic performance. For this concert, two LoLa racks were connected to each other: one from the SMC portal (LOLA4) and one from Salen downstairs (LOLA3). Both audio and video were passed through the LoLa program, and routed as seen in the diagram above. The portal performers were positioned in a way where they could easily see the downstairs performers through a monitor at any time, as shown in the pictures below. For a more detailed explanation of our setup and the performer’s rig, please visit our wiki page.\n\n\n   \n   Monitor facing the performers\n\n\n\n   \n   Audio setup\n\n\nPreparations\n\nDuring the few rehearsals and test-runs in advance, the time was mostly spent on setting up, troubleshooting, getting some noises flowing between the two stages, and then troubleshooting some more. From the musicians perspective, there was a lot of time to play around and develop musical ideas, and fine-tune the instrument setup. But this was more of a duo endeavor with the musician seated next to you. We got little time to actually practice together as the full quartet, to explore and develop musical ideas together, except for earlier on the very day of the concert. But, as our concept was to engage in a collective granular improvised session, this wasn’t necessarily a hindrance. It made us keep the ideas fresh and our minds sharp for the concert.\n\nGoing live\n\nOnce we were set to go and the lights were on, we did experience some transfer issues resulting in a lot of clipping and noise from the downstairs audio stream. We crossed our fingers that our stream passed through more fluidly the other way, and started playing. After a few minutes the quality of the sound improved, and it was a lot more rewarding as an improvisational experience. Some interesting textures were created, and it felt quite natural to perform together although not located in the same room. The monitor with the visual feed of the performers from Salen was surely a great aid in order to achieve a sense of interplay and connection. The rhythms were mainly instigated from Salen, and there were never any issues with latency in regard to playing along to the rhythmic structures. We had opted not to use any midi clock or other synchronization tools, so all delay effects and rhythmic components had to be done by ear, which is always interesting when working with electronic music.\n\n\n   \n   Performers hard at work creating that granjular christmas mood\n\n\nDealing with the technical part of the concert was an experimental experience. In the rehearsals, we had to move a lot of equipment up and down and get used to a certain setup for which we took plenty of notes. However, the day of the concert, we were given the LOLA4 rack, which was appreciated while a little nerve-wrecking. The notes we had taken up until now, were rendered somewhat useless and could not help feeling nervous until we were taught how to get around the new environment. Once all instructions had been given, everything seemed to run smoothly which allowed us to focus on the “stage” decor and on the rehearsals. Nonetheless when it was time for the concert, we experienced some jitter which forced us to restart the LOLA computer. That somewhat improved the issue, and the remaining problems were solved on the go.\n\n\n   \n   The LOLA4 rack\n\n\n\nFull concert.\n\n\nThe whole concert was streamed on YouTube and is available here. We had a really good time and are already looking forward to our next concert.\n\nBibliography\n\nConservatorio di musica G. Tartini. LoLa, Low Latency Audio Visual Streaming System: Installation &amp; User’s Manual. Retrieved from https://lola.conts.it/downloads/Lola_Manual_1.5.0_rev_001.pdf\n\nOliveros, Pauline, et al. “Telematic Music: Six Perspectives.” Leonardo Music Journal, vol. 19, 2009, pp. 1–29., https://doi.org/10.1162/lmj.2009.19.95.\n",
        "url": "/networked-music/2021/12/10/arvidf-granjular-concert.html"
      },
    
      {
        "title": "'Chasing Stars': An interactive spatial audio application",
        "author": "\n",
        "excerpt": "Let’s explore an interactive 3D-audio application under an ethereal sound landscape.\n",
        "content": "This project is an interactive spatial audio application considering an artistic approach as a starting point for its development. A description of the design and implementation is provided as well as the interaction of several subsystems in the development chain (motion capture, remote control, system behavior, audio generation, spatialization, and sound playback).\n\nThe Concept\n\nThe music and sound composition is based on an Ethereal environment, I interpret this concept as sounds “out of this world” that have to do with a fantasy experience.\n\nThe inspiration for this type of composition comes from the way in which artists have depicted the outer space regarding color and shapes, and their translation with “non-existing” sounds that can be achieved by the human imagination through sound synthesis. Additionally, I believe that this kind of composition can be also focused on a human perspective in terms of emotions. The picture below shows an illustration that tries to describe the concept of this project considering the fantasy component (art style), emotions (character expression), and situation (trying to reach a star).\n\n\n   \n   'Chasing Stars' by MrPerv\n\n\nAs such, the goal of this application is to reflect the situation shows in the illustration mentioned before, which is Chasing Stars. The idea is to fill an immersive sound ambience based on the space theme with some sonic elements moving around. In this environment, a participant is able to listen sounds that come from random directions and he or she should approach towards the right direction so that a new sound is triggered, this new sound informs you that a “star” has been caught.\n\nIn order to implement this application, a motion capture system was used. The next video shows a demo performance and the data from the motion capture regarding the position of the participant (red circle), and the position of the guidance sound that calls the user to a specific direction (green circle). The participant is trying to go to the place where he or she thinks the sound is coming from, and move the hand towards the guessed direction. Note that the participant is holding an object which is tracked by the motion capture system so that the position can be sent to the controller to confirm whether it is close to the right direction or not.\n\n\n   \n   Video Demo\n\n\nSound Elements\n\nThe sound elements that are part of the application are the following ones:\n\n\n  “Strings-like” sound: This is the base sound that fills the soundscape and is present from all directions. It is a chord progression that leads the musical motif.\n  Reverse Hi Hat: This is a rhythmical sound that is composed of two sources (each stereo signal) traveling around the space. It sets the musical tempo and the spatial perception of two elements in motion.\n  Guidance sound: This a soft but strong sound that is triggered in the random direction to localize. It is a random musical note from the A minor scale (440Hz to 880Hz)\n  “Vibraphone-like” sound: This is a sharp and percussive sound that represents a “hit” when the guidance sound is localized and is also triggered as a musical note as the guidance sound.\n  “Sparks-like” sound: This is a continuous sound that is played in the direction in which the performer move the tracking object. It receives the same chords from the “Strings-like” sound but synthesized differently.\n\n\nSpatial Features\n\nUna spatial audio technique used is pairwise amplitude panning, in which the relative amplitude of two adjacent loudspeakers is adjusted to create a phantom image between them. The figure below shows the contribution for a pair of speakers depending on the angle position in an 8-loudspeakers array. This configuration was implemented in order to increase the spatial resolution so that the participant can identify the direction more precisely.\n\n\n   \n   Pairwise amplitude panning\n\n\nThe sounds that use this strategy are: guidance, vibraphone-like (hit), and sparks-like (continuous), which are the ones that are closely related to the interactive part. Although this technique depends on the speaker configuration, it is possible to translate to a different setup. The following picture depicts the complete solution that was used for a 8-loudspeaker array and a binaural setup. Note that an encoding and decoding process is needed for the binaural format, which basically consists in treating the speakers as sources placed in a space where the stereo system is located.\n\n\n   \n   Pairwise amplitude panning - Encoding and decoding process\n\n\nOther strategy that was used is High Order Ambisonics (HOA) for the string-like (base), and reverse hi hat sounds. The next figure shows how every channel of these sources is placed in the sound field. Note that these two sounds are composed of a stereo signal in which source 1 and 2 are the ones corresponding to the hi hat sound and are constantly moving around the center in a epicycloid and cardioid path respectively. Sources 3 and 4 are the ones for the base and are placed permanently in the same positions to fill the environment.\n\n\n   \n   Base and Hi hat sources placed in the sound field\n\n\nThe process to encode and decode to the 8-loudspeakers array and the binaural setup is shown below. The four sources and the loudspeakers are positioned in the sound field and then encoded into a third order ambisonics format. The trajectories act over the sound field and are part of the encoding, then, two decoders are used for each setup.\n\n\n   \n   Ambisonics - Encoding and Decoding process\n\n\nThis model is used for the mentioned sources because it reinforces the spatial environment, and a high spatial precision is not required. The advantage of this configuration is the management of any loudspeakers array. The final mix of the whole application is the sum of the pairwise sources and the ambisonics ones into one only multi-signal output according to the required setup (8-loudspeakers or binaural).\n\nImplementation\n\nThe overall implementation is illustrated in the next diagram. The main framework for the development was Max/MSP with the interconnection of several modules explained below:\n\n\n   \n   System Implementation: Modules and Subsystems\n\n\n\n  Motion Capture (MoCap): The OptiTrack motion capture system was used to detect a specific object in space. This object is called rigidbody and is an element that provides position ( X, Y, Z), and rotation (roll, pitch, yaw). The application uses the position projected to the plane (X, Y) that is parallel to the floor to control the ‘vibraphone-like (hit)’ and ‘sparks (continuous)’ sound position in real time. The data is sent from the PC that runs the software that controls the MoCap (Motive), to the machine that runs the application, through OSC messages over the local network. An implementation used from the N-Place project to take the data streamed from Motive to an OSC module in the MoCap PC was used to transmit the data from machine to machine.\n  Remote Control from Smartphone: There are toggle controls sent by a smartphone to start and stop the application and, start and stop the guidance sounds and the game behavior. This remote control can be used with any mobile application that sends OSC messages. The spatial application just needs to take the right path for every trigger.\n  System behavior (Controller): The controller implements the logic for the interactive behavior. It generates the random angles for the guidance sound and triggers them every four beats when it is started. It receives the position data from the motion capture and translates the cartesian coordinates to spherical coordinates, then it takes the azimuth angle which is compared with the random angle to trigger the hit sound if necessary. Additionally, this azimuth angle is mapped to the “sparks-like” continuous sound so that it can follows the rigidbody.\n  Audio Generation: All the sounds, except the hi hat (an audio sample), are synthesized by the plugin Hybrid3. There is a pre-recorded MIDI score for playing back the chords through this synthesizer. Moreover, the application manages an array of numbers that represents an A minor scale, which is converted to MIDI notes that feed corresponding instances of the same plugin.\n  Spatialization: The spatial features used in the implementation are detailed before. The library Spat for Max/MSP was used for ambisonics, and a custom implementation for the pairwise panning was developed by the author. The solution takes into account an 8-loudspeakers configuration and a stereo setup, both simultaneously. There are several streams of audio that are mixed to compose one only output as shown in the implementation diagram.\n  Sound Playback: Max/MSP is capable of sending audio signals to independent channels in a sound card. This implementation considers 3 type of outputs that are assembled at the same time: Two set of 8 signals (two 8-loudspeakers array), and one set of two signals (stereo).\n\n\nConclusions\n\nAs an interactive spatial audio application, this project demanded to solve technical challenges for its development. One of them was to use the right set of tools in order to reduce complexity and increase reliability. Thus, it is important to study the problem carefully to choose the platforms that best fits the application.\n\nChoosing the proper spatialization techniques is crucial according to the problem. In this work, as explained, a pairwise panning approach was used instead of an ambisonic one to have full control of the gain sent to the loudspeakers, so that it has a higher spatial resolution to easily identify the guidance sound. However, due to perceptual reasons, it is unclear if there is a major difference between both techniques for this specific application, thus it can be part of a future testing.\n\nA personal remark regarding the whole process to build this application is that a combination in sound exploring and spatialization techniques can be a source of inspiration for artistic work, and it can easily become in a never-ending creative workflow.\n\nAs future work, the musical composition can be enriched with more spatialized sounds and musical layers, however there is a trade-off that should be considered if the interactivity is affected by a dense sound environment. The interaction can be increased by adding more degrees of freedom and mappings, which has to take into account principles for human-machine interaction. This application can be scaled to an interactive music system by exploring music-making capabilities in terms of gestures and movements. Finally, visual feedback by using technologies such as virtual or mixed reality can improve a complete 3D experience for a user.\n",
        "url": "/spatial-audio/2021/12/10/pedropl-chasing-stars.html"
      },
    
      {
        "title": "Micro and Macro: Developing New Accessible Musicking Technologies",
        "author": "\n",
        "excerpt": "Most of us will never be professional musicians, even though we have the musicality and the motor ability to achieve it…\n",
        "content": "\n   \n   The Macro 2.0 version\n\n\nIntroduction\nMost of us will never be professional musicians, even though we have the musicality and the motor ability to achieve it. Of course this is mostly due to coincidences, culture and personal preferences, but it is also dependent on our motor abilities. Most musical instruments demand very precise and fast motor control, which not all of us have. Of course this is something a musician will train up with thousands of hours of practice. However, some people I have talked with, were hindered from being able to even start up by playing the instruments they wanted because of their motor skills. This made me reflect on this question: would it be possible for me to create musical instruments that were engaging and challenging to play, but at the same time possible to play with only gross motor skills? How is it possible to design and implement accessible musicking technologies that can be controlled with motion in the air?\n\nAccessibility\nAccessibility in digital music instrument design is not a new thing, and already there exist instruments and interactive music systems that are used in music therapy and aimed at being accessible for people with disabilities. Emma Frid (2018) researched what trends and solutions of accessible digital musical instrument design that exist in NIME, SMC and ICMC conferences the past 30 years. The most common approach was to create tangible controllers which were used in 13 of 30 cases, like for instance an electronic hand drum designed for music therapy sessions. Non-tangible solutions, like air instruments, was the second most common approach with five of 30 papers (Frid, 2018).\n\nSome examples of musical instruments that have been designed with the purpose of being inclusive to people with physical disabilities are Soundbeam and Motion Composer. Common for these two instruments are that they are based on motion in the air. Motion Composer uses complicated video tracking technology, while Soundbeam uses ultrasonic sensors to track motion. Both systems are used in music therapy, and Soundbeam claim on their webpage that their instruments have been thoroughly tested and evaluated - especially in use cases for children with learning difficulties.\n\nHowever, both Soundbeam and Motion Composer are commercial products, and at the time of writing this blog post, a Soundbeam 6 costs about 100 000 NOK (about 10000 EUR). In other words, it is not possible for a person with a normal income to obtain such a thing. A Motion Composer is not really possible to get your hands on (yet?), at least not from what I can understand from their german webpage.\n\n\n  \n\n   \n   Soundbeam. Source: SoundbeamFilms Soundbeam youtube channel.  \n   \n   Motion Composer. Source: Motion Composer Facebook page \n  \n\n\n\nAll of this made it clear to me that the world needs a web-based musical instrument that can be controlled with motion in the air. There are several ways to do this, however, I chose to try out two different approaches: one by using a web camera and one by using a mobile phone with accelerometer and gyroscope sensors. Both approaches have two apps each, so In total I have developed four apps. However, to reach this stage, the prototypes have been through several development steps with collection of user feedback in each stage.\n\nThe evolution of the Macro apps\n\nThe Diff Cam Engine\n\n\n   \n   The Diffcam Engine\n\n\nThe Diff Cam Engine is an open source core engine that enables motion detecting in JavaScript, created by Will Boyd. As I had learned audio programming in JavaScript through one of the SMC courses, I decided to start on the path of creating motion detecting web-apps for musicking.  Very quickly I found out how to use this technology to create sound:\n\n\n    The very first motion capture musical instrument attempt   \n\n\nMacro 1.0\n\nAs you can see from the video above, the latency between action and sound was pretty big. So the next step was to lower the latency, make the system more interactive and improve how it sounded. This led to the first official version which was released with a feedback form that asked people to test out the apps and come with feedback. This version was called Macro 1.0 and was named after the Macro level; one of the three “spatiotemporal levels of human action” (Jensenius, 2017) (this will make more sense in the next section when I describe the Micro apps).\n\nMacro 1.0 consists of three apps: App 1, App 2 and App 3. They are all based on web camera motion detection, but have different approaches for musical interaction. In the video below, you can see a demonstration of Macro 1.0, or try it out yourself from this link.\n\n\n   Macro 1.0 demonstration video   \n\n\nMacro 2.0\n\nWith the second release I took the system a step further by introducing a button free design. The intention was to make the instrument more accessible by avoiding fine motor skill dependent actions like clicking a button with a mouse. This version includes only two apps, as app 2 and app 3 from Macro 1.0 were combined into one app. Try out the apps on this link:\n\n\n   \n   The Macro 2.0 version with explanation\n\n\nMacro 2.1\n\nThe last iteration took all the feedback and tried to improve the instructions, the interaction and the visuals of the apps. App 1 is a Theremin-like musical instrument where you can control pitch on the Y axis (vertical) and select between two effects and two instruments on the X axis (horisontal). This iteration has less functionality than the previous iteration, as feedback from users showed that people had problems with unintended output. Still it is a problem that the app interprets all movement as input, and the next step will be to teach the system to separate between different kinds of gestures.\n\nApp 2 is a random music generator, where the user can interact by turning instruments on and off and by applying effects (on the Y axis). The web-app Synastheasia inspired me to create a system for random music generation.\n\nYou are welcome to try out the apps from this link. Check out the video below to see a demonstration of Macro 2.1 app 1 and app 2:\n\n\n\n\n\nMacro 2.1 App 1 \n\n\n\nMacro 2.1 App 2\n  \n\nThe evolution of the Micro apps\n\nAt about the same time as I was starting to create the Macro apps, I was developing the Micro web-apps for the MICRO project at RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion at the Aalborg University. The apps were developed as air instruments and explored micromotion by using accelerometer and gyroscope sensors on smartphones with JavaScript and Web Audio API. I decided to develop further on the apps as a part of this Master’s thesis.\n\nMicro 1.0\n\nThe first release of the Micro apps included three different apps with different approaches for exploration of sound and music. The apps have it’s own wiki where all of the three apps are explained. You can also watch this demonstration video which describes the apps:\n\n\n\n\n   Demonstration video of Micro 1.0 apps\n\n\n\nMicro 2.0 and Micro 2.1\nWith the second iteration of the Micro apps, there was a goal to remove the buttons and find a way to make the instrument entirely touch-free. This was achieved by implementing a way to make the user “hover” the buttons with a blue dot that monitored the motion of the phone. In this way, the user could both control melodies and musical parameters by tilting the phone, but also activate buttons by tilting the phone in different angles. However, it was not possible to avoid buttons entirely, as the accelerometer and gyroscope sensors has to be activated by a touch action. The solution was to let the button fill the whole screen.\n\nIn the app 2 of the 2.0 release, the same random groove generator as in Macro 2.0 app 2 was introduced. In the Micro 2.1 release, most functionalities were the same as in the 2.0 version, but the visuals were improved and also some of the sound design was improved.\n\nThe video below show a demonstration of the Micro 2.1 app 1:\n\n\n   Video demonstration of the Micro 2.1 app 1   \n\n\n\nUser studies, reflections and further research\n\nThe goal of this Master’s thesis has been to create accessible web based musicking technologies, which meant that it was important to test out how accessible the prototypes actually were. To narrow down the focus, I had from the beginning decided to focus on people with low fine motor skills. I was lucky and got in contact with three informants who had low fine motor skills, and as a control group I invited three informants with normal motor skills. The main findings from the studies showed that people with low fine motor skills and the people with normal motor skills were able to interact with the prototypes on almost the same level. Both groups could explore sound and music with the apps, however, both groups encountered problems with the Macro apps creating unintended sound by interpreting all motion as input. The informants with low fine motor skills self-reported in general less enjoyment with the apps than the group with normal motor skills.\n\nThe Micro 2.1 app 1 seemed to be the most enjoyed apps among the users. Interestingly, it was shown that the apps were probably less accessible for the users without touch and mouse functionality than it would have been if touch was implemented. Even though the users reported that they had low fine motor skills, they were used to touch and mouse technology and found this easier than making gestures in the air. The exception was one of the three users who said that tilting the phone to activate buttons was easier for him than clicking a button on the mobile screen.\n\n\n   \n   How the users self-reported the experience with the apps\n\n\nHowever, only three users with low fine motor skills are not enough to generalize or make any clear conclusions, and more development and research should be done to investigate further on this technology. The obvious advantage with developing web based software is that it is made easily available and accessible to a huge amount of people. In these days with pandemic lockdowns, we are again being prevented from seeing each other, and the development of online motion tracking musicking technologies might hence be of value both for future research, but also for artistic experience.\n\nThe most important findings from the user studies was that the musicking technologies should be able to adapt and optimized to the individual in order to offer accessibility for people with low fine motor skills. For many people, touch technology might increase the accessibility, and can possibly be used in addition to motion tracking technology. Based on other accessible instruments such as Motion Composer and Soundbeam, it is likely to assume that the air instrument approach still has a great potential for accessibility. The technology developed in this thesis offers a great potential for improvement, and when mobile and computer cameras are improved with e.g. the option for 3D interpretation of the video stream, more precise and complicated mappings can be implemented. Implementing image recognition and machine learning within web camera based apps, can help distinguish between different kinds of movements.\n\n\n   \n   Comparison of the Micro and the Macro apps\n\n\nLinks:\n\n\n  Web page of the project\n  The Master’s thesis pdf\n  Macro 2.1 github repository\n  Macro 2.0 github repository\n  Macro 1.0 github repository\n  Micro 2.1 github repository\n  Micro 2.0 github repository\n  Micro 1.0 github repository\n  Macro 2.1 web-app\n  Macro 2.0 web-app\n  Macro 1.0 web-app\n  Micro 2.1 web-app (must be opened on a smartphone)\n  Micro 2.0 web-app (must be opened on a smartphone)\n  Micro 1.0 web-app (must be opened on a smartphone)\n\n\nGood bye!\n\n   \n   Thanks for now!\n\nReferences\n\n\n  Frid, E. (2018). Accessible digital musical instruments-a survey of inclusive instruments. In\n  Proceedings of the International Computer Music Conference (pp. 53-59). International\n  Computer Music Association.\n  Jensenius, A. R. (2017). Sonic Microinteraction in “the air”. In M. Lesaffre, P.-J. Maes &amp; M.\n  Leman (Eds.), The Routledge companion to embodied music interaction (pp. 429-437).\n  Routledge.\n\n",
        "url": "/masters-thesis/2021/12/14/fractionMari-micro-and-macro.html"
      },
    
      {
        "title": "Rockheim - about an interactive exhibition",
        "author": "\n",
        "excerpt": "Anyone who visited Rockheim knows about the Time Tunnel, where 6 screens show the history of Norwegian music from the 50s until today. If you have no idea what I am saying, add a visit to Rockheim on your bucket list! For this project, we had to investigate and offer improvement ideas.\n",
        "content": "Rockheim - About an interactive exhibition\n\n\n   \n   Main exhibition at Rockheim\n\n\nIntroduction\n\nRockheim is one of Norway’s most exceptional museums. It is the National Museum for Popular Music from the 1950s to the present. It is housed in a former warehouse on the bay of Trondheim.\nThe primary mission of the museum is to preserve and document Norwegian popular music through diverse exhibitions.\n\nOne of their main permanent exhibition is known as ‘The Time Tunnel’ and it opened to the public on 5th August 2010.\nRockheim’s biggest attraction presents music and artists in a cultural-historical context. The time travel starts in the 1950s, when rock first came to Norway, and continues towards the present time. Using interactive dissemination technology, the public is invited to actively participate during their visit. As part of the museum’s staff the guides who are present on each floor of the museum help create a smooth experience, by assisting the audience and engaging them in conversation.\n\nThe massive grid of screens on the wall is located at the entrance of the floor, in front of the elevators. It consists of six displays with each orientated as portrait. From left to right they are assigned for the 50s, 60s, 70s, 80s, 90s, and 00s, respectively. Static images depicting artists of each decade are shown until someone interrupt the flow by “breaking the glass”. In front of the screens stickers with each decade are placed on the floor to mark where on the ceiling are the webcam sensors placed. When someone stands on a sticker, the camera detects the shape of hands or the body of person from above. The outline of the movement is over-layed on the screen with green, over the virtual glass layer over the static images. By moving the hands, the person can “break the glass over the screen”, resulting in a short video of that artist playing.\n\n\n   \n   Screen interaction\n\n\nOur applied project\n\nSince this main interactive exhibition was implemented in the museum, technology evolved considerably, and several problems became clear:\n    - it was not obvious that the exhibition was interactive;\n    - only one person at a time could use the entire exhibition;\n    - the division by decades meant that all the music created from the 2000s until now was piled on only one screen.\n\nOur external partner, the representative of the IT department at Rockheim, got in contact with the SMC program and suggested an Applied Project where the students would work on some ways to address (some of) this problems and give well documented suggestions of improving the screen. The focus was meant to be on practical work, and not so much on theory and literature reviews. If possible, integrating the feedback of actual museum employees and visitors was important.\n\nThe museum provided us with an Azure Kinect camera and ideas of how to improve the interactivity using machine learning (ML). We had in-depth talks with our partner and shuffled ideas back and forth, to decide what was reasonable to expect from both parties. Our main deliverables were to be a prototype and a comprehensive documentation that would also allow replication of our work.\n\nAnd that’s when we came up with a plan…\n\nWe researched gestures in human-computer interaction (HCI) and found out a lot of taxonomies of gestures and studies on mid-air gestures in HCI applications, pros, cons and suggestions for mapping on tasks. Most notably, a study from 2010 (…) found that gesture mappings proposed by end-users were more successful than the more conceptually and physically complex ones proposed by HCI researchers. Based on this idea, we decided to conduct an experimental study right at the museum and ask people for concrete feedback.\n\nGesture elicitation phase\n\nSo, Pedro and Alena packed their bags and flew to Trondheim over the weekend to get their hands dirty. We created a very short questionnaire, asking people how easy did they think it was to discover that the Time Tunnel exhibition was interactive, how much did they enjoy it once they discovered it was interactive, and what general suggestions of gesture mappings on actions and improvements they had. We arrived with this idea that people will be curious and ask and want to be involved, and we set up our questionnaires and emptied our pencil cases on high tables near the exhibition. We would have waited forever for people to express their curiosity about our presence there, so, after some time of observing their behaviour around the screen, we put on our adult pants and started asking them the questions instead, in smooth conversations instead of on paper.\n\n\n   \n   Rockheim visit: jukebox\n\n\n\n   \n   Rockheim visit: Thunderbird\n\n\nOur trip to Rockheim was extremely productive, in one day we understood a lot more about the exhibition and how it is integrated with the rest, gathered a lot of good information from the guides (who have been working with in the museum and with visitors for years) and spontaneous and out of the box feedback from visitors. We organized all the information in a separate section of the documentation and decided on what gesture mappings to implement, based on all the feedback and our own research and ideas.\n\n\n  Slide or Swipe right/left – moving between decades and artists\n\n\n\n   \n   Swiping Right\n\n\n\n   \n   Swiping Left\n\n\n\n  “Point at” – selecting an artist or to go back by hovering over something on the screen\n\n\n\n   \n   Pointing\n\n\n\n  Zoom in – choosing to hear a musical excerpt of the selected artist\n\n\n\n   \n   Zoom in\n\n\n\n  Zoom out – exit the music video\n\n\n\n   \n   Zoom out\n\n\n\n  Putting hands in an X (or close to each other) – exit music video\n\n\n\n   \n   Close gesture demonstration.\n\n\nHardware and software setup\n\nHardware\n\nThe required equipment for this application is shown in the picture below.\n\n\n   \n   Hardware Setup\n\n\nUsing camera technology such as the Microsoft’s Azure Kinect when placed in front of the user would enhance the results of detection and user’s feedback perception. The Azure kinect has a body recognition system which can be utilized to extract minute gestures and when mapped intuitively can have versatile applications. This sophisticated sensor needs a powerful PC to run properly. Microsoft define the minimum requirements here.\n\nSoftware\n\nThe default application was a basic slide show of images/artist, where the timing cannot be controlled, nor the selection of the image/artist. Our team proposed a system where the person can select the decade by pointing with his hand and using gestures to rotate and zoom in and out of the artist, he/she would prefer to listen to.\n\nIt was designed based on the modules showed below.\n\n\n   \n   Software Modules\n\n\nWe used the Azure Kinect SDK, Unity Game Engine, and  Wekinator as the three main tools to support the development.\n\nThe content behavior and the logic to manage a virtual pointer was implemented in Unity, which was communicated to Wekinator to perform gestures based on machine learning strategies. We render a virtual skeleton to provide feedback to the users which also was useful to control virtual objects through explicit programming and machine learning. In the following section you will see a demonstration of the system we achieved.\n\nDemo\n\nHere you can watch a short demo of someone without prior knowledge of the project discover the final version of the prototype.\n\n\n   \n   Demo of a user interacting with the prototype for the first time\n\n\nEvaluation\n\nThe evaluation of the prototype was done internally in the team. Each evaluation phase was approached slight different.\n\nInitially, the quality of the gesture recognition was tested, by making fast gestures and documenting the optimal speed for any gesture. Gestures which we obtained from the study in Rockheim were trained with different variations and training these gestures with Lindsay as the user and then testing it out, had promising results. The gestures we decided on after our trip to Rockheim were trainined into the model by Lindsay and then tested for accuracy.\n\nA few other tests were also done to find the optimal position of the user in relation with the camera. The “sweet spots” we found were between 3 and 4 meters away from the camera and inside 25 degrees to the right and left from the central axis.\n\nFor the final testing, we asked an external user wiht no previous knowledge about the project to interact with the prototype and recorded it. After this test, we observed that some gestures had to be done twice or thrice; we think it may due factors such as speed of movement, and/or training of the machine learning model with only one user. This could, however, be corrected using higher frame rates and by training the ML model with multiple and diverse users.\n\nAs our main objectives were to have a working, ‘fun and intuitive’ prototype, we think the last test (seen partially in the demo) was genuinely satisfactory.\n\nWhat next?\n\nAfter integrating everything we could from the feedback, our observations and research in the prototype, we came up with a couple things that could be improved - with a little effort, the prototype will become from great to extraordinary (no bias)!\n\n\n  \n    The setup could be extended based on the available space. One Azure Camera detected up to three people, but the computer didn’t have enough processing power to render all the skeletons. So, by using 2 or 3 cameras, each with its own computer, it would be possible to have more than one person using the prototype in the same time. It would also be possible to add collaborative features, such as a queuing mechanism for playing the videos in order.\n  \n  The ML algorithm can be very easily improved by training the existing models with more diverse people - different physical characteristics, some standing some sitting (to improve the recognition for people using wheelchairs). The model can be extended by mapping other gestures and actions together. For example:\n    \n      Crossing arms – exit, go back or stop a music video\n      Keeping a palm up in front of the body - pause or stop a video\n      Lifting shoulders in a shrug or tilting head - clues/instructions appearing on the screen\n    \n  \n  \n    From an aesthetic point of view, the graphical content should be designed properly by an expert. Our fun idea is to change the skeleton to fit the theme of the museum and perhaps based by decades - for example, if someone chooses the 70s, all of a sudden their skeleton feedback would have a big wig on.\n  \n  From the point of view of the trade-off between discover-it-yourself and instructions, there are two things that could be implemented to help people with hints, without taking away from the fun of discovering:\n    \n      Screen overlays for the gestures that would trigger actions, without also giving the instructions of what the gesture would do. These could appear only once every 20 seconds, to grab the attention of people passing by, but still give the opportunity for self discovery.\n      Screen brightness changes that correlate with the distance a body is being recognized away from the screen. The screens would start by being dim, and then the moment someone passes through the field of the camera they would get brighter and brighter until the person is in the ideal spot (distance wise). The screen would get dimmer again if the person gets too close to the cameras.\n    \n  \n\n\nConclusion\n\nThroughout the project we changed very few things against our original plan. We managed out time quite well considering our other courses and outside work, and organized our resources accordingly. Based on each of our background and experience we split the work to make the most of the available time. If you’d like to download and use our prototype and the documentation we wrote for it, or have any questions, get in contact with us! Due to the nature of this project we can’t disclose all the information publicly.\n\nOverall, this was a very fun project and a great opportunity for all of us to work on a real-world application. We learnt a lot and we’re very grateful to Rockheim.\n\n\n   \n   Retro team Rockheim\n\n",
        "url": "/applied-project/2021/12/20/alenacl-rockheim.html"
      },
    
      {
        "title": "Latency: To Accept or to Delay Feedback?",
        "author": "\n",
        "excerpt": "We tested two of the solutions—the Delayed Feedback Approach (DFA) and the Latency Accepting Approach (LAA)—so you don’t have to!\n",
        "content": "\n   \n\n\nIntroduction\n\nIf you’ve ever tried jamming or performing music with your band online, you’ve undoubtedly encountered latency issues. So have we, and while looking for ways to deal with these issues we came across an academic paper with a couple of potential solutions. We tested two of the solutions—the Delayed Feedback Approach (DFA) and the Latency Accepting Approach (LAA)—so you don’t have to!\n\nSetup and equipment\n\nIn our testing of DFA and LAA, we connected two computers together through LOLA, but you can also use other low latency software like JackTrip. We used the Midas M32 to add latency in location 1. The latency was added to the signal going out and the signal coming in, which simulates latency added by distance.\n\nDelayed Feedback Approach setup\n\nThe schematics for DFA can be found in illustration 1. The concept of this approach is that the master delays its listening to their instrument to compensate for the delay introduced by the system, while the slave plays normally as if there was no latency. The delay added to their own listening (B) should move towards a middle ground between no latency and the actual latency of what the system introduces (A). This approach is typically used for round trip latency times of 25 to 50 ms. For our test, the master was our guitarist and the slave was our drummer.\n\n\n  \n  illustration 1 \n\n\nLatency Accepting Approach setup\n\nThe schematics for LAA can be found in illustration 2. This is the baseline for telematic communication. It simply means accepting whatever latency the connection causes, and negotiating the latency to achieve some meaningful interplay. For this approach, we tried different amounts of latency ranging from 25 ms to 100 ms to simulate inherent latency. Normally this technique would be used for non-rhythmic musical performances, but for our purposes we attempted to play rhythmically to analyze how this approach holds up under such conditions.\n\n\n  \n   illustration 2 \n\n\nMeasurements\n\nMeasurements were done at roundtrip level. A short 1kHz sine wave was transmitted from location 1 to location 2, where a speaker transmitted the signal into a microphone placed into the speaker’s diaphragm. The signal returned was recorded in a separate timeline in the digital audio workstation Reaper, which is where the time measurements were done. The inherent system latency was measured to be 19ms roundtrip. The measurements can be seen in illustration 3.\n\n\n  \n   illustration 3 \n\n\nTesting\n\nThe song we chose to perform was “Seven Nation Army” by The White Stripes due to its straightforward rhythmic nature, which we hypothesized will accurately tell us if we are playing on beat or not.\n\nThe master (guitarist) had to manually add latency to the monitoring of the signal. To do this we used an Empress Superdelay guitar pedal set to full wet signal (preventing the original non-latency signal to go through), and added latency in a range from 5  ms to 55 ms manually with the pedals potmeter. Adding too much latency resulted in an awkward playing experience, but there was a fine area around 25-35 ms of latency (roundtrip of LOLA signal at this point was 50 ms)  which was acceptable to perform with and made it easier to play in sync with the slave (drummer).  The DFA is definitely a compromise for the master, as it does negatively affect the playing experience of the master. However, from the slave’s point of view, he could not feel any of the effects of latency using DFA and was able to play with relative ease. This approach would also not work for an instrument with a loud acoustic output level, as the master needs to only hear their own signal through the delayed monitoring.\n\nThe LAA removes the need for delayed monitoring, but showcases the limits of musical interplay when the latencies are too high. Playing in time is impossible and it is more suitable for more ambient music genres with less emphasis on beat, or extremely slow pulses. Instruments suited for the LAA approach would be instruments for example with very slow attack.\n\nFor improvements we would suggest a more controlled way of applying monitoring delay in the DFA.\n\nWhat do? (TLDR;)\n\nSo which solution do you choose for an online performance or jam? In our experience, accepting latency is generally not a good idea if the music is rhythmical and reliant on a constant pulse. If you are experiencing relatively high latency issues, introduce some delay to your master’s monitoring of their own signal instead. While it won’t make the problem go away entirely, it makes a world of difference in terms of how comfortable playing over a network feels.\n\nWhile we relied on an expensive rig to get the job done, there are more available tools out there which can achieve similar results. If you’re looking into low latency solutions for playing music online, check out JackTrip.\n\nSources &amp; Resources\n\n\n  https://journals.ucp.pt/index.php/jsta/article/view/6956/6689\n  https://ccrma.stanford.edu/software/jacktrip/\n\n",
        "url": "/networked-music/2022/02/21/josephcl-dfa-laa-latency-approaches.html"
      },
    
      {
        "title": "Mastering Latency",
        "author": "\n",
        "excerpt": "Testing two techniques to work with latency when playing music telematically\n",
        "content": "Introduction\n\nThe Internet is widely used for audio communication. Numerous collaboration applications exist that make it trivial to have a conversation with almost anyone, anywhere. So, why is performing music any different? The problem is trying to keep a common rhythm between musicians in different locations. Maintaining a shared beat is difficult if it takes too long for one musician’s sound to reach another’s ears, and if this time (called latency) is too long (around 25ms [1]), it can make playing music together almost impossible. However, some techniques can be employed to work with latency, and not against it.\n\nGoals/Plans\n\nWe decided to test two such techniques outlined by Alexander Carôt and Christian Werner [1] between the portal and the video room at UiO.\n\n\n  Master Slave Approach (MSA)\n  Laid Back Approach (LBA)\n\n\nThese techniques are similar in principle. One player assumes the role of the “master”, determining the rhythmic elements of the performance. For the MSA, the master does not listen to the slave, allowing performances to take place at any amount of latency, whereas for the LBA both listen to each other and play with the latency adding a sense that the playing is relaxed, or laid back. The limit for this is around 50ms [1] before timing difficulties start to become noticeable.\n\n\n   \n   Master Slave Approach (MSA)\n\n\n\n   \n   Laid Back Approach (LBA)\n\n\nWe decided to perform in two constellations testing various latencies. For MSA, Hugh performed electric drums in the portal, taking the role of the master, while Kristian performed keys in the video room, playing various funk grooves. For LBA, Kristian took the role of the master, on keys, while Joachim performed saxophone, testing two jazz standards, the faster Donna Lee, and more moderate Solar.\n\nTechnical Specs\n\nWhen playing music over the Internet sound passes through several stages, which all inevitably add latency [2].\n\n\n  Geographical latency\n  Internet service latency\n  Home network latency\n  Digital processing latency\n\n\nTo be able to control for latency, we had to measure these first in order to ascertain a baseline. We measured the entire system and the network connection between the two UiO portals and found a total round-trip time (RTT) of 20ms.\n\n\n   \n   Network Latency only (0ms)\n\n\n\n  \n    Midas M32 Latency Delay\n    \n  \n  \n    \n      \n        \n      \n      \n        \n        In order to control latency we added delay in the channel-strips of the individual instruments in the Midas M32 mixer in the portal.  We verified our 20ms measurement by setting a RTT of 300ms (setting the delay on the drums in the Midas to 280ms) and having the drums play a steady beat against a click at 100BPM with the keys playing along on each beat. With one beat at 100BPM having a duration of 600ms, the drummer should hear the keys with exactly two beats delay, which was the case, verifying that our measurement was correct. For the MSA and LBA, we employed similar baseline setups. However, there were some necessary differences to meet the requirements of each of the approaches. For the MSA, a click track was employed for the drummer, which was not to be sent to keys. This was generated within the drum kit, but the signal was split so that it was not also sent to the keys.Midas M32 Latency Delay\n        \n      \n    \n  \n\n\n\n   \n   Routing Diagram MSA\n\n\n\n   \n   Routing Diagram LBA\n\n\nThe biggest technical challenge was monitoring. For the MSA this was relatively simple to achieve: the drummer (master) directly monitored the drum kit since they were not to hear the keys (slave) as described in the approach, and the keys could monitor from the mixer. However, for the LBA, the keys (master) required monitoring from the Midas in order to also receive the saxophone (slave), and as latency was applied in the channel strip, directly monitoring this would mean that the keys would be hearing themselves with a delay. Therefore a solution was found by looping the keys into a second channel in which the delay was first added, with the original input channel being sent to the monitoring headphones.\n\nPerformance Reflections\n\n\n  Master-Slave Approach\n\n\nFor Kristian, it didn’t feel too strange to jam along with the drums. However, since Hugh couldn’t hear the keys, there was also no way to communicate musically. This also meant that latency at any delay was not an issue, as what Kristian was doing was irrelevant to Hugh. However, for Hugh this approach didn’t really feel like taking part in a communal musical experience, as he was simply performing a certain number of agreed bars against a click track, and not hearing the music as a whole. This made it feel more like a technical exercise than a musical performance.\n\n\n   \n   Visible Latency between the keys recordings in video room and portal with aligned master tracks.\n\n\nTherefore, when playing highly rehearsed music in which responding to another player is not so important over a high latency connection, this approach is viable. However, for simply jamming with a friend there’s another solution that might be preferable.\n\n\n  \n    \n  \n  Recording from the video room at 50ms latency.\n\n\n\n \n   \n \n Recording from the portal at 50ms latency. Note the audible click and that the keys are behind the beat in comparison to the video room recording.\n\n\n\n  Laid-Back Approach\n\n\nFor Kristian, the tempo and style of song had a direct influence on how much latency could be tolerated. For Donna Lee, difficulties in synchronizing timing could be felt at much lower latencies than for Solar, which didn’t feel unnatural until the latency was set to the upper limit of 50ms one-way. A factor playing into this could be that it isn’t as natural to play laid-back on a fast-paced jazz melody, compared to a medium tempo melody with improvisation. As a result, Kristian mostly paid attention to his own tempo on Donna Lee, while Solar offered the ability for more interplay between the performers. Joachim didn’t feel the changes in latency directly, except if Kristian unconsciously adapted to the increasing latency. In view of this, LBA is a better option for jamming with friends over higher latency connections. However, it is not as well suited for faster tempos and highly rehearsed music which requires a steady and synchronised tempo.\n\n\n \n   \n \n Solar at 50ms recorded in the Portal. The style sounds natural, but the latency was noticeable for Kristian.\n\n\n\n\n  \n\nDonna Lee at 35ms recorded in the Portal. This was the point where Kristian started to really feel the latency.\n\n\n\n   \n   Kristian playing the synthesizer as the master\n\n\nConclusion\n\nAttempting to work with latency, instead of treating it as something to overcome, leads to interesting results in spite of several drawbacks. The two approaches live up to their names, with the MSA definitely creating such a dynamic, and the LBA sounding appropriately laid back. Employing these techniques opens up new possibilities to play together online, even if it isn’t playing together in a way that we are used to.\n\nRelevant papers and links\n\n[1]  Carôt, A., &amp; Werner, C. (2009). Fundamentals and principles of musical telepresence. Journal of Science and Technology of the Arts, 1(1), 26-37. https://doi.org/10.7559/citarj.v1i1.6\n\n[2]  JackTrip. (2022). Technology. https://jacktrip.org/technology.html\n",
        "url": "/networked-music/2022/02/21/joachipo-measuring-latency.html"
      },
    
      {
        "title": "Using Live OSC Data From Smartphones To Make Music With Sonic Pi",
        "author": "\n",
        "excerpt": "Easy as pi!\n",
        "content": "\n   \n\n\nIntroduction\n\nLet’s be honest, networking isn’t exactly the most intuitive topic to grasp, especially in relation to machine learning. However, in this article we will explain the basics of using OSC for music creation that couldn’t be simpler to follow! Using the oscHook smartphone application, the Sonic Pi music creation tool, and a basic machine learning regressor, we can easily create and customize a tool using smartphones to make music over WiFi.\n\nCreating the Data Set\n\nOur first step in this process is to create a data set to train our machine learning regressor later on. We achieved this by using oscHook to send accelerometer data to our Phython file, and saving the x/y/z coordinates that the application outputs. For our application, we decided to use 4 different phone positions at face level: facing the user, facing the left side, facing upwards, and facing downwards. Once the data set is created, we must partition the data set manually based on each position to maximize the results of our regressor and to avoid outliers. In the end, I had around a little less than 1000 samples for each position, which produced farily high R2 scores.\n\nSending to Sonic Pi\n\nAfter our machine learning model has been trained, we must again create an OSC connection from our smartphone to Python yet again to receive live data from oscHook. However, this time our data will be used to output numbers that will correspond to parameters we will use to shape our sounds with Sonic Pi. This was not as straight-forward as first anticipated, since it involved using AsyncIO in combination with the Python-OSC library to make an async server for handling OSC inputs and ouputs concurrently. This asynchronous design uses a main loop to send OSC messages to Sonic Pi, while our OSC server receives accelerometer data from our phone through an event loop. A “side effect” of this design is that we had to run our application as a .py-script instead of using a Jupyter notebook.\n\nUsing Sonic Pi\n\nSonic Pi is a music creation and performance tool based on live coding. This makes our whole experiment kind of meta, as we’re training a small machine learning model and coding an OSC server in Python so we’re able to make use of our accelerometer sensor creatively when we’re further live-coding in Sonic Pi. For our Sonic Pi project, we decided to change the parameters of a rather simple loop mainly consisting of the Amen break. Our regressor’s output was scaled to control the cutoff filter and playback rate of the sample. This project could be considered a proof-of-concept, showing us that there is a lot of possibilites to make more advanced and creative use of this techonology.\n\nVideo Demo\n\n\n\nFiles\n\nYou can view the GitHub repository with all of our files for this project here.\n",
        "url": "/machine-learning/2022/03/23/josephcl-osc-sonic-pi.html"
      },
    
      {
        "title": "Generating Music Based on Webcam Input",
        "author": "\n",
        "excerpt": "Do you miss PS2 EyeToy? Then you have to check this out!\n",
        "content": "\n   \n\n\nIntroduction\n\nFor people, recognising if somebody is giving a thumbs up is easy. The arm might be a bit higher or lower, or it might be viewed from a slightly different angle, but we can instantly recognise the posture. To programme a computer to do this, however, is a lot of work. Every combination of pixels that can form a thumbs up has to be programmed individually, which requires a lot of time and effort. And even after that, there will probably be a lot of edge cases that aren’t recognised.\n\nHowever, through the use of machine learning techniques, this becomes a lot simpler. The programme can just be provided with a lot of examples of various thumbs up postures, and after training, will be able to infer that a new posture that it has never seen before is also a thumbs up. The problem here is finding and collecting all of these examples. But what if we could just generate them on the fly, and spontaneously train our programme to recognise new postures? And moreover, what if we associated these postures with sound generation parameters, enabling the flexible programming of posture-controlled music generation systems? This is one of the use cases of the Wekinator, developed by Rebecca Fiebrink [1]. And to really get an idea of how the system works, we created our own simplified version of this.\n\nHow it works\nOur system is based around a classifier, a machine learning technique that involves the programme learning to sort input examples into discrete classes. It takes the input from the computer’s webcam, and allows the user to provide examples of up to 5 classes of posture by pressing the ‘q’, ‘w’, ‘e’, ‘r’, or ‘t’ keys while holding the posture that they want the programme to learn. When this has been done, the user can press the ‘spacebar’ and a Support Vector Classifier algorithm is trained on the postures and classes. When the user then holds the posture again, the programme should recognise which class it is. This information is then sent to a Pure Data patch via OSC, which triggers one of 5 sounds. By default, this is just 5 sine waves spread over 5 octaves, but if the user wishes they can alter these to be any desired sounds or parameters for sound generation in the PD patch.\n\nIn the video below, you can watch Arvid providing a short demo.\n\nVideo Demo\n\n\nIf you want to try it out or modify the code, it can be found here !\n\nReferences\n[1]R. Fiebrink and P. R. Cook, “The Wekinator: a system for real-time, interactive machine learning in music.”, 2010, vol. 3.\n",
        "url": "/machine-learning/2022/03/23/jakobhoydal_posturiser.html"
      },
    
      {
        "title": "5G Networked Music Performances - Will It Work?",
        "author": "\n",
        "excerpt": "In collaboration with Telenor Research, we explored the prospects of doing networked music performances over 5G. Here are the preliminary results.\n",
        "content": "\n\n5G is the new paradigm of telecommunication and wireless networking. But what is so great about 5G? For the average user, the biggest difference from 4G to 5G will be a massive increase in speed (bandwidth). In reality, 5G is an entirely new networking infrastructure. This is one reason why there are so many high expectations of 5G. We imagine remote-controlled surgery from afar, distributed sporting events, and even stable networked music performances (NMP).\n\nAt SMC, we have been experimenting with NMPs for several years. One of the biggest challenges with playing music over the network is the exceptionally high demand for low-latency communication [2] [3] [4]. In fact, research tells us that the ideal roundtrip latency (back and forth between two locations) for synchronous music performances is around 25-30ms, with the maximum we can tolerate at around 40-50ms [5]. For some perspective:\n\nDemo - 30ms audio delay from left to right ear:\n\n\n\nIn late 2021, we got in touch with Telenor to explore the feasibility of conducting NMPs over 5G. Telenor is currently involved in multiple EU-funded projects that, in part, explore the application of 5G to music technology, including Fudge5G and 5GMediaHub. During a two-week period in late March 2022, we did a series of experiments on a commercial and private 5G network in collaboration with Telenor Research. In this post, we present these experiments in detail, explaining the technical setup, methods, and preliminary results.\n\nSetup\n\nWe conducted two experiments in two separate locations on two different 5G networks.\n\nCommercial 5G Experiment\n\nThe first experiment was carried out at the Musicology Department at the Aalborg University, on March 30th 2022, on a first-generation commercial 5G network. Most commercial 5G networks today rely on a so-called Non-Stand-Alone (NSA) core network, the same as 4G. However, Telenor pre-configured our routers to access specific Access-Point-Names (APNs). This configuration enabled Peet To Peer (P2P) connectivity between our machines with faster packet routing.\n\nPrivate 5G Experiment\n\nThe second experiment was carried out in Elverum, close to Terningmoen Army Base, on April 4th 2022, on a 5G Network-on-Wheels (5GNoW) solution. The 5GNoW is a private 5G network, or Non-Public Network (NPN), that relies on a Stand-Alone (SA) core network. As we understood, these kinds of systems are mostly used for experimental testing of various 5G applications in the field.\n\n\n  \n    \n        \n        Our 5G Routers in action outside the Musicology Department at UiO, ready to test the commercial NSA 5G network. \n    \n  \n  \n      \n        \n        Coverage report of the commercial 5G network at our UiO location.\n      \n  \n  \n   \n        \n        Telenor's 5GNoW van at Terningmoen Army Base, Elverum. Photo by Olai Bendik Erdal.\n    \n  \n  \n    \n        \n        Our setup, approximately 5 meters away from the 5GNoW.\n    \n  \n\n\nHardware and Software\n\nFor both experiments, we used a pair of Huawei H138-380 CPE Pro 3 5G Routers to connect to the network.\n\nTo send audio and video back and forth, we used our own portable and custom-built NMP systems. These racks are essentially bundles of high-end software, audio/video peripherals and networking tools that can provide the lowest possible latency on audio/video transmissions over the network, given that all other the pieces of the puzzle are correct. Full documentation and more detailed info about these systems is available here.\n\n\n    \n        \n            \n            SMC's new NMP Portable Kits in action at UiO.\n        \n    \n    \n        \n            \n            Portable NMP kits ready for the road to Terningsmoen Army Base, Elverum.\n        \n    \n    \n        \n            \n            Setting up the NMP kits at Terningsmoen. From left: Stefano Fasciani and Aleksander Tidemann. Photo by Olai Bendik Erdal.\n        \n    \n\n\nFor NMPs, our go-to AV transmission software is LoLa (Low Latency AV STreaming System). This high-end application was developed at the Trieste Conservatory (Italy) in collaboration with GARR, the Italian Research and Academic Network. To provide ultra-low latency, Lola requires high-end GPU-equipped PCs, soundcard with very stable ASIO drivers (that support buffer sizes of 32 and 64 samples), and specialized Ximea video cameras.\n\nIn addition to Lola, we used the JACK2 and JackTrip bundle as our secondary software. JackTrip is another popular audio transmission application developed by CCRMA at Stanford University (USA). JackTrip is audio-only and accomodates a wider range of soundcards and buffer sizes. This is “bad” for latency optimization but essential to tolerate more unstable connections and network jitter.\n\nExperiments\n\nTo explore to what extent 5G can accomodate NMPs, we measured the stability, quality, and latency of roundtrip audio and video signals using Lola and JackTrip. In any real NMP scenario, we only care about technical configurations that render stable AV transfer over time with a minimal dropouts and other unwanted artifacts. Therefore, we only measured signal latency when the best possible tradeoff between stability and quality was found. We did three tests in each experiment:\n\n1) Measuring the Network Coverage and Bandwidth\n\nBy using the iPerf networking utilities, the Huawei routers’ own location-optimizing software, Telenor’s online coverage map, and Ookla’s online speedtester, we were able to make network bandwidth and coverage estimates throughout the experiments, ensuring that our load did not exceed the capacity of the network.\n\n2) Finding the Sweet-spots\n\nTo find the best tradeoff between stability and quality, we sent a constant stream of audio and video over the network and looped the signals back to their source, as depicted in Figure 1. With this, we were able to monitor the AV quality of our connections in real-time.\n\n\n    \n    Figure 1. Routing diagram of our method for monitoring the audio and video quality of a NMP system from a single location. Each side of the network represents a unique NMP kit.\n\n\nTo fine-tune the audio, we adjusted software and hardware buffer sizes to locate the lowest possible configuration that ensured a stable audio transmission over a significant period (maybe 10minutes total). For the video, we used a similar a approach, only adjusting the framerate, compression (M-JPEG) amount, and video resolution to find the sweet-spot.\n\n3) Measuring the Latency\n\nWith the software and hardware parameters fine-tuned, we measured the audio and video latency with a similar loopback system:\n\n\n    \n    Figure 2. Routing diagram of our method for measuring the roundtrip latency of NMP systems from a single location.\n\n\nWe measured the audio latency in two steps:\n\n\n  Digital roundtrip time (digital RTT)\n\n\nWith digital RTT, we refer to the measurement of audio latency from software to software (or PC to PC), and back again. With this method, we bypassed the latency induced by our external soundcards and mixers. For the measurements, we used jackTrip in P2P mode. By utilizing the -x1 argument client-side, we were able to record and monitor the digital RTT in real-time.\n\n\n  Analog roundtrip time (analog RTT)\n\n\nWith analog RTT, we refer to the measurement of audio latency through the entire chain depicted in Figure 2. To make these measurements, we used another laptop with a designated audio interface. From this secondary laptop/soundcard we sent audio impulses from output 2 to the NMP kits and received the signal back again on input 2. For reference, we closed output 1 to input 1 on the soundcard and sent identical audio impulses to output 1. Then, in software, we measured the analog RTT by looking at the temporal offset between inputs 1 and 2.\n\n\n\n    Routing diagram of our secondary system for measuring the analog RTT.\n\n\nFor video, we took advantage of the fact that our two NMP kits were in the same room. The measure the latency, we sent a Ximea video feed of me doing some claps 👏 from one NMP kit to the other. While displaying the video feeds in full-screen on both computer monitors, we filmed the monitors with a secondary camera. Then, we used the footage from the secondary camera to determine the video latency by counting the offset in frames between the two monitors.\n\n\n     \n    Routing diagram of our video latency measurement method.\n\n\nResults\n\nCommercial 5G Experiment\n\nInside the Musicology building at UiO, the 5G reception was poor. After inspecting Telenor’s coverage map of our location, we decided to place the routers outside and pre-configured them to be in Bridge mode, hoping it would generate better coverage, create a more stable connection between our routers, and boost overall performance. According to the routers’ location-optimizing software, we achieved a stable 75% 5G coverage at this location. From here, we measured a stable 60Mbps bandwidth.\n\nThe transmission sweet-spot for audio was achieved using jackTrip with a buffer size of 512. Unfortunately, experimenting with lower buffer sizes only resulted in massive jittery audio and dropouts. We found the optimal stereo audio settings to be the following:\n\n\n  \n  Compression\n  0%\n\n  \n  Bit Depth\n  16bit\n\n  \n  Sampling Rate\n  48Khz\n\n  \n  Buffer Size\n  512\n\n  \n  Notes\n  Good and stable audio\n\n\nUsing the above configuration, we measured a 110ms digital RTT and a 165ms analog RTT of the commercial 5G network at UiO. Considering that buffering 512 samples at 48Khz takes 10ms, and that during our analog RTT measurement audio had to pass through our soundcards a total of 4 times, this kind of delta between digital and analog RTT was expected.\n\nDemo - 165ms audio delay from left to right ear:\n\n\n\nFor video, we were able to use the Ximea low-latency cameras with Lola as the software utilizes a different buffering strategy for video transfer. After experimenting with various settings and buffering tools, we achieved a stable transmission with some drops (mostly not visible to human eyes) using the following settings:\n\n\n  \n  Compression\n  M-JPEG (quality 60%)\n\n  \n  Bit Depth\n  RGB24 bit\n\n  \n  Resolution\n  1024x576\n\n  \n  FPS\n  60\n\n  \n  Notes\n  Acceptable video quality\n\n\nWith this configuration, we measured the one-way video latency between our machines to be approximatley 7 frames. At 60FPS, this is equal to a 116ms latency one-way, or 232ms RTT.\n\n\n  \n    \n  \n  The capture of the video latency between our two NMP kits during experiment nr.1. The playback speed is 50%. Video by Stefano Fasciani.\n\n\nPrivate 5GNoW Experiment\n\nAt Terningmoen, Elverum, time was of the essence. We only had about 3-4 hours to set up our equipment, configure the network, and do our tests. From the start, we ran into unexpected issues in the 5G modem setup with the 5GNoW van. Also, establishing UDP and TCP/UDP connections over the network was unusually slow, illuminating further network issues. Therefore, we had to settle for limited bandwidth of approximately 13-14Mbps, enough to experiment with audio.\n\nThe transmission sweet-spot for audio was achieved using jackTrip with a buffer sizes of 256 and 512. At 256, we got an audibly ok quality audio, but one that was unstable over time with noticeable dropouts. At 512, the audio was clear and stable over a significant period. We found the optimal stereo audio settings to be the following:\n\n\n  \n  Compression\n  0%\n\n  \n  Bit Depth\n  16bit\n\n  \n  Sampling Rate\n  48Khz\n\n  \n  Buffer Size\n  256 and 512\n\n  \n  Notes\n   OK but unstable audio with 256. Good and stable audio with 512.\n\n\n\nUsing the above configurations we measured a 55-60ms digital RTT and an analog RTT at 74ms when using a buffer size of 256. Although impressive, this configuration rendered borderline audio quality that would be unpleasant in the long run. Using a buffer size of 512, we measured a 90-100ms digital RTT (similar to experiment nr.1, only slightly faster) and thus an analog RTT at about 140ms.\n\nDemo - 74ms audio delay from left to right ear:\n\n\n\nSummary and Concluding Thoughts\n\nDuring a two-week period in late March 2022, we investigated the feasibility of conducting NMPs over commercial and private 5G networks, in collaboration with Telenor Research. On two separate occasions, we measured the stability, quality, and latency of transferring uncompressed audio and compressed video over the networks with high-end hardware and software utilities. When accepting borderline conditions on a private 5G network (5GNoW), we managed to push the audio RTT latency down to 75ms. However, in more realistic conditions on a first-generation commercial 5G, we achieved an analog RTT audio latency of 165ms and 116ms one-way latency for video.\n\nCompared with audio RTT benchmarks mentioned in the introduction (between 25-50ms), our 5G test results were not particularly promising. However, I believe we could conduct successful NMPs over 5G if we could get the audio analog RTT down between 50-70ms. There are many documented strategies for coping with latency-rich environments [1]. In fact, SMC students have recently explored some of these strategies in detail (read more here). Also, considering that we had unfortunate testing conditions in Elverum, there is reason to be optimistic about achieving better RTT audio and video scores if we manage to resolve these issues at a later stage.\n\nOn the other hand, although we can do more testing, some things we cannot mitigate. For instance, we cannot change how the 5G protocol is written, to some extent how our routers/modems choose to buffer, or the inherent instability of using wireless networks. Because of this, we have to be practical and make the best with what we can get. Moreover, lower latency should be possible when the Ultra-Reliable Low Latency Communications (URLLC) feature of 5G is implemented on future Telenor networks.\n\n\n    \n      \n        \n        A big thanks to the Telenor Research team! From front to back: Aleksander Tidemann (Department Engineer UiO), Stefano Fasciani (Professor in Musicology UiO), Kashif Mahmood (Senior Researcher Telenor), Olai Bendik Erdal (Senior Researcher Telenor) and Ole Grøndalen (Senior Researcher Telenor) \n      \n    \n    \n      \n        \n        Also, a big thanks to research assistant Henrik Haraldsveen for helping out with the Oslo-side experiment.\n      \n    \n\n\nGoing Forward\n\nOur plans are now to do more testing at Telenor’s Oslo Hub at Fornebu in the summer of 2022. As discussed, we hope that we can resolve the private 5G networking issues and improve on our results from Elverum. We look forward to doing more tests and will keep you posted on the results.\n\nReferences\n\n\n[1] Carôt, A., &amp; Werner, C. (2009). Fundamentals and principles of musical telepresence. Journal of Science and Technology of the Arts, 1(1), 26-37. https://doi.org/10.7559/citarj.v1i1.6  \n[2] Chafe, C., Gurevich, M., Leslie, G., &amp; Tyan, S. (2004). Effect of Time Delay on Ensemble Accuracy. ISMA. Center for Computer Research in Music and Acoustics, Stanford University  \n[3] Rofe, M., &amp; Reuben, F. (2017). Telematic performance and the challenge of latency. Journal of Music, Technology and Education, 10(2–3), 167–183  \n[4] Rottondi, C., Chafe, C., Allocchio, C, Sarti A. (2016). An overview on networked music performance technologies. IEEE Access 4: 8823-8843.  \n[5] Schuett, N. (2002). The Effect of Latency on Ensemble Performance, Technical Report at CCRMA\nDepartment of Music, Stanford University, Stanford, USA.  \n\n",
        "url": "/networked-music/2022/04/11/aleksati-5g-nmp.html"
      },
    
      {
        "title": "Managing Network Performance",
        "author": "\n",
        "excerpt": "Managing Network Performance using Python\n",
        "content": "Introduction\n\nWhen working in the digital domain and on the internet, latency is determined by many factors. In this blog-post we will talk about how you can measure your network performances using a simple Python script in order to transfer data over a local network.\n\n\n   \n   Network Performance Word Cloud\n\n\nInternet Protocol Suite (TCP/IP)\n\nLet’s start by introducing what is the Internet Protocol Suite, also known as TCP/IP. Basically, this represents the set of communications protocols used in the Internet and similar computer networks. The main protocols in the suite are the Internet Protocol (IP), the Transmission Control Protocol (TCP), and the User Datagram Protocol (UDP).\n\n\n  IP is responsible for delivering packets from the source host to the destination host by looking at the IP addresses in the packet headers. IP has 2 versions: IPv4 and IPv6. IPv4 is the one that most of the websites are using currently. But IPv6 is growing as the number of IPv4 addresses are limited in number when compared to the number of users.\n  TCP is responsible to provide reliable and error-free communication between end systems. It performs sequencing and segmentation of data. It also has acknowledgment features and controls the flow of the data through flow control mechanisms. It is a very effective protocol but has a lot of overhead due to such features. Increased overhead leads to increased cost.\n  UDP on the other hand does not provide any such features. It is the go-to protocol if your application does not require reliable transport as it is very cost-effective. Unlike TCP, which is a connection-oriented protocol, UDP is connection-less.\n\n\nMoreover, TCP/IP provides end-to-end data communication specifying how data should be packetized, addressed, transmitted, routed, and received. This functionality is organized into four abstraction layers, which classify all related protocols according to each protocol’s scope of networking.\n\n\n  Application Layer\n  Host-to-Host Layer\n  Internet Layer\n  Network Access Layer\n\n\nHere, we focused on the Host-to-Host Layer (also known as the Transport Layer) as it is mainly responsible for the end-to-end communication and error-free delivery of data. In other words, it helps us to shield the upper-layer applications from the complexities of data. Within this layer, TCP is quite handy when it comes to detect packet loss and perform retransmissions to ensure reliable messaging. Packet loss occurs when one or more packets of data travelling across a computer network fail to reach their destination. This can either be caused by errors in data transmission, typically across wireless networks or network congestion and can be measured as a percentage of packets lost with respect to packets sent.\n\nNetwork performance metrics\n\nFurthermore, there are different ways to measure the performance of a network, depending upon the nature and design of the network. Here, we needed to assess the factors which were responsible for affecting the delivery of data using most common network performance metrics such as:\n\n\n  Bandwidth\n  Throughput\n  Latency\n\n\nThat way, we could design a simple Python script in order to measure the service quality of the network.\n\nMeasuring Network Performance locally\n\nTo measure the network performance, we choose to focus on latency and bandwidth metrics. For that we started by creating a local http Server, that we connected to another Client device. Then, we measured the ping and the bandwith between the two machines.\n\nPing\n\nWhen measuring the ping, we basically send a network message with no other purpose than to get an immediate response from the Server device. Here, we used this to measure the Round Trip Time (RTT) that we converted in milliseconds. It could have been also used to verify that another system is able and willing to respond to network messages at all.\n\nBandwidth\n\nWhen measuring the bandwidth, we usually get two numbers related to the upload and the download bandwidth. The upload number is usually smaller than the download number. This is mainly due to the fact that we normally download more than we upload. However in AV applications we can stream as much data as we receive so this can considerably change.\n\n\n  \n    \n  \n   Network Performance Tutorial\n\n\nSetting up a simple HTTP Server using Python\n\nTo create the Local Server we used Python and the following command line with the open port number 8080:\n\npython -m http.server 8080\n\nMoreover, we had to make sure that both devices (Server and Client) were connected over the same LAN or WLAN network. Also, the Client device needed to known the IP address of the Server device. This was possible using the following command line: ipconfig (on Windows)\n\nThen, we could open a web browser on the Client device and type in the IP address of the Server device, along with the open port 8080: http://[IP address]:8080\n\nFinally, we could measure the ping (ms) and the download (Bps) checking the local server address with our nice GUI.\n\n\n   \n   SMC Network Performance GUI\n\n\nIf you want to try it out, note that it is mandatory to install the following packages.\n\npip install pyspeedtest\npip install tkinter\n\n\nThen you can run the following script.\n\nimport pyspeedtest\nfrom tkinter import *\n\n# Get the right byte suffixe\ndef byte_suffixe(nbytes):\n    suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n    i = 0\n    while nbytes &gt;= 1024 and i &lt; len(suffixes)-1:\n        nbytes /= 1024.\n        i += 1\n    f = ('%.2f' % nbytes).rstrip('0').rstrip('.')\n    return '%s %s' % (f, suffixes[i])\n\n# Measure ping and download\ndef speedtest():\n    t = pyspeedtest.SpeedTest(e1.get())\n    ping.set(str(round(t.ping(),4))+' ms')\n    download.set(byte_suffixe(t.download()))\n\n# Create a nice GUI using tkinter library\nmaster = Tk()\nping = StringVar()\ndownload = StringVar()\n\nLabel(master, text=\"SMC Network Performance\\n\").grid(row=0, sticky=W)  \nLabel(master, text=\"Local Server\\n\").grid(row=1, sticky=W)\nLabel(master, text=\"Ping Result:\").grid(row=3, sticky=W)\nLabel(master, text=\"Download Result:\").grid(row=4, sticky=W)\n\nresult = Label(master, text=\"\", textvariable=ping).grid(row=3, column=1, sticky=W)\nresult2 = Label(master, text=\"\", textvariable=download).grid(row=4, column=1, sticky=W)\n\ne1 = Entry(master)\ne1.grid(row=1, column=1)\nbutton = Button(master, text=\"Check\", command=speedtest)\nbutton.grid(row=1, column=2, columnspan=2, rowspan=2, padx=5, pady=5)\n\nmainloop()\n\n\nHave fun!\n\nRelevant links\n\n[1]  `pyspeedtest` Python library https://github.com/fopina/pyspeedtest\n\n[2]  `tkinter` Python interface to Tcl/Tk https://docs.python.org/3/library/tkinter.html\n",
        "url": "/networked-music/2022/04/22/joachipo-network-performance.html"
      },
    
      {
        "title": "How to Set Up Hybrid Learning Environments",
        "author": "\n",
        "excerpt": "Learn how to set up a hybrid learning environment, ranging from simple to complex, serving as a starting point to help your classroom catch up to the digital age.\n",
        "content": "\n   \n\n\nIf you’re a lecturer or public speaker wondering how to improve your setup by allowing participants to attend both physically and virtually, keep reading. We will be focusing on the needs of students and teachers, but these setups can be used for public speaking across the web in general. We are going to discuss a few ways of setting up a hybrid learning environment, ranging from simple to complex, serving as a starting point to help your classroom catch up to the digital age.\n\nWhat is a Hybrid Learning Environment?\nWhen we talk about hybrid learning environments, we refer to environments in which both online and on-site learning/teaching takes place. For such an environment to work, there needs to be an appropriate setup, both in terms of equipment and the room itself. We have to make sure that the room or rooms that we are going to use as the on-site learning space allow the technology to effectively capture everything in it. This includes the positioning of chairs and tables within the range of cameras and microphones as well as computers and such within the range of the cables and plugs.\n\nEquipment List\nThis is the minimum you need to set up a hybrid learning environment. Deviations are listed under the various setups.\n\n  A Video Conferencing Software (Zoom, EduMeet)\n  A laptop with webcam\n  A screen/monitor\n  A USB microphone\n  Camera (Integrated or USB)\n  An internet connection (preferably by ethernet cable).\n\n\nHybrid Scenarios\nScenario 1: Attention on the Teacher\nThis setup is for sessions in which the teacher is the main focus. This is the simplest scenario to set up for a hybrid learning class. The teacher presents, the students take notes.\nSetup:\n\n  The teacher’s laptop is connected to the conference call (HDMI), which is in turn shown on the monitor.\n  The teacher’s camera is on, visible to the online students through the call.\n  Screen sharing relevant content.\n  For a richer classroom experience, add a second camera capturing the class and encourage online students to turn on their webcams.\n  If the teacher is using a whiteboard, not a screen, substitute the teacher’s webcam with an external camera\n  To keep it simple, use the laptop’s integrated microphone and speakers to address online students. For a clearer sound, use an external USB microphone.\n\n\nTip:\nTo handle questions, the teacher would function as an intermediary, repeating the questions for everyone to hear before answering.\n\n\n   \n    A diagram of scenario 1\n\n\nScenario 2: Interactivity between Students and teachers\nWhen we introduce interactivity between students, the setup becomes more complicated. On-site students need to hear the online students and vice versa. The main difference is audio.\n\nAdditional equipment:\n\n  Audio interface/digital audio mixer\n  1 condenser microphone per 2 students, 1 additional microphone used by the teacher.\n\n\nSetup:\n\n  Connect the interface/mixer to the laptop.\n  Place 2 speakers in front of the class. Connect them to the mixer (TRS-to-XLR and regular XLR cables).\n  Connect microphones to the interface/mixer (XLR).\n  Capture the class with a camera (HDMI to Capture Card to Laptop, or USB to Laptop if using a well placed external webcam).\n  As before, if using a whiteboard, add an additional camera to capture this along with the teacher.\n  Switch your laptop with a stationary PC if you’re experiencing overheating or jitter.\n\n\nTips:\n\nFor small classrooms or budgets, capture larger portions of the room with fewer microphones. Add speakers to ensure the audio is heard evenly throughout the room.\n\n\n   \n    A diagram of scenario 2\n\n\nScenario 3: Attention on the students\nFinally, we propose a setup for a scenario focusing on the students. This scenario arises in lectures based on debate where the teacher acts only as a guide or based on group activities.\nSetup:\n\n  Connect a mixer to the laptop.\n  Place 2 speakers in front of the class, connect them to the mixer.\n  If students are sitting in clusters, use 1 omnidirectional mic per cluster. If students are sitting in a row, use 1 cardioid microphone per 2 students like before.\n  Connect the screen to the computer.\n  Add cameras to cover the on-site students to the best of your ability. How many you can add depends on your video conferencing software.\n\n\nTips:\n\n  If you do not have access to many microphones, a couple should suffice for passing around.\n  As earlier, the online students should be encouraged to enable their webcams.\n  For group activities, either:\n    \n      Use the same setup. Online students will be grouped with online students through breakout rooms. On-site students will work with the other on-site students.\n      Provide tablets or computers running Zoom to on-site participants. Break online students into smaller rooms, so they can collaborate with the on-site students through chat. The on-site students can still communicate through microphones.\n    \n  \n\n\n\n   \n    A diagram of scenario 3\n\n\nProblem Solving\nThese three scenarios will be applicable in most situations, but here are some common problems you might encounter when setting up your hybrid environment.\n\nMultiple Classrooms\nIf your lecture requires more than 1 physical classroom, it’s generally a good idea to mirror the room setup, excluding the teacher’s desk which is only in 1 room.\n\nTeacher Training\nTeachers cannot be expected to understand all the technology in a hybrid environment from day 1. To avoid delays or interruptions during classes, teachers must be taught how the equipment works and how to troubleshoot issues. If you need help understanding the equipment available to you, communicate this to your institution.\n\nAudio Feedback\nIf you’re experiencing feedback, there could be multiple reasons for this:\n\n  Have students mute both microphones and speakers to avoid feedback.\n  Avoid placing microphones in front of speakers, or adjust their volume accordingly.\n\n\nEquipment Placement\nWhen placing equipment, keep in mind that the students need a clear view of screens, whiteboards, and any potential teachers that may occupy the room.\n\nVideo Tutorial\nFor a more in-depth explanation, check out our video:\n\n\n\n\n\nWe hope that this tutorial has been useful and that you manage to successfully set up your hybrid learning environment!\n",
        "url": "/networked-music/2022/04/24/sofiagon-hybrid-learning-environments.html"
      },
    
      {
        "title": "Ambisonics: Under the Hood",
        "author": "\n",
        "excerpt": "What happens when we encode/decode Ambisonics\n",
        "content": "Introduction\n\nAmbisonics is a spatial audio format that enables a quick and easy creation of a 3D audio mix. We created a video tutorial to show you how to create an Ambisonics mix using Reaper and the IEM plugin suite, which you can watch below.\n\n\n\n\nVideo tutorial.\n\n\nThis blog post goes into a little more detail in how exactly Ambisonics works and why Ambisonics is speaker independent, for those interested in what is going on ‘under the hood’.\n\nThe Channel Based Approach\n\nWhen traditionally mixing, we assign track information to a channel of audio, and each channel is reproduced over a speaker. This approach is carried over for traditional surround formats. So in a 7.1 setup, for example, we mix using eight channels, assigning our different channels to each track to create the spatialisation effect that we desire, and then sending each track to an individual loudspeaker. This is called a channel based approach. However, this can lead to problems when we don’t have 8 loudspeakers to playback our mix. What do we do with the extra channels if we have fewer speakers? And what if they’re in a different configuration to the setup on which the mix was created? Ambisonics offers a solution, in that the mix is speaker independent. This means that your mix can be played back over whatever loudspeaker configuration you have available. Let’s examine how this is possible!\n\nThe Ambisonics Approach\n\nIn contrast to a channel based approach, in Ambisonics we don’t mix by assigning a track to a channel, but rather we mix by placing a sound source on the surface of a spherical soundfield. We then encode this soundfield into a format called b-format where each channel carries the information of a spherical harmonic of the soundfield. These can be used to reconstruct the soundfield onto our loudspeaker array through the use of a decoder. A useful way of thinking about this is that when mixing, we’re creating a virtual soundfield, and through the use of the encoder we are ‘recording’ portions of this soundfield that we then reproduce for playback when decoding. Let’s look at an example of this.\n\nRecording the Soundfield\n\nIn the visual below, we can see the spherical harmonics up to the seventh order.\n\n\n   \n   Spherical Harmonics up to seventh order\n\n\nIf you’ve had experience with microphones, the zeroeth and first order spherical harmonics should look familiar, resembling an omnidirectional and figure-of-eight polar pattern respectively.\n\n\n   \n   Omindirectional and Figure-of-Eight Polar Patterns\n\n\nAnd this is correct! If we placed an omnidirectional microphone and three figure-of-eight microphones (each pointing along a different Cartesian axis) at the same point, we would record the spherical harmonics of the soundfield up to the first order, and could then decode this as Ambisonics! So if we encode our soundfield to preserve the first four spherical harmonics, we are in effect ‘recording’ our virtual soundfield using the microphone setup described above. As the order of the spherical harmonics increases, these ‘polar patterns’ become more complex, but can still fundamentally be thought of as the portion of our soundfield that we are ‘recording’.\n\nIntuitively, the more spherical harmonics we have, the more accurately we can reproduce our soundfield, thereby increasing the ease of sound source localisation and the size of the ‘sweet spot’. When choosing the order of Ambisonics, we are actually deciding the number of spherical harmonics that we are ‘recording’. So if we choose to encode in first order Ambisonics, we are ‘recording’ the first four spherical harmonics, in third order the first 16, and so on. As we need a channel for each harmonic, we can find the number of channels we need for an order by using the following formula:\n\n\n\nSo to encode seventh order Ambisonics, we would require\n\n\n\nchannels.\n\nSoundfields in 2D\n\nIf we only want to reproduce a 2D soundfield, the number of required channels is dramatically reduced. We don’t need to ‘record’ any of the vertical information, and so only need to encode the spherical harmonics which include horizontal information. If we take a seventh order encoding as an example, we would only need the harmonics highlighted below.\n\n\n   \n   Required harmonics to represent a 2D soundfield\n\n\nWe can find the number of channels we need for a 2D encoding using the following formula:\n\n\n\nSo for a seventh order encoding, we need\n\n\n\nchannels.\n\nHowever, remember that even in 2D, for first order, you still need a minimum of four channels.\n\nSumming Up\n\n\n  When we mix for Ambisonics, we place objects in a virtual spherical soundfield.\n\n  We then decide which spherical harmonics of the soundfield we wish to preserve by encoding the soundfield into b-format at a desired Ambisonics order. We can think of this as ‘recording’ portions of the soundfield with microphones that have polar patterns corresponding to the harmonic.\n\n  As each harmonic requires a channel for encoding, we can use the formulas above to find how many channels we require for the order that we have chosen.\n\n  When we decode the b-format, we are choosing which harmonics to reproduce. We can decode at a lower order (and therefore for fewer speakers) than we encoded in because the information of the lower order harmonics is still contained in the b-format signal.\n\n\nSources and Further Reading\n\nArteaga, D. (2015). Introduction to Ambisonics.\n\nZotter, F., &amp; Frank, M. (2019). ‘XY, MS, and First-Order Ambisonics’. Ambisonics: A Practical 3D Audio Theory for Recording, Studio Production, Sound Reinforcement, and Virtual Reality, pp. 1–22. Springer International Publishing: Cham. DOI: 10.1007/978-3-030-17207-7_1\n\nImage Sources\n\nhttps://upload.wikimedia.org/wikipedia/commons/7/74/Real_Spherical_Harmonics_Figure_Table_Complex_Radial_Magnitude.gif\n\nhttps://dt7v1i9vyp3mf.cloudfront.net/styles/news_large/s3/imagelibrary/Q/QA_4-0811-qSuciyY05bdnpJjU8IQDmj8ZBfB.PaBl.jpg\n\nVideo Tutorial Media Sources\n\nAudio\n\nAngels in Amplifiers - I’m Alright; From https://www.cambridge-mt.com/ms/mtk/\n\nImages\n\nhttps://notam.no/wp-content/uploads/2019/10/siriusoriginal-studio3Web.jpg\n\nhttps://notam.no/wp-content/uploads/2016/09/SiriusstWeb.jpg\n\nhttps://blog.bestbuy.ca/wp-content/uploads/2016/07/44354i9E45BD1951EE047B.jpg\n\nhttps://dxarts.washington.edu/sites/dxarts/files/styles/large_full_width/public/images/dxarts_media_lab1_sm.jpg?itok=UlD5573s\n\nhttps://upload.wikimedia.org/wikipedia/en/f/f6/AmbisonicLogo.svg\n\nhttps://www.researchgate.net/publication/341128068/figure/fig1/AS:887453950939143@1588596918992/Ambisonics-spherical-harmonics-for-orders-up-to-three-FOA-includes-the-top-two-lines-of.png\n\nhttps://ocl-steinberg-live.steinberg.net/_storage/asset/60417/storage/JPG_large_2000px/60417-large.jpg\n\nhttps://i.imgur.com/Ts2fWMx.png\n\nhttps://i.ytimg.com/vi/l0sxyrAuB0k/maxresdefault.jpg\n",
        "url": "/networked-music/2022/04/24/hughav-ambisonics-mix.html"
      },
    
      {
        "title": "Setting up a controlled environment",
        "author": "\n",
        "excerpt": "Taking advantage of light-weight control messages to do Networked Music Performances\n",
        "content": "\n\nAs SMC students, we have been exposed to Networked Music Performances (NMP) – both in theory by reading up on the technology and research in the field, and in practice by arranging our own NMP concerts. We’ve been introduced to different approaches in coping with latency issues introduced when playing together in real-time by sending raw audio between each location. Are there other ways to perform music together over the internet which are not as demanding in regards to bandwidth and latency? We’ve looked into using lightweight control messages to create and manipulate music together. This can be done in an interactive, interdependent setting where each participant is creating music locally while influencing the music-creation process of the other(s). It can also be a setting where some participants are manipulators and others are creators, where the manipulators influence the outcome of the creators. In this tutorial, we will introduce and explain three simple, network-based musical examples using UDP messages through network connections and with the OSC protocol.\n\n\n   \n   Photo: Yasin Hasan\n\n\nUDP\n\nLet’s start with the User Datagram Protocol (UDP), which is one of the main protocols computer applications use to send and receive messages over an Internet Protocol (IP) network. The other common protocol for sending messages through the network is known as the Transmission Control Protocol (TCP). This is slower than UDP, but much more reliable. TCP is connection-oriented, establishes a connection before transmitting, and expects confirmation messages back from the receiver. UDP on the other hand is a connectionless communication model which does not send or receive confirmation messages. This makes it more unreliable overall, but is much simpler and faster – the perfect choice for us! UDP is often ideal for musical contexts, since we care more about fast connections than sending and receiving every message exactly correct.\n\nExample #1: Pure data\n\nOne simple way to use UDP in a musical context is through the application Pure Data. Music in Pure Data is created and manipulated using “objects”, such as a phasor and reverb. Two of the key objects in our Pure Data patch are “netsend” and “netrecieve”, which give the user the ability to collaborate with others across a network. In our small example, two users are controlling each other’s synthesizer patch. The user on the left is controlling song tempo and presets, while the user on the right is controlling vibrato and low-pass filter cutoff. Notice how when a user change each of these parameters on their device, it concurrently changes other user’s patch.\n\nOSC\n\nSo now that we know what UDP is, what is OSC? OSC stands for Open Sound Control, and it is a protocol that allows different devices connected on the same networks or through the internet to send messages to each other. It is much like MIDI in many ways, but way more flexible. For our upcoming OSC example, one is manipulating the other music creator over internet through UDP.\n\nTo do this, each participant first needs to know their public IP address. Preferably, you would want a static IP address for this, but that could prove difficult – as there are not enough static IPs for everyone. This usually won’t be an issue, but you would want to check if your public IP is still the same before starting a new NMP session, since dynamic IPs are – well – dynamic. Your public IP can easily be found by simply googling it.\n\nThe second thing to take care of before jamming out and broadcasting OSC messages across the internet, is to configure port forwarding on your LAN router[1]. (Look up your router address in your network settings, e.g. 192.168.0.1) This part makes it hard, or even impossible, to do this through a public Wi-Fi, at campus, or even at your friend’s house. Some ISP’s won’t even let their customers access such settings on their router.\n\n\n   \n   An example of port forwarding in the router settings.\n\n\nThe good thing is that this setup only needs to be done on the receiver’s end, due to the nature of UDP messages not establishing a two-way connection. If we assume a setting where one is controlling the other, the sender only needs to know the public IP of the receiver and decide on a port. The receiver then needs to know the public IP of the sender and do port forwarding of messages from the chosen port of that IP to their local IP address (on an arbitrary port). Note: Under such circumstances, TCP would’ve not worked.\n\nExample #2: BespokeSynth\n\nLet’s show an example of a one-way NMP using BespokeSynth where Joseph is located at the SMC campus in Oslo, manipulating the music creation process of Kristian residing in Nittedal. We are using the modules ‘oscoutput’ to send, and ‘osccontroller’ to receive messages.\n\nAdding layers of complexity\n\nThe cloud is the limit (pun intended) for how you could take advantage of this technology for creative purposes. In our final example we’re seasoning OSC with accelerometer data from a phone, and a Machine Learning model in Python. The model is trained on 4 different gestures, and its output are scaled and converted back to OSC messages. We’re running Python and the music software on the same computer, but this could’ve been happening on different devices in different locations.\n\nExample #3: Sonic Pi\n\nHere is our last brief example, using Sonic Pi and based around a loop of the ‘Amen’ break. The gestures are controlling cutoff and beat_stretch rate of the loop.\n\nConclusion\n\nIn summary, we have gone through the basics of UDP and OSC, as well as three different ways to use these tools to create and manipulate music across networks. What we have shown here is only the beginning in terms of the possibilities afforded by playing music through networks. This blogpost is accompanying our video tutorial found below.\n\n\n\n\n\nWe hope that you take what we have taught in this video as a basis to explore further and create your own NMP someday soon!\n\n\n\nFootnotes\n[1] Take a look at the Port Forward webpage for more information on how to configure your router.\n\n[2] For more advanced and reliable use of OSC over UDP, look up ‘Time Tags’ in the Open Sound Control Specification.\n\n[3] CNMAT have released ODOT, a set of externals for even more advanced use of OSC in Pure data and Max/MSP.\n",
        "url": "/networked-music/2022/04/24/kriswent-using-udp-and-osc-for-nmp.html"
      },
    
      {
        "title": "Generating Samples Through Dancing",
        "author": "\n",
        "excerpt": "Using a VAE to build a generative sampler instrument\n",
        "content": "Introduction\n\nWhen we dance to music we are reacting to the sounds that we hear, interpreting elements of the music that stand out to us with our bodies. For example, if the music is louder we might make larger movements or we might time our movements to be on the beat. Therefore if we look at a video of somebody dancing we have a record of several elements that were in the music that they were dancing to. Can this record be used to reverse the process, and generate new music that possesses characteristics that were found in the music was originally danced to?\n\nThis was the question underlying the Sampling Through Dance project. Using a dataset of videos and audio files of the accompanying music and a machine learning architecture called the Varational AutoEncoder (VAE) [1] a generative model was trained. This was then attached to a live camera input, creating a sampler style instrument, and through dancing the generative sampler is played.\n\nSystem Design\n\n\n   \n   The System Design\n\n\nThe system comprises three main components:\n\n\n1. A component to capture the audio and video input from the computer soundcard and a camera and create a dataset to train the model with. These data are chopped into sample segments of the length desired.\n\n\n2. A VAE component that is trained on the data captured. A VAE is an artificial neural network architecture which is trained on an identical input and output. The data is compressed into a bottleneck, the ‘latent space’, and then reconstructed at the output. However, what’s interesting about VAEs is that this latent space is actually a representation of the parameters of a distribution of the data, and this is what provides the architecture with its generative capacities.\n\n\nHere, the data of the dance motion and the corresponding audio segments are encoded together in a shared latent space. After the model has been trained, the input for the audio and the outputs for the motion data are removed, meaning that motion data can be passed to the model to sample the latent space and generate new audio samples.\n\n\n3. A performance system, centred on this trained model. A live camera input is attached to the model and through dancing, music is generated.\n\nRepresenting Motion\n\nPassing raw video files to the model for training and in the performance system was not a good idea for a number of reasons. Firstly, raw video files are extremely large and would greatly increase the time required to train the model and the time required to generate new material. Secondly, when using the model after training, the environment and camera setup would have to be identical to that in which the data to train the model was collected. Instead, motiongrams [2] were extracted from each sample segment and used to represent the dance motion.\n\n\n   \n   How a motiongram is created (© A.R. Jensenius) from [3] \n\n\nA motiongram is a representation of motion along an axis through time and is created by calculating the mean of each column or row (depending upon whether an x-axis or y-axis motiongram is desired) of the difference between each frame in the video data. In this way, each row or column represents a single frame of the original video, greatly reducing the size of the data. In addition, they offer a good representation of both the quantity and location of motion, capture the temporal dependencies of the motion, and are also environment ‘agnostic’, due to the fact that they are calculated from the frame difference. This means that the system can be used to perform in environments different to those in which the data was collected.\n\nThe Prototype\n\nA prototype of the system was developed using a dataset of around one hour of dancing to the album Reward by Cate Le Bon.\n\n\n\n\nMiami by Cate Le Bon, the first song used in the dataset\n\n\nTwo models were trained, one with a sample segment length of 0.1 seconds and 0.5 seconds. The following videos demonstrate a short performance with each, showing the video as captured by the camera (at a low resolution of 64x48) along with the motiongrams that were passed to the model.\n\n\n\n\n  A performance with the 0.5 second model system (audio adjusted for latency)\n\n\n\n\n\n  A performance with the 0.1 second model system (audio adjusted for latency)\n\n\nThe results are rather interesting and not what were expected at the outset of the project. Instead of distinct audio samples, there instead is a shifting textural quality to the sound over a droning bass that occasionally changes pitch. There is also a repeated phrase underlying all of the segments. The VAE also adds a lot of noise, but this isn’t spread evenly throughout the frequency spectrum. Instead, this is mostly found in the higher frequency bands.\n\nLooking at several characteristics of the sound and motion, it’s possible to see that the element best captured by the models was the link between the quantity of motion and the dynamics of the audio segment, with larger motion resulting in a louder segment. The rhythm is set by the length of the sample segment, meaning that there this element cannot be captured by the model.\nThese three aspects create an interesting collaborative effect when dancing with the system. The timbral and harmonic elements of the texture feel very out of the control of the dancer, while the dynamics feel very manipulable, with the rhythm being set in advance. This creates an effect that one is dancing with the system, and not fully in control of it.\n\nThe Issue of Latency\n\nThere is an issue when it comes to using motion data as an input for machine learning model in a performance system. There are several figures provided on the maximum latency between motion and sound in a motion controlled system in order to create a sense that motion is creating the resulting sound, ranging from 0.02 to 0.15 seconds (see, e.g. [4], [5]). However, as a machine learning based model requires the motion to be finished before the resulting sound can be generated, the length of the motion itself is already creating latency (for the two models here 0.1 seconds and 0.5 seconds respectively). This is in addition to the time taken by the model to generate the audio, which (on my computer) was around 0.1 seconds for the 0.1 second model and 0.2 seconds for the 0.5 second model. This results in a total latency of around 0.2 seconds for the 0.1 second model and 0.7 seconds for the 0.5 second model, both of which exceed the maximum to create a sense of link between motion and sound.\n\nAlthough this latency was noticeable for the 0.1 second model, there was still a sense that the motion is creating the alterations in sound. However, for the 0.5 second model the latency was so large that there is a feeling that the system is reacting to the motion, not that the dancer is creating it through their motion. This adds to the feeling that the dancer is dancing with the system instead of controlling it. For the above videos demonstrating the system, the audio was aligned with the motion to remove this latency, in order to show the direct link between the motion and the musical elements.\n\nThe code for a pipeline system to capture the video and audio for a dataset, train a model and perform with it can be downloaded here.\n\nSources\n\n[1] D. P. Kingma and M. Welling, ‘Auto-Encoding Variational Bayes’, 2013, doi: 10.48550/ARXIV.1312.6114.\n\n[2] A. R. Jensenius, ‘Action-sound: Developing me-thods and tools to study music-related body movement’, Ph.D Thesis, Aalborg University, Oslo, 2007. [Online]. Available: https://www.duo.uio.no/bitstream/handle/10852/27149/jensenius-phd.pdf?sequence=1&amp;isAllowed=y\n\n[3] A. R. Jensenius, ‘Some Video Abstraction Techniques for Displaying Body Movement in Analysis and Performance’, Leonardo, vol. 46, Feb. 2013, doi: 10.2307/23468117.\n\n[4] T. Mäki-patola and P. Hämäläinen, ‘Latency Tolerance for Gesture Controlled Continuous Sound Instrument Without Tactile Feedback’, in Proc. International Computer Music Conference (ICMC, 2004, pp. 1–5.\n\n[5] R. Gehlhaar, ‘Sound= Space: an interactive musical environment’, Contemporary Music Review, vol. 6, no. 1, pp. 59–72, 1991.\n",
        "url": "/machine-learning/2022/05/03/hughav-VAE-sampler.html"
      },
    
      {
        "title": "Cross-modal correspondence: Different modes, common codes? Investigating musical engagement with an Ecological cognitive approach",
        "author": "\n",
        "excerpt": "Investigating musical engagement with an Ecological cognitive approach\n",
        "content": "\n   \n   Project timeline\n\n\nContext and background\n\nOur senses are interconnected, we constantly make sense of the information that is available in our environment. We give meaning to the information based on our knowledge and experience (mental models). Something we seem to know from ordinary life in that moods have colours, colours suggest flavours, sounds can be heavy or light and personalities can be spiky. We take for granted that the stage lighting for a concert is the “right” stage lighting and we seldom experience mismatched senses. When we do, it´s strange: a piece of slow pace music paired with yellow, rapidly flashing lights, for example, would seem wrong to some individuals.\n\nThe work in this project with Bjørn Dreyer and Olivier Lartillot touches a little on these kinds of topics but in a less dramatic way. I wanted to inquire into how the things we pay attention to affect the way we hear things. Artistic researcher and guitarist Bjørn wanted to share his cross-sensory experience with the audience; researcher Olivier created visualisation in the concert. He also wanted to make an app called “Synesthesizer” that allowed listeners to assign colours to the music. I was able to work with these amazing people to find out how the audience reacted to the sounds and colours. I also asked people who weren´t at Bjørn´s concert about their sensory reactions. My main roles here were to design the interactions, to conduct user research, and to develop data collection and data analysis methods and evaluate the data with the 4E framework. There were two phases of experiments (concert and non-concert settings). Do music listeners share common codes when they listen to the music in different modes?\n\nMy research questions were:\n\nRQ 0: How is music perception affected by the sensory channels a subject attends to?\n\nRQ 1: What is the relation of the listening conditions to the way the music content is understood in a concert setting?\n\nRQ 2: What is the relation of the listening conditions to the way the music content is understood in a non-concert setting?\n\nRQ 3: What is the difference between these two sets of results (RQ1) and (RQ2)?\n\nShort project summary\nIf you are interested in the topic, read my whole thesis!\n\nThis project explored music listeners’ cross-modal associations. A 2-stage experiment was conducted in a live concert setting and an offline non-concert setting. An ecological cognitive approach was applied as a foundation of the project. Data collection methods such as interviews, observations and open-ended surveys were used. Due to the complexity of participants’ responses, I used content analysis and coding to categorise the variables into themes and tags. Four major codes were generated: 1) emotional, 2) colour, 3)conceptual and 4) attentional. These suggest aspects of cross-modal music listening that research can look into further. A mixed-methods analysis is recommended for future research such as the correlation analysis to investigate individual feature-to-feature correspondence. Moreover, I found that timbre (i.e. the sound of a guitar or plucking string instrument) affords a calm sensation and elicits the cooler tone colour (i.e. blue, yellow and green). Also, the use of a C major, quiet in dynamic and slow in tempo appeared to elicit imagination to the themes such as Nature and Narrative such as walking in nature or a forest. This cross-modal mental imagery effect of music suggested that listeners could use slow, quiet, guitar/plucking strings music in combination with cool tone colours as an emotional regulation strategy. In HCI, this could further be tested and implemented as a music therapeutic tool. Another finding between music and imagined finger movements and colour suggested that using colour and music or even letters to teach motor skills to young children may be feasible.\n\nThe general conclusion I draw from the findings of this work suggests that cross-modal correspondence is a potentially rich topic of exploration that the small-scale nature of this project only touched upon. The relation of music to vision and emotion is a three-way relation of three complex phenomena with numerous variables. Part of the phenomena are based on physical causal relations and part of them are inevitably cultural and related not to causation but to meaning and interpretation. This obvious conclusion is that research in music requires all the tools available to researchers to adequately do justice to music´s physical, cultural and emotional aspects.\n\nGraphical summary\n\n\n   \n\n",
        "url": "/masters-thesis/2022/05/12/jonimok-crossmodal-correspondences.html"
      },
    
      {
        "title": "Visualizing Psyhophysiological Responses to Music Stimuli",
        "author": "\n",
        "excerpt": "Music can induce down-regulation of the nervous system, and in combination with technology, it has the potential to promote wellness.\n",
        "content": "\n   \n   Breath Painting Art\n\n\nVisualizing Psychophysiological Responses to Music Stimuli\n\nThe world is going through a global mental health crisis, although the traditional pharmaceutical drugs available can be effective and are an important support for people with mental disorders, they often impinge harmful side effects on the patients making use of it. However, some alternative methods and tools can help support treatments in the mental health field. From the physiological realm to the cultural perspective, research show that music has the capability to modify and drive human emotions, mood, and movement. And so, music can be a powerful tool and serve as means to diminish the dependence, volume of use, and therefore side-effects of such medicine.\n\nMusic can induce down-regulation of the nervous system, providing relaxation, and in combination with technology, music technology has the potential to supply powerful tools to promote not only better mental health, but also raise awareness of internal bodily processes, supplying access to physiological data that can be valuable to manage psychophysiological processes.\n\nThis blogpost is a synthesis of my master’s thesis developed for the SMC program - so if you are interested in the subject, you can read the full thesis, check the code (p5js) developed, explore the literature review with scientific research on the field, as well as the supplementary material on Zenodo publication.\n\nTo play with and explore the prototype’s (very simple) interface, access the link Breath Painting and scroll down to find the buttons.\n\nMotivation\n\nPersonal Reasons\n\nMusic and emotion are deeply connected in the human experience. Throughout my entire life, music has been an essential support to deal with traumatic events and difficult emotions. Since I was young, I found music a safe space where I can express myself and let emotions flow through. During my bachelor’s in Graphic Design I realized how music is deeply associated with visual representations in my perception, and that seeded my interest in creating immersive visual and musical interactive experiences, which brought my focus to Interaction Design within the graphic design field.\n\nRecently in my adulthood, I started to deal with symptoms of post-traumatic stress disorder (PTSD) as a consequence of acute traumatic events categorized as “Big T Trauma”, being diagnosed later on with complex PTSD.\n\nAs a person that plays and composes music, music has been always an important part of my well-being and expression as a human, but only in the recent years, during this master’s program, I have found out the healing music of great composers such as Hiroshi Yoshimura, Brian Eno, Jon Hopkins, Harold Budd, East Forest, and many others which has opened up a new world for me. I have been using this type of ambient music (or healing music if you will), in my self-work and it has shown me clear benefits and potential out of my own experience. That led me to connect the dots, and so I decided to investigate how music can be beneficial for mental health through the lenses of science.\n\nMusic, Technology, and Mental Health\n\n“The general response to both physical and psychological stress is the activation of the sympathetic nervous system (SNS) with inhibition of the parasympathetic nervous system (PSNS).” (Ziegler, Michael G., 2004)\n\nResearch show that music can affect the autonomic nervous system in different forms. Music can entrain the respiratory and cardiovascular systems, and induce different types of emotions.\n\nThe purpose of this project is to use music and technology combined to support mental health treatments and maintenance. Some key concepts were explored and are worth mention here in this post.\n\nBiofeedback is a technological component that fits the purpose, helping to bridge the gap between internal physiological processes and the patient’s perception of these processes, which creates a vast number possibilities. The use of biofeedback in gamification systems for example, can help patients to be aware of their own physiological internal state, while engaging on activities that can help to regulate their autonomic nervous system’s activity. It can be specially interesting for children, but not only children, studies show that adult individuals also benefit from the effectiveness of the use of these tools.\n\nGamification is another interesting concept for this matter. It is a technique that utilizes features of the gaming experience in non-gaming contexts. It is widely used to provide users engagement with a product, and it is effective for educative purposes. Studies observed that gamification techniques have great potential in stimulating individuals for the accomplishment of tasks. In the article “User-Centred Design and Usability Evaluation of a Heart Rate Variability Biofeedback Game” (Wollmann, 2016), results validate this hypothesis, showing that visualization techniques in a gamification context were effective in creating engagement for specific applications such as heart rate variability biofeedback training.\n\nHealthcare and well-being can benefit from the use of gamification and biofeedback since these are tools that potentially support maintaining the physiological system working in balance. The paper “Help me Relax! Biofeedback and Gamification to improve interaction design in healthcare”, analyses the user experience by utilizing the combination of biofeedback and gamification for well-being purposes. The former demonstrated to be an efficient tool to provide joy and engagement to healthcare applications (Spillers, 2012).\n\nAim\n\nThe goal of this project was to investigate how music affect human psychophysiology and the potential uses of music in combination with technology to benefit the mental-health and well-being of individuals.\n\nThe prototype proposed further on in this project aimed to raise awareness of the capability of music as a tool to modify physiological patterns and consequently regulate an individual’s autonomic nervous system. Music technology can support health care and psycho-therapeutic treatments, helping to prevent mental disorders such as anxiety, depression, stress, and bipolar disorder. Altogether, the combination of a biofeedback interactive system, with gamification techniques and conscious breathing can be a powerful tool to support mental health and wellness, and this project will describe how.\n\nThe prototype consists in a code that generates visualizations of breath and music data, which I am calling “Breath Painting”. These visualizations have the potential to feature music apps focused on relaxation and wellness, integrating gamification tools for mental health support applications, or educational purposes by raising awareness of the importance of music and respiration for the human mental health. It also can be used for the practice of resonance frequency breathing, which is proven to be an efficient technique used for regulating the autonomic nervous system.\n\nBreath Painting: A biofeedback based data visualization tool\n\nBreath Painting is the name given to the prototype that I developed for this project. The objective of this prototype is to provide unique visualizations of the correlation between breath data and music data from an individual’s music listening experience. The prototype takes numbers as input and translates them into dynamic geometric shapes and colors, as means to convey information about the synchronization in behavior between the two types of data. For this project, the objective of the visualization is to display technical data with an artistically engaging approach. In sum, people from any educational background will be able to visualize how different types of music modulate different breathing patterns. That is why the prototype was named Breath Painting.\n\nBiofeedback and data representation techniques are not a new technology in our time, the term “biofeedback” has been explored since the late 60’s, while the first heart rate data sonification device, a room-sized machine called electrocardiograph, has been in use since 1902. The same applies to data visualization techniques, they have been around with well-developed cartography since the 16th-century. On the other hand, the combination of all these techniques together, with modern technology and advanced apparatus, opens up a whole universe of possibilities.\n\nThe possibilities provided by data representation techniques such as data visualization and data sonification are appealing to me. The fact that we can visualize the internal bodily process, and listen to alterations regarding an individual’s psychophysiological activity sparkle endless creative solutions that can be at the same time, effective for medical use, and artistically instigating.\n\nThis is the reason why the idea for this prototype was born. By combining such techniques, it is possible to create an interactive system that receives biofeedback input from the user’s respiration data and translates it into graphic information. The idea is to raise awareness and induce people to breathe mindfully and with quality. That process can be supported by data sonification, where sounds indicate when the user’s breath is in resonance and stable, or when it is too fast and unstable.\n\nProgramming Language\n\nDuring the product research process, I started searching in parallel for a suitable programming language in the context of visual arts. Amongst several options, highlighting Pyhton, JavaScript, and Processing, I concluded that JavaScript was the most convenient programming language, using the library p5.js and the p5 web editor as the framework. This choice was taken considering economical accessibility, time frame, features, experience with coding languages, and open knowledge available.\n\nData\n\nThe breath and music data inputs used in this project were collected from the experiment “Headphones/speakers Experiment, 2018” (Jensenius, 2021) at the Aalborg University, part of the “MICRO - Human Bodily Micromotion in Music Perception and Interaction” project, which studied how music and rhythm influence the human body motion. The goal of the “Headphones/speakers Experiment, 2018” experiment was to analyse the body motion and rhythm entrainment while listening to music, exploring how the motor responses would differ if the listener were using headphones or speakers.\n\nThe participants were 42 persons signed up at the Aalborg University, without any impairments or medical conditions that would impact the results of the study. All the subjects listened to the same audio sample through headphones and speakers with systematic variation of the order of the samples and the listening device used. Every session starts with 30 seconds of silence, and presents the audio stimulus (45 seconds each) in different orders, having a 30 seconds interval between each song, and at the end.\n\nAudio Stimuli list:\n\n\n  Andre (EDM track with a low level of complexity)\n  Metronome (Metronome track based on a drum sample)\n  Neelix 1 (Trance track with complex rhythmic structure)\n  Neelix 2 (Same as Neelix 1 track but different section: a rhythmic and glissando build up)\n  Pysh (Deep house track with a steady, but slightly laid-back beat)\n  Rhythm (Simple two-measure drum pattern)\n\n\nAudio Data Visualization\n\nAfter organizing the data received, I started working on solutions for the audio data visualization. The first step taken in the process of transforming sound data into visual information, was to obtain musical information from the audio file that I prepared.\n\nThe most common type of audio signal visualization which most people recognize is the waveform. A waveform graphic displays the audio signal’s amplitude changing in time. Adding another dimension of information along with the audio amplitude, spectrograms are a more complex and detailed tool for audio data analyses. Spectrograms provide a visualization of the frequency content intensity and amplitude varying with time. Commonly, spectrograms are displayed as heatmaps. Whereas the frequency content is typically represented by the colour location on the y-axis, the amplitude is classically displayed as the brightness of the colour, and the x-axis is the time dimension.\n\nAs a first approach to this project, I opted for the spectrogram to represent the audio stimulus data visually. I took this choice considering that the spectrogram provides information regarding the frequency content and amplitude of the audio, allowing interesting correlations between the audio stimulus and the breathing data responses over time. In this case, however, I opted for developing a circular spectrogram for artistic purposes. A circular representation denotes the impression of a cyclic narrative to the visualization, which relates to the cyclic character of the breath, and also to the way music has been reproduced on analog media like a vinyl record for instance. Moreover, a circular visualization looks artistically appealing to the general user, dissociating it from the classic chart style and other classical technical representations for audio data.\n\n\n   \n   First test with circular audio data visualization\n\n\n\n   \n   Breath Painting: Audio Data Visualization (audio data only)\n\n\nWith this visualization, it is possible to see the audio data behavior in time, possible silent gaps, parts where the audio is mostly in the low-frequency range, and some emphasized frequencies. Although it was an interesting figure by itself, it still can be improved in terms of clarity and aesthetics, and more importantly, it needs yet to interact with the breathing data.\n\nBreath Data visualization\n\nChoosing a visual structure to represent the breath data was an iterative process. After several tests, I found a structure that represents breath from a conceptual perspective but also that carries aesthetic consistency regarding a symbolic representation of breathing. The most important in this case was that the chosen shape has a friendly code structure that allows the mappings to provide meaningful progressive changes on the shape, that can be understood in terms of data behavior.\n\nFollowing the process described below it is possible to see the iterative progression, starting with the breath data represented by dots in a circular structure overlaying the circular spectrogram generated by the music stimuli of the session represented by the data. Both audio and breath stimuli visualizations respect a linear progression of the data in time.\n\n\n   \n   Breath data of the full session including silence gaps, represented in dots in a circular plane + respective audio spectrograms\n\n\nAfter plotting the overview graph including the song and silence sections I could observe that the intervals between each wave get gradually shorter as the session develops towards the end, suggesting that the breath rate becomes higher in the second half of the session. It also shows that the inhalations become shallower contrasting to the deeper contraction of the chest indicating deeper exhalations by the end of the session. Another observation is that it is not clear if the silence sections are influenced by the preview music stimuli or if it is directly showing the result of the silent moment on the breathing patterns. That interpretation can be more accurate if one knows how fast the breath patterns are responding to the immediate stimuli, be it silence or music.\n\nThe initial approach taken was to map the mean, standard deviation, and variance of the sections of the data in order to understand more about the breath characteristics of the selected section, and to synthesize this information into a shape that can convey such information.\n\nMappings\n\nOnce I understood the structure building the shape, I started testing how different measurements of the data would behave as input of the main variables in the structure. The measurements are listed below:\n\n\n  Mean\n  Standard Deviation\n  Variance\n  Maximum value\n  Minimum value\n\n\nThese measurements were used to create the following mappings, which are a result of a testing process with several iterations. Some of the values were mapped to fit the variable value range, e.g.: the color Hue that goes from 0 to 360.\n\n\n  Number of lines = Standard Deviation\n\n\nloop(i = 0; i &lt; Deviation; i++)\n\n\n  Number of petals = Standard Deviation\n\n\nmap(Std. Deviation, 0, (dataMax-dataMin)/4, 2, 15)\n\n\n  Inner circle circumference = Minimum session’s value (exhalation)\n\n\ndataMin/10\n\n\n  Outer circle circumference = Maximum session’s value (inhalation)\n\n\nmap(dataMax, dataMin, dataMax, 200, 290)\n\n\n  Stroke Hue = Variance\n\n\nmap(Variance, 0, 1000, 350, 0)\n\n\n  Stroke Saturation = Mean\n\n\nmap(Mean, dataMin, dataMax, 1, 20) * i (the saturation value is mapped Mean value to a range, between 1-20 that when multiplied for the i of the for loop, generates a movement effect)\n\n\n  Rotation angle = Mean\n\n\nmap(Mean, dataMin, dataMax, 1.5, 3)\n\nThese correlations allow the user to understand the behaviour of the breath by observing the characteristics of the shape generated from the breathing session. Therefore, one can read the different arts produced from different “music-breath” sessions and compare them,  also understanding how the music ‘looks like’ during each session, by analysing the spectrogram of the sound listened during the session.\n\nTo simplify the understanding of the mappings above, I decided to create a legend that can serve as a guide for the user to read the art generated by their “breath-painting” sessions.\n\n\n  \n    The greater the number of lines (and complexity) on the shape, the bigger the deviation, which suggests that the breath session was unstable, varying in quality (deep or shallow breathing).\n  \n  \n    The same applies for the number of petals, more petals meaning more variation on the breath pattern.\n  \n  \n    The smaller the inner circle circumference is, the deeper exhalation performed during the session.\n  \n  \n    The bigger the outer circle circumference is, the deeper inhalation performed during the session.\n  \n  \n    For the color Hue value, the warmer the color (from purple-blue-cyan-green-yellow-orange to red), the greater the variance of the session was, which indicates that the user performed an inconsistent breath pattern. The closer it gets to the red color, suggests a tendency to aroused states, while the more consistent the breath is, suggests calmer states represented by colder tones closer to the blue color.\n  \n  \n    The stroke saturation also indicates the level of variance. If the saturation is low, it means that the variance of the session was low, suggesting stable breathing patterns. The opposite applies: if the saturation is high, it indicates an unstable breathing pattern.\n  \n  \n    The rotation angle affects the shape creating unique results for each session. The interpretation of this variable is not as clear and informative as the other mentioned above in this list. It is based on the Mean of the breathing session, and it was mapped for aesthetic purposes.\n  \n\n\n\n   \n   Breath Painting: Hue Scale Legend\n\n\nArts Generated (results)\n\nTo make viable the correlation between the musical stimuli and the respiration behavior, both types of data are drawn on the same canvas and can be displayed simultaneously with a harmonized aesthetic. By displaying the draws simultaneously, the interface allows the user to compare and visualize the development of the breath session in parallel to the music stimuli within the time domain.\n\n\n   \n   Stimulus 1 \"Andre\" - Breath Shape (type 1) alone. The relatively high number of petals (8), as well as the high number of lines, indicate high deviation and variance levels. The circumference sizes of this shape compared to the other examples in this document, suggest deeper breath patterns during this session. The hue value and saturation of the lines reaffirm the high variance within this session.\n\n\n\n   \n   Silence section 3 - Breath Shape (type 1) alone. With slightly less number of petals (7), as well as less number of lines, this shape indicates smaller deviation and variance levels. The circumference size is also smaller compared to the shape generated from the Stimulus 1 \"Andre\", suggesting shallower breath patterns. The hue value is low, again indicating lower variance, meaning that the subject S10 performed a more stable breathing pattern during this session.\n\n\n\n   \n   Silence section 3 - Breath Shape (type 1) + dotted breath. By plotting the dotted breath data together with the breath shape, it is possible to confirm the shallower breath pattern by observing the low crests (peaks) and shallow troughs. The dotted graph also confirms the low deviation and variance levels.\n\n\n\n   \n   Stimulus 2 \"Metronome\" - Breath Shape (type 1) + Spectrogram. Showing considerably less number of petals (6), as well as less number of lines, this shape indicates even smaller deviation and variance levels. The circumference size is also much smaller compared to the shape generated from the Stimulus 1 \"Andre\", suggesting shallower breath patterns. The hue value is even lower than the one presented by the \"Silence 3\" session, indicating small variance. These changes suggest that the subject performed gradually more stable patterns throughout the entire session S1.\n\n\nPrototype interface\n\nTo help test and export all the arts mentioned previously, it was helpful to develop a basic interface. The interface of the prototype is extremely simple but it serves the purpose, as it is not a final product. It is basically eight buttons in sequence on the bottom of the screen, that were implemented for testing the arts in different combinations. The first and second buttons are for audio reproduction. The first one is a PLAY button that switches to the PAUSE function, so the user can reproduce and pause the audio loaded in the code. By default, the audio stimulus pre-loaded is “1-Andre.wav”, matching the data segment pre-selected by default as well. It also draws the spectrogram in real-time with the audio reproduction. The second is a STOP button, to stop the audio and the spectrogram drawing action. The third button is a RESET button, made for resetting not only the audio reproduction, starting again from the beginning, but also to reset the canvas, erasing any art plotted. The fourth button is called BREATH DOTS, and as the name indicates, it plots the dotted breath data. The fifth button is called BREATH SHAPE, and it plots the breath shape, as the name suggests. This one should be used when the breath shape is not plotted, for instance when the user resets the interface. Then, the sixth and seventh buttons are TYPE 1 and TYPE 2, with the function of switching the breath data types. And finally, the last button is a SAVE function, with which the user can export/download the art generated.\n\n\n   \n   Prototype's Interface: Buttons\n\n\n\n   \n   Prototype's Interface\n\n\nFuture work\n\nA lot of improvement can be done to this project. There is room for exploration of the breath and sound visualizations, and the interaction between them.\n\nThe breath shapes, for instance, can be improved in terms of clarity. One way to do that is to increase the differentiation between sessions, making the interpretation faster and clearer. As this type of data is not dramatically changing between sessions (if the user performs natural breathing), the differentiation can be enhanced with mappings that make smaller variations output greater modulations on the structures, broadening the spectrum of changes.\n\nThe audio data can also be approached differently. It could become a modulator of the shapes generated by the breath data. With smart mappings, colors and other details of the geometry can convey the audio data keeping the time dimension, frequency, and amplitude information by translating it into different visual attributes of the breath shapes.\n\nMoreover, user experience research can improve significantly the efficiency of this prototype. It would be interesting to test different compositions for the visualizations and ask questions about the understanding of the users regarding the information conveyed. That would also provide input concerning user engagement. An addition that could improve the user’s experience and help educate the users about their breathing patterns would be to implement a legend for understanding the quality of their breathing patterns according to the art generated. A legend can help the users to compare their type of art to reference arts that can guide them to evaluate their Breath Painting. In addition, useful tips for healthy breathing can be implemented in the user interface of the prototype. These tips can be displayed according to the user’s breath session, for instance, if the session shows inconsistent breath patterns, or shallow breathing, the user can get tips on how to improve their breath practices, and can be oriented to follow and align their breath pace with a suggested sedative slow music with visuals that induce a deeper and consistent breath pattern.\n\nThis prototype can also be developed to be interactive in real-time with sensors and devices that can open doors for biofeedback systems and gamification. It can be used for educational purposes applied to health by stimulating the user to breathe deep and slow, or leading the user to discover their particular resonant breathing frequency. For example, the visuals can respond to the user’s breath in real-time, with visual rewards such as the appearance of beautiful elements, or the unpleasant visual indicators that the user is out of track with their respiration. The interactive system can play with the user’s engagement by displaying new appealing elements and sounds that are triggered by the correct bio-data (in this case the breath data) input provided by the user.\n\nAnother possible application of this type of system is generating visual elements and composing generative music simultaneously, providing an immersive experience to the user. That can be done by using the biofeedback data output coming from a breath sensor as a trigger, and modulator, for a generational music system that interacts with the visuals that are also modulated in real-time by the breath data of the user. For example, a calming environment like a forest, with nature sounds that blend into the music, together with visuals that are revealed as the user’s breath syncs into a desired rate, would be an effective way to educate and induce relaxed states for the users. Kids could also benefit from other environments more appropriate for children. With playful elements, as the kid breaths slower and deeper, the environment displays colorful objects that bounce, for instance. That creates engagement, educating and motivating the child to learn healthier breath practices, consequently providing well-being.\n",
        "url": "/masters-thesis/2022/05/15/rayam-visualizing-psychophysiological-responses-to-music-stimuli.html"
      },
    
      {
        "title": "The Algorithmic Composition Explorer",
        "author": "\n",
        "excerpt": "For my masters thesis, I proposed a novel design for an interactive system that introduces people to algorithmic composition.\n",
        "content": "\n   \n   The Algorithmic Composition Explorer\n\n\nAlgorithmic composition has a long and rich history, and encompasses a wide range of approaches and techniques. It can been defined simply (and quite broadly!) as the application of formal rules to generate musical material and ideas.\n\nWhile it is a well known tool for creating new music and ideas, it can also be a valuable approach in teaching music and composition. Being able to describe music in terms of algorithms can contribute to a learners understanding, helping focus their attention on the structure and organisation of that music. The application of algorithmic thinking to music can be used to understand form and style. And algorithmic composition can be used as a vehicle for concept development, with the algorithms producing raw material that is then edited and assigned musical meaning by the learner.\n\nMy plan was to build an interactive system for learning about algorithmic composition with the following aims:\n\n\n  Take a “learning through practice” approach, allowing users to manipulate and compare different algorithmic approaches, offering a visual representation of the generated music as well as audio playback.\n  Allow users to take away their generated musical ideas in the form of MIDI scores and downloadable music notation.\n  Use ubiquitous, standards-compliant technologies that allow it to have the greatest reach, while requiring little previous technical knowledge on the part of the user.\n  Make the code open-source and ready to be built upon with more algorithms, export formats, and visual representations of the music.\n\n\nAs one of the aims is to open up the world of algorithmic composition to those with little technical knowledge, it is important to convey that it is not necessary to be a accomplished programmer or have an advanced understanding of mathematics in order to apply algorithmic thinking and techniques to composition. There are many historical and contemporary algorithmic composition techniques that do not require the use of a computer, and these are no less interesting for it!\n\nWhat is Algorithmic Composition?\nIn the 1680s, Leibniz argued that the rules for composition are so well defined that anyone can be instructed to compose following certain rule sets. Cope (2015) goes so far to say that all composers use algorithms while composing whether they are aware of doing so or not - “all composers are algorithmic composers” (Cope, 2015).\n\nIf we are all, consciously or subconsciously, composing with algorithms, it would make sense that there would be almost as many different approaches to algorithmic composition as there are composers, and that classifying them into “methods” or “schools of thought” is going to be a difficult undertaking. An algorithm can be a complete approach to composing a piece of music, but more commonly a hybrid approach is taken, with algorithmic music being composed using a combination of multiple different methods.\n\nAleatoric methods involve the use of chance or randomness. Mozarts Musikalisches Wuerfelspiel is an early example of using chance, an approach adopted and developed by composers such as John Cage in the twentieth century. For true aleatoric methods, the probability of any event happening is the same for all events - there is no preference for one result over another.\n\nThen there are the methods which use probability. Xenakis was a pioneer here with an approach he called stochastic music, which involves the use of randomness together with statistical principles (Aschauer, 2008). Unlike purely aleatoric methods, stochastic methods allow for more control over the output while still working with random data.\n\nRules-based methods and constraint systems allow the application of certain rules to musical material. These rules can be widely applicable, covering general music theory models such as counterpoint and voice leading, or more targeted and focused towards a specific idea or intention, depending on composers musical goal.\n\nThen there are algorithms that use machine learning and systems that work with fractals. There are genetic algorithms which take a biologically inspired approach to composition. Then there are cellular automata, chaotic systems and game of life algorithms, and many, many more. It was beginning to feel like I had bitten off more than I could chew!\n\nBuilding the Algorithmic Composition Explorer\nAs explained above, the three main criteria for the Algorithmic Composition Explorer were:\n\n\n  To be widely available and requiring little in the way of previous technical knowledge. Avoiding the need for any installation or setup to get started.\n  To allow users to experiment with and compare different algorithmic approaches, and offers a visual representation of the generated music as well as audio playback. Allowing users to take the generated music with them and incorporate it into their own work.\n  To provide a platform that can be built upon and extended with new algorithms, instruments and components.\n\n\nIf the goal is to build something widely available, building for the Web is a good option. There’s nothing to install or configure - you just need a browser. It works across devices - a responsively designed website can (in theory!) work as well on a phone as on a desktop. And the Web offers a lot of potential for building an Algorithmic Composition Explorer with the Web APIs for audio and MIDI. Tone.js is a great example of a library that is built on top of the Web Audio API and, in addition to lots of sound generating loveliness, offers global transport and sequencing options which make it suitable for working with algorithmic composition. But before I could jump into the details, I needed a bit of structure and the ability to modularise the code. I ended up going with static site generator Jekyll, as it offered enough flexility while still keeping things simple. To make sure the app worked as well on mobile devices as on desktops, I employed the responsive talents of the CSS framework Bootstrap.\n\nRegarding the algorithms themselves, I wanted to use examples that were straightforward enough for learners to get an understanding of simply by adjusting parameters and figuring out what’s going on under the hood. The learning-through-practice approach is about designing the system so that it provides intrinsic feedback - in this case, demonstrating a clear relationship between the changing of an algorithms parameter and the results that are heard. Any instructions provided should encourage exploration and ask questions, as apposed to simply explaining what an algorithm is doing.\n\nWith all of this in mind, starting at the beginning made sense here. Guido of Arezzo’s vowel-to-pitch algorithm is one of the earliest known, dating back to the 11th century. And while simple to comprehend, adds an interesting relationship between the source text and music produced.\n\nMozarts Musikalisches Würfelspiel was an algorithm that was almost discounted, due to the lack of control the user has over the music produced. However, it is a good example of using randomness to generate music, and could encourage the learner to think about form and style, and what makes the song sections work together.\n\nLike Mozarts dice game, the Markov chains algorithm also works with previously prepared music, but the algorithm itself is more complex in this instance. The underlying concept is understandable however, and much can be learned about this algorithm simply by varying the source material and adjusting the order.\n\nArvo Pärt’s Tintinnabuli takes a step back from Markov chains in terms of the complexity of the algorithm, but opens up a world of exploration. While this is an approach that can be more easily grasped, the results are more customisable and potentially more easily transferable to the learners own compositions.\n\nIn addition to the algorithms themselves, an audio player and some sort of notation viewer was needed to be able to listen to and visualise the music generated. I used Tone.js for sequencing the generated music, as well as for creating a couple of instruments - a sample-based piano and a synth - that could be used for playback. ABC.js was used for displaying the music notation.\n\nFinally, in order to get music in and out of the app, importers/exporters were written for ABC notation and MIDI files.\n\nConclusion &amp; future work\n\nThe end result of all this work can be found at The Algorhthmic Composition Explorer.\n\nDue to the limited timeframe of this thesis, there are many aspects of the software I would have liked to have developed further. Implementing more algorithms is perhaps an obvious one. Cellular Automata could be an interesting algorithm to tackle next. There are also many improvements that could be made to the existing algorithms. For example, allowing the user to upload songs to the Markov Chains algorithm, or even being able to populate the Musikalisches Würfelspiel grid with user-created phrases.\n\nAdding MIDI functionality could be an interesting area to explore. Being able to input notes using a MIDI controller would be a great way to feed the Markov Chains algorithm with data, or add phrases to Musikalisches Würfelspiel.\n\nRegarding the rendering of notation using ABC.js, there is definitely room for improvement! It works, but the flexibility of an alternative like VexFlow could make for more accurate notation.\n\nDuring development I found myself wanting to take ideas generated and incorporate them into my own compositions. This lead to several brainstorming sessions where I explored how to adapt the system more for this purpose. I have spoken much about the modularisation of the code, but this is an approach that would be interesting to take with the interface as well. Creating a ‘playground’, where different algorithmic components can be combined in order to build a customised algorithm without the need for coding would be an interesting next step. In some ways, this could be seen as moving the focus away from learning about different algorithmic composition approaches and more towards creating an environment for algorithmic composition itself. This thesis has very much been focused on the learning through practice approach, and I believe learners could only benefit from more opportunities to do just that.\n\nMy hope is that the Algorithmic Composition Explorer creates a spark for people curious about this fascinating subject, helping them realise the creative potential of the application of algorithms in creating new musical ideas, and inspiring them to dig deeper.\n\nCheck out the app here: https://algorithmic-composition-explorer.com.\n\nFor those that want to contribute, the source code can be found on Github.\n\nReferences\n\n\n  Cope, D. (2015). Algorithmic Music Composition. In G. Nierhaus (Ed.), Patterns of Intuition (pp. 405–416). Springer Netherlands. https://doi.org/10.1007/978-94-017-9561-6_19\n  Aschauer, D. (2008). Algorithmic composition [Thesis]. https://repositum.tuwien.at/handle/20.500.12708/11036\n\n\n",
        "url": "/masters-thesis/2022/05/15/stephedg-algorithmic-composition-explorer.html"
      },
    
      {
        "title": "A Human-Machine Music Performance System based on Autonomous Agents",
        "author": "\n",
        "excerpt": "Let’s make music with virtual fellows in mixed reality.\n",
        "content": "This blog post presents a brief summary of my thesis. The main intention is to provide an overview of this work and multimedia material (video, audio, source code) for a complete understanding of how the system works and the results obtained from the research process. For more details you can read the full manuscript.\n\nAbstract\n\n\n  This thesis proposes the design and implementation of a multimodal system for human-machine music performances in real-time. The machine behavior is modeled under the concepts and paradigms related to an artificial Swarm of Autonomous Agents. The system used three advanced technologies as subsystems: Motion Capture, Spatial Audio, and Mixed Reality.  These subsystems are integrated in one only solution that is evaluated regarding ‘system measurements’ and ‘music improvisation sessions’. The ‘system measurements’ determine the advantages and limitations in terms of effectiveness and efficiency; and the ‘music improvisation sessions’ evaluate user interaction through the analysis of data recording and a survey. The results provide latency, jitter and other real-time parameters that are contrasted with user data. Moreover, the user analysis shows that the system is easy-to-use and highly enjoyable. These findings indicate that the strategy to conceive the system is validated and can be used for further investigation for autonomous agents and musicology aspects.\n\n\nBackground\n\nThis system is based on concepts and paradigms related to Musical Agents (Tatar &amp; Pasquier 2019),  Live Algorithms (Blackwell et al. 2012), Swarm Intelligence (Blackwell 2007), and Music in Extended Realities (Musical XR) (Turchet et al. 2021). The relevance of this topics relies in bringing the interaction with artificial musicians in a physical space, which requires to know how they react to the human musician and among themselves, how they can work collectively (as simple individuals) so that complex behaviors emerges, how to structure a solution in an architecture for music improvisation, and how to perceive and participate with them in a physical-virtual environment that foster an effective and efficient communication.\n\nThe main technologies that were integrated to this systems are: Motion Capture, Spatial Audio, and Mixed Reality (MR). There are related works described in the manuscript that used the concepts and technologies mentioned before in an implicit or explicit way, nevertheless, at the moment of the conception of this work, there was no a system that integrates those paradigms and technologies as one only solution for a human-machine music improvisation.\n\nSystem Overview\n\nThe design is supported by the three cutting-edge technologies mentioned above (Motion Capture, Spatial audio, and Mixed Reality) and a musical input source through a MIDI controller. These components are connected to the core of the solution, which means that there are five elements that assemble the complete platform as shown in the figure below. An agent is the representation of a sound source located in space that contains musical material from the performer and is able to adopt an autonomous behavior when the user decides it.\n\n\n   \n   System Design Overview. The wider arrows represent inputs and outputs from the component to the system and the thinner ones show inputs provided by the user and outputs received from the system.\n\n\nThe technologies illustrated above allow the user to interact with the system and receive feedback in real-time. An agent (sound source) is initially an audio track that is created when the human performer finishes a musical line that is input from the MIDI controller and looped it. The sound source can be moved in space through a trackable object called rigid body whose position is detected by an optical motion capture system (OptiTrack). The position is fed into the system to place the agent in space in terms of sound and image. For the audio output, a spatial audio system (SMC Portal) composed of a circular array of loudspeakers reflects the location of the sound source. For visualization, a mixed reality headset (Microsoft HoloLens) is used to render the agent (as a 3D colored sphere) in the physical space where the performer is, also the headset allows to interact with simple gestures to select the agent to control it or let it to behave autonomously (generate new musical material and move independently), that is why the mixed reality device receives and sends data to the main system.\n\nThe communication between components depends of the technology that is using. For the spatial audio system, the infrastructure from the SMC portal (8-channel loudspeakers array) is used. The motion capture system and the mixed reality headset are connected through a local network (LAN) over a router. The motion capture uses a wired (Ethernet) connection while the mixed reality headset uses the wireless network.\n\nThese interconnections have an impact in the size, time, and rate of data transmission back and forth, which is significant for the user experience in a real-time setting. That is why it is relevant to take system measurements as part of the methodology to evaluate this system, since those measurements can help to improve and find a balance between the experience and the system efficiency, or identify the feasibility for music performances under these limitations.\n\nThe video below demonstrates the operation of the system from several perspectives.\n\nVideo Demo\n\n\n    \n    System operation from 4 perspectives for a human-machine music improvisation session.\n\n\nEvaluation\n\nThe system was evaluated for a number of 8 agents. This size was chosen from several experimentations considering the trade-off between latency and audio dropouts. The summary for the measurements based on this size is shown in the table below.\n\n\n   \n   Summary of system measurements for an agent size of 8.\n\n\nSeven users tested the system. In a testing session a user had to familiarize with the equipment, improvise a musical piece as long as he or she decided (relevant anonymize data was captured on each music performance), and complete a survey. These user testing sessions demonstrated that the previous measurements do not produce significant issues, except for the MIDI controller to sound output latency when a high tempo (BPM) is set. Moreover, the system was stable on every performance and tolerated a long music improvisation of 43 minutes (the longest time that a participant spent).\n\nComparisons were established between the system measurements, the captured user data, and the survey applied to the participants. In general, the system provided to the users a positive unconventional experience that was considered enjoyable.\n\nYou will find below an excerpt of 1 minute for the audio that the musicians produced. This is a binaural render taken from the ambisonic output that they experienced in the room.\n\nAudio Excerpts from Participants\n\n\n  \n    \n    Output Audio File\n  \n  User 0\n\n\n\n  \n    \n    Output Audio File\n  \n  User 1\n\n\n\n  \n    \n    Output Audio File\n  \n  User 2\n\n\n\n  \n    \n    Output Audio File\n  \n  User 3\n\n\n\n  \n    \n    Output Audio File\n  \n  User 4\n\n\n\n  \n    \n    Output Audio File\n  \n  User 5\n\n\n\n  \n    \n    Output Audio File\n  \n  User 6\n\n\nConclusions\n\nThe results obtained validates the way-of-making for the system, and the research question about how to design and implement such system is answered with the endorsement of the key findings of this work. Therefore, it is possible the conception of the envisioned system as an integrated solution for human-machine musical performances.\n\nThe contributions of this work are: the design and implementation of the proposed system, the evaluation strategies, and the results of this study.\n\nThe limitations are related with: equipment (low resolution for spatial audio, limited field of view for HoloLens, PC no powerful enough for supporting more than 8 agents), measurement equipment (low frame rate cameras), and time restrictions (not enough participants for generalization, and there is still more data that can be analyzed). Despite of these limitations, the findings are relevant to validate the system and answer the research question.\n\nIt is recommendable to address the limitations described above, reduce latency and packet loss values, and search for more efficient implementations. For future work it is considered the exploration of other strategies for multi-agent behaviour and their level of participation in human-machine musical performances. Besides, a further evaluation of human aspects in the musicology field can be conducted by using this system.\n\nSource Code\n\nThe core system was implement in Max/MSP and the source code can be found below:\n\n\n  https://github.com/pedro-lucas-bravo/agent_music_improviser\n\n\nThe mixed reality application was developed in Unity3D for the Microsoft HoloLens version 1. The source code is placed in:\n\n\n  https://github.com/pedro-lucas-bravo/mr_agent_visualizer\n\n\nThe data analysis was carried out in Python. The data sets and code can be found in the following repository:\n\n\n  https://github.com/pedro-lucas-bravo/master_thesis_data_analysis\n\n\nReferences\n\n\n  Tatar, K., &amp; Pasquier, P. (2019, 1). Musical agents: A typology and state of the art towards Musical Metacreation. Journal of New Music Research, 48 (1), 56–105.\n  Blackwell, T., Bown, O., &amp; Young, M. (2012). Live Algorithms: Towards Autonomous Computer Improvisers. In Computers and creativity. Berlin, Heidelberg: Springer Berlin Heidelberg.\n  Blackwell, T. (2007). Swarming and Music. In Evolutionary computer music (pp.194–217). London: Springer London.\n  Turchet, L., Hamilton, R., &amp; Camci, A. (2021). Music in Extended Realities. IEEE Access, 9 , 15810–15832.\n\n",
        "url": "/masters-thesis/2022/05/15/pedropl-human-machine-impro.html"
      },
    
      {
        "title": "What is a gesture?",
        "author": "\n",
        "excerpt": "There are many takes on gesticulation and its meanings, however, I wanted to take the time to delimit what a gesture is, possible categories and gesturing patters.\n",
        "content": "\n   \n\n\nThere is some debate regarding what is the function of human gesticulation. Some believe it’s a tool we use to communicate with others [1], and some that it’s part of the language [2]. Regardless of the function behind gesturing, I decided to take on a study about gesticulation to explore and delimit what a gesture is, possible categories and to find some gesticulation patterns. Therefore, I conducted a small experiment that allowed me to delve into this exploration of human gesticulation.\n\nExperiment Design\nI devised an experiment in which three participants watched a short animation episode and, afterwards, they were interviewed with questions about said episode while being recorded. The questions were divided into summary, descriptive, detail and movement questions. In addition, the recorded interview was processed by a video analysis program that also provided some movement data. This, together with the comparison of the interview answers provided enough material for an in-depth gesticulation analysis.\n\nTypes of questions and gestures\nI categorized the interview questions as well as the gestures to facilitate the discussion of the results later.\nInterview questions\n\n  Summary question: asks the participants to narrate something in general terms, hoping for a summary of events. There was one summary question in the interview that asked participants to narrate everything they remembered from what they had watched.\n  Descriptive question: asks for a scenery description. There was one descriptive question included asking to describe the inside of the pyramid that appears in the episode.\n  Detail questions: ask for a specific detail, usually a visual element. Eight of them were included.\n  Movement questions: ask about a scene that contains movement (objects/characters moving, other actions that involve movement). Six of them were included.\nDetail and movement questions were included in the interview together as a set of short questions.\n\n\nGestures\nI devised the categories for the gestures according to the motion included in the content of the participants’ speech.\n\n  Detail gestures: those used to depict details or still visual elements as can be shapes, sizes, locations etc.\n  Movement gestures: those used to depict moving bodies or actions that involve movement.\n\n\nMotion Capture Tools\nFor this experiment, I made a recording of each participant’s interview and processed the videos through a program that analyzes the movement frame per frame called Video Analysis. From this program, csv files are exported containing data on timestamps, centroid of motion, quantity of motion and other motion-related data.\nQuantity of Motion\nThe most relevant motion data feature for this experiment exported from the Video Analysis program is the Quantity of Motion (QoM). The term refers to a calculation of “the overall movement of the motion” [4]. There are different approaches to achieving this calculation depending on the tools used to capture the motion. I used QoM to compare the total amount of movement in each participant’s interview to be able to draw informed conclusions from that.\n\nExperiment Results\nObservations from the videos and the QoM.\n\n\n   \n\n\nParticipant 1\n\n  Participant 1 was sitting in a somewhat stiff position and did not gesture much throughout the interview.\n  Most of the detail gestures depicted objects rather than characters, however, most of the movement gestures were mostly about actions/movement carried out by characters.\n  There were about the same amount of detail and movement gestures.\n  Sometimes, P1 would use detail gestures to describe movements and vice versa.\n  There were some head movements whenever P1 was trying to recall something, since they either moved their pupils or their entire head left or right.\n  There was plenty of fidgeting movements with hands, legs and feet which are justified since P1 has been diagnosed with ADHD.\n\n\nParticipant 2\n\n  Participant 2 is the participant that gestured the most and also whose interview is longer. When asked the summary question, P2 went into a lot of detail.\n  Their position was more relaxed and comfortable.\n  The participant would sometimes use movement gestures to depict details and vice versa.\n  P2’s gestures were more complex and specific, specially the detail gestures.\n  They performed about the same amount of detail and movement gestures\n  There was hand fidgeting whenever the participant was not gesturing.\n\n\nParticipant 3\n\n  Participant 3 was also sitting in a straight, somewhat stiff position and did not gesticulate in abundance.\n  P3’s interview was the shortest out of the three, lasting only around half of the length of the other two.\n  Some of P3’s gestures were performed with the head instead of the hands and arms but were still clearly recognizable as a gesture of the aforementioned categories.\n  Most of the gestures were movement gestures and most were pretty discreet and small.\n\n\n \n\nAfter going through the results of the experiment various times, that’s when I really felt the need to delimit what a gesture is. There were so many types of movements recorded throughout the interview that I couldn’t help but wonder which would make the cut and be categorized as such.\n\n\n   \n\n\nWhat are gestures?\nThe Merriam-Webster dictionary defines gestures as “a movement usually of the body or limbs that expresses or emphasizes an idea, sentiment, attitude” [4], which I believe is an accurate enough definition. However, and with this in mind, when we think of a gesture we generally think of hand, finger or arm movements. If I present a case in which a participant, as it was the case with P3, gestures the action of “returning” with a semi-circular motion of their head, a fair amount of people would also agree that head movements can be used to gesticulate.\nBut what about facial expressions? When narrating the summary, P1 was describing that a character was shocked at the same time as P1’s eyes widened and their eyebrows raised. Same happened with P2. This specific use of a facial expression to accompany the content of the narration, specifically depicting the character’s emotion, fits with the definition of what a gesture is. Additionally, when indicating an object’s location, P3’s limbs and head did not move, but their pupils did.\nWould this mean that fidgeting, posture checks or clothes re-arrangement [5] count as gesturing too? These other movements can have a meaning behind them sometimes, e.g.: fidgeting out of nervousness, changes of posture due to insecurity or discomfort, or fixing one’s clothing because of self-awareness. However, they do not always have a specific meaning and whatever they may communicate is not an intentional communication.\nTherefore, and after a careful consideration of everything mentioned above, I conclude that a gesture can be considered any movement that expresses a message or that fulfills a goal intentionally.\n\nTypes of gestures (reprise)\nI have already delimited and defined two very clear types of gestures at the beginning of this post: detail and movement gestures, both related to motion within the speech content. In their paper, Alibali et al. [1] use two different categories instead, one of which I found especially interesting: representational gestures which are “gestures that represent some aspect of the content of the speech” [1]. Within that classification, both of my gesture categories, detail and movement, belong to the representational gestures. Nonetheless, after observing more carefully the participants’ gestures, I noticed that there was a third type of gesture I had overlooked and that needed its own category:\n\n  Additional gestures: those that communicate something to the listener in addition to or regardless of what the speaker is saying.\nAn example of this type would be when P3 was summarizing the episode: They did so within a sentence, then they ended the summary with an “In short” accompanied by a gesture with both open hands moving outwards towards the interviewer. By saying “in short” they were specifying that they were doing a short summary, but their hands were not gesturing something small or short. Instead, their hands were metaphorically handing to the interviewer their response, saying something like “that’s it” or “there you have it”. The meaning behind the words and the meaning behind the gesture were different. This last category is the only one with no relation to motion compared to the other two.\n\n\nWith this, I can conclude my gesture classification with three distinct categories: detail, movement and additional gestures.\n\nGesticulation patterns\nCommonalities\nGetting back to the analysis results, I think some interesting patterns came to light in the common factors that the participants shared throughout their interviews:\n\n  When narrating in general terms, participants didn’t feel the need to gesture. P1 and P2 gestured during the summarizing question only because they decided to narrate in detail certain parts, taking their time to recall scenes and images etched in their heads. However, whenever they didn’t detail the events, the gesticulation stopped. Moreover, P3, who did the most general summary, didn’t gesticulate at all during that section.\n  If the participants were unsure about an answer, no detail nor movement gesticulation was produced. They did raise their eyebrows or squint their eyes in what we could maybe consider an additional gesture, denoting that they were doubtful.\n  No gesticulation (including facial expression) was made by the participants when talking about emotions or facial expressions, with one exception from P1 and P2 when describing “shock”. No gesticulation was performed either when talking about sound-related actions like “screaming” or “crying”. But it’s true that I failed to ask specific questions about sounds.\n  No gesticulation was made when describing colors, most likely because colors are an abstract concept hard to describe with movement or shapes.\n  Whenever the participants were trying to recall something, they would look away from the interviewer either with their pupils or with their entire head. This might be due to a need to feel ‘safer’ or less pressured while thinking, away from the interviewer’s stare.\n  All participants (ADHD or not) fidgeted to some degree when they weren’t gesticulating. Nevertheless, it’s visible in the general overview that P1’s fidgeting was more intense.\n  All participants used detail gestures to depict movement and movement gestures to depict details. This, I believe, would be the equivalent of using nouns as verbs and verbs as nouns when one cannot find a fitting word within the language’s vocabulary.\n\n\nDifferences\nThere were many difference as well, but the most relevant ones were these:\n\n  While P1 and P3 were in a stiff, straight posture and gesticulated little, P2 was in a comfortable position and gesticulated more. This is not consistent evidence that the posture is related to the amount of gesticulation but there might be a correlation. If the participant felt more comfortable or less aware of the interview (and therefore was sitting more comfortably), they might gesticulate more and vice versa.\n  While P1 and P2 used at some point a facial expression as a representational gesture, P3 didn’t at any point. This might be related to each individual’s personality, perhaps.\n  While P1 and P3 performed an additional gesture when told the interview was over, P2 didn’t and just sat up from their relaxed position.\n  While P1 and P2 used a balanced amount of movement and detail gestures, P3 used a majority of movement gestures. This as well might be related to the participants having different types of personalities or mental processes.\n\n\nFinal Conclusions\nAfter conducting this study, I established that a gesture carries an intentional meaning or use and I found three types of gestures: detail, movement and additional. Moreover, I traced some patterns between the participants’ gesticulation, such as gesturing while detailing, no gesticulation for colors or when unsure of an answer, general fidgeting and movement (detail) gestures being used to depict detail (movement) elements. I also found some differences such as posture and gesticulation, facial expressions as gestures and amount of detail and movement gestures which might be related to the participants’ different personalities or mental processes.\n\nI hope this blogpost has been an interesting read and thank you for reading!\n \n\n   \n\n\n \n\n \n\n \n\n \n\nReferences\n[1]\tM.W. Alibali, D.C. Heath, and H.J. Myers, “Effects of Visibility between Speaker and Listener on Gesture Production: Some Gestures Are Meant to Be Seen” in Journal of Memory and Language, no.44, pp.169-188 2001. doi:10.1006/jmla.2000.2752, [Online]. Available: http://www.academicpress.com\n\n[2]\tL. Mol, “Language in the Hands”, PhD thesis, Tilburg University, Tilburg, 2010. [Online]. Available: https://research.tilburguniversity.edu/en/publications/language-in-the-hands\n\n[3]\tOggy, (NEW SEASON 5) Oggy and the Cockroaches The Great Pyramid Mystery (S05-E01) Full Episode in HD. (Sep. 05, 2017), Accessed: May 12, 2022. [Online Video]. Available: https://youtu.be/XJYxOumSiIY\n\n[4]\t“Gesture.” Merriam-Webster.com Dictionary, Merriam-Webster. Accessed May. 10, 2022. [Online Dictionary]. Available: https://www.merriam-webster.com/dictionary/gesture.\n\n[5]\tI.H. Smith and K. Larry, “Gesticulation and Effective Communication” in International Journal of Linguistics and Communication, vol. 3, no. 1, June 2015, doi: 10.15640/ijlc.v3n1a1. [Online]. Available: http://dx.doi.org/10.15640/ijlc.v3n1a1\n",
        "url": "/motion-capture/2022/05/19/sofiagon-what-is-a-gesture.html"
      },
    
      {
        "title": "Playing music standing vs. seated; whats the difference?",
        "author": "\n",
        "excerpt": "A study of saxophonist in seated vs standing position\n",
        "content": "\n   \n\n\nIts common that musicians learn and adapt their teachers playing/performing-technique and style. One technique that I have adopted from one of my former teachers, is to use the pose of your body to influence your playing, by standing up and sitting down to regulate the tempo and excitement of what’s played. This type of reaction is called embodied cognition. It’s the psychical reaction to a physical state; in this case position of playing.\n\nMy hypothesis was that musicians will move more, play faster and play with more energy in the standing position vs. the seated position. An experiment was constructed where three saxophonists played both given repertoire and improvisation.\n\nAlso, a small instrument was built on the basis of the hypothesis. Using a gyro-sensor, OSC data was sent to a Pure Data patch that speeded up an arpeggiator and a drum-machine when the player leaned towards the computer.\n\nFirst we have to ask the question what an optimal pose for a musician is. The easiest answer is that there is no optimal physical pose. This is because all musicians have their own way of expressing themselves both in their style and instrument. The pose of a pianist is completely different from the pose of a vocalist. Shoebridge et. al [1] tried to make a definition of optimal pose by asking musicians. Their conclusion was that each musician must find their optimal pose on the basis of five concocted subprocesses:\n\n\n  Maintaining ease\n  Finding balance\n  Challenging habits\n  Expanding the framework\n  Barriers to change\n\n\nPrevious research also shows that the pose has an effect on mood and performance[4], stress and anxiety [2][8], improve the abdominal muscle activity [7], and self esteem [8]. This is linked to different positions in upright vs. stooped. The research done on musicians is limited, but the findings in the papers mentioned, shows that different poses can have an effect.\n\n\n   \n   Figure 2: Plotted data of participants sitting\n\n\nThe experiment was conducted in the SMC-portal at the department of musicology at UiO. The participants did not know the cause of the study, and was asked to follow the instructions. Afterwards, they were asked questions about what they find the most comfortable position, and how they think they are affected by their position when playing.\n\nThe participants was given Baa Baa Black Sheep as material to play. This was because it was easy to read, and could be played prima vista. The participants was given five minutes to play through the song for the first time. The participants was asked to play in the two different positions, both when playing the given repertoire and improvisation. Between each of the four settings, participants was given a video to watch. This was to reset their state, and become comfortable in seated or standing positions.\n\n\n   \n   Figure 3:  Plotted data of participants standing\n\n\n\n  \n    \n       \n      Performer 1\n      Performer 2\n      Performer 3\n    \n  \n  \n    \n      Saxophone Used\n      Tenor\n      Soprano\n      Tenor\n    \n    \n      Tempo repertoire sitting\n      64\n      63\n      58\n    \n    \n      Tempo repertoire standing\n      57\n      61\n      52\n    \n    \n      Difference\n      -7 bpm\n      -2 bpm\n      -6 bpm\n    \n    \n      Mean RMS repertoire standing\n      -12.2dB\n      -12.8dB\n      -14.12dB\n    \n    \n      Mean RMS repertoire sitting\n      -12.6dB\n      -14.9dB\n      -13.3dB\n    \n    \n      Difference\n      +0.6dB\n      +0.9dB\n      -0.82dB\n    \n    \n      Mean RMS imrpov standing\n      -13.9dB\n      -13.2dB\n      -13dB\n    \n    \n      Mean RMS imrpov sitting\n      -11.3dB\n      -14.4dB\n      -12.9dB\n    \n    \n      Difference\n      -2.6dB\n      +1.2dB\n      -0.1dB\n    \n    \n      Preferred position\n      Seated\n      Standing\n      Seated\n    \n  \n\n\nSeeing the results, it’s clear that the participants uses a wider area of movement in the upright position compared to the seated position. But this is not the case for all participants. Participant 2 does not move as freely as the other participants. Participant 1 has a clear shift in movement together with the music.\n\nIn figure 5, we can see the the change of frames of the participants improvising. The motion can be higher because the performers are less constraint to the note-stand. My hypothesis here was that all of the performers will have higher motion, since they can focus more on themselves. As we see, this is the case for participant 1 and 3. But participant 2 is still not moving as much.\n\n\n   \n   Figure 4: Frames of motion, given repertoire\n\n\nAll participants had a higher average tempo when sitting compared to standing. Nair[8] found similar results in their research, with higher speech times in an upright position. This is not an apples-to-apples comparison, since these are two different states, but the lean of the upper body in a seated configuration vs. the upright position in a standing configuration can lead to similar results. As seen in figure 4, participant 1 and 3 has a slight more lean forward in the seated configuration, versus participant 2 that has similar lean in both positions. This correlates to the tempo findings in table 1 with a lower difference in tempo for participant 2.\n\n\n   \n   Figure 5: Frames of motion, improvisation\n\n\nRegarding energy-levels in the different recordings, we can see that there is little conclusion to draw from the data. Participant 1 and 2 has a higher difference in volume when playing improvisation vs. given repertoire. Participant 3 has little difference. The difference is too little to see any large correlations. The results form Ackermann’s study [7] saw an increase in abdominal muscle activity in upright position. The same results could not be measured using the technique in this study measuring the average RMS levels. Ackermann also stated that its hard to recommend a seat- ing position on the same basis that Awad[4] does, with each musician finding optimal positioning. But they saw that their participants has a higher preference in the standing position vs. the seated. The findings in table 1 suggest that 2 of the 3 participants favours seated position.\n\nTo conclude, the data collected suggest that there is a link between pose of a musician and the performance. But it did not prove that there is an optimal performing position. This is determined by factors the musician has learnt from years of experience, and is not something that can universally be applied to all musicians. In regards to the hypothesis of enhanced tempo, express more energy, and more movement in the standing position is partially true, but more participants are needed to see correlation between the factors.\nThe number of samples used in this study makes it hard to draw conclusions, but similarities can be seen.\n\nReferences\n\n[1] A. Shoebridge, N. Sheilds, K.E. Webster, (feb. 1. 2017) Minding the body: An interdisciplinary theory of optimal posture for musicians [Online]. Available: doi.org/10.1177/0305735617691593\n\n[2] E.R. Valentine, D.F.P. et al., (Oct. 1. 1995) The Effect of Lessons in the Alexander Technique on Music Performance in High and Low Stress Situations [Online] Available: doi.org/10.1177/0305735695232002\n\n[3] I. Foxman, B.J. Burgel, (Jul. 1. 2006) Musician Health and Safety: Preventing Playing-Related Musculoskeletal Disorders [Online]. Available: doi.org/10.1177/216507990605400703\n\n[4] S. Awad et al., (Jun. 16. 2021) Embodiment: I sat, I felt, I performed - Posture effects on mood and cogni- tive performance [Online]. Available: doi.org/10.1016/j.actpsy.2021.103353\n\n[5] M. E. Bates, E.P. Lemay, (May, 2004) The d2 Test of attention: construct validity and extensions in scoring techniques [Online]. Available: doi.org/10.1017/S135561770410307X\n\n[6] P. Briñol, B.C. Wagner, R.E.Petty (Oct. 2009) Body posture effects on self-evaluation: A self-validation approach [Online]. Available: doi.org/10.1002/ejsp.607\n\n[7] B.J. Ackermann, N.O’Dwyer, M. Halaki (Aug. 25. 2014) The difference between standing and sitting in 3 different seat inclinations on abdominal muscle acti- vity and chest and abdominal expansion in woodwind and brass musicians [Online]. Available: doi.org/10.3389/fpsyg.2014.00913\n\n[8] S. Nair et al. (Jun. 2015) Do Slumped and Upright Postures Affect Stress Responses? A Randomized Trial [Online]. Available: 10.1037/hea0000146\n\n[9] PureData (Pd-0.52), IEM. Accessed: 01.04.2022 [Online]. Available: puredata.info/downloads/pure-data\n\n[10] GyrOSC (2.5.1) Bit Shape. Accessed: 01.05.2022 [Online]. Available: bitshapesoftware.com/instruments/gyrosc/\n\n[11] A.R (2005) Developing Tools for Studying Musical Gestures within the Max/MSP/Jitter Environment. [Online]. Available: urn.nb.no/URN:NBN:no-32299\n\n[12 ]Qualisys Track Manager (2021.2) Qualisys [Online]. Available: qualisys.com/software/qualisys-track-manager/\n",
        "url": "/motion-capture/2022/05/19/jakobhoydal-Sitting_vs_standing.html"
      },
    
      {
        "title": "Emulating analog guitar pedals with Recurrent Neural Networks",
        "author": "\n",
        "excerpt": "Using LSTM recurrent neural networks to model two analog guitar pedals.\n",
        "content": "Using a Recurrent Neural Network to model two analog guitar effects\n\nAs a music producer and guitarist, I love distortion. It’s a bit like champagne, it goes with everything. So for my Machine Learning (ML) project I chose to emulate two analog guitar effects using Recurrent Neural Networks (RNNs).  \nAn analog effect is a non-linear system which changes some of the sonic characteristics of an audio source. The dry sound is input into the unit, and the resulting output is the wet audio. My chosen effects where the Boss SD-1 distortion pedal and the Wampler Black ‘65 tube saturator.\n\n\n   \n   The Boss Sd-1 and Wampler Black 65'\n\n\nAudio examples of a dry sound and how it sounds after being processed through the effects:\n\nDry Sound\n\n\n\nSD-1\n\n\n\nBlack 65’\n\n\n\n\n\nUsing ML techniques, we are able to emulate an effect without having any prior knowledge of the effect we emulate. This is called black-box modeling. By feeding raw dry audio to a neural network, we can train it to predict the wet audio. More specific; we feed it a segment of dry audio and the model predict the value of the target sample of wet audio.\n\n\n   \n   Frame of samples as input and one sample of wet audio as target output\n\n\nNormal fully connected feed forward neural networks aren’t very good at this, because they are not able to keep any memory of former events. Even though my chosen effects had very short time-variances, they still affect the audio source in ways that needs to be understood in the time domain. RNNs are able to keep some memory of former events in their cell state which makes them quite good at learning how a non-linear system like an analog distortion effect works.\nRNNs come in many forms, from the simple recurrent unit to the Gated Recurrent Unit (GRU). But the most popular and successful when it comes to audio effect modeling is the Long Short Term Memory unit (LSTM).\n\nDuring my preliminary tests I quickly realized that the LSTM networks performed much better than the other RNNs. I also found that LSTMs had some peculiar challenges with audio prediction which I decided to explore. The result of this exploration was a comparative study of different LSTM architectures and different hyperparameters and how they performed modeling my chosen effects.\n\nDataset\n\nI created an 8 minute audio file consisting of different audio material. This audio was then recorded both dry and wet through my two effects. 20% of the recorded audio were set aside to be used for evaluation (The test set). The remaining 80% were the foundation for the dataset used for training the models.\n\n\n   \n   The full dataset used in the project\n\n\nTraining the models\n\nI made five different model structures, which has either one or two LSTM layers, 16 or 32 hidden units (inside the LSTM), and accepts as input either a frame of 32 samples or 64 samples. These different structures were then trained on different dataset sizes and different number of epochs, and compared to see who performed the best.\n\n\n   \n   The five model structures.\n\n\nUsing Tensorflow with the Keras API, I trained hundred of models, and collected evaluation metrics from all of them which I used for analysis. \nIt took quite some time.\nAnd every week or so I would come up with a tiny improvement and as a result I had to redo everything from scratch.\n\nResults\n\nTo put it simple, the results can be explained this way:\n\n\n  The models performed better on the SD-1 than the Black 65’ when we look at the evaluation metrics. But when you audition the predicted audio and the target output it’s hard to tell which effect is most similar.\n(The following examples are one layer, 32 hidden units trained on 40k frames of audio.)\n\n\nSD-1 True output\n\n\n\nSD-1 Predicted Output\n\n\n\nBlack 65’ True Output\n\n\n\nBlack 65’ Predicted Output\n\n\n\n\n  The bigger the model structure, the better the evaluation metric scores. Evaluation metrics are the Mean Absolute Error (Mae), Coefficient of Determination (R2) and Error to Signal Ratio (ESR).\n  The bigger the dataset, the better the evaluation metric scores gets.\nAll the best performing models were trained on 500k frames of either 32 or 64 samples.\n\n\n\n   \n   Evaluation metrics for the best performing models on the SD-1.\n\n\n\n   \n   Evaluation metrics for the best performing models on the B 65'.\n\n\n\n  The evaluation metric scores do not necessarily correspond to my subjective perception of the similarity of the predicted versus the target output.\n  The longer the models train, evaluation metric scores improves, but the more they added unwanted high frequency material (noise and aliasing).\n  Visually inspecting spectrograms and waveforms often tells a different story than the evaluation metrics.\n\n\n\n   \n   Waveforms of dry audio, target output and predicted output.\n\n\nNoise and aliasing\n\nIn ML, an epoch is one training iteration through the whole training set. The number of epochs then determines for how long the model is allowed to train. Training for too many epochs can result in overfitting, or in the case of this project; noise and aliasing.\nHere you can see how the models first learn to emulate the low frequency content, then slowly learn to add the high frequency content. After 50+ epochs they start to add erroneous high frequent noise and aliasing artifacts.\nThese examples were made with a dataset of 116 seconds of dry audio, rather small compared to the biggest datasets used for my experiments. However bigger datasets would cause the same behaviour:\n\n\n   \n   Spectrogram of Target Output.\n\n\nTrue output\n\n\n\n   \n   Spectrogram of prediction after 20 epochs.\n\n\nPredicted output after 20 epochs\n\n\n\n\n   \n   Spectrogram of prediction after 35 epochs.\n\n\nPredicted output after 35 epochs\n\n\n\n   \n   Spectrogram of prediction after 50 epochs.\n\n\nPredicted output after 50 epochs\n\n\n\n   \n   Spectrogram of prediction after 80 epochs.\n\n\nPredicted output after 80 epochs\n\n\n\n   \n   Spectrogram of prediction after 200 epochs.\n\n\nPredicted output after 200 epochs\n\n\n\nThis could however be because the LSTMs are doing a great job emulating the analog effects. All analog effects are non-linear, and non-linear systems will always produce content above the Nyquist Frequency, called the intermodulation product. Whenever audio goes through the process of Analog-to-Digital conversion, this is handled by a low pass filter filtering out the information around and above the Nyquist frequency. However because the high frequency content predicted by the models happens inside the digital domain, no such filtering is possible.\n\nTakeaways\n\n\n  LSTM networks are pretty good at modeling analog effects with short time-variances. However they don’t work that well if the effect has longer time-variances (phasers, chorus) or memory (delay, reverb).\n  It’s hard to evaluate how similar a predicted audio signal is to its target audio signal. Evaluation metrics underestimate low energy high frequency information, in other words they don’t “hear” the noisy stuff.\n  Smaller and less computationally expensive models can produce pretty good results. The performance gain achieved by adding layers or more hidden units to a LSTM network are not necessarily worth it compared to the added computational cost and increased inference time.\n\n\nThe code for this project is available at Github.\n\n\n\n\n\n\n\n\n\n\n\n",
        "url": "/machine-learning/2022/05/19/arvidf-ml-guitareffect-modeling.html"
      },
    
      {
        "title": "Myo My – That keyboard sure tastes good with some ZOIA on top",
        "author": "\n",
        "excerpt": "Extending the keyboard through gestures and modular synthesis.\n",
        "content": "As my final project in the “Motion Capture” course, I chose to explore an extended keyboard instrument. There are several ways to perform motion capture, some of them demanding more equipment and being more complex than others. I wanted to keep my extended instrument setup lightweight, not too costly and not too complex. By deciding on this framework, I wanted to be able to use my design even after the course was finished – and ultimately bring it to the stage. While we have access to state-of-the-art equipment and facilities as SMC students, a full-on Infrared Motion Capture system might be hard to bring to the next festival gig or summer tour. My choice was to use an Inertial Measurement Unit (IMU), usually a small devices with a combination of accelerometers and gyroscopes. I also wanted to make use of modular synthesis to accompany and process the audio from my keyboard.\n\nGestural control – Myo\n\nI wanted an IMU able to stream sensor data in real time for my project, and which easily could be worn on an arm. It so happened that there existed a perfect gadget for my purpose, in the shape of an armband: Thalmic Lab’s Myo. This device also has eight electromyography (EMG) sensors and can recognize five gestures using underlying machine learning technology. The only drawback is that this device has been out of production for some years now. By stroke of luck though, I was able to get my hands on one such Myo in mint condition from eBay. This means that I can continue to use my extended keyboard instrument in the future as well!\n\n\n   \n   Myo armband by Thalmic Labs\n\n\nModular synthesis – ZOIA\n\nTo treat, accompany, stir up and mess with my keyboard, I chose the Empress Effects ZOIA as my secret sauce. This is a digital modular synthesizer system in the shape of a guitar effect pedal. Even though I’ve dipped my toes slightly into a certain rabbit hole, I have neither the money nor the time for a Eurorack modular synthesizer setup just yet. To build a system with just half of the possibilities as the ZOIA would definately be costly, and quite possibly humongous. I also had an interest in exploring this intriguing piece of gear, which have been patiently awaiting on the shelf for too long now.\n\n\n   \n   Empress Effects ZOIA\n\n\nThe patchwork\n\nNow that I had settled on some lightweight, not too costly, and not too complex ingredients, I needed to blend everything perfectly together. I chose to wear the Myo on my left arm and use my right arm for playing the keyboard. The sensor data from my Myo got extracted using Myo Mapper, which then transmitted Open Sound Control (OSC) data to a Pure data patch. This patch basically transformed the Myo OSC data to MIDI control change (CC) signals, which was sent to the ZOIA using a MIDI cable. I used MIDI CC signals instead of MIDI note-on signals for the synthesizing part as well to make the signalflow more like that of a modular synth, which uses control voltage (CV) for transmitting audio and modulation signals between modules. The ZOIA has its own internal CV-like signalflow, with float values ranging from -1 to +1. A major drawback in using MIDI, which was my only viable option for transmitting multiple sensor signals to the ZOIA, is its 1980’s low resolution 7-bit integers ranging from 0-127. To get around this, I added several slew limiters to smooth out the stairway-like MIDI CC values to continuous-like ZOIA CV signals. The ZOIA also had my keyboard connected to its audio inputs, which got flavorized with the different patches I built.\n\nVideo examples\n\nBelow are a couple of performances I did with my extended keyboard instrument, accompanied by flowcharts of the ZOIA patches. Hope you like it!\n\n1. Myo Accompaniment\n\nThis patch has a 4-operator FM voice for synthesizing melodies to accompany the keyboard, which gets slightly randomized using a modulation matrix affected by the EMG sensors. Holding a fist gesture will activate the synth voice, while “yaw”, “pitch”, and “roll” data from the Myo gyroscope change frequencies of different oscillators. There is also an angry bit-crusher in parallel with an aliasing effect, and a delay with modulation in the end. These effects get activated/deactivated using wave out, double tap, and wave in gestures respectively. The complete routing of effects and gestural signals can be seen in the flowchart below.\n\n\n\n\n\n\n   \n   ZOIA Patch #1: Myo Accompaniment\n\n\n2. Myo Processor\n\nThis patch doesn’t have a synthesizer voice of its own – it’s all about treating the keyboard audio. The main flavors here are the not-so-sober looper, which records while you hold a fist gesture, and a granulizer, which catches and freezes grains when doing a wave-in gesture. Different arm movement affect grain size, position, density, and pitch/speed control. By doing a wave-out gesture, the looper’s content get ring-modulated, also modulated by arm movement. By double-tapping, you add a “shimmer” effect to the reverb. See the flowchart for a more detailed description.\n\n\n\n\n\n\n   \n   ZOIA Patch #2: Myo Processor\n\n",
        "url": "/motion-capture/2022/05/20/kriswent-extending-the-keyboard-through-motion-capture-and-modular-synthesis.html"
      },
    
      {
        "title": "When Hearts Beat as One – Cardiac Dynamics and Synchrony in String Quartet Performances",
        "author": "\n",
        "excerpt": "Investigating Cardiac Dynamics and Synchrony in String Quartet Performance\n",
        "content": "\n   \n   MusicLab Copenhagen Project\n\n\nAbstract\nIf you are interested in this study, please read the whole thesis!\n\nThis master’s thesis examines the musicians’ cardiac rhythms in string quartet performances. It attempts to capture and demonstrate the cardiac dynamics and synchrony in musical ensembles by analyzing two cases, including a student string quartet (the Borealis String Quartet) and a world-renowned quartet (the Danish String Quartet) performing in different experimental configurations. Two string quartets measured resting heart rate as the Quiet Baseline and repeated Joseph Haydn’s String Quartet in B-flat major in conditions that differ in communication constraints such as the Blind, Violin-isolated, Score-directed, Normal, and Concert. Besides, the Danish String Quartet performed an additional Moving Baseline condition in which they played a scale together, as well as a Sight-reading condition involving a music excerpt they had never heard or practiced before. Unlike most previous studies on music and physiological responses, this study employs both linear and nonlinear methods to reveal different aspects of cardiac dynamics from the individual to the group level. Firstly, we observed more predictable individuals’ cardiac dynamics during the musical performance than the resting baseline in both quartets. Secondly, group-level synchrony analysis demonstrated that both quartets’ cardiac synchrony levels increased during performance conditions relative to the Quiet Baseline. Moreover, the cardiac synchrony level of the Borealis String Quartet was affected to varying degrees by adverse conditions. However, the Danish String Quartet, as an expert group, was more resistant to constraints. Finally, we compared the cardiac synchrony level of the two quartets in identical pairwise conditions. We found the Danish String Quartet has a higher cardiac coupling rate relative to the Borealis String Quartet. Overall, our findings suggest that performing in the string quartet facilitates more predictable cardiac dynamics and synchrony. Different constraints may affect cardiac synchrony to the degree associated with the level of expertise.\n",
        "url": "/masters-thesis/2022/05/20/wenbo-when-hearts-beat-as-one-thesis.html"
      },
    
      {
        "title": "Estimating the repertoire size in birds",
        "author": "\n",
        "excerpt": "Estimating the repertoire size in birds using unsupervised clustering techniques\n",
        "content": "Introduction\n\nUnsupervised learning algorithms search for structures or patterns in a dataset without requiring labels. In the context of bird song recognition, this approach can be useful to draw inferences when manual labelling is inaccesible or too expensive. For example, unsupervised learning can be used to find hidden structures in bird songs [1].\n\n\n   \n   European Greenfinch © Rogério Rodrigues, accessed 11 May 2022\n\n\nIn this blogpost, we will focus on the greenfinch songs (Chloris Chloris). They are organized in a variation of elements which may go on for more than one minute. More precisely, the song of a male greenfinch has been characterized by having groups of tremolos, repetitions of tonal units, nasal chewlee and a buzzing nasal wheeze, which could be uttered on its own and categorized into four phrases classes [2]:\n\n\n  A trill\n  A pure tone\n  A nasal tone\n  A nasal “tswee”\n\n\nThe goal is to use an unsupervised method to estimate the repertoire size in the greenfinch songs. The proposed method follows five main steps which requires to (i) download audio recordings from an online collaborative database, (ii) segment the audio recordings, (iii) convert them into a reduced representation set called a feature vector, (iv) use t-SNE, a dimensionality reduction algorithm, to reduce the dimensionality of the data [3], and (v) automatically form homogenous groups using the DBSCAN algorithm [4].\n\nEstimating the size of the repertoire\n\nEstimating the size of the repertoire can be a challenging\ntask as it needs to perform syllable classification from audio\nrecordings. Therefore, there are many problems we can encounter:\n\n\n  Data set issues — the data can be highly imbalanced due to bigger popularity of one species over another, there is a large number of different species and recordings can have different length, quality of recordings (volume, cleanliness)\n  Multi-label classification problem — when there are many species singing at the same time\n  Background noise — road, rail, and air traffic\n  Different types of bird songs - elements, syllables, phrases, calls, and songs\n  Inter-species variance — there might be a difference in birdsong between the same species living in different regions or countries\n\n\nTo minimize these problems, we can start by (i) downloading only high quality recordings from the Xeno-Canto database, according to its quality ratings ranging from A (highest quality) to E (lowest quality), and (ii) removing recordings that have an other specie referenced in the background using Xeno-Canto’s Application Programming Interface (API v2) and the following fields of the recording object:\n\n\n  q: the current quality rating for the recording\n  also: an array with the identified background species in the recording\n\n\nSegmenting\n\nMoreover, to reduce the size of the dataset and facilitate computer processing, we segment the audio recordings into individual syllables. However, the detection of boundaries for the bird syllables is quite challenging because adjacent syllables can overlap in time and frequency and the onset/offset detection can occur below the background noise level.\n\n\n   \n   Segmentation of bird syllables\n\n\nThe common approach is to convert audio recordings into a spectrogram and apply image processing techniques to pick out the signal of interest. Here, we use the Wavelet Transform (WT) over the Fourier Transform (FT) as it allows a better temporal resolution and is well adapted for non-stationary signals such as bird songs [5].\n\n\n   \n   A schematic overview of the time and frequency resolutions of the different transformations in comparison with the original time-series. The size and orientations of the block gives an indication of the resolution size, A guide for using the Wavelet Transform in Machine Learning, accessed 12 May 2022\n\n\nThe Fourier transform is mainly used for stationary signals which are represented as a sum of sinusoids, meaning that the standard transform is only localized in frequency. Conversely, wavelets are localized both in time and frequency increasing the performance of our syllables segmentation because we are dealing with non-stationary signals.\n\nFiltering\n\nFurthermore, the WT is also useful to remove background noise as it allows to decompose the signal into details (high) and approximation (low) parts. At some scale, the details contain mostly the insignificant noise and can be filtered or zeroed out using thresholding without affecting the signal [6]. Using the free library for the Python programming language PyWavelets it is quite easy to implement.\n\ndef filtering(signal, wavelet='db6'):\n    # Calculate decomposition and reconstruction filter values \n    dec_lo, dec_hi, rec_lo, rec_hi = pywt.Wavelet(wavelet).filter_bank  \n    # Apply high-pass decomposition filter along one-dimension\n    y = scipy.signal.lfilter(dec_hi, 1, signal) \n    return y\n\n\nBefore filtering\n\nAfter filtering\n\n\n\nExtracting\n\nFurthermore, we convert each syllable into a reduced representation set called a feature vector. Since bird sounds are musical in nature, time and frequency-based features used in audio and music retrieval can be extracted. More precisely, we extract Mel-Frequency Cepstral Coefficients (MFCCs) and Descriptive Features (DFs) [7] as they are representative of the way humans hear.\n\n\n  \n    Time domain features\n\n    \n      Energy (EN)\n    \n\n    \n      Zero Crossing Rate (ZCR)\n    \n\n    \n      Duration of the Syllable (DUR)\n    \n  \n  \n    Frequency domain features\n    \n      Spectral Centroid (SC)\n    \n\n    \n      Spectral Bandwidth (SB)\n    \n\n    \n      Spectral Flux (SF)\n    \n\n    \n      Spectral Roll Off (SR)\n    \n\n    \n      Spectral Flatness (SF)\n    \n  \n\n\nImplementing the model\n\nClassification\n\nBecause we are dealing with high-dimensional feature vector, we facilitate the classification process by applying the non-metric dimensionality reduction technique t-SNE to project the data in two dimensions.\n\n   \n   Exploratory visualization using the t-SNE algorithm\n\n\nAdditionally, we cluster the samples rapidly and objectively using the DBSCAN algorithm. This algorithm is useful to find core samples with high density and expand clusters from them. One of the significant attributes of this algorithm is noise cancellation which is helpful to discard the noisy samples as well as the capacity to find the number of clusters while coping with unbalanced classes.\n\n   \n   Unsupervised bird song syllable classification\nusing the DBSCAN algorithm\n\n\nHowever, the biggest challenge with the DBSCAN algorithm is to find the right parameters (eps and min_samples values) to model the algorithm. This requires to find the minimum Euclidean distance between each samples and to identify the “knee” point, or the point of maximum curvature of the curve. That way, we can find the optimal eps parameter.\n\nEvaluation\n\nFinally, we evaluate the model through various ranges of min_samples values and find the right value according to the highest Silhouette score. Silhouette score always ranges between -1 to 1 with a high score suggesting that the objects are well matched to their own cluster and poorly matched to their neighborhood clusters. This is computed as follows:\n\n\n\nIn this study, we iterate over minimum samples values ranging from 20 to 75 and find a maximum Silhouette score of ~0.88 for a minimum sample value of 75. This made it possible to estimate the size of the greenfinch song repertoire  at four syllables, corresponding to the previous observations that had been established in [2].\n\n\n  \n    \n      min_samples\n      Number of clusters\n      Silhouette score\n    \n  \n  \n    \n      20\n      4\n      -0.177\n    \n    \n      25\n      4\n      -0.165\n    \n    \n      30\n      11\n      -0.043\n    \n    \n      35\n      15\n      0.131\n    \n    \n      40\n      24\n      0.269\n    \n    \n      45\n      25\n      0.336\n    \n    \n      50\n      27\n      0.491\n    \n    \n      55\n      20\n      0.541\n    \n    \n      60\n      14\n      0.679\n    \n    \n      65\n      11\n      0.734\n    \n    \n      70\n      4\n      0.870\n    \n    \n      75\n      4\n      0.878\n    \n  \n\n\nCode and instructions for setting up this project can be found here.\n\nReferences\n\n[1] Ranjard, L., &amp; Ross, H. A. (2008). Unsupervised bird song syllable classification using evolving neural networks. The Journal of the Acoustical Society of America, 123(6), 4358-4368.\n\n[2] Hans R. Güttinger. Variable and Constant Structures in Greenfinch Songs (Chloris chloris) in Different Locations. In: Behaviour 60.3/4 (1977), pp. 304–318.\n\n[3] Maaten, L. van der, &amp; Hinton, G. (2008). Visualizing data using t-SNE. Journal of Machine Learning Research, 9(Nov), 2579–2605.\n\n[4] Ester, M., Kriegel, H.-P., Sander, J., &amp; Xu, X. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, 96(34), 226–231.\n\n[5] LI, J., ZHENG, J. M., WU, Y., &amp; XING, F. (2006). AUDIO SEGMENTATION BASED ON THE WAVELET TRANSFORMATION. In Wavelet Active Media Technology And Information Processing: (In 2 Volumes) (pp. 319-324).\n\n[6] Priyadarshani, N., Marsland, S., Castro, I., &amp; Punchihewa, A. (2016). Birdsong denoising using wavelets. PloS one, 11(1), e0146790.\n\n[7] Arti V Bang and Priti P Rege. Evaluation of various feature sets and feature selection towards automatic recognition of bird species. In: International Journal of Computer Applications in Technology 56.3 (2017), pp. 172–184.\n\n\n\n\n\n",
        "url": "/machine-learning/2022/05/20/joachipo-estimating-repertoire-size.html"
      },
    
      {
        "title": "Reconfigurations: Reconfiguring the Captured Body in Dance",
        "author": "\n",
        "excerpt": "Building cooperative visual, kinaesthetic, and sonic bodies\n",
        "content": "Introduction\n\nOptical motion tracking systems allow the body’s motion to be captured in precise detail in 3 dimensional space. These capture systems are used in many contexts, from analytical tools to performance capture for films and games and use a marker based approach, with individual markers placed on the body forming the basis out of which skeletons or rigid bodies are constructed. The assumption is also that these markers remain in a fixed position on the body. However, there’s an interesting idea in this: when we capture the motion of a body with such a system we aren’t actually capturing the full body, just the points we decide to. In a live performance capture context, this means that a performer need only reveal as much of their body as they wish. They have control over the representation of their body and in addition, the capture anonymises them to an extent. The flip side of this, however, is that just looking at the points from a capture, the audience tends to assume that anything vaguely resembling a part of a human body is just a part of a body, and, at that, a body which tends towards the idealised body. If a capture is purely the upper body, for example, we assume that the rest of the body is still there.\n\n\n   \n   Markers on a human body in OptiTrak Motive (from https://v22.wiki.optitrack.com/index.php?title=Skeleton_Tracking) \n\n\nReconfigurations is an attempt to work with this idea and encourage performers to cooperate to build bodies together bigger than their own. Two performers perform motions in a performance space setting markers on their bodies where they wish and moving them around at will. A third performer configures and links these markers on a visual display to form shapes and bodies that are beyond what is possible to make with as a single person. Each body is also sonified with a different timbre. The position of the markers on the horizontal plane determines pitch and on the vertical axis amplitude.\n\n\nThe system is designed to work for two use cases; choreographed dance performance, in which the dancers plan out specific shapes, bodies, and sounds and transition between these with purpose, and an interactive installation to allow non-trained members of the public to explore the visual, kinaesthetic, and sonic space offered by the system.\n\nSystem Design\n\n\n\n   \n   Left: Control Display, Centre: Performance Display, Right: Performers in Performance Space\n\n\n\nThe entire performance system is run over four components; the optical motion capture system that tracks the markers and passes on their position data, a control display that enables the third player to link the markers and assign them to shapes and bodies, and which passes this data onwards to the further two components, a performance display that visualises the results for the performers and audience, and a sonification patch. Data is passed between these components over OSC. A prototype system implemented these specifically to fit the specific space of the portal.\n\n\n\n   \n   Performance space in the portal\n\n\n\n\n\n   \n   System Data Flow\n\n\n\nOptical Motion Capture System\n\nThe optical motion capture system used in the prototype system was OptiTrak Motive. Motive enables the live broadcast of position data, so fits the needs of the Reconfigurations system. In addition, instead of using basic markers, rigid bodies are employed in their place. This means that each broadcast position has a unique identifier, which helps in regards to marker dropout and occlusions.\n\n\n\n   \n   The 30 rigid bodies used as markers in performance\n\n\n\nControl Display and Performance Display\n\nThe control display and performance display show the positions of each rigid body as a single marker. The control display is interactive and used by the third performer in order to assign markers to bodies and shapes, enable and disable individual markers, and link the markers together visually. The performance display mirrors the control display without any of the visual information required for interaction. This is displayed on a screen visible both to the audience and the performers.\n\nSonification\n\nThere are five bodies available to the performers, each with their own timbre and characteristic. Each enabled marker in the body drives a sound generator with the pitch determined by the position of the marker on the horizontal plane and the amplitude determined by the position on the vertical axis. However, there are two different pitch mappings, one for the choreographed dance use case and one for the interactive installation use case.\n\nThe first case maps several bodies markers to a continuous frequency range, enabling microtonal playing, and several to the 12-tone equal temperament chromatic scale. This is to ensure that the majority of spatial configurations result in dissonance, allowing the choreography to shift into consonances with purpose.\n\nThe second case maps the markers for every body to the minor pentatonic scale. This is to enable the untrained users of the interactive music system to work purely in consonances so that no choreography has to be planned.\n\nPerformance with the Prototype System\n\n\n\n   \n   Two performers forming and moving as a 'guitarist' in a test session\n\n\n\nBelow are two videos of a performance combining both use cases. Joachim and Joseph performed as dancers, with Hugh controlling the configuration. Two performances took place, with each performance being semi-planned out. The sonification version used was the pentatonic version for the second use case.\n\nFor performance 1, four rigid bodies were used, one placed on each of Joseph and Joachim’s hands. Conceptually, the idea was to play with the quadrilateral configured by the positions of their hands, exanding this outwards into 3 dimensional projections of a tetrahedron and unmooring points from the main body.\n\n\n\n\n\nPerformance 1\n\n\n\nFor performance 2, 12 rigid bodies were used. These were attached to the head, left shoulder, elbow, hand, hip, and knee of Joachim and head, right shoulder, elbow, hand, hip, and knee of Joseph. Conceptually, the performers were exploring the idea of creating a whole body out of their two halves, seeing how this allows that body to move in ways that neither Joseph or Joachim could achieve alone. As the performance continues, this body slowly becomes reconfigured, shifting towards more abstract shapes and representations.\n\n\n\n\n\nPerformance 2\n\n\n",
        "url": "/motion-capture/2022/05/20/hughav-mo-cap-reconfigurations.html"
      },
    
      {
        "title": "Developing Techniques for Air Drumming Using Video Capture and Accelerometers",
        "author": "\n",
        "excerpt": "Creating MIDI scores using only data from air drumming\n",
        "content": "Introduction\n\nIn this blog post, I’ll be descibing my process to create a system for air drumming using a combination of accelerometer data and video input, and showing my results via a video synchronizing the generated MIDI files to a test video file. To do this, the system will be broken down into two parts: drum hit detection and drumming position prediction.\n\nHit Detection\n\nIn order to tell when a drum hit should be registered in the MIDI file, we will use one AX3 accelerometer in each hand. They will both be held in between the thumb and index finger, much like a drum stick would. In order to detect a hit, we will only be analyzing the y axis of acceleration since this was found to be the most accurate.\n\nPosition Prediction\n\nWe must also be able to tell exactly what drums the drummer is playing at any given time using purely video data, which is a much harder task. To do this, we will be using a variety of machine learning (ML) techniques and tools. First, we need to train our ML systems using there will be 7 different postions we will use, each corresponding to which drum the left and right hand are playing. Those postions are:\n\n\n  Left and right hand playing a hi-hat\n  Left hand playing a snare, right hand playing a hi-hat\n  Left and right hand playing a snare\n  Left hand playing a snare, right hand playing a ride cymbal\n  Left hand playing a snare, right hand playing a floor tom\n  Left and right hand playing a floor tom\n  Left hand playing a snare, right hand playing a crash cymbal\n\n\nYou can view one of the test videos below, with the left hand playing a snare and right hand playing a hi-hat.\n\n\n  \n\n\nFeature Extraction\n\nNow, the question is how we can tell each of these positions apart. To accomplish this, I came up with 4 different feature extraction methods to attempt this: using raw video data in a system similar to the Wekinator, using raw CSV data from the VideoAnalysis software, using smoothed CSV data from VideoAnalysis, and using smoothed VideoAnalysis data along with smoothed AX3 accelerometer data.\n\nRaw Video Data\n\nIn order to use raw video as an input to ML systems, we will import the videos into python using the pacakge OpenCV, convert them to grayscale, downsample the video to 50x50 pixels, and parse each training video frame by frame to train our systems to predict positions.\n\nVideoAnalysis Data\n\nThe VideoAnalysis software was developed by the fourMs lab to perform simple motion analysis on video inputs. The inputs we will use for our ML systems that are output into a CSV by the VideoAnalysis software will be the quantity of motion, the x and y coordinates for center of motion, and the x and y coordinates of the motion bounding box’s minimum and maximum values.\n\nWe will use this data in two different ways. The first will be similar to the raw video data, where we will parse the output CSV file row by row (equivalent to frame by frame) and use the afformentioned values to train the ML systems. The second will be using a rolling average in order to smooth the data, and then the ML systems will be trained in the same way as before. Finally, we will be using a combination of smoothed VideoAnalysis data and smoothed accelerometer data as inputs to the ML systems.\n\nML Techniques\n\nFor each feature extraction method, we’ll use both an MLP regressor and an MLP classifier. In regards to this project, the main difference between the two are that the classifier will use input data to predict a label that corresponds to a drumming position, while the regressor will output two continuous values which will be rounded and used to correspond to the position of each hand (ex. [0, 1] corresponds to left hand snare, right hand hi-hat).\n\nEvaluation Metrics\n\nWe will be evaulating the ML systems using both qualitative and quantitative methods. For regressors, we will be using R2 score as our quantitative metric and for classifiers we will be using accuracy. We will also sync the generated MIDI to the test video file, which you can view below.\n\nResults With Raw Video Features\n\nRegressor\n\n\n  \n\n\nClassifier\n\n\n  \n\n\nResults With Raw VideoAnalysis Features\n\nRegressor\n\n\n  \n\n\nClassifier\n\n\n  \n\n\nResults With Smoothed VideoAnalysis Features\n\nRegressor\n\n\n  \n\n\nClassifier\n\n\n  \n\n\nResults With Smoothed VideoAnalysis And Accelerometer Features\n\nRegressor\n\n\n  \n\n\nClassifier\n\n\n  \n\n\nQuantitative Metrics\n\nRegressors\n\n\n   \n\n\nClassifiers\n\n\n   \n\n\nConclusions\n\nAs you can see from these videos, hit detection with accelerometry was very accurate, but none of the attempted techniques for position prediction worked particularly well. This could be due to a variety of factors, but my main theories are that I tested the ML systems on too many positions and due to some of the movements being incredibly similar (ex. left hand snare and right hand hi-hat vs. both hands snare), the resulting ML system just ended up getting confused. However, there were some brief flashes of success in the results for the regressor using smoothed VideoAnalysis CSV data as an input.\n\nIn general, using raw video input with as inputs to ML systems seems to not be a good way to produce accurate MIDI scores through air drumming. One possible alternativesthat may work better is using raw video data, but defining boundaries for each instrument so the onus is on the user to perform correctly instead of relying on an ML system. If ML were to still be used, a more accurate 3D rendering of arm motion using a system such as OptiTrack should be used instead of just flat, 2D video data.\n\nFiles\n\nYou can view the GitHub repository with the files for this project here.\n\nImage Source\n\nhttps://jamaddict.com/wp-content/uploads/2019/11/bigstock-Boy-listening-to-music-57379796.jpg\n",
        "url": "/motion-capture/2022/05/20/josephcl-air-drumming.html"
      },
    
      {
        "title": "Recognizing and Predicting Individual Drumming Groove Styles Using Artificial Neural Networks",
        "author": "\n",
        "excerpt": "Can we teach an algorithm to groove exactly like a specific drummer?\n",
        "content": "\n   \n\n\nIntroduction\n\nAs shown in the picture above, a MIDI drum beat can be divided into two sections: the score, which is the quantized notes that the drummer plays, and the groove, which is how the drummer plays the notes. In this blog post, we’ll be examining groove and attempting repliacte a groove style an individual drummer. First, we have to define what the elements of a groove are.\n\nBackground Information\n\nWhat is Groove?\n\nGroove can be defined as a combination of velocity, which is how hard a note is played, and microtiming, which is how far a note is off of a quantization grid. When microtiming and velocity are applied to a quantized drum score, it creates a sense of humanization for listeners.\n\nPrevious Work\n\nIn 2019, Google’s Magenta project created GrooVAE, which they define as a “class of models for generating and controlling expressive drum performances” using machine learning. Through this project, they were able to sucessfully apply a realistic, human sounding groove to any drum score.\n\nData Set\n\nWhile creating GrooVAE, the team also developed the Groove MIDI Dataset. For this project, we will be using an expanded version of this data set, known as the \nExpanded Groove MIDI Dataset (E-GMD), which features 444 hours of MIDI data from 9 different drummers.\n\nProject Description\n\nThis project design can be broken down into five sections: extracting data from the MIDI database, training a multilayer perceptron (MLP) classifier to recognize a drummer’s unique groove, training MLP regressors to generate microtiming histogram bins based on each drummer’s MIDI velocity while playing, generating MIDI based on the regressor’s output, and testing if the previously trained classifier can recognize the drummers based on the MIDI that was generated.\n\nMIDI Data Extraction\n\nThe Python package Mido was used in this project to extract relevant data from MIDI files, which includes the velocity and timing of each note played. Additional features, such as tempo and beat type (beat or fill), were extracted through the name of each file in the E-GMD (ex. “4_latin-brazilian-samba_89_beat_4-4_6.midi”). These features were extracted into a series of classes, including a class for each drummer, MIDI file, and MIDI note.\n\nTraining the Classifier and Regressors\n\nFeature Extraction\n\n\n   \n\n\n\n   \n\n\nThe features that the classifier and regresors use are histogram bins generated by analyzing microtiming and velocity for each snare, bass drum, and hi-hat note of every file. When extracting features, the system iterates through all files for every drummer and place the velocity and the microtiming in their respective histogram bins. The bins for velocity are in increments of 8 on a scale of 0 to 127, corresponding to the velocity value supplied by the MIDI note, resulting in 16 total velocity histogram bins. The microtiming section requires a few more steps to get histogram bins. Since there is no guarantee that the ticks per beat of each file are the same, the values must be normalized. This was accomplished by creating a function that computed the percentage of how far off beat each note was from a perfect sixteenth note using a given tick per beat value. Ostensibly, this function showed how far a note is from a thirty-second note, since the furthest a note can be away from sixteenth note quantization is a thirty-second note. Once we have a percentage ranging from -50 (directly on the thirty-second note behind the sixteenth note) to 50 (directly on the thirty-second note ahead of the sixteenth note), the value is placed into one of 10 histogram bins, each of which corresponds to 10 percent of the quantization percentage range.\n\nClassifier Testing\n\nThe features that the classifier is trained on are the microtiming and velocity histogram bin values of each file and target value is the drummer. Once the features are extracted, the classifier is trained and tested with 10 k-folds. Through testing, I found that the classifier could predict a drummer based on these features with 90-97% accuracy.\n\nRegression Testing\n\nThe features that the regressors are trained on are the histogram values for velocity, while the target values that it is trying to predict are the histogram values for microtiming. For each drummer, we will create a new drummer and train and test it with 10 k-folds. The R2 score and varience for each drummer’s regressor can be seen on the table below.\n\n\n   \n\n\nMIDI Generation\n\nAs we can see from the table, drummers 3, 7, and 9 had the best results, so from here on out we will only be focusing on these three drummers. To generate MIDI, I used the python package MIDIUtil. The generated MIDI files consist of constant sixteenth notes for the bass drum, hi-hat, and snare to ensure each instrument is equally weighted when evaluated with the classifier in the next step. Each of these notes have a random velocity and microtiming applied to it based upon an array generated using weighted random values based on the microtiming and velocity histogram inputs, which were supplied to us from the regressor. For velocity, each histogram bin percentage is divided by 8 and applied to every velocity value, so that 0 through 127 all have a probability of being selected. The microtiming histogram uses the same system, but there is a probability for every 1 percent off a sixteenth note (from -50 percent to 50 percent) and each histogram value is divided by 10 to accomplish this. These audio files will be used to test with the previously trained classifier. An example audio file for each drummer can be found below.\n\nDrummer 3 Generated MIDI Classifier Input\n\n  \n\n\nDrummer 7 Generated MIDI Classifier Input\n\n  \n\n\nDrummer 9 Generated MIDI Classifier Input\n\n  \n\n\nTesting Generated MIDI Files\n\nAfter the MIDI files are generated, they are then fed into the previously trained classifier to evaluate if it can accurately classify our generated MIDI files. Unfortunately this ended up not working that well, with accuracy scores around 40-66%.\n\nFinal Thoughts\n\nThrough testing, we found that an MLP classifier can reliably predict an individual’s drumming groove using microtiming and velocity histogram values alone. However, we also found that only some drummers had accurate R2 scores for MLP regressors predicting microtiming histogram values based on velocity histogram values. When examining the input files for each drummer, the common link between each drummer with good R2 scores was relative consistancy in playing style. For example, drummer 3 played mostly rock beats, with a few jazz and hip hop beats as well, and had excellent R2 scores. Meanwhile, drummer 5 was trained on a significant amount of MIDI files, but due to the diversity of different styles they played, which included samba, blues shuffle, funk, rock, afro cuban jazz, and more, R2 results were significantly lower than other drummers that were trained on the same number of files. Therefore, future studies on individual groove should focus on specific styles of music.\n\nIn terms of the generated MIDI files, you can listen to a more “normal” 4/4 beat generated with the same aformentioned techniques below.\n\nDrummer 3 Generated MIDI\n\n  \n\n\nDrummer 7 Generated MIDI\n\n  \n\n\nDrummer 9 Generated MIDI\n\n  \n\n\nAs we can see, the MIDI files sound pretty random and there are no distinguishing characteristics between each drummer to the naked ear. Future studies may have to use a windowing function to create more human and less roboting sounding MIDI files.\n\nFiles\n\nYou can view the GitHub repository with the files for this project here.\n\nImage Sources\n\nhttps://magenta.tensorflow.org/assets/groovae/score-groove.png\n\nhttps://i.pinimg.com/originals/b7/db/69/b7db69a654b84e86467d5aa0b28ee35d.jpg\n",
        "url": "/machine-learning/2022/05/20/josephcl-recognizing-and-predicting-groove.html"
      },
    
      {
        "title": "Piano Accompaniment Generation Using Deep Neural Networks",
        "author": "\n",
        "excerpt": "How I made use of Fourier Transforms in deep learning to generate expressive MIDI piano accompaniments.\n",
        "content": "How can you generate piano accompaniments to a given melody using machine learning (ML)? That was my question. The answer, my friend, seemed not to be blowing in the wind, but revealed itself after extensive googling, trial, and error. My goal was to somehow generate piano accompaniments by conditioning the ML model on melodies. To do so, I had to look into the field of Natural Language Processing (NLP) and deep neural networks. I settled on working with symbolic music representation in the form of MIDI for my project, not raw audio. There has been a lot of research in translation applications using ML. Let’s pretend that melodies are Chinese, and accompaniments are Swedish. Now we simply need to translate the Chinese melody into a Swedish accompaniment. Easy, right? Not when you’re completely new to the field, as I was. But I slowly and steadily managed to learn some basics of ML translation, and a lot of general ML theory along the way.\n\nSequence to Sequence\n\nTranslational tasks are usually tackled by using sequence to sequence (seq2seq) models. This architecture takes one sequence in and spits another sequence out. One of many challenges is that the sequences probably won’t be of the same length. And especially for my use case, where there should be several more notes (analogous to words) in the generated accompaniment than there will be in the incoming melody. One ML model design which has gained much attention (pun intended) the last years, is the Transformer model [1]. This model uses self-attention mechanism to learn the relationship between notes and can potentially learn long-term and high-level structures and phrases in music. This model performs even better than the Long Short-Term Memory (LSTM) layers, which previously was used by Google Translate, among others.\n\n\n   \n   The Transformer architecture, sourced from a book [2] I couldn't do without this spring.\n\n\nFNet\n\nFor my project, I chose to use the FNet architecture [3], which is a modification of the Transformer. Here, the attention layer of the encoder is exchanged with a Fourier transform layer to project the input sequence into the Frequency domain. This makes the model much faster, as multiplication in the frequency domain is equivalent to convolution in the time domain, while the performance is only slightly worse. I’m not too concerned about the accuracy scores anyway when working with a generative model like this.\n\nModel Architecture\n\nSeq2seq models are made up of an encoder block and a decoder block – in my case, the melody (source language) is fed into the encoder, and the accompaniment (translated language) is fed into the decoder when training the model. The encoder output is connected to a multihead attention layer in the decoder, to condition the accompaniment generation on the melody, while the decoders output is the predicted accompaniment. When the model is used for inference/generation, you’ll still feed the encoder with the melody, but the decoder will only receive its last predicted output token at any given step. (You can also feed the decoder a portion of the original accompaniment to give it a flying start, as I did when prediction my accompaniments.) Output token you say, what’s a token?\n\n\n   \n   The Transformer (left) and FNet (right) encoder blocks\n\n\nTokens\n\nThe food for seq2seq models is tokens. Tokens would usually be a word representation in an NLP task. For symbolic music translation, this could be a note. There are several ways to tokenize music, and I chose to use a modified version of the design found in both Google Magenta’s Performance RNN and in the research done by the creators of my dataset [4]. I encoded MIDI notes into the following four categories:\n\n\n  128 note-on events (One for each of the 128 MIDI pitches – to start a note)\n  128 note-off events (One for each of the 128 MIDI pitches – to release a note)\n  32 time-shift events (Making 32nd notes the shortest possible time-shift)\n  32 velocity events (MIDI velocities quantized into 32 different values)\n\n\nDataset and processing\n\nAs my dataset, I chose the mentioned POP909 dataset, consisting of 909 Chinese pop songs. I wanted to work with popular or groove-based music, as much research has been done using classical piano music. This dataset conveniently had MIDI files with separated tracks for melody and accompaniment, which made the whole process easier for me. I chose to quantize the MIDI files into a 32nd second note resolution and chop the files into 32 bar segments to expand my dataset. Now, I only needed a model to train!\n\nModel training\n\nAs my tool for building a machine learning model, I used the Keras ML framework, which is a part of the TensorFlow library. I spent a lot of time in building the model and tweaking different hyperparameters to make it perform in the best way possible. In the start, I used my poor old laptop, and kept it running overnight. (Disclaimer: Do not try this at home! I was a bit worried if the laptop would heat up too much during sleepy time.) After a while, I switched to using UiO’s ML Nodes. This, in the end, made it possible to fit my model in time. The final training of 695 epochs took about 6 hours and 45 minutes using 2 GPUs on the ML Nodes, while it would’ve approximately taken 10 days and 15 hours on my laptops CPU. (20 seconds per epoch vs 22 minutes pr epoch.) In the end, I chose to also train a vanilla Transformer model, to compare with the FNet.\n\nAccompaniment generation\n\nIn the end, I was quite surprised how the generated accompaniments turned out. It played in the same key as the melody, but the timing and rhythm was oftentimes a bit off. The model seemed to be able to generate accompaniments with some musical phrases and elements audibly present. My virtual pianist is not replacing a professional accompanist anytime soon and will not always sound completely sober. But hey, it works better than I expected it would. Below, you can take a listen for yourself. Impressive or not, this has been a nice entry point for me into deep learning and music generation. I hope to expand my knowledge and experience in this field in future projects.\n\nAudio Examples\n\nThe piano predictions below have the 256 first of 1024 tokens feed as an input condition. The first example was made by me recording “Make You Feel My Love”, to see how well my models performed on a completely different file with complex harmony.\n\n“Make You Feel My Love” – Bob Dylan\n\nOriginal Accompaniment\n\n\n  \n\n\nFNet Predicted Accompaniment\n\n\n  \n\n\nTransformer Predicted Accompaniment\n\n\n  \n\n\n“Song 186 – Segment 5” – POP909 dataset\n\nOriginal Accompaniment\n\n\n  \n\n\nFNet Predicted Accompaniment\n\n\n  \n\n\nTransformer Predicted Accompaniment\n\n\n  \n\n\nReferences\n\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in Neural In- formation Processing Systems 30: Annual Conference on Neural Information Processing Systems (NeurIPS), 2017, pp. 5998–6008.\n\n[2] F. Chollet, Deep learning with Python, Second edition. Shelter Island: Manning Publications, 2021.\n\n[3] J. Lee-Thorp, J. Ainslie, I. Eckstein, and S. Onta-non, “FNet: Mixing Tokens with Fourier Trans-forms,” arXiv:2105.03824 [cs], Sep. 2021, Ac-cessed: May 04, 2022. [Online]. Available: http://arxiv.org/abs/2105.03824\n\n[4] Z. Wang et al., “POP909: A Pop-song Dataset for Music Arrangement Generation,” in Proc. of the 21st Int. Society for Music Information Retrieval Conf., Montréal, Canada, 2020\n",
        "url": "/machine-learning/2022/05/20/kriswent-generating-piano-accompaniments-using-fourier-transforms.html"
      },
    
      {
        "title": "Reverb Classification of wet audio signals",
        "author": "\n",
        "excerpt": "Differenciating reverberation times of wet audio signals using machine learning.\n",
        "content": "\n   \n\n\nMachine learning is hard, and it takes a lot of time. It’s a lot of ups and down to figure out how to actually create a working program, or to find the data you are searching for. In this blogpost, I will talk about my trial of classification of reverberation times using machine learning.\n\nClassification of reverberation time is usually done by measuring a reverberant rooms RT60 value; the time multiple echoes uses to drop 60 decibels from the end of the transmitting audio. But when measuring a stream of continuous audio containing reverberation, this is not possible. Therefore, I wanted to see if it was possible to train a classifier using 21 reverberation lengths, applies to 100 random audio samples. Spoiler-alert: It did not work as intended.\n\n\n   \n      Figure 1 and 2\n\n\nThe existing literature that seeks to predict RT60 vary in design. But the similarities in most of the systems are that most of them want to create a machine that can predict RT60 times from blind samples, meaning reverberate recording that the model has not seen. The most common papers found, was from researchers that participated in the ACE Challenge - Corpus Description and perfor- mance evaluation[1]. It is a competition to identify the most promising and non-intrusive Direct-to-Reverberant Ratio (DRR) and RT60 estimations in noisy reverberant speech. They offer a dataset that researchers can compete with, to accomplish the best RT60 and DRR measurements.\n\nOne paper that seeks to accomplish the ACE challenge [2] suggests that Attention-CRNNs are best suited for classification of reverb, where an attention vector is present in lower amplitudes of the signal, such as tails after peaks. As the other ACE-challenges, this is done on human speech and not music. In another paper [3], the authors hypothesise that using both temporal features in combination with spectral features is the best way to train a CNN to estimate RT60. Their results show that the estimation can be modelled as a regression problem and implement with a convolutional neural network. Their neural network outperform state-of-the-art algorithms form the ACE Challenge evaluation corpus for T60 estimation.\n\nWhen looking at previous research that focuses on music related problems, one paper tried to match artificial reverb settings to unknown room recordings [4]. This can be a useful application, since matching organic reverberant recordings with artificial reverb is a time-consuming task. They have used MFCC as feature extraction, in a Gaussian Mixture Model (GMM). Existing recordings gets recommended reverberation settings from three selected plugins.\n\n\n   \n      Table 1\n\n\n\n  \n    \n    Output Audio File\n  \n  Class 0, Audiofile 1\n\n\n\n\n  \n    \n    Output Audio File\n  \n  Class 9, Audiofile 1\n\n\n\n\n  \n    \n    Output Audio File\n  \n  Class 18, Audiofile 1\n\n\n\nThe database constructed had its main origin from the OpenAir library of impulse responses[6]. Choosing 20 suitable reverberations from their library of 100 impulse responses, was done on the basis of their length and general quality. The audio-samples was sourced from different dry audio signals mainly from the apples loops library [5], [7]. As we can see in the graph below, the 100 audio-samples was a large mix of everything available with limited reverberation. Other audio-samples were sourced from anechoic recording databases with high quality recordings.\nIf you want to try this database yourself, you can find it here.\n\n\n   \n      Diagram 2: Source of audio-samples\n\n\nFeature extraction was done with librosa’s MFCC. Originally developed for automated speech recognition, it can be used as a temporal feature in music analysis. The chosen size of the MFCC was 40 mels, over a frame size of 2048 samples (42ms) and a window-length of 512 samples (10ms). The main problem with using MFCCs is that they are not robust to noise. Therefore a filter was applied at 400Hz to try to remove as much of the fundamental tones in the music as possible.\n\n\n   \n      Figure 3: Unfiltered MFCC\n\n\n\n   \n      Figure 4: Filtered MFCC\n\n\nThe model that I created had some flaws. As you can see in the diagram below, the LDA does most of the work. The tensorflow-model is trained on the basis of how the LDA is projecting the data, and the blind-samples was fed into the same transformation as the train-data.  This is an issue when validating the data with the test data, since its projected in a way from the LDA where it already has been classified on the basis of the label given in the labelling-process (true label). The realisation of this fundamental issue came too late, mouthing in poor performance.\n\n\n   \n      Diagram 1: System architecture\n\n\nWhen running the LDA projection through a separate test-set with the same audio-samples that the model had seen, but different impulse responses, the model was able to predict the samples fairly well. But with blind samples that the model had not seen before that was similar but not the same, the model was not able to predict much. In diagram 4, we can se that the overfitting that happened in diagram 3 in class 5 is the fault for this.\n\n\n   \n      Diagram 3 and 4: Normalized matrix \n\n\nIn conclusion, the model created suggest that it is possible to differentiate between reverberation-times in wet audio-signal from monophonic instruments, opera, percussion, to quartets playing music together. But the model doesn’t work well with samples that is has not been trained on (blind-samples).\n\nThe model performs well when given impulse responses that are different from each other, both in length and in timbre. But with samples that are too similar, the model starts overfitting or under-fitting one of the similar classes. Compared to state of the art papers, this model is both of lower quality, and does not show the same results.\n\nAll in all, machine learning is hard. The joy of having a «working» model did that I didn’t validate the results. This lead to a later realisation that the model did not work at all.\n\nReferences\n\n[1] J. Eaton, N. D. Gaubitch, A. H. Moore and P. A. Naylor «ACE Challenge - Corpus Description and Performance evaluation» in 2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics October 18-21, 2015, New Paltz, NY\n\n[2] C, Papayiannis, C. Evers, P. A. Naylor «End-to-End Classification of Reverberant Rooms using DNNs» in IEEE/ACM Transactions on Audio, Speech and Language Processing Volume 28 2020\n\n[3] H. Gamper and I. J. Tashev «Blind Reverberation Time Estimation Using a Convolutional Neural Network» in 2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)\n\n[4] N. Peters, J. Choi, and H. Lei, “Matching Artificial Reverb Settings to Unknown Room Recordings: A Recommendation System for Reverb Plugins,” Paper 8700, (2012 October.) AES Convenction 133\n\n[5] Apple, «Apple Loops Library» last modified 04 December 2022 https://support.apple.com/en-gb/ HT201907\n\n[6] Open Air Library, «Anechoic and IR Data», visited 04 April 2022 https://www.openair.hosted.york.ac.uk/\n\n[7] AVAD-VR «Audio/Video Anechoic Database», visited 04 April 2022 http://www.lam.jussieu.fr/ Projets/index.php?page=AVAD-VR\n",
        "url": "/machine-learning/2022/05/20/jakobhoydal-Reverb_classification.html"
      },
    
      {
        "title": "Spring Telematic Concert 2022: Portal Perspectives",
        "author": "\n",
        "excerpt": "Guitar and MoCap in the Portal + behind the scenes documentary!\n",
        "content": "To cap off a long semester of machine learning and Motion Capture (MC) we held our final Portal course telematic concert, this time between the Portal and the Science Library at UIO. For this concert we decided to add in a little complexity; not only would we be employing interactive music systems based around machine learning and motion capture with visual components, but we would also allow the audience to participate in one section of the concert. Integrating all of these components proved a challenge, but one that we were happy to take on.\n\n\n   \n   \n  Take a look behind the scenes!\n\n\nPerformances\n\nThe concert was split into two halves, comprising two performances. The first performance was an improvisation between Arvid on electric guitar in the Portal, and Kristian on keys and Joseph on percussion at the Science Library. Kristian also added effects and looped the guitar sound from the portal using a MYO armband and a Zoia synthesizer (more on this system here).\n\nThe piece started out minimalistic and quiet, and slowly grew in intensity. The mix between the raw guitar sound coming from the amplifier and the treated audio signal from the Science library created an interesting listening experience in the portal. Some interesting timbral textures and motifs were created during the piece, and after slowly reaching a climax the improvisation found its end in a slow, drifting fade out of reverb and delay. The performers in the Science Library were visible on two monitors in the Portal, enabling visual contact and helping the single performer in the Portal to feel as part of an ensemble.\n\n\n   \n   Arvid performing\n\n\nThe second performance involved integrated two motion controlled systems. In the Portal, Hugh set up the optical MC marker reconfiguration and sonification system (more on this system here), with the resulting audio being sent over LOLA to the Science Library for manipulation and effects added by Joachim’s machine learning based, gesture parameter controller. This performance was carried out by audience members briefed on the systems’ operation and asked to interact with each other. This led to some interesting results with the interactions providing musical ideas novel to both Hugh and Joachim.\n\nHowever, there were several difficulties in this setup. Firstly, due to the LOLA camera and microphone setups there were no methods through which the audience members using the system on each side could communicate with each other. Secondly, due to both sides’ musical output being gesture controlled and involving synthesized sound, there was some confusion on the portal side of what exactly the motion was controlling. One audience member even started moving their rigid body according to the pitch modulation effect being applied in the Science Library. Nonetheless, reports from the Portal side indicated that the audience members had fun and enjoyed both the motion controlled systems and the interaction over the LOLA network.\n\n\n   \n   The motion capture performers\n\n\nTechnical Setup\n\n\n\nThe portal technical setup was centred on the LOLA rack, used for sending both audio and video to the Science Library. For the first performance, the guitar’s output setup was sent directly for Kristian to manipulate. The MC system was a little more complex, as this involved using four computers communicating via OSC. The audio output of the system was sent directly over LOLA.\n\nAdditionally, the MC system also involved a visual component, showing the rigid bodies’ position which needed to be displayed in the Science Library. However, sending two video feeds (the performers’ camera and the visuals for the MC system) caused some display problems.\n\nThe solution we found was to simply place a monitor showing the MC output directly in front of the camera. This worked to an extent, however this obscured the performers and could be difficult to make out on the screen sometimes. Moreover, it was difficult to draw the audience’s attention to it in the Science Library. Jakob (Science Library) suggested streaming the visual output over Zoom, as this allowed picture in picture placement on the Science Library’s side. Despite adding a large amount of latency between the Science Library’s view of the Portal performers and the visualizations, we decided that this would be the best option to clearly display them both.\n\n\n   \n   Routing in the Portal\n\n\nProduction\n\nIn order to promote our concert, we contacted the people responsible for the IMV’s and the Science Library’s social media accounts. We created a Facebook event for the concert that we sent to the social media representatives to share and that we also shared on the IMV Facebook group. Additionally, the Science Library representative permitted us to take over the official Instagram account on both the day before and the day of the concert to upload promotional stories. Finally, we put up posters around campus, and we were covered and ready to concert!\n\nBehind the Scenes Documentation\n\nOliver documented the process and music systems by making a behind-the-scenes video, to showcase what can be done for telematic music concerts. A video also makes it easier to explain these complex processes in a relatively short amount of time. We used the only equipment available to us, a camcorder. As a result, some footage had to be thrown out due to clipping audio, as audio monitoring is limited on these devices. In retrospect, we should have narrated the video a bit more to make the timeline clearer and ensure the viewers know what’s happening and why. Some background music might help fill empty space in the video as well, but we managed to reduce this emptiness by overlapping dialogue between clips.\n\nConclusions\n\nAs a final Portal concert, this felt like a culmination of the skills learnt over our first year in SMC. Our comfort with the Portal equipment, LOLA connections, and telematic performance made the process smooth and simple, allowing us to expand to include other techniques that we’ve acquired, such as using machine learning and the Portal’s motion capture system. Now we’re looking forward to taking the role of the audience, and seeing how next year’s SMC intake surprises us in their concerts!\n",
        "url": "/networked-music/2022/05/26/hughav-telematic-concert-portal.html"
      },
    
      {
        "title": "One Last Hoorah: A Telematic Concert in the Science Library",
        "author": "\n",
        "excerpt": "In which we go over the most ambitious telematic concert of our SMC careers.\n",
        "content": "After one year at the SMC master program, we finally went out into the masses to arrange a location, away from familiar locations. This time, we arranged a concert between the science library at Vilhelm Bjerknes hus, and the SMC Portal at ZEB. In terms of audiovisual setup, we still used the same NMP-kits as before, but at this concert we devided it into two section: one that was a traditional telematic concert with gestural interplay, and one with instruments created for the audience to interact with. This post is about the methods and techniques we used to create the Vilhelm Bjerknes side of the concert.\n\n\n   \n   \n  Video recording from the opening act.\n\n\nTelematic Gestural Interplay\n\nOur first part of the concert was a performance by Joseph Clemente, Arvid Falch and Kristian Wentzel. This performance centered around the idea of a telematic interplay where gestures at one site manipulate the audio coming from the other site. At the Science Library, Kristian used a Myo armband controlling a ZOIA modular system to process and manipulate Arvids electric guitar playing from the Portal. What is a Myo armband? It is an inertial measurement unit (IMU) with gyroscope and accelerometer sensors, which also sports eight electromyography (EMG) sensors. This handy device is also capable of recognizing five different hand gestures out of the box. For a more in-depth look at Kristians setup, take a look at his other blog post.\n\nTo make a performance out of our concept, we added Joseph with a Roland SPD-SX sampling pad at the science library, and put a keyboard in front of Kristian as well. Our feature was an improvised piece, with some agreed guidelines. Our performance was made up of four sections, where one section seamlessly flowed into the next. We decided that Kristian should start the performance with a solo piece on an extended keyboard instrument using his setup. After some time, Arvid would join in for a duo performance where Kristian manipulated Arvid. In the third section we expanded into a trio, introducing Joseph on the drum pads. For the fourth section, we added more prominent and coherent grooves with a beat to the mix before finishing off.\n\nTelematic Interactive Systems\n\nThe second part of the concert has been thought as a telematic performance involving the audience of the two different locations (i.e. the Portal and the Science Library). The idea was to allow the audience to (i) generate and (ii) process audio data in real-time while interacting with two types of program:\n\n\n  A program able to generate audio from the body motion of users\n  A program able to control audio effects using the hand gestures of users\n\n\nBy receiving the audio data created by the motion of body of the users in the Portal, the users in the Science Library were able to interact by adding effects and send the processed audio data back to the Portal. The Science Library program is composed of two parts:\n\n\n  A gesture interaction script written with the programming language Python\n  An audio patch written with the open source visual programming environment Pure Data\n\n\n\n   \n   Gestural Interactions for Multi-parameter Audio Control\n\n\nGestural Interactions for Multi-parameter Audio Control\n\nTo recognize hand gestures, Joachim use the Python library Mediapipe, a simple way to build world-class ML solutions and applications. MediaPipe Hands is a high-fidelity hand and finger tracking solution. It employs machine learning (ML) to infer 21 3D landmarks of a hand from just a single frame. That way, the user can use the index finger to trigger audio effects. Moreover, using Open Sound Control server and client implementations in Python, this makes it possible to send UDP packets from Python to Pure Data, to finally output audio from the soundcard of the computer. The hand gestures are thus used as controller for the multi-parameter audio effects (pitch, reverb, delay and modulation) in Pure Data.\n\n\n   \n   Audio effects in Pure Data\n\n\nCode and instructions for setting up the program can be found here.\n\nPromotion\n\nAs a quick note, with the threat of COVID finally becoming more of a distant memory, we were finally able to promote our concert in more effective ways. This both physical and virtual posters featuring the poster created by Jakob, a Facebook event, and a takeover of the science library’s Instagram page, where we were able to post stories and advertise our concert to the library’s 1000+ followers.\n\nAdditional Information\n\nFor more information on the technical details of this concert, visit our wiki page.\n",
        "url": "/networked-music/2022/05/27/joachipo-telematic-concert.html"
      },
    
      {
        "title": "Emotional Responses to Vibro-tactile Music",
        "author": "\n",
        "excerpt": "What happens when music is felt instead of heard? When music is just vibrations… can it still make people feel emotions?\n",
        "content": "An Exploratory study of Emotional Responses to Vibro-tactile Music\n\n\n   \n   Thesis Word Cloud\n\n\nIf I were to ask you what is the first word that comes to mind when I say MUSIC … what would the answer be? Some might say their favourite genre, band or instrument, others might say sounds, or even feelings. Most of us would agree that music conveys and makes us feel certain emotions. If music is sound, and sounds are essentially mechanical vibrations, then would vibrations make us feel something as well? My thesis project revolved around this question…\n\nThis blogpost presents the motivation behind the study, as well as the aim, and main findings of the pilot study and the main experiment. For more detailed information about the study, please read the manuscript (available on GitHub). Additional materials such as the datasets and the R-scripts can be found in the same repository.\n\nMotivation\n\nThe sense of touch\n\nAs humans, we perceive the world around us through our senses. I like to imagine that, if they would be a group of friends, the visual and auditive senses would be the extroverts who are always in the middle of the action and who need constant (new) stimulation, while the sense of touch would be the quiet introvert who notices everything and who is the glue that holds the group together - just because it is not always central it does not mean it is any less important. If you make an effort, you will realise that you associate certain actions, people, objects with how they feel – if I ask you to imagine that you are in a truck speeding on the highway, the first thing to come to mind might be the roaring noise of the engine and the passing of fields on the window, but close behind will be the feeling of being jilted from one side to the other or up and down (awful suspensions!), perhaps the constant vibration of movement and the heat of the cab (no air conditioning, of course…).\n\nDeaf and hard of hearing individuals\n\nFor each person, the sensory focus is different. For deaf or hard of hearing people, touch becomes an even more important sense that helps them navigate the world around them. It substantially shapes their perception of music, compared to people with unaffected hearing who are used to associating music with sound first. Many studies have used tactile feedback in combination with the auditive to enhance their musical experiences. It is still unclear whether it is a viable solution. Firstly, to hear anything, most deaf or hard of hearing individuals wear hearing aids or cochlear implants to amplify the volume of the auditive stimuli reaching their ears - which distort the sounds to some extent. These also do not help with sound localization, so using them in an installation together with vibro-tactile stimuli might not be a very pleasurable experience for people wearing them. Secondly, an important consideration is whether it is fair to even ask them to wear aids in the first place, to fit into our idea of society – where the lack of auditive feedback can be a major drawback. It might be egotistical to deny them the pleasure of experiencing music, or to expect them to find ways around sound in order to perceive it.\n\nMusic for touch and tactile illusions\n\nThat is why the idea of music designed only for the sense of touch holds a lot of appeal. In order for it to be comparable with music meant for ears in terms of complexity and capabilities to evoke or convey emotions, it would need to use more than just the vibrational feedback that accompanies sounds anyways (since all sound is after all vibration). A distinction is made in this project between the emotions recognized in a musical piece and those emotions evoked in the listener. For example, you might intellectually recognize that a piece of music is sad, but that doesn’t mean that it actually makes you feel sad (for an interesting paper about whether sad music can really make us feel sad see this paper).\n\nA novel approach to music for touch with a lot of potential is the idea of using tactile illusions (i.e., a surprising illusory sensation resulting from the mismatch between actual stimulation and what is perceived) to enhance the vibro-tactile feedback based on an audio signal. By having more than one stimulation point (which is required for an illusion to work), dimensionality is added to the perception of the vibration. Dynamics that are often used in music (tempo, rhythm, melody) can be preserved or translated through a sensation close to panning – when wearing headphones, a stereo signal uses panning to give someone the feeling of the sound travelling from one side to the other; similarly, with two points of tactile stimulation, vibro-tactile feedback can evoke a similar sensation.\n\nThis figure is a visual representation of the tactile illusions known as funneling (retrieved from this paper). When the arm is stimulated in two places under specific circumstances, another, apparent stimulation point is sensed. In this thesis, the actual stimulation points were across the body - one point in each hand - and the apparent stimulation point was in the air, between the two hands (as presented below in the pilot study section).\n\n\n   \n   On the body funneling\n\n\nAlthough it is somewhat intuitive in which ways tactile illusions can enhance dynamicity, it is still unclear how they influence the perception of emotions, or even if they can carry emotional information at all. Thus, this project explored vibro-tactile music in form of tactile illusions and how individuals responded to it from an emotional point of view.\n\nUltimately, this type of music is intended for people suffering from hearing loss, but it is not an exclusive experience reserved to them. If their auditory sense is muffled to aid concentration on the sense of touch, it was considered that individuals with unaffected hearing would respond in a fairly similar manner. Although considered sufficient for this novel exploratory thesis, later studies should focus on learning more about the emotional responses of people suffering from hearing loss in particular.\n\nAim\n\nThe main goal of this study was to explore the emotional content conveyed by vibro-tactile music rendered from musical excerpts used in previous research (Vieillard et al., 2008) to tactile illusions. Empathy, musicianship level and level of hearing loss were considered as individual characteristics which might have an influence on the perception of vibro-tactile (musical) emotions.\n\nMoreover, the best apparatus for relaying clear tactile illusions was tested from a choice of three pairs of actuators (two commercially available and one purpose-built based on literature indications).\n\nFindings\n\nIn this study, three tatile illusions were used when rendering musical excerpts: Phantom Motion or apparent movement (a continuous movement reminiscent of an apparent sweeping motion), Cutaneous Rabbit or saltation (an impulsive movement of short impulses from one side to the other reminiscent of hopping), and Funneling (an error of localization, the illusory feeling of an apparent actuator).\n\nPilot Study\n\nThe first experiment was a pilot study with the goal of determining which pair of actuators conveyed the clearest tactile illusions and which were most comfortable to hold. For this purpose, 10 experts in relevant fields (such as music technology and music cognition) tested three tactile illusions with a pair of commercially available piezoelectric actuators, a pair of commercially available voice-coil actuators named Basslets, and a pair of purpose-built actuators named Hap-phones (see the picture below).\n\n\n   \n   Hap-phones\n\n\nThe majority of the experts decided that the hap-phones were the actuators that conveyed the clearest illusions. Therefore, for the main experiment, this was the pair that was used. Participants tested them both strapped around their wrists and held between their fingertips, and found that holding the actuators between fingertips as seen in the image below was best.\n\n\n   \n   Hap-phones held between fingertips\n\n\nMain Experiment\n\nThe goal of the main experiment was to determine the emotional responses that individuals have to vibro-tactile music. Eight excerpts were rendered to tactile illusions from Vieillard and collegues (2008) (2 happy, 2 sad, 2 scary and 2 peaceful), and were presented in a random order. Following each excerpt, participants answered questions about the emotions they recognized in theexcerpt and the emotions they felt. This was all structured as a questionnaire, and at the end they answered 14 questions as part of the Interpersonal Reactivity Index - used to measure their empathy level (specifically, their fantasy and empathic concern). After completing the questionnaire, they were asked if they experienced any sort of associations during the vibro-tactile music.\n\nIn total, 55 individuals participanted. Only 5 suffered from different levels of hearing loss - and 52% of all were musicians.\n\nOf all participants, 75% experienced musical associations (e.g., EDM, musique concrete, soundtracks), imaginative associations (e.g., memories or life situations like dancing in a night club, skipping down the road, being at the sea side), or both. With one exception, all deaf or hard of hearing participants experienced both types of associations. Musicians experienced more musical associations than non-musicians. Interestingly, music-loving non-musicians experienced more associations overall.\n\nThe most important finding is that yes, music as tactile illusions can make people feel emotions. In almost all cases, the excerpts were considered happy or peaceful, and participants rarely felt sadness or scariness. This suggests that the experience of feeling vibro-tactile music is pleasant overall, which is in line with participants’ feedback. Some even said that it feels like a “massage for the fingertips”. Another interesting finding is that, instead of fine grained emotions, participants felt broaded emotions such as “liveliness”, “mellowness”, “startlement” or “unease”, which suggests that the lack of melodic information has quite a big impact on conveying emotions.\n\nFuture work\n\nThe most obvious suggestion for future studies is to focus on a deaf or hard of hearing population, and even comparing them to individuals with normal hearing. If we want to create technology for them, we need to know more about the way they perceive the world aroud us. It is also important to have bigger or at least more equal samples, to have reliable results of statistical analysis.\n\nThe findings of this study seem to indicate that music as tactile illusions indeed makes people experience emotions, more research is needed to find a consistent correlation between (type of) illusion and emotion. So far, sweeping, continuous tactile illusions seem to be associated with peacefulness, and impulsive, highly dynamic illusions with liveliness. Looking more into this and designing better experiments will also help and provide support for composing vibro-tactile music from scratch - it would then be easier for composers to know which illusions to use to convey certain emotions.\n\nRegarding individual characteristics that might influence the perception and recognition of emotions in vibro-tactile music, empahty did not seem to be extremely relevant. Other characteristics should be considered in order to discover more about why and how certain people feel emotions.\n\nThis thesis could be considered as a stepping stone towards more research combining the fields of musical haptics (music technology) and music cognition, specifically regarding the study of emotions in vibro-tactile music.\n\nContact me for any questions and suggestions!\n",
        "url": "/masters-thesis/2022/05/30/alena-exploration-of-emotional-response-to-vibrotactile-music.html"
      },
    
      {
        "title": "Unsupervised Classification of Sub-Genres of Electronic Music",
        "author": "\n",
        "excerpt": "unsupervised machine learning classification and clustering of sub-genres of electronic music\n",
        "content": "This thesis project is a study of unsupervised machine learning techniques for the classification and clustering of sub-genres of electronic music. New sub-genres of electronic music are frequently introduced and most have similar audio characteristics, having a proper distinction between them is a laborious task. Therefore, it becomes essential to explore tools and techniques that help us differentiate between these genres easily and efficiently. Two approaches suggested by Barreira and Rauber have been employed for the clustering of music. Barreira’s approach uses a model-based clustering technique by employing Expectation-Maximization for Gaussian Mixture Models. Whereas, the Rauber approach uses Growing Hierarchical Self Organizing Maps which is an extension of Self Organizing Maps. Moreover, Low-level audio features that mathematically show characteristics of audio are extracted for feeding into these algorithms.\n\nBackground and motivation:\n\nMusic Information retrieval (MIR) is an interdisciplinary field concerned with the development of innovative approaches to streamline the plethora of digital music and provide easy accessibility by extracting features from music (audio signal or noted music) and by developing different search and retrieval schemes. Given the importance of music in our society, it’s surprising that MIR’s research is still relatively new, having widely started around two decades ago, MIR has however undergone a transition since then. Some of the widely popular applications in music information retrieval are signal analysis, music recommendation, classification of mood and genres/sound/instruments, etc.\nSeveral clustering and classification techniques have been introduced and implemented for the classification of genres of music. Ranging from several approaches using supervised machine learning techniques (Ahmad et al., 2014; Asim &amp; Ahmed, 2017; Bahuleyan, 2018; Tzanetakis &amp; Cook, 2002), etc. In these techniques, the model is trained on input data that has been labeled in correspondence to a particular output. Moreover, some employ semi-supervised classification (Poria et al., 2013; Song &amp; Zhang, 2008) approaches which are a combination of supervised and unsupervised machine learning approaches. However, these techniques necessitate manually tagged data. As a result, these approaches are restricted in their ability to evolve with new data, music, and genres, as manually constructing and updating a big dataset for machine learning models is not always viable.\nThe unsupervised approach is a branch of machine learning in which the model is not given any labeled data to train itself, rather it autonomously works to find patterns and information, and it is not constrained to the requirements of manually tagging the data. However, just a few solutions have been presented, ranging from using hierarchical self-organizing maps(Ahmad et al., 2014), employing the Hidden Markov model (Barreira et al., 2011), or more recently using an unsupervised artificial neural network(Pelchat &amp; Gelowitz, 2020) (Raval, 2021). Moreover, these solutions majorly explore the classification of relatively easily distinguishable genres which are widely popular like pop, hip-hop, rock, classical, etc. The elements that differentiate between them are very distinct from each other and have very less overlapping. The classification of genres that are similar to each other or have similar elements present in them has very minimal exploration, like (Quinto et al., 2017) wherein the classification of subgenres of Jazz music using deep learning techniques has been performed.\nMotivated by the concepts of unsupervised machine learning, audio signal processing, and feature extraction techniques, my research question became: How to efficiently implement unsupervised machine learning techniques that cluster or classify mainstream genres to different sub-genres of electronic music? and subsequently enhance and evaluate them.\n\nSystem Description and Implementation:\n\nThe first step in the project is the audio signal processing step, which is to extract features, these features are related to the properties of the audio sample such as spectral analysis, timbre, loudness, and melody among others. These features when extracted are large in numbers and have a large number of dimensions, therefore a feature reduction technique is required for optimizing computational time, storage space, and redundancy. But before that, a normal standardization is employed after which the features dimensionality reduction technique based on Principal Component Analysis (PCA) is performed. The results are then fed into two separate systems that are being implemented in this thesis. The first system as proposed by (Barreira et al., 2011) uses a model-based approach for the classification of genres. The second system uses psychoacoustic models and self-organizing maps for classification purposes and is proposed by (Rauber et al., 2002).\nThe dataset used in this project is called MTG-Jamendo dataset (Bogdanov et al., 2019). It contains audio for 55,701 full songs, with a duration of at least 30 seconds, is in the MP3 format, and has the quality of 320kbps at the sampling rate of 44.1kHz. The tracks in the dataset are annotated with 692 tags encompassing genres, instrumentation, moods, and themes that have been applied to the music in the dataset. The sub-genres chosen for this project are: “house”, “trance”, “funk” and “minimal”. Now since there are thousands of tracks present in the electronic genre category and a few hundred in each subcategory, it becomes crucial to extract a limited amount of tracks for the testing and training of the algorithm, therefore, to save computational expenses, 100 tracks from each genre are selected randomly. All the tracks are down-sampled to 22,050 Hz and are turned into mono tracks. Finally, the full-length tracks are sliced into a duration of 30 seconds clip is taken for extracting the features and standardization of the duration. After the preprocessing, the next step as discussed above is the feature extraction.\nThe features extracted in the project are classified into two categories, the first one is called computational features, meaning they do not represent any musical information but only describe the mathematical analysis of the. The second category is called perceptual features, these features mathematically represent musical properties based on the human hearing system. To limit the context of this project, only computational features are considered. The features extracted are, spectral centroid, spectral roll-off, spectral flux, Mel frequency cepstral coefficients, root mean square energy, and spectral bandwidth. Moreover, the spectral properties are calculated over each window of the spectrogram and a mean is taken of the calculated values to get one value for the entirety of the audio sample. Moreover, RMS energy values, along with this Mel frequency cepstral coefficients, zero-crossing rate, and power spectral value are also calculated.\n\n\n   \n   Dataset distribution based on audio samples RMS and MFCC values\n\n\n\n   \n   Dataset distribution in 3D based on audio sample’s RMS, ZCR, and MFCC values\n\n\nBarreira’s Model-Based Approach:\n\nThe system as proposed by (Barreira et al., 2011) uses a model-based approach for the classification of genres. In this approach, the clustering method consists of a learning approach that clusters the music samples only based on their audio features without any previous information about the genre of the samples. After extracting the above-mentioned features suggested in the (Barreira et al., 2011) approach to clustering of music, a matrix of the features concerning the audio samples is created followed by standardization and feature reduction techniques. Finally, the clustering is obtained by the Expectation-Maximization with the Gaussian mixture model algorithm.\nThe Expectation-Maximization for Gaussian Mixture Models(EM-GMM) algorithm iteratively evaluates the parameters of a GMM. We start by taking an initial parameter set randomly, then update it by alternating between the two Expectation-Maximization stages. In other words, we first start by randomly choosing some points as the center of the clusters, then for each data point, we calculate the probability of being in each cluster, this is called as Expectation stage. Lastly, using this probability we recalculate the means and variances of the clusters this is the maximization step. We do these iterations until a convergence criterion is met.\n\n\n   \n   System pipeline (Barreira et al., 2011)\n\n\nRauber’s Approach:\n\nThe system described by Rauber (Rauber et al., 2002) uses a widely popular neural network algorithm called Self Organizing Maps (SOM) (Kohonen &amp; Somervuo, 1998), along with its extension called Growing Hierarchical - Self Organizing Maps (GHSOM). It is a model in which by placing related data items next to one another on a map display, the neural network does cluster analysis. The GHSOM, in particular, is capable of recognizing hierarchical correlations in data and so generates a hierarchy of maps depicting distinct musical genres into which the pieces of music are arranged. SOM is used in clustering and mapping (or dimensionality reduction) techniques to map multidimensional data onto lower-dimensional data, making it easier to grasp complicated situations. It essentially does cluster analysis by projecting the data space onto a two-dimensional map space in such a way that related data items on the map are close to each other.\n\n\n   \n   A simple depiction of SOM training and model vector adaptation (Rauber et al., 2002)\n\n\nThe Growing Hierarchical Self-Organizing map (GH-SOM) is based on the usage of a hierarchical structure with numerous levels, each of which includes a collection of separate Self-Organizing Maps (SOMs). At the top of the hierarchy, one SOM is utilized. A SOM might be added to the next tier of the hierarchy for each unit in this map. This approach is replicated with the GHSOM’s third and subsequent levels\n\n\n   \n   Architecture of a GHSOM (Dittenbach et al., 2000)\n\n\nThe figure has one map in layer 1 consisting of 3×2 units and It gives a rough arrangement of the input data’s key clusters. The second layer’s six separate maps provide a more thorough look at the data. To offer adequate input data representation, two units from one of the second-layer maps were further enlarged into third-layer maps. As a result, comparable data can be discovered on surrounding map boundaries in the hierarchy\n\nResults and Future Work:\nThe evaluation is done based on two metrics Silhouette coefficient and the Calisnki-Harabasz coefficient. The silhouette coefficient is defined in the interval [−1, 1] for each example in the data set. When it is close to zero means that the points are uniformly distributed throughout the Euclidian space, negative values indicate that the clusters are mixed and overlapped and positive values indicate high separation between the clusters. Calisnki-Harabasz coefficient is the variance ratio criteria, often known as CH, and is a metric based on the premise that clusters that are compact and well-spaced from one another are desirable clusters. The result obtained by the models can be seen in the figure and the table given below\n\n\n   \n   Clusters generated by Barreira's clustering approach(EM-GMM)\n\n\n\n   \n   Clusters generated by Rauber's clustering approach(SOM)\n\n\n\n   \n   Barreira and Rauber's reult comparison\n\n\nWith limited prior knowledge of machine learning and audio signal processing techniques, the core challenge of the project was to thoroughly understand the algorithms and audio features so that an optimum implementation can be achieved. With only one semester to conduct the research, these difficulties were magnified, resulting in several technical and methodological compromises. Although both the techniques were successful to a large extent in the classification of the given four different types of sub-genres of electronic music. It should be noted that the dataset fed into the system was still quite small and the approaches for future work, need to be tested on larger datasets with more data points as well as a greater number of subgenres of electronic music. Additionally, hyperparameter tuning and optimization are highly required to make the models and algorithms robust and perform even better. Moreover, applying them to sub-genres of other popular genres like Classical, Jazz, Rock, etc can also yield interesting results. Redesigning the approaches so that they can be deployed and migrated in a real-life application should be seen as an aspect of future development. Such that they can be used in distinct Music Information Retrieval concepts like a song retrieval system based on genres, or for building a personalized music recommendation system.\n\nReferences\n\nAhmad, A. N., Sekhar, C., &amp; Yashkar, A. (2014). Music Genre Classification Using Music Information Retrieval and Self Organizing Maps. In M. Pant, K. Deep, A. Nagar, &amp; J. C. Bansal (Eds.), Proceedings of the Third International Conference on Soft Computing for Problem Solving (pp. 625–634). Springer India. https://doi.org/10.1007/978-81-322-1768-8_55\nAsim, M., &amp; Ahmed, Z. (2017). Automatic Music Genres Classification using Machine Learning. International Journal of Advanced Computer Science and Applications, 8(8). https://doi.org/10.14569/IJACSA.2017.080844\nBahuleyan, H. (2018). Music Genre Classification using Machine Learning Techniques. ArXiv:1804.01149 [Cs, Eess]. http://arxiv.org/abs/1804.01149\nBarreira, L., Cavaco, S., &amp; da Silva, J. F. (2011). Unsupervised Music Genre Classification with a Model-Based Approach. In L. Antunes &amp; H. S. Pinto (Eds.), Progress in Artificial Intelligence (pp. 268–281). Springer. https://doi.org/10.1007/978-3-642-24769-9_20\nBogdanov, D., Won, M., Tovstogan, P., Porter, A., &amp; Serra, X. (2019). The MTG-Jamendo dataset for automatic music tagging.\nDittenbach, M., Merkl, D., &amp; Rauber, A. (2000). The growing hierarchical self-organizing map. Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium, 6, 15–19 vol.6. https://doi.org/10.1109/IJCNN.2000.859366\nKohonen, T., &amp; Somervuo, P. (1998). Self-organizing maps of symbol strings. Neurocomputing, 21(1), 19–30. https://doi.org/10.1016/S0925-2312(98)00031-9\nPelchat, N., &amp; Gelowitz, C. M. (2020). Neural Network Music Genre Classification. Canadian Journal of Electrical and Computer Engineering, 43(3), 170–173. https://doi.org/10.1109/CJECE.2020.2970144\nPoria, S., Gelbukh, A., Hussain, A., Bandyopadhyay, S., &amp; Howard, N. (2013). Music Genre Classification: A Semi-supervised Approach. In J. A. Carrasco-Ochoa, J. F. Martínez-Trinidad, J. S. Rodríguez, &amp; G. S. di Baja (Eds.), Pattern Recognition (pp. 254–263). Springer. https://doi.org/10.1007/978-3-642-38989-4_26\nQuinto, R. J. M., Atienza, R. O., &amp; Tiglao, N. M. C. (2017). Jazz music sub-genre classification using deep learning. TENCON 2017 - 2017 IEEE Region 10 Conference, 3111–3116. https://doi.org/10.1109/TENCON.2017.8228396\nRauber, A., Pampalk, E., &amp; Merkl, D. (2002). Using Psycho-Acoustic Models and Self-Organizing Maps to Create a Hierarchical Structuring of Music by Musical Styles.\nRaval, M. (2021). MUSIC GENRE CLASSIFICATION USING NEURAL NETWORKS. International Journal of Advanced Research in Computer Science, 12(5), 12–18. https://doi.org/10.26483/ijarcs.v12i5.6771\nSong, Y., &amp; Zhang, C. (2008). Content-Based Information Fusion for Semi-Supervised Music Genre Classification. IEEE Transactions on Multimedia, 10(1), 145–152. https://doi.org/10.1109/TMM.2007.911305\nTzanetakis, G., &amp; Cook, P. (2002). Musical genre classification of audio signals. IEEE Transactions on Speech and Audio Processing, 10(5), 293–302. https://doi.org/10.1109/TSA.2002.800560\n \n",
        "url": "/masters-thesis/2022/06/02/abhishec-thesis-unsupervised-classification.html"
      },
    
      {
        "title": "RoboCapo: A Digitally Controlled Actuated Capo for Enhanced Guitar Playing ",
        "author": "\n",
        "excerpt": "An augmentation device for the guitar, a robotic capo mechanism that explores the emergence of new complex and meaningful modes of interaction.\n",
        "content": "\n   \n   RoboCapo\n\n\nAbstract\n\nThis thesis presents a novel tool, RoboCapo; an augmentation device for the guitar, a digitally controlled actuated capo mechanism that explores the emergence of new complex and meaningful modes of musical interaction. The device enables the musician to incorporate new techniques of guitar playing, freeing up cognitive bandwidth and promoting human and technological capabilities that enables gestures that may be rarely used, prominently the little finger in the musical endeavor. The concept design and prototype are presented, along with an exploration into its potential applications as a semi-controlled and a fully automated device. The mechanical and electronic latency, and the force applied by the capo which affects the sustain of the vibration of the string are evaluated. Artistically, a set of composed excerpts with the use of the proposed device are presented in the form of video and audio recordings. The RoboCapo in itself is not meant to produce sound via string manipulation, but is seen as a tertiary finger that evokes intimate control of the sound which can be compared as a human augmentation that aids in fretting the strings. Lastly, the thesis opens a discussion on the emergent behaviours exhibiting sound producing elements and expands on other unexpected facets that have been observed in the making.\n\nIntroduction\n\nThe conception of this idea, inspired in its basic form, weaved out of the necessity to be able to incorporate fingering flexibility in the root notes of chord voicings or harmony in general. Thus, leading to design this system and thereby exploring various symbiotic applications and new features, not only as fully automatic system but also as an electronically controlled mechanized motion which affords substantial human involvement in the sound production process.\n\nThe system provides a middle ground for digitally controlled autonomous instruments and passive controllers, a dynamic system where the human performer and the system mutually influence each\nother in a musical context. Such integrated concepts with real, physical motion producing acoustic sounds that exhibit meaningful behaviors are sought to engender rich diverse and fluid modes of interaction in this sparsely researched area.\n\nThe design process was practically motivated, demanding an explorative approach with a functional prototype. this thesis\nexplores a set of design philosophies, and what is considered as appropriate freedom to make artistic choices, the author takes a mixture of technological and artistic approach towards its entirety. [Lim et al., 2008]\n\nResearch Question\n\nMy main research question has been;\n\nHow can we design a controllable actuating capo system that can be used to explore alternative modes of interaction with a guitar?\n\nTo go about answering the question, the aim of the project is to incorporate an actuating capo into a guitar in order to explore alternative methods of leveraging performers’ consciously controlled gestures without hindering pre-learned gestures and sharing some of the control with the machine while still providing proactive and intimate human control of the sound.\n\nImplementation\n\nFor reasons of brevity, I shall present only the final prototype here, and if you are interested in the machine, I recommend to read the whole thesis.\n\nIn the third and final iteration of the prototype, the design was made as an improvement in its weight distribution, mounting mechanism to be integrated within the guitar, a final design of the 3D printed arm and the button housing to be placed on the body of the guitar.\n\nLinear Motion System\n\nExisting techniques for the moving mechanism in various robotic instruments such as MechBass [McVay et al., 2015], GuitarBot [Singer et al., 2004] employed the use of carriage slides on beams often made of aluminium extrusion due its simplicity and speed.\nCommercial solutions were reviewed and avoided due to its high costs and delivery time. Instead, this linear motion system was approached with a customised and off the shelf material. An aluminium extrusion was used as a rail, an aluminium carriage with low friction wheels, a motor at one end, an idler pulley at the other end and a timing belt\ndrive attached to the motor were used.\n\n\n   \n   T-Slot\n\n\nConsidering the force required to move a carriage carrying about\n150grams of weight with an optimal speed and with high precision, the NEMA 17 stepper motor, one among the most common motors used for linear motion drives with variety of torque and speed ratings was chosen and mounted on the plate. A timing belt drive of width 5mm was attached to the motor, the carriage and around the idler pulley. The motor actuated the movement of the carriage. The carriage then provided a base for the capo/ fretting mechanism discussed below.\n\n\n   \n   Stepper motor\n\n\nCapo/Fretting Mechanism\nThe action of the traditional capo mechanism in its basic form must press/push the top strings of the guitar and it must have a fairly equal force on all the types of guitars because each type of guitar neck has different curvatures. These traditional capos have a large amount of force pushing against the fretboard on all the strings. It is known that the force required to fret the strings lies from 1N to 6N depending on the string gauge and the string action [Grimes, 2014]\n\n\n   \n   Fretter: Side view\n\n\nSince the fretting mechanism must deliver this much force, a high-power servo motor MG996r with rated torque of 11kg/cm, equivalent to 107N/cm, tested in the first prototype was chosen. The servo motor was fastened to the carriage and the horn of the servo motor is attached with a 3D printed arm which resembles a sleeve with a fret-pad on a traditional capo. The 3D arm has undergone thorough numerous iterative designs, the first design was a simple rectangular arm with two holes. The final design of the arm has a slight concave curvature resembling a traditional capo sleeve, it also two holes placed at the top of the arm, these holes are used to screw on to the horn and hold the arm tightly to avoid any movement when the large amount of force is applied. A clip like mechanism was designed to enable a fast attach and release of the arm. This was done in foresight of developing a system where the user can avail a set of customized arms for fretting any desired string or number of strings. Though the 3D CAD designs were made, it was decided that was over and beyond the scope of this thesis and shall not pursued.\nRefer appendix for CAD designs.\n\n\n   \n   RoboCapo: Front view\n\n\nInputs\n\nTwo scenarios in the two categories which are the supportive role and robot initiative role as discussed above are taken into account when designing the control mechanism, firstly, as a semi-robotic device, the control mechanism was designed with the intention of the human having total control of the movement of the capo in his/her hands/fingers.\nThe right hand’s gestures on the guitar are confined to a smaller area than the left and as Stone et al. states the dominant hand takes an exploring function [Stone et al., 2013]. It was chosen to place the control system in reach to the right hand. A button system was designed to activate the movements, 7 push buttons in a line are placed in 3D printed case which holds the buttons. The button housing is placed on the body of guitar, below the strings, in reach of the plucking hand (dominant hand), the buttons are such that the centre button is mapped to 0th position, the buttons on the right side of the centre button are +1 and +2 and +3 mapped to the nth number of position accordingly and the buttons on the left are mapped similarly such that they are -1, -2 and -3 accordingly. The frets on the right-hand side of the capo mechanism was chosen arbitrarily to be the positive, making the left side to be negative.\n\n\n   \n   Control Mechanism with buttons\n\n\n\n   \n   Button Housing, CAD\n\n\nTwo C-Clamps were employed to mount the system on the guitar. One C-clamp was placed on the head of the guitar, and the other was placed on the body, adjacent to the neck of the guitar. The rail is positioned on the mounts such that the distance between the neck and the rail is 11mm. This ensures that the capo arm rests on the 5th and 6th string of the guitar and also provides ample amount of space for the hand and thumb to hold and move freely along the neck. Another smaller clamp is used to tighten the rail and the body clamp in place to restrict any minor movements. The microcontroller was programmed and set for testing.\n\n\n   \n   RoboCapo: Front view\n\n\nEvaluation and Discussion\n\nCapo/Fretting Evaluation\n\nTo determine the speed and latency of fretting mechanism from point A to point B on the neck including the rotation of the arm from the fretted position at point A to the fretted position at point B, the video of the motion of the RoboCapo was recorded. The video was recorded at a framerate of 60fps. The 0th time stamp was considered at the moment when the arm lifted itself from the point of contact on the string, and the nth time stamp was considered when the arm pressed down on the string such that the string was touching the fret. A variety of fret potions at varying travel lengths were considered in the test.\nThe test included moving from and to all 9 accessible frets at steps of one, two an three.\n\n• The average time to move one fret forward : 0.354 seconds\n\n• The average time to move one fret backward: 0.333 seconds\n\n• The average time to move two frets forward : 0.477 seconds\n\n• The average time to move two fret backward: 0.46 seconds\n\n• The average time to move three frets forward : 0.568 seconds\n\n• The average time to move three frets backward: 0.574 seconds\n\nAcoustic performance evaluation\n\nFor this test, the audio output of the guitar was recorded while the RoboCapo fretted at stationary position and moving to different positions on the neck while the string was plucked by a plectrum. The audio was recorded at a sample rate of 48kHz, and the sample was determined to have full clarity if the string vibrated for long periods of time and did not consist of a buzz tone, which occurs when the string is not depressed with the required amount of force, causing the string to have an improper contact with the fret markers. The samples were then analyzed by listening and marking the decay times.\nThe test sample size was taken to be 82 The analysis is as follows:\nThe decay time of the notes was found to be in the range of 5.5 seconds to 7.3 seconds.\n\nOut of all the tests, 3 notes resulted in improper fretting, which caused a loud buzz sound and had a decay time of less than 3.5 seconds as indicated in the test recordings. 4 of the recordings had a decay time of less than 5 seconds but did not have the buzz\nsound.\n\nThe results indicate that the error chance rate of the fretting mechanism is then calculated to be 3.65%. And the rate of which a note has a chance to decay in less than 5 seconds is 4.87%.\n\nThe small decay occurs due to sudden changes in the pressure of the fretting arm which be caused by mechanical or electrical faults.\nFrom this, we can conclude that the force or the pressure applied by the fretting mechanism works with high consistency.\n\nApplications\n\nThese proposed applications glimpse into techniques which exploit the enhanced expressivity of the novel system, RoboCapo, which can be added into the instrumentalist’s arsenal of tools for self-expression, these are a number of ideas that have come up from the exploration and this list is not meant to be exhaustive. The performance technique and usage of this system is ripe with potential and has only begun its journey as a musician’s tool.\n\n• Extended Chords\n\n• Extended number of strings\n\n• Barre Chords\n\n• Multiple right-hand techniques\n\n• Slide Guitar\n\n• Partial capo\n\n• Auto Metronome\n\n• Automatic tertiary finger\n\n• Mechanical sound synthesizer\n\n• Scordaturas (alternate tuning)\n\n• Finger tapping alternative\n\n• Atypical; shared musical performance\n\n• Delay\n\nConclussion\n\nRoboCapo, an actuated musical instrument or a physical attachment device for Guitar endowed qualities that proved to provide intuitive and engaging forms of new interactions. As an augmentation that needed to be attached to the guitar, the process of integration and calibration was one of the major challenges, precise measurements were needed such that the RoboCapo fit into the guitar perfectly.\nIn conclusion, I would like to state that the RoboCapo system exhibits the key factors that make an actuated musical instrument effective [Overholt et al., 2011]:\n• Free up human cognitive bandwidth.\n• Enable and even compel the performer to interact with the technology in new ways.\n• Allow the performer to steer external sources of electrical energy by controlling when and how they are applied; such that the performer can physically attend to other aspects of playing.\n• Endows a physical instrument with virtual qualities that are adjustable in real time by a computer, but which are nevertheless tangible, fostering intuitiveness and possible intimate interactions.\n\nDemo Video\n\n\n",
        "url": "/masters-thesis/2022/06/02/lindsay-robocapo.html"
      },
    
      {
        "title": "Norway's First 5G Networked Music Performance",
        "author": "\n",
        "excerpt": "We played Norway’s first 5G networked music performance in collaboration with Telenor Research.\n",
        "content": "\n   \n      \n  \n\n\nWhen it comes to playing over wide area networks such as the internet, the main problem is trying to keep a common rhythm between musicians in different locations. Maintaining a shared beat is difficult if it takes too long for one musician’s sound to reach another’s ears, and if this time (called latency) is too long (around 25ms [1]), it can make playing music together almost impossible. However, some techniques can be employed to work with latency, and not against it.\n\nIn late spring 2022, we were engaged by Telenor Research Group to perform a Network Music Performance (NMP) over their 5G network at the Telenor Festival. Previously, We have had extensive experience with NMP over wired networks but this was the first foray into performing over a 5G cellular network.\n\nTechnical Setup\n\nWe used the Next Generation 5G platform by Telenor Research as an autonomous private network for the NMP. Telenor setted up a transmission with 80 Mhz in the C-Band with a pure standalone 5G network for the occasion.\n\n\n   \n   Next Generation 5G platform by Telenor Research\n\n\nTo connect, we used a pair of Huawei H138-380 CPE Pro 3 5G routers and NMP Portable Kits to facilitate ultra-low-latency music performances between two remote locations. These portable kits are essentially bundles of high-end software, audio/video peripherals and networking tools that can provide the lowest possible latency on audio/video transmissions over the network.\n\nAudio transmission\n\nFor transmitting audio through the network, we narrowed our options to LOLA Low Latency, developed by GARR and Conservatorio G.Tartini in Italy, is a unique audio-visual streaming technology, which enables musicians to play together in real-time and JackTrip, a popular audio transmission application developed by CCRMA at Stanford University (USA). In comparing the test results and requirements of the project, the 5G could not accommodate the buffering requirements (very small packets) to use LOLA, therefore, we had to use JackTrip which can work with larger buffer sizes [2]. We tested different parameters for the buffer and queue sizes and ended up with the following parameters.\n\n\n  \n    Buffer size\n    Queue buffer\n    Latency\n    Quality\n  \n  \n    256\n    4 (default value)\n    125 ms\n    Bad\n  \n  \n    512\n    4 (default value)\n    163 ms\n    Good\n  \n  \n    512\n    5\n    132 ms\n    Good\n  \n\n\nThis relatively large buffer size (512) and queue buffer (5) resulted in 53 milliseconds of latency being added at the receiving end as the audio is buffered before playback, for a total Round Trip Time (RTT) of 132 milliseconds. Regarding the network, we measured a ping delay of 6 ms with an upload bandwidth of 127.44 Mbps against a testing server (Blix Solutions AS). Upload speed was significantly improved over 5G compared to the LAN (average upload speed usually corresponds to 74.96 Mbps for fixed broadband, according to Speedtest). While downloading information is more common, NMP requires data to travel in the opposite direction as well. Playing live music on the network needs fast upload speeds in order to send data to the other musician’s server.\n\nVideo transmission\n\nFor transmitting video through the network, we used OBS Ninja a cutting edge Peer-to-Peer forwarding technology that offers privacy and ultra-low latency. We disabled the audio and added the maximum resolution for the video source.\n\nSetup locations : Mainstage and Villa Haraløkka\n\nSetup at the Mainstage\n\nThe mainstage was at the Telenor HQ itself, a tent was set up next to the 5G on wheels trailer.\n\n\n  NMP Portable Kit\n  A condenser mic for the saxophone (played by Joachim)\n  An SM58 mic to communicate and for announcements\n  A Pair of Genelec 8030a speakers as PA system\n  A Large screen for showing the video received from ‘Villa Haraløkka’\n  A XA series Canon video camera\n\n\n\n   \n   Mainstage\n\n\nSetup at the Villa Haraløkka\n\nThe Villa is located around 300 meters from the mainstage.\n\n\n  NMP Portable Kit\n  Two line in for the Keyboard. (played by Kristian)\n  Two SM58 microphones for communication\n  A pair of Genelec 8020a speakers for monitoring\n  A XA series Canon video camera\n\n\n\n   \n   Kristian performing at Villa Haraløkka\n\nPerformance Reflections\n\nKristian and Joachim decided to play two jazz standards (A Child Is Born written by Thad Jones in 1969, and A Night in Tunisia written by Dizzy Gillespie around 1940–42) with medium tempo as it can tolerate slight deviations in the playing of the melody and can leave some place for the development of the melody between the chords.\n\n\n   \n   \n  \n\n\nFor Kristian, the tempo and style of song had a direct influence on how much latency could be tolerated. Consequently, difficulties in synchronizing timing for up tempo standards were experienced. This has already been studied in a previous blogpost on Mastering Latency.\n\nIn preparation for this NMP, we practiced over the Local Area Network (LAN) at UiO with artificially added latency. For the ballad “A Child Is Born”, it felt quite good to perform even when we had high latency. The more rhythmic “A Night In Tunisia” was a bit harder with high latency, where Kristian had to keep the focus mostly on his own playing so as to not lose the groove.\n\n\n   \n   Network Musical Performance in Telenor Festival 2O22\n\n\nFurthermore, there was not a significant difference in playing over 5G compared to the UiO LAN with added latency from the musicians point of view. The advantageous part of it was that the 5G had higher upload speeds compared to LAN which is mentioned in the technical section.\n\nReferences\n\n[1] Carôt, A., &amp; Werner, C. (2009). Fundamentals and principles of musical telepresence. Journal of Science and Technology of the Arts, 1(1), 26-37. https://doi.org/10.7559/citarj.v1i1.6\n\n[2] Fasciani, S., &amp; Tidermann, A. (2022, April 11). 5G Networked Music Performances - Will It Work?\n",
        "url": "/networked-music/2022/07/05/joachipo-telenor-concert.html"
      },
    
      {
        "title": "Meet Triple A - SMC 2022",
        "author": "\n",
        "excerpt": "Hi! This is SMC-Team Triple A - Read more on who we are and what we do besides hanging around trash bins.\n",
        "content": "Our interests span classical music, film music, human-computer interaction, and much more. We are all musicians who developed a love for technology, and are looking forward to creating all sorts of wild and wacky musical tools over the next two years.\n\n\n\nThe Team\n\n \n\nKristian Eicke\n\n\n   \n\n\nI am a musician and technology enthusiast from Dresden, Germany. I originally started as a guitarist in bands and eventually found myself interested in more technology-driven fields of producing and recording music while finishing my Bachelor in Musicology in 2022. I see myself as a tech humanist and a knowledge hungry musician who is never too shy to dive into new territory. Besides being a student, I take joy in developing new ideas for weird instruments, listening to obscure metal and petting every cat I see on the way.\n\n \n\nChristian Thobroe Anda\n\n\n   \n\n\nChristian Thobroe Anda is a film/tv/game composer (or at least trying to be). He has a bachelor degree in film music from Leeds College of Music, took a “year course” (Årsstudium) in art history, and just finished a postgraduate certificate in education. Christian is a multi-instrumentalist, mainly focusing on guitar and piano\n\nHis interests, and reason for attending SMC, is to learn more about audio interactivity, and how to compose and use music to make people feel. While also feeding his addiction for new and old technologies that can change the ways of composing, interacting and perceiving music.\n\nhttps://soundcloud.com/christian-thobroe-anda\n\n \n\nJack Hardwick\n\n\n   \n\n\nJack is a violinist and computer musician from Mirfield, a small town between Manchester and Leeds in northern England. He’s played the violin for 18 years (a realisation which has made him feel old) and has a BA in Music from Carleton College in Northfield, Minnesota, USA, where he discovered his love for electro-acoustic composition and technologically extended violin performance, as well as film and video art.\n\nAfter spending the last few years working in arts marketing for theatre and opera on both sides of the Atlantic, he’s looking forward to continuing to explore his passion for all things SMC, particularly human-computer interaction and instrument design.\n\nOutside of music he can be found watching Formula 1, thinking he’s Gordon Ramsay while cooking questionable food, or playing strategy and simulation video games.\n\n[https://www.hathuwic.com/] https://www.hathuwic.com/)\n\n",
        "url": "/people/2022/08/26/kristeic_team_blog_post.html"
      },
    
      {
        "title": "Say Hi To Team B - 2022",
        "author": "\n",
        "excerpt": "Team B is made up of three enthusiastic individuals who share a common interest–a love for music and technology.\n",
        "content": "\n   \n   Team B\n\n\n As the first week of SMC came to a close, we had the opportunity to introduce ourselves and get to know our teams, with whom we will work until the end of the semester. Team B is made up of three enthusiastic individuals who share a common interest–a love for music and technology. During our first conversation we established our ground rules as we all agreed that working together can help us perform better. As a result, we promised to facilitate cooperation through our engagement and communication. We hope to improve our overall performance and productivity by providing authenticity and uniqueness. As we are a diverse group of people with very different cultural and professional backgrounds, combining our strengths can help to boost our creativity while also minimizing our individual weaknesses. In this case, the strengths and weaknesses of our team members can complement and enhance one another. We believe that we must work together in harmony to create a positive experience. Furthermore, as a team, we want to encourage specialization and industry knowledge. Therefore, we strive to build and develop competencies that will serve us and those around us for the rest of our lives. Moreover, we acknowledge the responsibilities of working and performing as a group. Each team member is responsible for the team's success. Lastly, we intend to encourage and support one another in order to overcome all future obstacles and chart a clear path to success.\n\n\n   \n   Endri\n\n\nEndri Alickaj\n\n Endri is the new SMC student. Recently, he graduated from NHH with a master's degree in business and now pursuing his musical passion at UIO. Endri has been making music since he was a teenager, and this time he wants to use his skills and knowledge to help society innovate. Currently, he is interested in the psycho-emotional aspects of music and how they affect humans and their well-being. Because technology is advancing at such a rapid pace, Endri believes that we should strive to build a deep neural network capable of producing high-quality music with both harmony and melody that is comparable to music composed by humans. Furthermore, by utilizing technology such as deep learning or motion capture, we can create phenomenal artistic scenes that will entertain and stick with people for a very long time.\n\n\n   \n   Emin\n\n\nEmin Memis\n\n Emin is a creative tech guy who is into creating new things using machines. After studying computer science, learning music and working on research on these, he is ready to combine everything he has to do fantastic things at SMC! His interests range from music, media, motion and visual arts to data science, algorithms and programming. He worked on AI-music and psychology-related research. Whether it was creating classical piano music using AI or seeking answers to behavioral questions about music, these experiences made him want to go deep into music and its complex structure. Besides that, he loves cats, classical music and creating videos and photographs.\n\n\n   \n   Nino\n\n\nNino Jakeli\n\n Nino has a background in jazz and she studied vocal performance at the conservatoire. There, she began leaning towards experimental music production. She is currently interested in collecting audio samples which she cuts, edits and arranges in many different ways. Another dimension she aims to explore during the SMC programme is silence and its impact on music perception. Beyond that, her subject of study incorporates music and machine learning, music psychology and sound exploration. She looks at this programme as an exciting journey for picking up the unique skills and experiences that SMC provides. During her free time, she likes taking ice bath and practicing martial arts.\n",
        "url": "/people/2022/08/26/ahmetem-team-b.html"
      },
    
      {
        "title": "Meet SMC Team C 2022",
        "author": "\n",
        "excerpt": "Hello! We are team C and new around here. Come and know us better!\n",
        "content": "\n    \n    Do not go into the dumpster!\n\n\nWho are we?\n\nThe members of Team C all share the ambition of really getting out of our individual comfort zones as we work our way through the SMC programme! We came together with quite diverse academic and professional backgrounds, but quickly found that there is a lot of common ground between us at various levels. We look forward to collaborating, learning, sharing and hopefully creating a lot of great things together over the coming two years!\n\nHave a look at our individual sections below to learn more about each of us.\n\n\n   \n   Team C Competences\n\nAs a whole, we see Team C as having a set of complimentary skillsets.  While Aysima and Olve have backgrounds in computer science and engineering respectively, Fabian and Alex have backgrounds more centred around music and production.  This complimentary combination of skills has already proved fruitful, with Team C competing and achieving second place in the C2HO opening hackathon.\n\nTeam members\n\nLets hear more about who we are!\n\nAlexander Wastnidge\n\n\n   \n   DJing in Kristiansand\n\n\nMy name is Alexander Wastnidge and my background is in Music Production, Audio Engineering and Education.  As a music producer I have been a signed artist under the moniker “From Beyond” since 2018.  I have also been involved in creating music for picture, AV and other projects.  As an audio engineer I have been a freelance recording, mixing and mastering engineer for music and online content working mostly with independent artists, labels, podcasters and film makers.\n\nI also have seven year’s experience of working in creative education teaching everything from DAW music production, audio engineering and film sound.  The intersections of personal creativity with the wider spheres of technology and society is an area of great interest to me which I look forward to exploring in this course.\n\nFrom Beyond - Soundcloud &amp; \nFrom Beyond - Instagram\n\nAysima Karcaaltincaba\n\n\n   \n   Creating happy noises in Frogner Park\n\n\nHello! I am Aysima and I am a Software Engineer with a bachelor degree in Mathematical Engineering. I graduated ten years ago and have been working on cloud systems as a full stack developer. In the last three years, my role has been developing services for an online meeting platform which focuses on hybrid and remote meeting solutions, therefore I am interested in learning how hybrid and remote setups can work for music performances.\n\nI have been playing accordion and learning different dances for some years. Music and performance arts have been something I tried to pursuit for a long time but I think I lacked dicipline and focus in order to be productive at those two areas.\n\nI know programming but I have no idea about sound, electronics and even music. I am seeing this programme as an oportunity to learn from others and produce solutions which brings programming, hybrid communication and music together.\n\nFabian Stordalen\n\n\n   \n   In action\n\n\nHello! I’m Fabian. I got Bachelor in Musicology from the Aalborg University specializing in music philosophy. My thesis explored the relationship between popular music and nostalgia, looking at different ways the past is used to todays music and how it affects our thoughts and feelings about the future. I have a passion for music production and (old) electronic music which naturally led me to the SMC program. What interests me in particular is getting a deeper understanding of the tools used in production and the interactivity between humans and computers. In my spare time I play bass in the band Ushikawa.\n\nUshikawa - Instagram\nUshikawa - Facebook\n\nOlve Skjeggedal\n\n\n   \n      Enjoying life on stage\n\n\nHi! My name is Olve. For more than 20 years I have been working in the international maritime and offshore energy sector with engineering, business development, project management and corporate leadership, but have now reached a point in life where I want to take a timeout to follow my long-running dream of dedicating myself full-time to music related activities, and see where that may take me!\n\nDespite my relatively long-running corporate career my greatest passion (apart from my children and wife) is music, which I have been doing in one form or another for most of my life. I learned basic skills and music theory in my childhood and early youth by taking lessons and playing the clarinet and classical guitar, and later I moved on to electrical guitar (and eventually also singing) which I have been doing since.\n\nI am Norwegian and live just outside Oslo with my wife, our two teenagers and a labrador retriever. By way of education I have a MSc degree in Engineering from the NTNU technical university in Trondheim, and have also done 1-year full time programme in Music Production and Recording at the University of Stavanger.\n\nThe integrated musical and technological nature of the SMC programme stands for me as a great way to combine my passion for music with my technical education and work experience, and I am really looking forward to it!\n",
        "url": "/people/2022/08/26/aysimab-hello-team-c.html"
      },
    
      {
        "title": "How to do No-Input Mixing in Pure Data",
        "author": "\n",
        "excerpt": "No-Input Mixing in Pure Data\n",
        "content": "What is No-Input Mixing?\n\nNo-Input Mixing is a way of using a traditional mixing board as an instrument which you can play. The technique was developed by japanese experimental composer Toshimaru Nakamura who started experimenting with plugging mixing board outputs into inputs which would then create feedback-loops. These highly unpredictable feedback-loops could then be controlled using the various knobs on the mixer. By using this technique you can easily create quite interesting noises and soundscapes. Take a listen to one of Nakamura’s pieces here.\n\nNo-Input Mixing in Pure Data\nSo what if you wanted to recreate the sound of a no-input mixer by using Pure Data? This can be done by creating an artificial feedback-loop using delays. Here is a simple patch showing how it can be done using white noise as the sound source for the feedback-loop. I am using a wrapper for Pure Data called Plug Data, but everything shown can be recreated in vanilla PD. You can also download the patch [here] If the “Feedback Volume” exceeds 1 then the delay will start creating a feedback-loop and if you change the “Delay Time” then the sound of the feedback will drastically change, especially if the delay time is short. \n   \n\nBy experimenting with different input sources you can easily create different textures. You can for example use an oscilator by using “osc~” or perhaps your computers microphone by using “adc~” You can also combine multiple feedback-loops which I have done in this patch.\n\n   \n\n\nHere I’ve combined two different oscilators and some white noise which are controlled by separate sliders. Then the signals are mixed together and sent to a reverb. Each feedback-loop has it’s own volume slider so you can adjust the volume of each element. The number box above each oscilator changes the frequency produced which can lead to interesting results. If you’re using frequencies which are close to each other it will create a pulsating effect like in the image above.\n\nExperimenting with different effects and where they’re placed in the signal chain can lead to interesting results. You can for example add a chorus effect on one of the oscilators and a phaser to the other one. Or maybe you can replace the oscliators with more advanced forms of synthesizers like an FM or a Granluar synth. Plug Data can also work as a VST which means you can use the patch inside of your DAW, record everything and then further process it.\n\nFinished\nFinally I made a nicer looking GUI for the patch.\n\n\n   \n\n\nFiles\n\nDownload the Pure Data patch here\n",
        "url": "/sound-programming/2022/10/01/fabianst-no-input-mixing-pd.html"
      },
    
      {
        "title": "The impact and importance of network-based musical collaboration (in the post-covid world)",
        "author": "\n",
        "excerpt": "The Covid-19 pandemic offered a unique opportunity (and necessity) to focus on the creative usage (and further development) of the technological tools used for network-based musical collaboration.\n",
        "content": "\n\nCollaborative music-making is a core aspect of human musicality (Small, 1998). Traditionally, collaborative music making has predominantly been face-to-face, synchronous and interactive, with reliance on recording and production technologies for dissemination and archiving. However, the emergence of electroacoustic synthesis and processing, along with interactive digital systems, has brought new technologies to the forefront of music practices. New genres such as electronic music and live coding, which alongside with the rapid development of the network-based telecommunication applications, that have made it much easier to communicate and collaborate efficiently with anyone around the world, have changed the shape of music making and collaborative music creation. The Covid-19 pandemic offered a unique opportunity (and necessity) to focus on the creative usage (and further development) of these technological tools for professional and semi-professional music-makers alike.\n\nTraditional music configurations such as orchestras, chamber ensembles, bands, etc., have been most affected due to the curtailment of coordinated simultaneous synchronous music performance. During the time of lockdown and social distancing, spaces of music production (e.g., rehearsal spaces, studios) and consumption (e.g., venues, nightclubs) had suddenly become unfit for their intended purposes. In their place, alternative approaches to music-making that prioritize accessibility and the use of technologies for remote musical interactions, such as the ubiquitous music making or ubimus community, thrived. Some musicians have also adapted their use of technology or have introduced new technological aspects to their creative and collaborative practice.\n\nMany network musical performance softwares such as LoLa, Jamulus and JamKazam, that enable real-time rehearsing, jamming and performing with musicians at remote locations, overcoming latency, saw a huge growth. They were further developed during the pandemic, in order to provide users with more features such as: having more uncompressed audio channels to allow for full surround sound and support larger groups, such as choirs with as many as 98 members etc. In April 2020 Jamulus was being downloaded two thousand times per day, with the trend increasing. LoLa has been used for live streaming by many individual professional musicians, as well as international concerts, while the famous violist and conductor Pinchas Zukerman described it as “the savior of the profession”. Furthermore, in 2020 JamKazam managed to raise over $100,000 through crowdfunding on GoFundMe.\n\n\n\nNevertheless, although streaming and network-based musical collaboration have contributed towards the survival of live music during the pandemic, the revenue levels from live streaming and Video-on-Demand (VOD) work have not matched the earnings from live performances for most musicians. In the long term however, stakeholders note that it is unlikely that the core value chain of the music industry will change too dramatically. It is therefore expected that the previous operating model whereby artists and labels retain close links to streaming platforms, venue operators and event promoters to distribute music, will remain dominant following the pandemic.\n\nStill, it is undeniable that in the post-covid world we are gifted to use to our advantage and convenience the latest applications and software, alongside with the experience that this period provided us with, as a shortcut to any sort of limitation that we may be faced with because of an inability to get in face-to-face touch with other musicians. It is now more apparent than ever, that we can musically collaborate with everyone from the comfort of our home.\n\nReferences\n\n[1] Cai, C. J., and Terry, M. (2020). “On the making and breaking of social music improvisation during the covid-19 pandemic”, in NFW ‘20 Symposium on the New Future of Work.\n\n[2] Lemaire, E. C. (2020). Extraordinary times call for extraordinary measures: the use of music to communicate public health recommendations against the spread of COVID-19. Can. J. Publ. Health 111, 477–479. doi: 10.17269/s41997-020-00379-2\n\n[3] Parsons, C. (2020). Music and the internet in the age of COVID-19. Early Music 48, 403–405. doi: 10.1093/em/caaa045\n\n[4] Taylor, I. A., Raine, S., and Hamilton, C. (2020). COVID-19 and the UK live music industry: a crisis of spatial materiality. J. Media Art Study Theory 1, 219–241.\n\n[5] “Jamulus - Internet Jam Session Software / Discussion / Jamulus Software: Audio Latency in Windows Vista”. sourceforge.net. 13 June 2006.\n\n[6] Prince, Andy (12 March 2014). “JamKazam Lets Musicians Play Together from Different Locations”.\n\n[7] Rosen, Peter (2020-11-15). “Social Distancing For Musicians: A Quick Guide To Low-Latency Audio”.\n\n[8] “LoLa, Low Latency Audio Visual Streaming System Installation &amp; User’s Manual, Version 2.0.0 (rev.001)”. Conservatorio di musica G. Tartini – Trieste, Italy.\n\n[9] ‘Collaborating in Isolation: Assessing the Effects of the Covid-19 Pandemic on Patterns of Collaborative Behavior Among Working Musicians’. Noah R. Fram, Visda Goudarzi, Hiroko Terasawa and Jonathan Berger. Center for Computer Research in Music and Acoustics, Music Department, Stanford University, Stanford, CA, United States, Department of Audio Arts and Acoustics, Columbia College Chicago, Chicago, IL, United States, Faculty of Library, Information and Media Science, University of Tsukuba, Tsukuba, Japan 2021.\n\n[10] The impact of the COVID-19 pandemic on cultural and creative industries. Marilena Vecco, Martin Clarke, Paul Vroonhof, Eveline de Weerd, Ena Ivkovic, Sofia Minichova, Miriam Nazarejova. Panteia 2022.\n",
        "url": "/networked-music/2022/10/03/the-impact-and-importance-of-network-based-musical-collaboration-in-the-post-covid-world.html"
      },
    
      {
        "title": "Euclidean Rhythms in Pure Data",
        "author": "\n",
        "excerpt": "What are Euclidean Rhythms and how can you program them?\n",
        "content": "The idea first posited by Godfried Toussaint in his paper “The Euclidean Algorithm Generates Traditional Musical Rhythms” for using mathmatics to generate rhythmic elements has been applied to modules in the Eurorack modular synth ecosystem for some time now.  The basic principal is that given a total number of “steps” in a rhythmic sequence and a chosen number of desired events within that sequence, the algorithm will attempt to space those events evenly, thereby creating pleasing rhythmic parts.  This video demonstrates one such module in the Eurorack ecosystem\n\nDrawn in by their balance of immediacy, unpredictability and musicality, I set out to program useable Euclidean Rhythm sequencers using Pure Data.\n\nRhythm Sequencer\n\n\n   \n   First implementation\n\n\nThis first implementation is based on the one found here by Andrew Brown: https://www.youtube.com/watch?v=lCcGeVXHkbE\n\nThis gives user input for number of notes and “rotation” which is an offset control.  You can see the signal flow and the maths at work here.  The output can be the float number or it can be taken from the “bang” to trigger something within a patch.  This works well when connected to percussive sounds.\n\nThe user interface allows for fast input of the available values.  Randomising these values can also yield unexpected but pleasing results.\n\nExpanding this concept further I have also been building sequencers for melodic purposes as well\n\nEucliean Melodies\n\nBy combining several sequencers in parallel, each with their own designated notes, this implementation of euclidean sequencing allows for the generation of melodic parts.\n\n\n   \n   Sequencer Swarms\n\n\nFor monophonic voices, four sequencers is a good number for generating patterns.  Bass lines, lead lines, arpeggios and more are possible in this configuration.  I have experimented with both chromatic and scaled note selections, the latter working especially well for generative/randomised patterns.\n\nFor polyphonic voices, greater numbers of sequencers can work well to create complex parts and swarms of notes.\n\nDo Try This At Home\n\nMy Euclidean Sequencer implementations are available from this Githib repository: https://github.com/ajwast/Euclidean-Sequencers-in-PD\n\nYou are free to inspect, use and adapt them as you see fit.\n\nBibliography/Further Reading\n\n“The Euclidean Algorithm Generates Traditional Musical Rhythms” by Godfried Toussaint\n\nA deeper dive into Euclidean sequencers in Eurorack from mylarmelodies\n\n\n\n",
        "url": "/sound-programming/2022/10/21/alexanj-euclidean-sequencers-pd.html"
      },
    
      {
        "title": "FAM Synthesizer",
        "author": "\n",
        "excerpt": "A simple digital synthesizer with the potential of sounding big, complex and kind of analog. Screenless menudiving included.\n",
        "content": "\n   \n   Final version of FAM.\n\n\nFor my SMC4054 Interactive Music Systems midway project I made a FM/AM synthesizer with analog modulation. FM and AM can be very digital sounding, so I wanted to explore and expand some features in this specific synthesis model. Everything was made in Pure Data.\n\nSound Engine\nIt consists of one carrier oscillator, one FM modulator and one AM modulator which i quitesimole, though the combination of FM and AM in itself opens for some new sonic possibilities as they have different sonic attributes. Ratio is selected from a set of harmonic intervals and modulation amount can be set from 0 to 100 %. To expand on this concept I have added stereo detune for all oscillators, which widens the sound and makes the modulation feel quite different from regular mono signal path modulation. This is also a way to access the non-harmonic ratio parameters for the modulators creating bell and metal like sounds. Lastly there is an ADSR for each of the oscillators and a distortion on the summed output for some analog ish grit.\n\nScreenless Menu\nOne thing is that I don’t know exactly how to integrate a screen into a setup. Another thing is that I in general feel like the screen is distracting me as a user from interacting with the sounds that I sculpt. So I wanted to make a menu system so I could have one encoder control many parameters, but without having to put information on a screen for feedback. The solution for this was using LED lights. At the end I got three buttons with integrated LEDs. Before this I used regular buttons and LEDs seperately. One button for “FM” menu, one for “AM” menu and one for “Amp” menu. Firstly I wanted to detect which button is pressed first to enter one of these three main menus. When the first button is pushed down, then the other buttons will enter the sub menus. The submenu is accessed once a held button is released. The button LEDs are indicating the main menu (FM, AM or Amp) and a RGB LED is indicating which submenu is active.The system is mapped as this:\n\n\n  \n    \n      Button Combination\n      Main Menu\n      Parameter\n      LED\n    \n  \n  \n    \n      B1\n      FM\n      Amount\n      Turquoise\n    \n    \n      B1  + B2\n      FM\n      Ratio\n      Pink\n    \n    \n      B1 + B3\n      FM\n      Detune\n      Yellow\n    \n    \n      B1 + B2 + B3\n      FM\n      ADSR\n      Red\n    \n    \n      B2\n      AM\n      Amount\n      Turquoise\n    \n    \n      B2  + B1\n      AM\n      Ratio\n      Pink\n    \n    \n      B2 + B3\n      AM\n      Detune\n      Yellow\n    \n    \n      B2 + B1 + B3\n      AM\n      ADSR\n      Red\n    \n    \n      B3\n      Amp\n      Cassette Analog FM Amount\n      Turquoise\n    \n    \n      B3  + B1\n      Amp\n      Stereo Modulation Overdrive and Filter\n      Pink\n    \n    \n      B3 + B2\n      Amp\n      Detune\n      Yellow\n    \n    \n      B3 + B1 + B2\n      Amp\n      ADSR\n      Red\n    \n  \n\n\nNote that there is a relationship between which button is pressed first and the main menu.\n\nI found that the gestural control of menu accessing was more intuitive than I expected it to be. Of course this may have to do with the fact that I designed the menu system, so I tested it on a friend and didn’t give any information in advance. At first the menu system wasn’t very understandable, but after a few minutes it seemed like it became clearer that the buttons were directing the encoder. This was at a stage in the design process where the buttons needed to be held down for the encoder to actually send data to the desired parameter. Also I should mention that the encoder was a potentiometer at this stage. Based on the feedback from our half our session I changed from holding buttons to pushing buttons and swapping the potentiometer for an encoder. These were the major feedback points on the instrument, and it really improved the experience of using the menuless button system for accessing parameter control. It benefits on using muscle memory to navigate on the instrument which can be very effective once you spend the time needed to learn the button combinations. One obvious downside is that there is no way to control multiple parameter at once, so it calls for programming a sound to be played.\nOn that note I should mention how expressive the synthesizer is. Both FM and AM modulation amount as well as amp velocity is full range MIDI velocity responsive. This gives the keyboard player a very large dynamic range regarding how hard each note is hit. Almost like playing a mallett. In addition to this I made the velocity impact the release of all envelopes so that hard hit notes have longer release and decay times than the ones played carefully. These mappings afford experimenting with expressive playing styles.\n\n\n  \n    \n  \n  FAM - Audio Demo\n\n\n\n   \n   Schematics for the hardware integration.\n\n\n\n   \n   Solderboard.\n\n\nCurrent state and future potential\nFor further development some sort of feedback on the encoder should be implemented, like a LED ring or strip showing the current value of the selected parameter. Without it the performer is left with listening at the sound and then assuming the current value. An interesting outcome of not knowing the value of every parameter is that the performer could rather spend energy on listening to the sound instead of thinking about the values shown for the parameter. Personally I find that when given information about the technical state of an instrument it can drag my attention away from the actual sound and towards the technical aspects of data and sound synthesis. This matter is highly subjective.\nSoundwise I enjoy the stereo FM and AM modulation. A very simple expansion on an established concept providing quite new sonic results. Musically it creates big textures that are easy to fit into mixes in subtle or aggressive ways. It can create subtle pads that can hide behind other sounds and work in the background to create a stereo room feel without adding reverb, or it can act as lead instrumentation. It naturally creates mallet type sounds of different kinds due to its modulation synthesis basis.\n\nThe analog cassette modulation source immediately gives the sound some dustyness and serves as a very effective way to create nostalgic sounds with a good amount of personality. The benefit of using this modulation instead of sampling is that the modulation can happen globally and not on a note basis, and it excludes the matter of noise accumulation when sampling tones through old analog gear. Another aspect of this that I value is how the magnetic tape serves as a sensor for physical degradation over time when the tape is exposed for different environments like heat, humidity and so on.\nI hope to further develop this instrument so I can test it more in my own work and maybe also have my friends use it to see how others use it.\n",
        "url": "/interactive-music/2022/11/10/henrikhs-FAM-synth.html"
      },
    
      {
        "title": "A Contact Microphone And A Dream: To Loop",
        "author": "\n",
        "excerpt": "Join a humble contact microphone on its quest for freedom!\n",
        "content": "\n   \n   Our Titular Hero\n\n\nChapter 1: Locked Away\n\nA contact microphone leads a lonely existence. While other sensors are free to move around and explore the world, the contact microphone is doomed to stay stationary to fulfill its purpose of playing audio vibrations of objects through contact. This is the story of Pieza, the loneliest little contact microphone, and its dream to break free from its shackles and fulfill its ultimate desire: to become the star of its very own interactive music system.\n\nChapter 2: Piezos Make Plans, God Laughs\n\nPieza’s dream involved being able to live loop its inputs. The plan was to have four different buffers that Pieza could record into using Pure Data in order to create a beat from scratch. Pieza would of course be free roaming in the system, so the user has free reign over what noises they can make with the contact microphone, including tapping and scraping along surfaces. Also, an element of sound manipulation must be present, with the ability to change the reverb amount in order to different different sounds so the end loop has distinct layers that people could pick out.\n\nChapter 3: Teamwork Makes The Dream Work\n\nOf course, any great escape can’t be done without a little help from some friends. Pieza enlisted a crack team of rogue agents, including a rotary potentiometer, a button, and two LEDs to help in its journey towards freedom, each with a special set of skills. The rotary potentiometer will set the amount of reverb placed on the contact microphone’s input. The button will trigger the recording of the contact microphone with a simple press. The LEDs, sometimes known as “the twins,” each serve their own purpose toward the mission. The yellow LED will turn on after pressing the button and stay on for the duration of the recording period to indicate when the user should stop playing. The red LED will flash according to the tempo, which was set to 120 BPM, so the user knows at what speed they should be playing.\n\nChapter 4: To Freedom!\n\nAt midnight on the night of a full moon (ignore the daylight in the video, Hollywood special effects) the team finally assembled to put their plan into action. View the results of their efforts below.\n\n\n  \n    \n  \n  Contact Mic Looper Performance\n\n\nFor the curious, view the blueprints of the escape below.\n\n\n   \n   Contact Mic Looper Circuit Diagram\n\n\nChapter 5: And They Lived Happily Ever After\n\nAnd so Pieza and the Gang celebrated through the night on their successful getaway. Pieza went on to have a long and luxurious life in the music business like it always dreamed, but tragically passed away after the circuit was dismantled by a cruel SMC student. What have we learned from this tale? No matter what your situation is, no matter how small you feel, no matter how hopeless the situation is, you too can waste your life writing about anthropomorphic piezo discs overcome the odds and become the hero in your own story.\n\nFiles\n\nYou can view the Pure Data file for this project here.\n\n",
        "url": "/interactive-music/2022/11/11/josephcl-contact-mic-looper.html"
      },
    
      {
        "title": "Dråpen, worlds largest interactive music system? ",
        "author": "\n",
        "excerpt": "This may be the largest interactive music system you have heard about\n",
        "content": "\n   \n      \n  \n\n\nSewer is not something that you often relate to art installations. But at Bekkelaget Renseanlegg this is the case. Oslo has a growing population, and in 2001 when Oslo’s newest sewage treatment plant was under construction some of the funds for the project was allocated for an art installation. However, art in physical form as images, sculptures or light-installations was not in mind. Arne Nordheim, one of Norways first contemporary electronic music composers was commissioned to ornament this «shitty» location with an installation. It was called «Dråpen» (The Droplet).\n\nThe plant is located on the east side of Oslofjord inside a mountain. Large halls treat sewage water and releases it into the fjord. It is a smelly site dimensioned for 270 000 peoples waste. The piece consists of concrete and diffuse sounds exited by real time properties of the treatment plants operation. You can argue that this IMS is controlled by the most amount of people. Of course weather also factors in with runoff water.\n\nThe system is controlled by the amount of fluid that runs trough the system. The selection of audio, consisting of concrete and obscure sounds is selected on the basis of control data from the plant of which part of the processing it is in. The concrete sounds consist of children crying, polish burial orchestra, airport sounds, gunshots and more. 32 speakers, 2500 meters of cabling and 12 reverb processors can reorganise the audio in 500 million ways based on the sensors of the plant.\n\n\n   \n   Arne Nordhiem in 1968. Source: Trondheim byarkiv, The Municipal Archives of Trondheim\n\n\nNordheim said «The objective is to tell the visitors of the places function, and let the elements of the treatment process choose the spatialisation of the audio. I can imagine that the droplet-inspired reverberations will tell something about the massive volumes [of the halls] that live their unknown life under Ekeberg-hill. The reverberations will give the visitors an idea of distanse and the material surroundings of stone and water»\n\nNordheim claimed that the piece was most reverberant after the morning showers and evening news. This is disputed, since the latency  from the source of sewer to the plant is quite long.\n\n\n   \n      \n  \n\n\nIn 2019 I made a small radio documentary about the site, not knowing about Dråpen. You can listen to it in Norwegian at this link.\n\nIf you want to check it out, look for public viewings either under Ultima Contemporary Music Festival or other happenings. Otherwise, viewings can be organised privately in groups by contacting the plant here.\n\nArt in public spaces - Dråpen\nUltima Festival 2017 - Dråpen\n",
        "url": "/interactive-music/2022/11/13/jakobhoydal-draapen.html"
      },
    
      {
        "title": "Rehearsing Music Over the Network",
        "author": "\n",
        "excerpt": "My experiences with rehearsing music telematically and some tips.\n",
        "content": "Rehearsing Over the Net with LoLa\n\nAs a part of the SMC4024 course we were asked to set up and organize a telematic concert. This means playing together in separate spaces over the internet. For this we used a software called LoLa running on specialized computer racks. LoLa is made for low-latency real time transfer of both audio and video which makes it an ideal software for telematic performances.\n\nOur Concert\n\nOur concert was a part of the Embodied Perspectives on Musical AI workshop held at UiO where we were asked to put on a 20-minute performance. The performance was held between the UiO Science Library and the RITMO research center and consisted of two parts. First part consisted of two cover songs while the second part was a semi-improvised electroacoustic piece. For this concert I played bass and the No-Input Mixer which I’ve written a post about earlier\n\nIf you want to watch a video of the performance it can be found here\n\n\n   \n   Concert Setup used at RITMO\n\n\nThe Virtual Room\n\nOne thing I quickly noticed while rehearsing using LoLa was how disconnected you initially feel from the other performers. This feeling can be avoided by moving the cameras closer to the performers. This also helps with visual cues which can be a lot harder to pick up on unless they’ve been explained beforehand. This means that all movements have to be exaggerated, at least during the beginning of rehearsals. Since our rehearsals were done on the stage there was quite a bit of distance between the performers and the cameras. This made communication while playing difficult at times. Especially compared to our earlier rehearsals where we all practiced together in a small room where we could easily communicate with each other.\n\nTips for Listening to Each Other\n\nThe listening situation is also quite interesting while using LoLa. When rehearsing for our concert we were using headsets to listen to each other. Although this gave us better control over the overall level, I did feel like I lost the sense of being located in the same space as the other performers. Using speakers instead of headphones is one way of bringing back the feeling of being situated in the same space, but be careful since it could easily create a feedback loop if you’re using microphones. Since us performers in RITMO were using headphones we became a lot more aware of not only our own playing but also the nuances of the other performers. This made it easier for me to adjust my playing to what the others were doing, so even though verbal and visual communication was harder. I found communicating musically to be easier.\n\nFinal Thoughts\n\nOverall I found both rehearsing and playing telematically to be an interesting and enjoyable experience. As someone with lots of experience playing in different groups and bands I can clearly see the potential of this technology. Although some tweaking and experimenting is required if you want to recreate the feeling of playing together in the same room.\n",
        "url": "/networked-music/2022/11/21/fabianst-telematic-rehersal.html"
      },
    
      {
        "title": "Audio Engineering for Network Music Performances",
        "author": "\n",
        "excerpt": "How much more difficult could it POSSIBLY be?\n",
        "content": "From hybrid online conferences to network music performances, the job of the sound technician is growing in complexity.  Here we will take a look at the added considerations for the FOH engineer working with network music performances (NMPs).\n\nHow much more difficult could it POSSIBLY be?\n\nIf you’re already a competant sound engineer who’s comfortable in front of the board you might wonder what the fuss is about.  Is it not just more of the same?  If you already know your high shelves from your aux sends, then the basic principals will still serve you well, however, expanding your live sound across a network connection will throw up a few key complications, some of which are covered here. In these examples we will assume a connections between two locations.\n\nDoubling the Setup\n\nA NMP essentially means you are working across more than one location.  Firstly, your “front of house” where your audience and the main focus of your performers will be located.  Secondly, whatever other location (or locations) your remote performers are performing from.  With two locations this inevitably means two sets of audio equipment for audio routing, analogue/digital conversion, networking etc.\n\nThis represents a clear addition to the workload in terms of sheer quantity, a longer load-in and setup across two locations.  Take this into consideration when planning.\n\nDouble the Sound Check AND Fold Back\n\n\n   \n   NMP FOH\n\n\nThe diagram above illustrates the additional layers of complexity for the FOH engineer. Notably that as well as bringing in sound sources “locally” (ie in the same location as you), you will also need to consider the sound sources you receive from the network.  Will you receive a submix of the remote sources or will they be separated? Do you have enough channels on the mixer? Remember to label your channel strips meaningfully.\n\nYou will then need to take into consideration the balance of local and remote sources when creating your FOH and monitor mixes.  Speaking of monitoring, you will have to create an additional monitor mix of your local sources to send to the remote musicians for monitoring.  This added complexity will extend your sound checks significantly.\n\nAs well as testing the sound, you will also need to establish and test the network connection and receiving of the remote audio.  Ideally you will have a separate technician handling network connectivity but it will still need to be factored in to sound checks.\n\nCommunication is Key\n\nTalkback microphones are easily overlooked but absolutely vital when setting up for NMPs.  Lines of talkback communication will be necessary across several situations:\n\n\n  Local and remote musicians will need to communicate with each other via monitors, isolated from the FOH.\n  Likewise, local and remote audio engineers will need to communicate with each other, also isolated from the FOH and possibily also the performers.\n  Musicians may need to communicate with remote engineers to change monitor mixes or identify issues.\n  Remote musicians and technicians need to be cued for the beginnings and ends of performances\n\n\nAnd a Whole Host More\n\nThese were just some of the most immediate additional complexities for audio engineering a NMP.  Hopefully this has helped you begin thinking about the technical considerations of such a performance.  This is just the beginning, though.  Below are some further complications to take into consideration when planning your own network music performance.\n\n\n  Making sure you pay attention to the remote musicians when mixing (they are easy to deprioritise)\n  Having a desk with enough I/O to accommodate the added complexity\n  Signal noise/ground hum on signals received remotely\n  Gain staging local and remote signals\n  Network connection stability and audio fidelity\n  Video monitoring\n  Staging/stage plotting\n\n",
        "url": "/networked-music/2022/11/24/alexanjw-audio-engineering-nmps.html"
      },
    
      {
        "title": "Popsenteret's Music Producer Experience",
        "author": "\n",
        "excerpt": "The top 4 components of a physical computing music production station - you’ll never guess what #3 is!\n",
        "content": "\n   \n   Music producers, old and new\n\n\nIntroduction\n\nThe term “music production” means many different things to different people. For some, it conjures an image of a person talking a band through the process of the album they’re recording. Others might imagine a teenager hunched over a computer working on an electronic track. But what are the commonalities that unite these figures and how can this be present interactively to the public? This is the question we were tasked with answering by Popsenteret, the museum for Norwegian popular music.\n\nFUZZ\n\nOne of the museum’s interactive experiences is the FUZZ exhibit. This presents various musical instruments to visitors, offering an overview of how they function and showcasing how to recreate several sounds. Currently there are four different stations which present the guitar, bass, drum-kit, and synthesiser.\n\n\n  \n   Popsenteret's FUZZ exhibition\n\n\nOur job was to create a fifth station: music production. Since there aren’t instrument commonly associated with music production, we instead explored creative nature of music production.\n\nOur Station\n\n\n   \n   Our final solution for the FUZZ music producer station\n\n\nOur station is intended to be a collaborative experience, operated by four visitors simultaneously. Each visitor controls a separate section, and together they produce music.\n\nStation Sections\n\nSequencer\n\n\n   \n   The station's sequencer section, which uses computer vision to detect if a hole is covered\n\n\nA sequencer allows users to create a musical pattern, typically rhythmic or melodic. While sequencers are usually presented on a computer screen or small electronic device, we decided to go big.\n\n\n   \n   Other common sequencers available on the market\n\n\nEach of the rows represents a separate instrument, and each column represents a step in time. The sequencer loops through the steps and plays back a sound if the instrument is active.\n\n\n   \n   The sequencer in action!\n\n\nBut the sequencer needs some sounds to play, and this is where the next two stations come into play.\n\nDrum Pad\n\n\n   \n   The drum pad section of the station\n\n\nEach drum pad represents a different drum sound, which are played back by the bottom four rows of the sequencer. When a drum pad is hit, its sound changes. A hard hit makes it sound louder and reverberant, a soft hit makes it quiet and clean.\n\n\n  \n    \n  \n  An example of an unprocessed drum sound, played when the drum pad is hit softly\n\n\n\n  \n    \n  \n  An example of a processed and reverberated drum sound, played when the drum pad is hit hard\n\n\nThe visitor can also play along to the rhythmic sequence in real time.\n\nDeconstructed Guitar\n\n\n   \n   The deconstructed guitar section of the station\n\n\nThe noises from the guitar are sampled played back by the fifth row of the sequencer. We decided to present the guitar slightly differently by deconstructing it into three separate components: the neck, strings, and body.\n\n\n   \n   Left: The guitar neck\n   Middle: The guitar strings, which consist of a bass string and electric guitar string, both sharing the same input, on a custom built body\n   Right: The guitar body with a kick pedal next to it to encourage users to hit it\n\n\nInstead of playing the guitar as normal, the user can interact with these components by scraping along the neck, plucking a string, hitting the body with a kick drum pedal, or any other way that they can imagine!\n\n\n   \n   Interaction with the guitar neck\n\n\n\n  \n    \n  \n  Sound of the guitar neck\n\n\n\n   \n   Interaction with the guitar strings\n\n\n\n  \n    \n  \n  Sound of the guitar strings\n\n\n\n   \n   Interaction with the guitar body\n\n\n\n  \n    \n  \n  Sound of the guitar body\n\n\nMIDI Keyboard\n\n\n   \n   The MIDI keyboard section of the station\n\n\nThe visitor at the keyboard section does something a little different. On top of the rhythmic sequencer the station also has a melodic sequencer. This is represented by a row of eight blocks on the screen.\n\n\n   \n   The interactive melodic sequence view, which is featured on the station's center touch screen in front of the MIDI keyboard section\n\n\nEach block represents the same step in time as the rhythmic sequencer, and the position of the block determines the pitch of the note in the melody. The keyboard player creates this sequence on the touchscreen. Then then can press a keyboard key and play back the sequence in the specified key.\n\n\n   \n   Interaction with the MIDI keyboard\n\n\n\n  \n    \n  \n  Sound of the MIDI keyboard\n\n\nGenres\n\nBefore they start using the station, the visitors pick a genre that they want to recreate. The selected genre determines the sound of the drums and the synthesizer controlled by the two sequencers. It also turns several lights above holes on the rhythmic sequencer red. These holes represent a common rhythmic pattern for the genre.\n\n\n   \n   The genre selection screen, which is featured on the station's center touch screen facing the deconstructed guitar section\n\n\n\n  \n    \n  \n  Sound of the \"Pop\" genre snare\n\n\n\n  \n    \n  \n  Sound of the \"House\" genre snare\n\n\n\n  \n    \n  \n  Sound of the \"Pop\" genre synth\n\n\n\n  \n    \n  \n  Sound of the \"House\" genre synth\n\n\n\n   \n   An example of the genre sequence being filled in\n\n\nEffects\n\n\n   \n   The effects grid view, which is featured on the station's center touch screen facing the drum pad section\n\n\nBy moving the Popsenteret logo around the above grid, the visitors can control two effects. The X-axis controls the pitch of the drums, and the Y-axis controls an echo effect.\n\n\n   \n   An example of the effects grid being used\n\n\n\n  \n    \n  \n  Sound of the effects being used\n\n\nVideo Demonstration\n\n\n  \n    \n  \n\n\nFiles\n\nIf you want to build your own interactive museum exhibit, our design documents and code can be found here.\n\nImage Sources\n\nhttps://upload.wikimedia.org/wikipedia/commons/2/2e/Engineer_at_audio_console_at_Danish_Broadcasting_Corporation.png\n\nhttps://upload.wikimedia.org/wikipedia/commons/7/7c/Beatles_and_George_Martin_in_studio_1966.JPG\n\nhttps://in2english.net/wp-content/uploads/2019/03/marshmellow-electronic-music-producer.jpg\n\nhttps://s3-ap-northeast-1.amazonaws.com/miroc-web-corporate-uploads/wp-content/uploads/2019/05/11133012/20190510SB191DAY_24.jpg\n\nhttps://support.apple.com/library/content/dam/edam/applecare/images/en_US/proapps/logic/logic-pro-x-step-sequencer.jpg\n\n",
        "url": "/applied-project/2022/11/25/josephcl-popsenteret.html"
      },
    
      {
        "title": "Zeusaphone - The singing Tesla coil",
        "author": "\n",
        "excerpt": "Have you ever seen choreographed lightning?\n",
        "content": "\n   \n      \n  Daft Punk's Derezzed performed with musical Tesla coils.\n\n\nYes, you can make music with lightning. The wonderful singing Tesla coil known as Zeusaphone (sometimes called a Thoramin), is a form of plasma speaker with one-to-one mapping. It is a variety of a solid-state Tesla coil that has been modified to produce musical tones by modulating its spark output. The resulting pitch is a low-fidelity square wave like sound reminiscent of an analog synthesizer.\n\n\n   \n   The Zeusaphone in action!\n\n\nThe high-frequency signal acts in effect as a carrier wave; its frequency is significantly above human-audible sound frequencies, so that digital modulation can reproduce a recognizable pitch. The musical tone results directly from the passage of the spark through the air. Because solid-state coil drivers are limited to “on-off” modulation, the sound produced consists of square-like waveforms rather than sinusoidal (though simple chords are possible).\n\nWhile early versions of the Zeusaphone generally used zero crossing threshold detectors as a method of producing music through their spark output (an example of this circuit was released by ctc-labs.de in 2009), Scott Coppersmith was the first person to design a complete MIDI-based Tesla coil system. Since then, the systems work by means of a microcontroller that is programmed to interpret MIDI data and output a corresponding pulse-width modulation (PWM) signal. This PWM signal is coupled to the Tesla coil through a fiber optic cable, and controls when the Tesla coil turns on and off.\n\n\n   \n   live performance in Cyber Rodeo at Giga Texas\n\n\nThe term “singing Tesla coil” was coined by David Nunez, the coordinator of the Austin, Texas chapter of Dorkbot, while describing a musical Tesla coil presentation by Joe DiPrima and Oliver Greaves during DorkBot’s 2007 SXSW event. The term was then made popular by a CNET article describing the event. They had been doing public performances with the technology since March 2006. Shortly after that, DiPrima named their performance group “ArcAttack” and became the first musical group to ever use this technology in live performance.\n\n\n   \n   ArcAttack performing live with singing Tesla coils\n\n\nThe name zeusaphone was coined after a public demonstration of the device on June 9, 2007, at DucKon 16, a science fiction convention in Naperville, Illinois. The performance was by Steve Ward, an electrical engineering student at the University of Illinois at Urbana–Champaign, who designed and built the Tesla coil he used. Since the invention of the Tesla coil by Nikola Tesla in 1891, never had a Tesla coil product achieved such an incredible level of precision and control as to be able to amplify music through lightning-like arcs exceeding one million volts of electrical potential. Thus, the revolution in Tesla coil technology began with the Zeusaphone.\n\nThe term “Zeusaphone” was conceived by Barry Gehm, of Lyon College, on June 19, 2007, in a conversation with his friend Bill Higgins. It is a play on the name of the sousaphone, giving homage instead to Zeus, ancient Greek god of lightning. The name was adopted by Ward on June 21, 2007. The alternative name “Thoremin” was suggested by Dan Butler-Ehle; it is a wordplay on “theremin” incorporating the name of Thor, the god of thunder in Norse mythology.\n\nBelieve it or not you can actually buy one of these things and there are several models to choose from, with the largest model ZFX-160 standing at a wapping 7 ½ feet (around 2.30 meters) tall, generating 1.5 million volts of raw throbbing musical electricity. The prices for all models go from around $2,000 up to $8,500.\n\n\n   \n   2022 Zeusaphone ZFX-160 MK IV\n\n",
        "url": "/interactive-music/2022/11/26/iosifaragiannis-zeusaphone.html"
      },
    
      {
        "title": "Exploration of 5G networks for Networked Musical Performances",
        "author": "\n",
        "excerpt": "A latency optimization methodology for NMP.\n",
        "content": "Introduction\nThe objective we had when carrying out this Network Music Performance (NMP) was to test the latency of Telenor’s 5G network so as to find out whether currently musicians could use 5G to execute this type of performance. In order to do that, we first needed to optimize the latency with the factors under our control: finding the best configuration that would lower or just not add latency. We tested different networks, settings and distances to prepare. Finally, we travelled to Telenor Fornebu and, more specifically, to their magnificent villa, did the final tests and experimented with our music performance.\n\n\n   \n   Telenor Villa Hareløkken\n\n\nTelenor 5G Experimental\nAs a concluding part of our test phase, we spent the two days at the Telenor Testbed doing measurements on the experimental 5G network as well as preparing for the final NMP. We were eager to test the possibilities (and limits) of a closed-off version of the 5G network, in which you have all the network bandwidth you could ever wish for available for yourself. Sadly, the experimental network was not yet stable enough for a NMP, as the connection lasted about 40 seconds before disappearing for 20 seconds. We hope to be invited back when it’s up and running!\n\n\n   \n   Testing Telenor 5G Experimental network\n\n\nLatency\n\nWe conducted a total 144 audio test over different network configurations. The main network configurations were:\n\n  Ethernet\n  WiFi (LAN)\n  Commercial 5G\n  Telenor 5G experimental\n\n\nFor each network configuration we tested three different sample rates (44kHz, 48kHz and 96kHz), with different buffer sizes (64, 128, 256, 512, 1024). The latency was measured in RoundTrip Time (RTT). \nThe values obtained from testing with ethernet was considered as our baseline, which enables us to show just how much latency each different network configuration and setup adds.\n\n\n   \n   Added latency compared to ethernet baseline\n\n\nThe commercial 5G causes latency way above what is considered acceptable for a NMP, while the Telenor 5G Experimental delivers promising results in terms of latency.\n\nExample audio quality with packet loss\n\nBesides of latency, the quality of the audio can be affected by the network transmission. The following examples are taken from 5G commercial network tests between Oslo and Svalbard. When packet loss affects the audio quality to this extent latency itself is not the only problem. Notice how it improves as the buffer size increases.\n\nBuffer size 64\n\n\n\nBuffer size 128\n\n\n\nBuffer size 256\n\n\n\nNetwork Music Performance over 5G\n\nOn basis of the test results with the Telenor’s commercial and experimental 5G, the commercial network was chosen for the Networked Music Performance (NMP) even though latency was significantly higher, due to dropout problems with the experimental network.\n\nWe used a Master Slave Approach (MSA) since the latency was greater than 25ms. This approach is a compromise for conventional rhythmical music, where one performer (Master) ignores the return signal from the other performer (Slave), as illustrated in the figure below. It can be hard for for the master and slave to be in musical sync, since the master has such a high latency back to them. It is also important that the master can perform independent form the slave.\n\n\n   \n   Master-Slave approach\n\n\nClicks and pops were audible but can be acceptable when practicing or doing a small showcase. It is not pleasant to listen to a concert with constant popping. This ocurred due to the low buffersize in JackTrip. A higher buffersize will increase latency, but improve quality. Nonetheless, latency could have been further lowered on the experimental network at 48kHz sample rate, 128 bffer size.\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\nConclusion\n\nThe next generation of mobile communication - 5G can be used for network music performances. But the commercial standard today lacks the required low latency needed for communication without applying Master Slave Approaches or Latency Accepting Approaches. Telenor’s experimental 5G network performs much better than the commercial network in terms of latency and jitter, but it is too unstable to be used for an extended time.\n\nIt is clear that the experimental network has advantages in comparison to the commercial network. A comparison between Oslo-Oslo vs. Oslo-Svalbard shows that it is the RANs contributes the most to the latency in the total system, and not the transport layer of the 5G network.\n\n\n   \n   The team. From left to right: Kristian Wentzel, Joachim Poutaraud, Jakob Høydal, Arvid Falch,  Sofía González\n\n\nAdditional ressources\nSource code for streaming audio over UDP and monitoring latency is available here: https://github.com/joachimpoutaraud/udp-audio-stream-latency.\n\n\n\n\n\n\n\n",
        "url": "/applied-project/2022/11/27/joachipo-telenor-5g-report.html"
      },
    
      {
        "title": "Performing & Recording with JackTrip",
        "author": "\n",
        "excerpt": "Performing and recording network music at home? Thanks to JackTrip, now you can do it too.\n",
        "content": "What is a Network Musical Performance?\n\nA network musical performance or NMP is a musical performance that happens over a network, be it a private network or a public one like the internet. In SMC we spend a lot of time experimenting with various software for performing and rehearsing network music. Two of our most used solutions are LOLA and JackTrip. First let’s briefly compare them, and then we will dive deeper into JackTrip.\n\nLOLA and JackTrip\n\nIn SMC we most commonly use LOLA for our network performances. The advantages of LOLA are many – with an ideal network, such as the LOLA network we use at UiO, we can stream uncompressed audio and video between two to three locations. This is just what we need for these concerts in which high production values are essential.\n\nHowever, LOLA requires specific cameras and audio interfaces as well as powerful PCs to perform optimally, meaning it is mostly limited to performance scenarios over a dedicated network. What if we want to rehearse a larger ensemble across the internet on the go? What if we can do without low-latency video? Step forward JackTrip.\n\nUnlike LOLA, JackTrip does not support video, but neither does it require specific hardware. It is available for Windows, Mac and Linux and can support a far higher number of participants. Testing has shown that it can support up to 500 participants connecting to a single JackTrip server. Check out this article for more details on that experiment. As with LOLA, a limiting factor will always be the network itself. Any client connecting to JackTrip with a poor connection will both send and receive with latency.\n\n\n   \n   The JackTrip user interface\n\n\nNow let’s take a look at some of the performance and recording possibilities afforded to us by JackTrip.\n\nPerforming with JackTrip\n\nDuring the recent COVID lockdowns, many ensembles tried their hand at remote performances and recordings. Anyone who attempted this through Zoom will be intimately familiar with the chaos that can ensue due to latency. However, by combining video through Zoom (other videoconferencing solutions are available) with audio through JackTrip, we can maintain the visual element of a concert while also opening the door to low-latency audio, allowing for real-time performances despite physical separation.\n\nTake a look at this JackTrip performance featuring almost 100 performers:\n\n\n\nRecording with JackTrip\n\nWe can take advantage of internal audio routing within our laptops to record high-quality audio received over the network into a DAW. In Windows, JackTrip provides a handy utility called JackRouter, a virtual audio device which we can use to send and receive audio between the JackTrip and a DAW. It appears as a send and receive location in the JackTrip patch bay, at which point one can simply send channels to it to receive them in a DAW using JackRouter as its audio device.\n\nIn our own testing with JackTrip in SMC4024, my fellow SMC student Kristian and I recorded two channels, one local and one from the network, while also sending out a drum track for us over which we would both improvise. We routed my microphone connected via XLR into an audio interface and Kristian’s synth received over the network into JackTrip and out into Ableton via the JackRouter receive patch point. There I recorded both of our instruments directly to Ableton audio tracks. Both of these received channels plus the pre-recorded drum machine track were then all routed back into JackTrip through the JackRouter receive. This allowed us to monitor ourselves and the drums with negligible latency, while also recording the whole session.\n\n\n   \n   The JackTrip routing for the above setup. Note both live audio sources being sent into Ableton and out again before going to our headphones.\n\n\nSee the result of our experiment in the video below:\n\n\n  \n    \n  \n  JackTrip performance over the LOLA network\n\n\nWe recorded the video separately but simultaneously using our laptop webcams because we were unable to access the internet, and therefore Zoom, while also being connected to the LOLA network. While is this not an issue if you are using JackTrip over the internet, in our case it meant we could not communicate visually while recording. As a result it did dampen the feeling of a live performance somewhat.\n\nWe found JackRouter to be the easiest solution for internal audio routing on Windows when using JackTrip. JackRouter is unfortunately Windows-only for now, but the above can also be replicated on other operating systems using audio routing utilities such as BlackHole on Mac.\n\nConclusion\n\nJackTrip opens up a wealth of possibilities for network music performance and recording between many participants. I highly recommend you give it a try!\n",
        "url": "/networked-music/2022/11/27/jackeh-performing-recording-jacktrip.html"
      },
    
      {
        "title": "Music Mode for Online Meetings",
        "author": "\n",
        "excerpt": "It is possible to use popular meeting products for music!\n",
        "content": "What is music mode?\n\nOnline meetings and hybrid platforms have become a mandatory tool in our daily lives. Many use those platforms for educational or business purposes but from healthcare to art, hybrid communication is everywhere; but what about music? Music mode is a feature which many commercial meeting products supports. It enables users to have music performance over network with the audio at highest quality.\n\n\n   \n   Performing music in an online meting, photo by Omar Prestwich\n\n\nEven though you dont care about the latency, you may still face many problems when performing music in an online meeting. Some sounds may not be heard or heard differently. I personally have been taking accordion classes during the pandemic and I can briefly say that it was a very bad experience to have to change the octave of the sound or the tone all the time in order to keep things “perceivable”.\n\nThere are many reasons behind these problems and I will try to go through the reasons for them while telling about how music mode feature solve these problems. My knowledge about the topic is based on what I learned from music mode announcement of commercial software products and from our talk with Bjørn Winsvold, a Principal Engineer working on music mode in Cisco Webex Room Devices. Lots of thanks to him for giving me his time and explaining to me the journey of the sound from the speaker of a participant to the microphone of another participant!\n\n\n   \n   Music Mode vs normal audio path as Bjorn Winsvold explained in a Webex Board\n\n\nThe most popular meeting software platforms are focusing on creating a smooth meeting experience regardless of how quiet or meeting friendly your environment is. So it wouldn’t be wrong to say that the success of a meeting software is very dependent on how good the noise cancellation is or how smooth and plain your sound is transferred to the other participants. However, the software modules that allow us to have a smooth meeting experience can also be the reasons why we have problems when we play music in a meeting.\n\nI will talk about these modules and what kind of solutions Cisco Webex, Zoom and Microsoft Teams offer with music mode features of theirs.\n\n\n   \n   Music Mode in Webex\n\n\nNoise reduction\n\nNoise reduction is a very useful feature we can get from commercial meeting software. It removes your coffee machine beeping constantly, your washing machine screaming and gives you a plain environment to present by removing static noises. On the other hand, the very same feature may think of music as noise and remove it too. For example if you are an accordion or violin player and want to keep playing a chord for a second, then the noise reduction algorithm may remove the sound by accepting it as “noise”. So the meeting products should disable these modules or change them. For example Webex solves the problem by increasing the duration it uses in order to categorize a constant sound as “noise” while Microsoft disables the module.\n\n\n   \n   High Fidelity Audio Mode in Zoom\n\n\nAudio compression\n\nThe audio needs to be compressed and decompressed because most of these solutions are using standard audio codec for the transmission of the audio packets. This also means compression algorithms are very essential for audio quality. For music mode, the compression algorithm and compression rate becomes even more important and all three products promises higher audio quality in music mode.\n\n\n   \n   High Fidelity Music Mode in Microsoft Teams\n\n\nAuto gain controls\n\nAnother step in the journey of audio is the gain control. The purpose of this is to reduce the dynamic range of gains in order to make them be heard clearer. Zoom eliminates this step in music mode while microsoft teams gives the option to eliminate to the user. Cisco Webex replaces this module with a limiter to prevent overload of the signal before it is sent to the codex.\n\nHigh pass filtering\n\nIt is common in these systems to apply a high-pass filter to the microphone signal in order to remove low-frequency noise. For music mode this is not a good idea since you then will lose the low tones from your music especially if you are using a bass guitar etc.\n\nConclusion\n\nLong story short, it is actually possible to have music lessons when you can’t meet with your teacher and it is as efficient as an in person lesson and also as fun! You just need to know what to look for and choose the optimal software for your needs. It is also possible to use Cisco Webex Devices with Zoom or Microsoft Teams meetings so that you can benefit from both solutions’ features.\n\nAs I have more insights for Cisco software, I viewed the solutions mostly from their architectural choices but if you want to read more about differences, below you can find more information about different “Music Mode” solutions of the three software platforms:\n\n\n  Music Mode for Cisco Webex Devices\n  Music Mode for Cisco Webex App\n  High Fidelity Music Mode in Microsoft Teams\n  High Fidelity Audio Mode in Zoom\n\n",
        "url": "/networked-music/2022/11/28/aysimab-music-mode.html"
      },
    
      {
        "title": "Telematic Concert between Salen and Portal - Performers’ Reflections",
        "author": "\n",
        "excerpt": "Reflections on semester’s first Telematic Concert\n",
        "content": "A Telematic Experience\n\nAs SMC master students, one of our focus areas has been practicing low latency software solutions which allow us to host real-time music performances over the network. In our first semester, we did try performing together by using different software solutions. As part of these efforts, we were expected to perform two telematic concerts. Each concerts were hosted in two different locations at the same time over the network using a low latency audio visual streaming software, Lola.\n\nAs one of the performer teams, we will describe this journey and provide some details about our first experience performing a concert over the network. Our performance, ‘A Telematic Experience’ was hosted in the Portal and Salen seminar rooms on the 4th of October and thanks to all the practices we did in the class, everything worked well and the performance and rehearsals turned out to be a lot of fun!\n\n\n   \n   Our concert\n\n\nMusic for the Performance\nFor this performance, we prepared four instrumental, semi-improvisational pieces, each led or originating from different members of the group. A theme which emerged from our first rehearsal was a general trend towards cinematic music. This allowed for a balance between playing synchronously and freedom in structure while also necessitating communication between musicians.\n\nThe lineup of musicians was:\n\nSalen\n\n\n  Christian - Piano and confransier\n  Kristian - E-Drums and Synth\n  Alex - MPC/Synths and FX pedals\n\n\nPortal\n\n\n  Jack - Violin through laptop FX\n  Joseph - Guitar through laptop FX\n\n\n\n\nPlaying Telematically\n\nIn our case, probably owing to the fact that both locations were in the same building, latency was imperceptible during both the final rehearsal and concert, retaining the feeling of being in the same room.\n\nLearning how to communicate through Lola was very important, especially due to the improvisational nature and somewhat fluid structure of our music, proving to be our biggest challenge. Specifically, it was difficult to communicate verbally during the final rehearsal, since if one of us had something to say we needed to give that microphone priority and pause the rehearsal, which overall slowed us down and made the rehearsal less efficient.\n\nA dedicated talkback microphone in the Portal may have helped with this. However, during the day of the concert we did have satisfactory communication, thanks to the rehearsals and time we spent practicing together as well as individually, resulting in the pieces working efficiently from the first run-through over the network. Additionally, we had also decided who was going to cue which pieces and sections while we were rehearsing in the same room, making the transition to Lola less jarring. The biggest take away we think is that practice and great planning makes perfect.\n\n\n   \n   Another rehearsal\n\n\nA Telematic Rehearsal\n\nUltimately we did not execute all the marketing ideas we intended due to time constraints. It could have helped attract a larger in-person audience if we had shared the event more widely with printed posters and sharing through email lists.\n\nWe were very pleased with the music we were able to create together in just three rehearsals, since the pieces had variety and made good use of all the instruments we had available. Finally, we were lucky with the instrumentation and our music preferences, because everything fitted together very adeptly from the very first rehearsal.\n\n\n   \n   Our first rehearsal\n\n",
        "url": "/networked-music/2022/11/28/aysimab-telematic-experience-performers.html"
      },
    
      {
        "title": "Bringing the (Optic) Fibre Ensemble to Life - Behind the Scenes of a Telematic Music Performance",
        "author": "\n",
        "excerpt": "What does it take to put on a telematic music performance? Cable spaghetti of course!\n",
        "content": "Introduction\n\nOn Tuesday 22 November 2022 we held our second network musical performance of the semester between the Science Library and RITMO on the UiO campus. Read below to find out about all the technology that went into bringing the music of the newly-formed (Optic) Fibre Ensemble to life.\n\nThis concert formed part of a two-day workshop arranged by Çağrı Erdem called Embodied Perspectives on Musical AI. We were the evening’s entertainment for some of the brightest minds in musical machine learning, so the stakes were high.\n\n\n   \n   Setting up the concert at the Science Library\n\n\nThe two locations for this concert were 650m apart, or about 1.9 seconds at the speed of sound. Meanwhile LOLA, the specialist low-latency software we used, allows us to send audio and video between these locations in around 20-30ms.\n\nYou can watch the full concert online here as part of the workshop.\n\nTechnical Background\n\nThis concert was an example of telematic music, or music that is performed between multiple physical locations in real time thanks to network communications technology. We can also call such an event a network musical performance, or NMP. At UiO, we use specialised software called LOLA to stream real-time, high-quality audio and video over a network, meaning performers at both ends can see and hear each other with almost imperceptible latency.\n\nAt both locations we used the NMP portable kits. These kits allow us to connect to our UiO LOLA network from a variety of locations around campus, including the two locations for this performance. Our kits include everything we need for a barebones NMP performance, such as the specialised PC, audio interface and camera required by the LOLA software, as well as a 16-channel mixer, a dedicated headphone amplifier, a projector and more. You can read more about the specific equipment included in these kits here.\n\nScience Library\n\n\n   \n   Alex works the mixer while Nino practices piano\n\n\nThe technical team at the Science Library consisted of our colleagues Alex, Iosif and Aysima. This location served as the front of house for the event, and was also where two of the four performers were located. Nino sang and played piano and guitar, while Emin played the e-drums and ney. Alex was primarily responsible for audio routing and front of house sound, Aysima handled the LOLA software and associated video, and Iosif was in charge of the livestreaming of the event.\n\nThe original plan two weeks before the concert was to use a spatial audio system run through a Max patch designed by Masoud. However, we later realised that this was not quite feasible due to the sheer amount of outputs required from the mixer as well as the CPU demands on Masoud’s laptop. As a result, we scaled down our ambitions to an extended stereo system. The wall-mounted speakers in the Science Library provided the majority of the front of house sound, while a pair of large Genelec speakers on the stage were used for the final piece of the concert.\n\n\n   \n   Science Library stage plot\n\n\nRITMO\n\n\n   \n   Team RITMO getting ready to soundcheck over LOLA\n\n\nThe RITMO technical team consisted of your authors Kristian and Jack, while Masoud and Fabian were the remote performers. Masoud sang and performed a patch in Max/MSP, while Fabian played the bass guitar and the no-input mixing board. Helpfully for the technicians, both of these setups connected to the performers’ audio interfaces. This meant that we only needed to send line inputs into the mixer on the NMP kit and connect a simple Shure SM58 vocal microphone for Masoud.\n\n\n   \n   RITMO stage plot\n\n\nChallenges\n\nWe were pleased with how seamless this concert was for the technicians. However, no NMP is ever without its challenges. The technical requirements for the performance shifted significantly in the week prior to the concert, which meant we did not know until we were in the space whether we had all the equipment we needed. Luckily we did not overlook anything major, but nonetheless we learnt that making an exhaustive list of necessary equipment beforehand would have been ideal for peace of mind. Any missing equipment would have been much more problematic if we had been holding the concert outside of the UiO campus, so this was an important lesson to learn going forward.\n\nConclusion\n\nWe had nearly two full days set aside to assemble, rehearse and perform this concert, and it filled a vast majority of it. NMPs certainly take time to get working correctly, but when you get there the results can be brilliant. In this case we were very happy with the concert we managed to pull off, and look forward to getting stuck into more NMP wizardry next semester.\n\nOther Links\n\n\n  For a full technical breakdown of the concert, read our wiki page here.\n  Check the SMC blog for a full report by the performers in this concert. They composed some stunning original music which you should also read about.\n\n",
        "url": "/networked-music/2022/11/28/jackeh-kristeic-optic-fibre-ensemble-technical.html"
      },
    
      {
        "title": "The Optic-Fibre Ensemble",
        "author": "\n",
        "excerpt": "Folk inspired soundscapes + No input mixing=True\n",
        "content": "End Semester Concert\nAt the end of the fall 2022 semester us SMC4024 students were tasked with putting on a concert as part of the Embodied Perspectives on Musical AI workshop being hosted at UiO. This was no ordinary concert. It was the second telematic concert where we played together over the net synchronously while being located in separate buildings. Our concert was played between the UiO Science Library and the RITMO Research Centre. To achieve this we used a software called LoLa which makes it possible to transfer both high quality audio and video over the network with very low roundtrip latency.\n\nThe Long Journey of Team B and C\nWe have 4 separate teams in SMC4024, during this concert team A and D were tasked with handling all the technical aspects of the concert while team B and C were the performers. We named our group the “Optic-Fibre Ensemble”. When we first started planning the concert our performance team consisted of 6 people but the week before the concert we down to only 4 performers due to unfortunate circumstances. This meant our concert plans had to be revised multiple times due to an ever decreasing group size. So the program we ended up with was quite different from our initial plans, but we are all happy with the results.\n\nOur pieces\nOur final set list consisted of 3 pieces. 2 cover song and an original piece. The cover songs we chose were the jazz standard “Autumn Leaves” and “Ain’t No Sunshine” by Bill Withers. Our final pieces was an electroacoustic semi-improvsational piece written by one of our team members: Masoud Nikafs. Masoud’s electronic piece consisted of a pulse and drone, field recordings, a layer of synthetic wind that was live mixed to fit the context of the Improvisation in the second part. Emin’s ney played in Saba, while Masoud’s vocals improvised microtonal Persian phrasings. Nino played both the piano and did vocal improvisations in georgian while Fabian added textures to the piece with his no-input mixing technique.\n\nPlaying Together\nThe last two days before the concert we move all our equipment to the different venues where we were playing. This gave us the possibility to rehearse together using LoLa to prepare ourselves for the concert.\n\n\n   \n   Science Library\n\n\n\n   \n   RITMO\n\n\nOne problem we encountered while setting up is that we couldn’t play at full volume at the science library since other people were present reading for their exams. So whenever we got time to practice we had to do it at a low volume. If you want to read more about the experience of rehearsing together with LoLa, Fabian has written a blog post about it\n\nConclusion\nOverall, this was an excellent experience. We had lots of fun (and some frustrating) moments working together and considering the circumstances we are very happy with what we achieved together and are looking forward to another semester with more NMP craziness.\n",
        "url": "/networked-music/2022/11/28/fabianst-performer-blog-post.html"
      },
    
      {
        "title": "Blackhole, Open Source MacOS Virtual Audio Device Solution for Telematic Performance",
        "author": "\n",
        "excerpt": "MacOS guide to Blackhole\n",
        "content": "Blackhole\n\nBlackHole is a modern MacOS virtual audio driver that allows applications to pass audio to other applications with zero additional latency. As a newly migrated user from Windows to Mac I found Blackhole, pretty reliable, easy to use, and easy to map between platforms, So I decided to share the features and make an installation and user guide! Enjoy!\n\n\n  Supports 2 , 16 , and 64audio channels.\n  Customizable to 256+ channels if you think your computer can handle it.\n  Supports 44.1kHz, 48kHz, 88.2kHz, 96kHz, 176.4kHz, and 192kHz sample rates.\n  No additional driver latency. BlackHole is also available using brew cask install blackhole\n  It is Also available for Windows but here we only introduce the MacOS version\n\n\nBenefits:\n\n\n  Versatile user interface\n  Stable working with no crashes (Also on Apple Silicon)\n  Different versions available for specific needs\n  It is possible to rename the Blackhole\n  Customizing Channels, Latency, and Sample Rates\n\n\n\n   \n\n\nInstallation of MacOS:\n\nhttps://existential.audio/blackhole/\n\nInstallation guide:\n\nApproach 1: Download Installer\n1.Download the latest installer\n\n\n   \n   \n\n\nYou must subscribe for the free version\n\n2.Close all running audio applications\n\n3.Open and install package\n\n\n   \n   Press continue\n\n\n\n   \n   select its destination\n\n\n\n   \n   Finalize your installation\n\n\nApproach 2: Install via Homebrew\n\n\n  2ch: brew install blackhole-2ch\n  16ch: brew install blackhole-16ch\n  64ch: brew install blackhole-64ch\n\n\nFeatures:\n\nIt has the following routing options for Logic Pro X\n\n\n  Logic Pro X to FaceTime\n  Logic Pro X to Google Meet\n  Logic Pro X to Skype\n  Logic Pro X to Zoom\n\n\nIt has the following routing options for GarageBand\n\n\n  GarageBand to FaceTime\n  GarageBand to Google Meet\n  GarageBand to Skype\n  GarageBand to Zoom\n\n\nIt has the following routing options for Reaper\n\n\n  Reaper to Zoom\n\n\nIt has the following routing options toRecord System Audio\n\n\n  Setup Multi-Output Device\n  In Audio Midi Setup → Audio Devices right-click on the newly created Multi-Output and select “Use This Device For Sound Output”\n  Open digital audio workstation (DAW) such as GarageBand and set input device to “BlackHole”\n  Set track to input from channel 1-2\n  Play audio from another application and monitor or record in your DAW\n\n\nRoute Audio Between Applications\n\n\n  Set output driver to “BlackHole” in sending application\n  Output audio to any channel\n  Open receiving application and set input device to “BlackHole”\n  Input audio from the corresponding output channels\n\n\nCreating Aggregate device on mac:\nIn this example zoom H5 recorder is used as a sound card in combination with Blackhole to be used in Ableton\n\n\n  Open Audio Midi Setup\n\n\n\n   \n   2) You can modify I/O and audio settings of the blackhole here \n \n\n\n   \n   3)\tOr create an aggregate device by hitting the plus Icon on the bottom left \n\n\n\n   \n   4)\tYou can use this aggregate device in all DAW or Musical programming languages or communication software \n\n\nFor Further exploration visit:\n\nhttps://github.com/ExistentialAudio/BlackHole#readme\n",
        "url": "/networked-music/2022/11/29/masoudn-blackhole.html"
      },
    
      {
        "title": "An instrument, composition and performance. Meet the Transformer #1",
        "author": "\n",
        "excerpt": "A review of Natasha Barretts Transformer #1\n",
        "content": "A review of Natasha Barretts presentation at IMV November 24th\n\nAt our department this fall, we’ve been so fortunate as to have a series of great talks presented by the Department of Musicology called Women in Music Technology. November 24th I was present at the talk of Natasha Barrett presenting her Interactive Music System (IMS)  the “Transformer #1”.\n\nIn this blog post I will present this work as I understood it, and linger around some interesting aspects of it that Natasha Barrett brought up herself during the talk.\n\nThe “Transformer #1” is a very complex concept to wrap your head around if you try to categorize it as an IMS. Here’s my attempt to explain how it works in very broad terms:\n\nOk, so on stage (We were mostly just shown video excerpts of the actual performance)  you would see a performer singing (or producing sounds) and dancing/moving, and behind her there’s a screen showing visuals. But the sounds you’re hearing are not just the sounds of the performer, but some very interesting musical results that for me as an observer seems to be both affected by the movement and the original sounds produced by the human performer. In addition to that, the visuals also seem to be affected by the actions and sounds of the human performer. And notice the word affected; as a spectator it does not sound/appear like it is directed or controlled by the human performer, my intuitive feeling as a spectator was actual musical interplay between human performer and the system.\n\nHopefully you have an idea of how I experienced it (if not cheat and watch the youtube video), but here are my attempts to categorize it (based on my experience as an observer and the very thoughtful observations by the creator herself about the IMS):\n\n\n  \n    It is an IMS, using advanced many-to-many mappings (Machine Learning, Music Information Retrieval and specifically designed algorithms) of audio and motion capture data from a performer to create sounds (both through sound synthesis and in order to choose audio from a huge database of samples)\n  \n  \n    It is a visual generative system, using the input from the human performer (audio or motion capture) to decide on parameters for computer generated visuals.\n  \n  \n    It is a composition (!), as it is constructed as one performance with four different acts/parts, where different algorithms in both visual and audio systems are used for each section. Also, the system utilizes a huge dataset of samples recorded by the composer for sound production, and the composer has designed the whole system and supervised the training of the Machine Learning algorithms.\n  \n  \n    It is a performer on its own, playing an integral part in the performance; taking inputs from the human performer and creating an output which I believe is hard to replicate based on the complexity of the system.\n  \n\n\nAs a composer myself, the fact that the creator thought this system should be considered as a composition was eye-opening. From what I’ve seen and heard from the short talk I fully support this claim. And that amazes me.\n\nI would really encourage you to to experience it yourself (Oh, I forgot to mention that for the actual live performance all the audio is spatially encoded, giving the audience a 360 degree sonic experience). Run to this link and get a ticket for the show at Henie Onstad Kunstsenter 3.12.2022 or 4.12.2022, or if you’re reading this too late check out this video.\n",
        "url": "/interactive-music/2022/11/30/arvidf-transformer1.html"
      },
    
      {
        "title": "In search of sounds",
        "author": "\n",
        "excerpt": "Are you looking for sounds to inspire your next project? At synth.is you can discover new sounds by evolving their genes, either interactively or automatically.\n",
        "content": "In search of sounds\n\nAre you looking for sounds to inspire your next project? There are many ways. Trying new physical instruments or new configurations of familiar ones, finding or recording samples and making them into something new. At synth.is you can discover new sounds by evolving their genes, either interactively or automatically.\n\nWhen evolving sound genes interactively, you’re presented with generations of individual sounds, where you can preview, or -listen, each individual and choose the ones most suitable as parents for the next generation. This type of Interactive Evolutionary Computation has been employed before in different scenarios, such as for evolving 2D images, 3D objects and in an interactive media installation. The evolution can be approached from two different angles, either starting from scratch where the first generations contain primitive individuals, or continuing evolution from published sounds. The latter approach is intended to allow evolvers to build upon the progress made by others, with fresh ears - in a way standing on the shoulders of giants - instead of starting from scratch. Either approach can be interesting, paving an evolutionary path from genesis of exploring in what sonic directions already published sounds can be taken.\n\n\n\nAutomatic evolution of sounds employs search algorithms to allow the computer to discover novelty. The current implementation of automated search for sounds is supervised by pre-trained classifiers, so the discovered novelty is somewhat recognisable. Elite sounds for each class can be listened to and liked, in a similar way as can be during interactive evolution, adding those found interesting and useful to a catalogue of favourite sounds. Favourite sounds can also be published, for others to use and continue evolving.\n\n\n\nThe main source of sounds comes from pattern producing networks. Those audio sources are routed through audio signal networks, which can be considered as synthesiser patches. Those patches can be configured manually or they can also be evolved alongside the pattern producing networks.\n\n\n\nFavourite and published sounds can be rendered as sample based instruments in the SFZ format. That format can be directly used by DAWs such as Bitwig Studio and the Renoise tracker, or with specialised plugins like sforzando and sfizz.\n\nVarious settings can be chosen for the rendering, such as the duration of each sample, number octaves and velocity layers, and sample rate. Rendered sample instruments can be previewed in the web browser interface with an on-screen keyboard, which also supports Web MIDI, so you can try them out with your favourite hardware controller or DAW. Along with the SFZ instrument definition, a Sonic Pi live coding template can be downloaded with the rendered sample instrument.\n\n\n\n\n\n\n\nA recent addition to this sound evolution web interface is the integration of a live coding environment based on the Glicol engine. Underlying genes of favourite and published sounds can be copied to the clipboard (as a JSON text string) and pasted onto the live coding screen, where they can be rendered with the desired configuration and used directly in the accompanying live coding text area. Furthermore, code modification steps can be recorded during a live coding session, so they can be played back. This is a form of musical recording, but not where the audio signal is inscribed but rather the symbols from the live coding, along with their temporal placement, resulting in a JSON structure containing both the audio gene descriptors and their compositional arrangement, clocking in at a few kilobytes. That can be viewed as outperforming any audio compression.\n\n\n\nThe project itself has evolved from the application of those pattern producing networks for single cycle oscillator waveform descriptions, to the rendering of sounds that develop over time, of any desired length. Recent explorations into automated evolution of sounds will be investigated further, possibly utilising custom reference sounds and audio features of preferred sounds for navigating the sound space. New neighbourhoods in the sound space may be defined with unsupervised classification. Extraction of the core functionality behind synth.is into a reusable (NPM) library is underway. A command line interface (CLI) will demonstrate one possible use case. Stay tuned at https://twitter.com/wavekilde (Mastadon / ActivityPub presence is under consideration : )\n",
        "url": "/other/2022/12/01/bthj-synth-is.html"
      },
    
      {
        "title": "New Interface for Sound Evolution (NISE)",
        "author": "\n",
        "excerpt": "Robiohead is an attempt in a box to explore how an Interactive Music System (IMS) can offer ways to explore sound spaces by evolving sound producing genes in a tactile manner.\n",
        "content": "New Interface for Sound Evolution (NISE)\n\nRobiohead is an attempt in a box to explore how an Interactive Music System (IMS) can offer ways to explore sound spaces by evolving sound producing genes in a tactile manner. A motivation is to create a musical interface alternative to those already available at the sound evolving web application at synth.is, as discussed in a previous blog post. The IMS will explore embodied / physical interactions with sample based virtual music instruments, which can be rendered by that web application. Sensor data is used to navigate samples along dimensions of notes (pitches), velocities and durations. Approaches to evolutionary interaction are explored by testing how different gestures can be sensed for conducting evolutionary search. The name comes from a second external-design iteration in the form of a robot head, discussed below, with a twist that is probably influenced by the name of some English rock band.\n\nA design premise is that the IMS is as much a toy as a musical instrument. It should invite children as well as adults to playfully explore and discover new sounds. Why not just a mobile app that runs on phones that most have? Young children commonly don’t have smartphones and they can benefit developmentally from tangible, engaging toys that speak to a primitive form of communication. The instrument should be robust and child safe, with no parts sticking out that could break off.\n\nIMS design notes\n\nThe design has so far gone through two (concept) iterations. Following are notes form a (thought) process that has lead to the current prototype design:\n\nFirst concept iteration\n\nInitial idea for the overall design of the IMS was simply based on a sphere with short embedded cylinders sticking out from the two adjacent sides. The cylinders would offer connectivity and support for mounting the IMS.\n\nOn the surface of one half of the sphere could be metal, touch sensitive plates for triggering different chord combinations of the samples. The plates can be arranged in different tetromino shapes, where each shape would represent a chord. The shapes, or chords, would be moved around a tuning lattice, e.g. with tilting / rotation, to transpose them to different roots. The tuning lattice might be microtonal, e.g. just intonation. Exposing children and adults to different tunings may be suited to help them to become more receptive to unusual stimuli.\n\nThe backside of the sphere could be reserved for evolutionary interactions, where individual sound genes, local or remote, can be paired and evolved while interacting with sounds from newly developed individuals on the tetromino-chord side. The following sketch depicts the tetromino-chord side and the evolutionary side with buttons for selecting and favoriting sound-genes, which may not be a very novel interface. In any case there was a struggle in coming up with a good representation for that interaction with clear affordances.\n\n\n\nThe tuning-lattice navigation with tetromino-chords is inspired by this video: https://youtu.be/CSL_Axohw94\n\n\n\n- shared by Alexander Szorkovszky.  \n\nSecond concept iteration\n\nWith that sphere-based design in mind, observing a robot head drawn by a nine-year-old on a misty glass lead to that form being considered as a basis for the IMS design: It may offer the desired invitation to play and discovery, as it looks intriguing and a bit playful. The box shape might also be easier to work with than a sphere, for an early prototype:\n\n\n\nDiscussing this with the nine-year-old prompted him to sketch out the ideas:\n\n\n\nObserving a decades-old Desigual towel, during a bathroom break from the impromptu design session with the nine-year-old, offered new input to a possible design direction for the robot-head. The straight and diagonal lines for the mouth and nose, on the oddly designed head of that towel decoration, were somehow associated with the soft membrane potentiometer in the SMC4054 hardware kit.\n\n\n\n\n\nHearing of those associations, the nine-year-old was quick to whip up new sketches:\n\n\n\n\n\nThinking of the upcoming winter solstice celebrations, the design associate came up with the concept of a beard and a pointy cap, where the former could serve as a tilting support and the latter as a cover for any connectivity and charging ports.\n\nThe two soft membrane potentiometers could offer navigation of notes / chords and sample durations, respectively. Eyes might conceal sensors for filter and reverberation / echo controls, while also providing led-based feedback. With those sensors as the only visible ones, the low control dimensionality may lead to exploration of hidden affordances.\n\nThe evolutionary interactions mapped to buttons in the initial concept iteration might be mapped to accelerometers, gyroscopes and/or compasses, where e.g. shaking the instrument might lead to discoveries or evolution of new sounds. The shaking-gesture could be perceived as mixing up the sound-genes. Tilting the instrument might provide navigation of note (lattices) instead of adjustments of visible potentiometers (and those tilts might lead to subsequent shaking-gestures). Then a potentiometer would rather be reserved for other sample-playback controls, such as for timbre stretching (2.4) or sustain control.\n\nAlternative mapping might have the shaking-gestures responsible for triggering arpeggios of the most recently tilt-navigated notes, with a potentiometer controlled tempo. Evolutionary interactions might then have to resolve to the button gestures from the first concept iteration or some other gestures that are yet to be conceptualised.\n\nThe design sketches from the nine-year-old associate may require 3D printing. Should time not allow such materialisation has not yet been realised, where a cardboard box currently serves as an initial substitute:\n\n\n\nCurrent prototype\n\nOf the possible sensor combinations considered during the second concept iteration, the tilting gesture has been implemented for navigating sounds on a tuning lattice and a (gentle) shaking gesture unravels new sounds. This prototype doesn’t offer actual evolution of sounds with shakes, but rather selects new pre-rendered sample based virtual instruments, to provide a tangible experience of how such a gesture might come up with new sounds during an evolutionary process. The soft membrane potentiometer is used for choosing different tetromino shapes for chord combinations, with seven positions for the one-sided tetrominoes (where 19 positions would be required should all the fixed tetrominoes be covered). A sliding potentiometer is used to control the duration of the sounds (where each duration is pre-rendered) and a flex sensor is used to affect the sound velocity / filtering (each velocity is also pre-rendered). Those three dimensions of pre-renders (pitch, duration, velocity) result in quite large sample based instruments and the corresponding sensors provide a three-to-one mapping of control. In addition, a button stops (repeating) playback, allowing silent navigation to new sound parameters, e.g. a new tuning lattice position, instead of the default audible sequential navigation.\n\nWhile not exactly adhering to the initial design constraint of child safe robustness, without any parts sticking out, it is a start:\n\n\n\n\n\nTesting and performing\n\nVideo recording of one test session:\n\n\n\nLive performance with Robiohead in Salen after an SMC4054 exam, on Monday December 12, 2022:\n\n\n\nFirst step implementation (milestone 1) tried out:\n\n\n\nAlexander Szorkovszky trying out the Robiohead:\n\n\n\n\n\nOpen source\n\nThe source files behind this IMS can be accessed from:\n\nhttps://github.com/synth-is/robiohead\n",
        "url": "/interactive-music/2022/12/01/bthj-robiohead.html"
      },
    
      {
        "title": "Why Your Exhibit Tech Failed (and how to fix it)",
        "author": "\n",
        "excerpt": "Why are visitors not using your installation?  You might just disagree with yourself, the visitors, and your department or client about their needs and wants. The reason for this disagreement is always the same.\n",
        "content": "\n    \n\n\nYou had a great idea for an installation, convinced your stakeholders of said idea, took it through design\nand execution, only to realize that it doesn’t perform as intended. Why, you think to yourself after\nmonths of hard work, are visitors not using your installation? Why is the client unhappy? Why do people\nnot understand how to use it? It’s so simple!\n\nChances are it’s not a bad installation at all. You might just disagree with yourself, the visitors, and\nyour department or client about their needs and wants. The reason for this disagreement is always\nthe same: you didn’t lay the project foundations early enough. Or worse, you didn’t lay down any\nfoundations at all.\n\nAsking The Right Questions\nIt is rarely a good idea to jump into a project and just start solving the most apparent problems. If you\ndon’t spend time diagnosing, how confident can you really be that you’re solving the correct problem?\nMaybe you’re doing the right thing, but maybe not, and I probably won’t have to tell you that leaving\nthe success of a project to chance is a bad idea.\n\nWhy should this project exist?\nInstallations can be commissioned by a third party, be part of a public funding deal, be requested by\nvisitors, or be requested by an exhibit. These are not good enough reasons by themselves to create an\ninstallation. They will not guide your decision making, nor will they ensure a successful project.\nSometimes these commissions come with a set of requirements. It’s important to understand the basis\nfor those requirements and, if that basis is unreasonable or unrealistic, work with the stakeholders to\ncreate a better frame for the project.\nIs this truly a change for the better?\nHave you ever made a change to a product because you thought it was what your boss wanted when\nthey really had something else in mind? Have you been stuck making minor changes because your client\njust isn’t happy with the result?\n\nStakeholders often request a feature or installation without having a reason beyond that they want it to\nexist. Other times they do have reasons, but they are unable to articulate those reasons in a way that’s\neasily digestible, and you spend an unnecessary amount of time finding common ground. This might be\ntrue for you as well. Your gut might say yes to a new idea but, while there is value in gut feelings, take a\nstep back and see how a change relates to your goals.\n\nHow do you know if the project is successful or not?\nGoals and evaluation go hand-in-hand. You cannot accurately evaluate the success of a project unless\nyou set highly specific goals during the outset of your project. Worst case scenario, you will evaluate the\nproject based on the wrong criteria and think the outcome is better or worse than it is. So, what do you\ndo?\n\nThe MUSETECH Framework\nDeveloped by Areti Damala, Ian Ruthven, and Eva Hornecker, the MUSETECH model provides a strong\nfoundation for judging the performance of technology. While intended for museums, any exhibition,\ngallery, or experiential learning center will benefit from this model, the only difference is terminology.\nThis tool will speed up your decision-making process and give you the means to justify your design and\nimplementation choices.\n\nThe model lists 121 criteria serving to guide you in your quest to understand what works and what\ndoesn’t work about your installations and exhibit technologies. These criteria are grouped into 4\ncategories:\n\n  Design: These criteria will help you make mindful design choices and articulate why these\nchoices were made.\n  Content: These criteria are concerned with the kinds of tech curators and establishments use\nto create and maintain content, but also visitor-personalized or visitor-created content.\n  Operation: These criteria focus on resource requirements, installation, and maintenance of\nthe installation.\n  Compliance: These criteria deal with accessibility, health, safety, ethics, and potential legal\nissues.\n\n\nThese categories can in turn be approached from 3 different perspectives:\n\n  The Visitor: Whomever is likely to attend the exhibition, the end users.\n  The Cultural Heritage Professional: Outside of museums, I would call them curators or\noperation managers. They will be interacting with the same tech daily.\n  The Museum (or exhibit/gallery/learning center): They have a unique understanding of their\nown purpose and how they envision the future of the establishment.\n\n\n\n   \n   The MUSETECH Model\n\n\nWork together with your stakeholders to select criteria and do so early in the development process to\nminimize misunderstandings and waste of resources.\n\nSelecting Appropriate Criteria\nThe model presents a heap of criteria to choose from, but in reality you will rarely have the resources\nto fulfill each of them. Write down every criterion you think is relevant, then eliminate half of those\nwith your main stakeholder to determine what’s truly important. What you’re left with is common\ncriteria to measure your project against, giving you leverage to eliminate requests that do not support\nthe goals of the project and more power to make changes that do, saving both time and money. If\nyour only criterion is to promote social interaction and a feature request or design choice does not\nsupport this criterion, it is a poor use of resources.\n\nAdditional Readings\nFor a deeper understanding about this framework, I highly recommend reading the original article. If\nyou just want to get started, check out the MUSETECH Companion for a comprehensive list of criteria.\n",
        "url": "/interactive-music/2022/12/03/olivegr-musetech.html"
      },
    
      {
        "title": "Yggdrasil: An Environmentalist Interactive Music Installation",
        "author": "\n",
        "excerpt": "Plant trees and nurture your forest to generate sound!\n",
        "content": "\n   \n   Yggdrasil, in it's finished form\n\n\nI keep reading about how most new electronic instruments are short-lived and rarely picked up by\nothers than the person who created them. Making an instrument is difficult, and convincing others it’s\nworth playing is even more so. Knowing this, why go against the flow? I decided to create a music\nsystem designed to be ephemeral, while also experimenting with ideas for making users stick around.\n\nYggdrasil is an installation piece that promotes environmentalism. The concept was derived from the\nU.N. Sustainability Goals by exploring potential ways these can be translated into gestures and sound. I\nsettled on “Climate Action,” where suggested actions are planting trees and working towards clean\nenergy.\n\nInteractive Music System\nThe user can perform an overtone series using their left hand. The overtone series is first performed\nwith an uninteresting sound, but the user can develop it over time. To push the music forward, the user\ncan place trees on a platform or shine an accompanying flashlight through a hole in the base of the\ninstallation to grow their overtone sound. In theory, the user can place anything on the platform, but\nthe installation is calibrated for a set of accompanying whittled tree models.\n\nConcept Design\nResearching newer music systems for inspiration, I came across the\nCicada, a physical synthesis module. While it also\nsounds great, I was drawn to the idea of moving physical pieces around to change the soundscape. As\nthe core of my project was the idea of planting trees, this naturally led me to the idea of a platform on\nwhich you place whittled tree models. I used the same hexagon shape to avoid a boxy, uninteresting\nlook, while leaving space on opposite sides of the case for different sensors.\n\nSensors\nI’m using 3 different sensors to drive the system:\n\n  Force Sensing Resistor: senses pressure generated by the platform and trees, which drives the\nmusic forward.\n  Infrared Camera: the user moves their left hand in front of the camera to play an overtone\nseries.\n  Light Dependent Resistor: an accompanying flashlight will be used to nurture the forest. This\nchanges the timbre of the overtone series.\nMultiple sensors allow for different kinds of mapping strategies and plenty of exploration while also\nsupporting my climate action design goal.\n\n\nWhile developing the system, I experimented with temporal mapping strategies—postponing the result\nof an action to be experienced later. This was inspired by the idea that, for better retention, an\ninstallation should gently introduce a user and gradually become more complex. I suggest only to use\nlong-term mapping together with short-term mapping, as users may be thrown off by not receiving any\nfeedback at all during their interactions.\n\nHardware Components\nThe casing was planned out in Inkscape before it was laser cut and oiled. I designed a light chamber to\nmake the base ambient light absorbed by the light resistor more stable, 3 holes for cables, and a mount\nfor the IR camera.\n\nAt the core of the system is a Bela board, built on a BeagleBone Black computer. I\nbuilt my own circuit on top of the Bela to retrieve analog data from my sensors. The translation from my\nbreadboard prototype to the perfboard is practically 1 to 1. I used some extra male headers to better\nsupport the attachment, making it feel more robust.\n\n\n  \n  Pro tip: solder one component at a time. It's easier to debug.\n\n\nA button was added to be able to reset the calibration process, should the result not be ideal the first\ntime. A rotary encoder was also added to be able to tweak the master output level without diving into\nthe code.\n\nSound Generator\nSound generation was done in Pure Data. To ensure the installation can be used in different\nenvironments with different lighting, I created a calibration system that, with the push of a button, gives\nthe user 10 seconds to use the system to its extremes. These extremes are then normalized before they\nare mapped to musical parameters.\n\nAdditional Remarks\nSpecial thanks to Kjetil Normand for whittling the miniature trees, and to Skaperkraft\nValdres for laser cutting my design.\n",
        "url": "/interactive-music/2022/12/03/olivegr-yggdrasil.html"
      },
    
      {
        "title": "Live Streaming A Telematic Concert",
        "author": "\n",
        "excerpt": "Whys, How-tos and Life-Saving Tips on Telematic Concert Streaming.\n",
        "content": "\n   \n   Photo by Caleb Oquendo\n\n\nMassive development in network technologies and passing through a strong pandemic make telematic performances more and more common every day. Today, splitting your band to different locations and playing music together and in real-time is not a big deal. It’s not only that the performers are lazy for a meet-up in a concert hall, but also the audience is becoming more remote in time. If you want your show to reach people, you may have to live stream your telematic show!\n\nWhen it comes to streaming a telematic performance, a couple of things may be essential. Primarily how you represent multiple venues may be crucial in terms of transmitting a telematic feel. Also, being aware of technical obstacles may help you on the go. In this blog post, I gather thoughts and advice based on my experience and observations on telematic concerts throughout the first semester in SMC.\n\nEquipment\nThere are tons of equipment that can go well in your streaming setup. However, you can perform an excellent live stream with just a few. Below are lists of must-have and good-to-have equipment. You don’t need any specific equipment when live streaming a telematic show. But remember that quality streaming/recording starts with the right equipment.\n\nMust Have Equipment\n\n  PC\n  Video Source (Camera, Webcam, PC Screen)\n  Audio Source (Mic, Mixer, Audio Interface)\n  Internet Connection\n\n\nGood to Have Equipment\n\n  Multiple Cameras - possible on different locations\n  Mixer, Audio Interface\n  Camera Control Interface\n  Lighting Solutions\n  Headphones\n  Streaming Software\n  Post-Production Software\n  Bonus: A Friend Watching Your Stream Live\n\n\nFork in the Road: Streaming One Place or More\nOne of the essential duties of a director is creating a space perception in their audience’s mind. It’s a bit tricky when it comes to a telematic performance. There are different places, and you have to make your stream represent the multiple places properly to the audience without killing the telematic vibe.\nIn my opinion, to transmit the most telematic experience to your remote audience, the best setup is doing the mainstream from one room (filming the physical screen) and showing cameras from other rooms occasionally (e.g., only in opening and introduction speeches). This will draw an excellent telematic picture!\n\nIf you are livestreaming using cameras in different rooms, consider going split-screen and labeling the videos to highlight the telematicity.\n\nIf you would like to live stream mainly from multiple rooms, make sure you still represent a telematic ambiance. For example, don’t stream using cameras from different rooms sequentially without showing the screens in the rooms. Otherwise, it may seem as if they were different cameras in the same room and could not transmit the telematic spirit. You may also think of going split-screen and labeling the videos (e.g., Science Library in Blindern and RITMO Lab in Gaustad) to highlight that the performers are at different locations.\n\n\n   \n   SMC students playing in their telematic concert between RITMO and Science Library.\n\n\nFilming A Screen\nSimply, cameras are designed to film humans and objects, and screens are not designed to be filmed by cameras. Even so, having screens on the stage inhered in telematic setups may lead to problems you may have to cope with.\nScreen Brightness and Color\nToo bright or too dark screens can be an issue when using cameras. Even though human eyes are perfect at seeing bright and dark in the same frame, that’s not the case for cameras. In such a scenario, first, try to adjust the cameras’ diaphragm and aperture settings. If it doesn’t help, try changing the light flashing on the monitor or projection screen.\nRefresh Rate Issues\nIf your screen is flickering or banding, although screens have different technologies, the underlying cause is similar: the camera’s framerate doesn’t match the monitor’s refresh rate. A monitor’s refresh rate is how many times the picture is changed in a certain amount of time. On the other hand, a camera’s refresh rate represents the number of pictures taken in a certain amount of time. When they don’t match, problems begin. You can fix it by setting the camera’s fps to multiples of the refresh rate of the screen.\n\nImportant Things to Remember\n\n  You may have the audio slightly offset when you take video and audio from different sources (e.g., video from cameras, audio from a mixer). Remember to synchronize them before going live. Most streaming programs (e.g., OBS Studio) have built-in tools to shift the audio slightly. For testing, you can either run a test stream and watch it live or run a test recording and check it on your local machine. If you are recording only, you can use post-production software (e.g., Adobe Premiere Pro) to automatically synchronize everything you recorded. See this article to see how to sync audio and video in seconds: https://www.evercast.us/blog/syncing-audio-in-premiere\n  If you are filming a screen, you may experience poor color quality. Remember to fine-tune the screen’s color settings (e.g., TV, projector) to make sure it looks cool on the camera.\n  To overcome flickering when filming a screen, try changing the refresh rate of the screen to multiples of your camera/stream refresh rate (when streaming 24 fps, the screen refreshes at 48 or 96 fps)\n  Consider different platforms, device types and screen orientations.\n  Running a test stream beforehand always helps!\n  Always have the stream open.\n\n",
        "url": "/networked-music/2022/12/04/ahmetem-livestreaming-telematic.html"
      },
    
      {
        "title": "Making A Telematic Concert Happen - A Quick Technical Look",
        "author": "\n",
        "excerpt": "Background of A Telematic Experience\n",
        "content": "\n   \n   A Telematic Experience\n\n\nWhen it comes to telematic performances, there is always some technical processes in the bakcground, and a bunch of people taking care of all. This blog post is a brief technical overview of the 2022 fall mid-semester telematic concert performed by SMC students between two locations, Salen and Portal at UiO.\n\nGetting Ready\n\nAs technicians, we gathered five times to determine technical needs and actions. In the first meeting, we gathered with the performers for the initial instrumentation plan and locations. Secondly, we did an internal session where we discussed our roles. On the 27th of September, we presented our final plans for the concert in the class. Next, we explored the venues with performers. After a final rehearsal on the last day before the show, we met on the concert day morning to prepare for an outstanding performance.\n\nConcert Setup\n\nSalen\n\nA full backline and professional concert sound system were available in Salen to ensure flawless performance. Sounds fancy, right? Yup! These include the Midas M32 digital mixer, Hifi PA system, 24x input/output patching panel, and microphone stands. To enable musicians to perform together between Salen and Portal, we used a low-latency Audio/Video streaming system called LOLA. For this concert, Salen was considered the front of the house. Performers consisted of Alex on the synth and MPC sampler, Kristian on the E-drums and a mono synth, and Christian on the electric piano while also hosting the event. Technicians Emin, Fabian, and Masoud were eager to guarantee top-notch live-streaming, audio, and video service delivery.\n\nPortal\n\nThe audio setup in Portal was quite similar to Salen. On Both sites, we used portable LOLA racks instead of stationary pcs. In the Portal, two musicians were present: Jack on the violin and Iosif on the electric guitar. Olve, responsible for audio engineering aspects, did a fantastic job by making a multitrack recording of the entire performance, which we encourage for future events for documentation and audio post-production for the video recording. Nino helped with the video connection and ensured Portal musicians to see Salen performers in real-time.\n\nNetwork Connection\n\nVideo connection between two places was made using the LoLa software. We used Portal’s dedicated LoLa computer for connection. In the Salen, we used a portable LoLa pc. On both ends, a screen was routed to a monitor where performers were able to see other musicians. The network connection was flawless. The performers experienced a natural latency without any fluctuations.\n\nLive Streaming the Concert on YouTube\n\nWe live-streamed the whole concert from Salen on SMC’s YouTube channel. We used the cameras and the streaming station in Salen. We had scheduled the concert beforehand and our stream reached around 387 people online. The recording can be seen here.\n\n\n   \n   \"A Telematic Experience\"\n\n\nFinal Thoughts\n\nMaking our first telematic concert happen was a great experience. We are very pleased that we were able to provide the necessary technical needs for that show. We are looking forward to diving into the next ones!\n\nPlease See Also\n\n\n  \n    2022 Fall Mid Semester Telematic Concert Salen and Portal Documentation (Technical)\n  \n  \n    2022 Fall Mid Semester Telematic Concert Salen and Portal Documentation (Performance)\n  \n  \n    2022 Fall End Semester Telematic Concert RITMO and Science Library Documentation (Technical)\n  \n  \n    2022 Fall End Semester Telematic Concert RITMO and Science Library Documentation (Performance)\n  \n\n",
        "url": "/networked-music/2022/12/04/ahmetem-making-a-telematic-concert-happen.html"
      },
    
      {
        "title": "The SMC Audio Vocabulary",
        "author": "\n",
        "excerpt": "Click this post if you need some explanation on jargon, mainly related to the Portal and the NMP kits.\n",
        "content": "After my first SMC semester I found answers to my tech questions and also asked my fellow students what part of the audio jargon they need clearance on. So what I will do in this post is providing a short portal-related list of vocabulary and explanations some of you will find helpful, as well as tips and tricks I learned over the years as a rookie guitar player and some general workflow for a happy day in audio-land. Bear in mind that I will only cover hardware audio routing – so basically everything you can touch. I also plan on updating this post with more relevant vocabulary after the semester break.\n\nMix Bus/Bus/Buss, Subgroups and Aux/Returns\n\nYou question yourself: “What do words mean?” I hope I can help.\nIn short, a mix bus is a channel where signals from several other places are combined on your mixing board.\nYou can literally think of it as a public bus that picks up individuals along the way and drops all of them at a final location. The one bus you will always find in a DAW or on a mixing board is the Main Stereo or Master (Bus) where all your individual channels are pre-routed to.\n\nThe concept of a Mix Bus comes in handy if you want to group together signals of similar instruments, typically percussion, to be able to adjust the level of all instruments mixed together and then potentially add effects. This is also called creating a subgroup. The NMP kit mixer has two group channels next to the Main Stereo with which you can exactly achieve this.\n\nOccasionally, you come across a channel called Aux(iliary) or Return. It is similar to a mix bus insofar that it can receive input from various channels. The simple difference is how the signal is routed to it. It receives a copy from the source channel via a send option rather than from the channel output, so you can add effects for example without affecting the source. \nYou can read more extensively on the matter here.\n\n\n   \n   The NMP-kit Yamaha Mixer and different routing options\n\n\nPre-fader vs. Post-Fader\n\nThose terms refer to send options, meaning that you can send the signal to wherever you desire before or after it is affected by the channel fader.\nThe Pre-Fader option is mostly used for monitoring situations where you want to send a separate personal mix to an artist without affecting the room mix.  The button PFL (Pre Fader Listen) on the mixer for the NMP kits is another handy option for you as an aspiring sound tech. You can listen to the raw signal from the channel on your headphones to correctly equalize it. Your outputs will not be affected by that.\n\nPost-Fader Sends are generally used to send the signal to an effects channel (Aux), so when you fade out the instrument, the effect on that instrument will fade out similarly. Easy!\n\nPhantom Power\n\nis a positive voltage for equipment that contains active electronic circuitry, mainly condenser microphones (if you wonder about all the different types of microphones click here). All currently installed microphones in the Portal (3 ceiling mics and two gooseneck tabletop mics) require phantom power, so if you someday find them not working check the MIDAS settings for each mic channel and enable the Phantom +48V button (phantom power can range from +12V to +48V output). It gets its cool name from the fact that it provides the voltage through the same XLR cable that the audio signal goes through – on pins 2 and 3 to be specific – no extra power plug needed. To be reasonable with your equipment, do not use phantom power with dynamic mics as they do not require it.\n\nADAT vs. S/PDIF\n\nADAT originally referred to the Alesis Digital Audio Tape recorder module developed in the 1980s. The ADAT Optical interface protocol was introduced later for sending audio between recorders, and to this day we use the abbreviation for the protocol (commonly referred to as ADAT Lightpipe) rather than for the recorder. \nADAT and S/PDIF (Sony/ Philips Digital Interface) are optical audio cable formats that allow the transfer of digital audio between devices without a conversion to analog to retain high quality. \nS/PDIF can be implemented either through an optical cable (TOSLINK) or a coaxial cable (RCA), whereas ADAT requires an optical cable. \nSo what are the differences besides that?\n\n\n  \n    \n      ADAT\n      S/PDIF\n    \n  \n  \n    \n      -\tsupports up to 8 channels\n      - supports only 2 channels\n    \n    \n      -\tfixed sample rate of 48khz/ 44.1kh\n      - no fixed sample rate, the clock is embedded in the protocol\n    \n  \n\n\nAnd: optical cables can be damaged more easily but are the preferred choice if length matters and electromagnetic interference needs to be avoided.\n\n\n   \n   Possible Connections\n\n\nHierarchy of On/ Off - Plugging/ Unplugging\n\nSome people might care more for the technology they own than for their fellow human beings whereas others do the opposite. I think a balance between both ways is vital. So, here are some general rules on what to plugin first and power off last to prevent damage on your speakers or microphones.\n\n\n  Always switch your speakers on last or off first. Powering on your mixing desk after the speakers results in a loud pop that can damage your devices.\n  Always mute the corresponding channel or switch off your device before unplugging cables to prevent damage.\n  Correspondingly, turn off phantom power before plugging in a microphone.\n  If you do not know how loud a signal will be, always start with the fader down and gradually increase volume.\n\n\nHow to roll a cable correctly\n\nI feel this is a very spicy topic, many cables have suffered a slow death because of that. Seriously, the right way to roll a cable prolongs the live of it exponentially and it is a nice routine, so here is how I do it. (Please correct me if you see some room for improvement.)\n\nFirst, make sure that the cable is untangled and lies on the floor in a more or less straight fashion without any bumps. This makes it a lot easier to roll it. Please avoid the sailor-rope-method. Let the cable hang loosely in your hand and roll it into an O-shape like in the video.\n\n\n  \n    \n  \n\n\nIf you feel that there is strain on the cable try to roll it to the left or right with your fingers to release that strain. The final product should look like this.\n\n\n   \n   A good rolled cable\n\n\nBonus easter egg: the speaker array in the Portal is routed counter-clockwise on the MIDAS.\n",
        "url": "/networked-music/2022/12/06/kristeic-the-mct-audio-vocabulary.html"
      },
    
      {
        "title": "JackTrip Vs Sonobus - Review and Comparison",
        "author": "\n",
        "excerpt": "Low-latency online music performance platforms\n",
        "content": "\n    \n   Source:npr\n\n\nYou probably know that it takes time for audio data to travel from person to person. This slight delay, A.K.A. latency, is a “good old friend” of any online music collaboration. If otherwise intended, you will end up looking for tools to eliminate it.\n\nThis blog post is a quick review and comparison of two platforms dedicated to low-latency audio network performances over the Internet.\n\nAudio Quality\n\nBoth provide all the details that audio nerds want to see. JackTrip uses uncompressed, 48 kHz, P.C.M. 16-bit audio and ensures the best latency. In Sonobus, audio quality can be instantly adjusted from fully uncompressed P.C.M. (16, 24, or 32 bit) or with various compressed bitrates (16-256 kbps per channel) using the low-latency Opus codec.\n\nConfiguration\n\nSonobus is primarily targeted at non-technical musicians and has a relatively easy user interface while setting up JackTrip requires some I.T. experience. Also, Sonobus comes with various features. If you want to incorporate audio streaming in D.A.W. It is available as a V.S.T. plugin. Unlike JackTrip, users can use Sonobus as a Mobile app which can be quite handy.\n\nArchitecture\n\nJackTrip and Sonobus are based on P2P architecture, meaning sharing all kinds of computing resources, such as processing power, network bandwidth, or disk storage space between peers. JackTrip also has Client/Server model where all parties are differentiated and respond to clients’ requests. Luckily, we can use both between any combination of machines, e.g., one end using Linux can connect to another using macOS. ‍If all the musicians have a fiber ethernet connection both enable up to 800 kilometres away or more coverage.\n\nConclusion\n\nJackTrip and Sonobus are audio-only streaming services for network music performances. Both are used to achieve low-latency(25-30milliseconds) uncompressed audio connection. Compared to JackTrip, Sonobus is available as a V.S.T. plugin and mobile application. The easy-to-use interface can also be another advantage of it because JackTrip requires being comfortable with running command-line programs.\n\nLinks for Downloading JackTrip and Sonobus\n\n\n  JackTrip\n  Sonobus\n\n",
        "url": "/networked-music/2022/12/06/ninojak-Jacktrip-Vs-Sonobus.html"
      },
    
      {
        "title": "Music By Laser: The Laser Harp",
        "author": "\n",
        "excerpt": "If you want to know how to play music with lasers, and maybe learn something about the laser harp, then you should give this a read.\n",
        "content": "\n\n\nPicture yourself on a Friday night. You’re standing in a dark room, and all you hear is the buzzing of voices from the people around you. In the distance, you can see the shadow of a person onstage. Suddenly, a deep sound fills the room, and the buzzing is replaced by cheer. The shadow lights up in neon colors and is now standing in front of something reminding of a sprinkler system of lasers.\n\nWhat you’ve now been picturing is a concert where the artist will perform with a laser harp. A laser harp is an electronic musical user interface with a laser display and is what I will enlighten your knowledge about. You can play music with the harp by cutting off the different laser beams, which will create various notes. It was patented in 1982 by the French composer Bernard Szajner, but the Australian Geoffrey Rose claims it was his invention and that he had taken out a provisional patent in 1975-1976. Later, the harp was popularised by Jean-Michel Jarre, which uses the instrument at almost every concert.\n\n\n    \n\n\nThere are two different styles of laser harps that are commonly used today, framed and unframed style. The framed style has, shockingly, a frame around the beams so that it looks like a traditional harp, while the unframed style, again shockingly, doesn’t have a frame and looks more like a fan or a sprinkler (as described earlier).\n\n\n    \n\n\nFramed style\nThe framed style is the one invented by Geoffery Rose. How the instrument detects the blocking of the beams by using an array of photodiodes, often called light dependant resistors, inside either the upper or lower part of the frame. When the laser beam is blocked, the light reflected off your hand is detected by the corresponding resistor and the harp gives a MIDI feedback signal. The number of lasers is not fixed and can be everything from one to 32 or even more. This is mostly dependent on the capacity of the MIDI controller(s) and the software used but is also a freedom of choice. The size of the laser harp is also optional considering all limitations, and some larger installations have been seen at Burning Man, an event in the US that focuses on art and self-expression among other things.\n\n\n    \n\n\nUnframed style (frameless)\nThe unframed style, or the sprinkler (yes, I think it looks like a sprinkler), is often built as one laser split into the wanted number of beams by using a mirror. Since there is no frame, the detection of blocking is a bit more complex than with the framed style. The detection is reliant on the timing of the light reflecting back to only one photoresistor. I have also found some examples that use a stepper motor that is connected to a mirror, and by changing the motor position fast enough, the single beam is split into multiple. This way, every laser beam has its own angle, or position, which is used for detecting which beam is being blocked.\n\nClosing words\nThese techniques are of course not the only ones used to create this type of control data with a laser harp. Other alternatives that have been tested are the use of an infrared rangefinder to determine the position of the hand that blocks the laser beam, a laser-based rangefinder to calculate the distance between the hand and sensor, a camera that tracks the position and motion of the laser, and I am guessing there are multiple other techniques which can be used. These are just some examples :)) Thank you for reading!\n\nAdditional reading\nInformationg about the installation of a laser harp on Burning Man from 2005 can be found here.\n\nMore information about the framed harp, invented by Rose, can be found on this link.\n",
        "url": "/interactive-music/2022/12/06/thyrala-music-by-laser.html"
      },
    
      {
        "title": "Wii controller as the Gestural Controller",
        "author": "\n",
        "excerpt": "Read this post to find information on a different use of a Wiimote than playing games on your Wii console.\n",
        "content": "\n    \n    Wii Controller\n\n\nYou might know the Wiimote from playing bowling, tennis, or even fencing on your Wii console back in the day. This might be the more traditional way of using the controller, but that didn’t stop anyone from exploring other possible ways of applying it. Though many tried the Wiimote on other game consoles, some thought about the use of it in a musical sense. With its 6 degrees of freedom and affordability, it had the potential to become a powerful digital interactive music system. This was tested by Wong et al. among others.\n\nDesign\nGesture is a big part, if not the biggest, of how we interact with interactive music systems today. And how these gestures are mapped into sound depends on how the system interprets the gestures. With 6 degrees of freedom: 3 linear translation directions in X, Y, and Z positions and 3 rotation angles e.i. pitch, roll, and yaw - the Wiimote has the opportunity to interpret movement from more than pushing buttons. The controller of Wong et al. uses the built-in accelerometer to measure acceleration in the 3 different direction positions. With this, moving around how you feel, can lead to music that matches your mood and feelings.\n\n\n    \n    Wii Controller Motion Sensing\n\n\nEvaluation by musicians vs. non-musicians\nSomething I found interesting in the paper by Wong et al. was that they compared the evaluation from musicians with the one from non-musicians. The system was designed to detect different beat patterns, and Wong et al. tested how well these patterns were detected in the hands of the different experienced people, as well as how they felt about the experience with the Wiimote.\n\n\n    \n    Conducting gestures of 2/4 and 4/4 beat pattern\n\n\n\n    \n    2/4 beat pattern in curve and linear shape\n\n\nThe people with music experience often listen more to the actual music, while the non-musicians mostly focused on the visual aspect of the system. And what the people with little to nothing experience in music did pay attention to with the sound, was the control of it, not necessarily the music directly.\n\nThe musicians often tested the gestural controller with motions corresponding to how they usually perform, which wasn’t the direct intention of this designed system. The “amateurs” however often used the system as intended, which also makes sense since the system wasn’t developed for professionals. On the other side, the experienced musicians got their gestures detected more than the non-musicians. The people with no background in music didn’t quite understand how the usage of force affected the controller, hence how important it is in the expression of emotions, ascent, etc. This led to less detection of the gestures.\n\nArticle of inspiration\nIf you liked this post, I recommend you to read the original article by Wong et al. Thanks for reading! :))\n\nElaine L. Wong, Wilson Y. F. Yuen, and Clifford S. T. Choy. 2008. Designing Wii controller: a powerful musical instrument in an interactive music performance system. In Proceedings of the 6th International Conference on Advances in Mobile Computing and Multimedia (MoMM ‘08). Association for Computing Machinery, New York, NY, USA, 82–87. https://doi.org/10.1145/1497185.1497205\n\nVideo not related to article\nHere is a YouTube video of someone making music with the Wiimote.\n\n\n\n",
        "url": "/interactive-music/2022/12/06/thyrala-wiimote-music-controller.html"
      },
    
      {
        "title": "The Feedbackquencer: Making Music with Feedback - Part 1",
        "author": "\n",
        "excerpt": "Using feedback in a sequencer\n",
        "content": "Microphones, Loudspeakers, and Feedback\n\nWe all know the sound of acoustic feedback, the screech that occurs when a microphone is held too close to a loudspeaker.\n\n\n   \n   How acoustic feedback occurs \n   From: https://www.toa.jp/soundoh_wiki/index.php?Soundindex/Acoustic%20feedback\n\n\n\n  \n    \n  \n  The sound of feedback\n\n\nUsually we try to minimise the occurrence of this phenomenon as we see it as intruding upon the musical performance which is taking place. This is part of a tendency we have towards attempting to make microphones and loudspeakers transparent. We see the focus as lying upon the performer and their instrument, and not the auxiliary technology that we are using to facilitate it.\n\nBut what if we draw attention to the fact that we are using microphones and loudspeakers, remove the transparency and use them as musical instruments? Then acoustic feedback can be seen as the sound of the instrument, and one of its most fundamental sounds at that! It is “the sound of microphones and loudspeakers themselves” [1, p. 4]. In fact, the composer Robert Ashley describes it as “the only sound that is intrinsic to electronic music” [2, p. 185].\n\nThis concept underlies two projects that I’ve worked on over the last few months. Both conceptualise a microphone and loudspeaker as a musical instrument, presented in different interactive contexts. The first of these is presented in this post, and is based around building musical sequences out of feedback. It takes the form of the Feedbackquencer.\n\nThe Feedbackquencer\n\nThe Feedbackquencer is focussed on the timbral aspects of acoustic feedback. It encourages the musician to explore the way in which the spatial relationship between loudspeaker and microphone alters and emphasises different aspects of the sound of the feedback. The instrument consists of two main components: a microphone mounted in a stable position and a loudspeaker held in the hand.\n\nThe musician plays the instrument by moving the loudspeaker towards the microphone until feedback is generated. This feedback is then played back by a four-step sequencer, with each note pitchshifted to a different musical pitch (the length of each sequence step and the pitches are set before the instrument is booted up). The musician then explores the timbral space offered by the acoustic feedback within this set pattern.\n\nBut there’s more! An LED is mounted next to the microphone which increases and decreases in brightness at a set rate. Adjacent to the LED is a photoresistor (a sensor that can be used to measure light). The signal from the photoresistor (corresponding to the brightness of the LED) is used to control the volume of the feedback, in effect working as an amplitude envelope. So the musician can also control this parameter by placing an object between the LED and photoresistor. There’s also a button that can be used to control the rate at which the LED flashes.\n\n\n   \n   The Feedbackquencer constructed on a breadboard\n\n\nThe use of the photoresistor also means that the system is sensitive to the environment. So you can also play the instrument through interactions as simple as turning on and off the light in the room.\nA short performance with the system can be seen below, where a piece of paper was used to control the light reaching the photoresistor.\n\n\n\n\n\nThe Feedbackquencer in action\n\n\n\nAll of the computation and signal processing is carried out by a Bela running a Pure Data patch. The block diagram below shows how all of the various components interact.\n\n\n   \n   A block diagram showing how the Feedbackquencer functions\n\n\nIf you want to try constructing the system and playing around with it yourself, all the software and design documents can be found here.\n\nThis instrument works well enough, but interaction by the musician is limited to control over timbre and the amplitude envelope. Can a more complex system be developed that offers the musician control over many more parameters, enabling much richer interaction with the feedback. Check out part 2 of this post to see the further development of these ideas.\n\nReferences\n\n[1] C. van Eck, Between air and electricity: microphones and loudspeakers as musical instruments. New York London Oxford New Delhi Sydney: Bloomsbury Academic, 2017.\n\n[2]\tT. Holmes, Electronic and Experimental Music, 3rd ed. New York: Routledge, 2008.\n\n",
        "url": "/interactive-music/2022/12/06/hughav-feedbackquencer.html"
      },
    
      {
        "title": "Out-Of-The-Box Sound Sources for your IMS",
        "author": "\n",
        "excerpt": "Exploring alternatives for generating sounds with your interactive music system.\n",
        "content": "Sometimes you don’t want to design the audio generating source completely from scratch in your up-coming Interactive Music System (IMS). Sometimes you’ve already put enough weight on your shoulders in knitting and stuffing your kawaii panda-shaped instrument or planing and polishing your pieces of oak for your pirate treasure chest-themed music box. Don’t despair! There is vendors making synthesizers who speak openly in voltages – just patch in and have a listen. Maybe you’d like some of them to produce the soothing sounds lacking in your system?\n\nBelow is a non-exhaustive survey of a few embedded systems and synthesizers who could possibly speak together through jumper cables.\n\n\n   \n   The beloved Bela.\n\n\nEmbedded Systems\nEmbedded systems are small computer systems consisting of computer processor and memory along with I/O pins for communication with the outside world. These pins could be dedicated analog inputs, dedicated analog outputs, or digital I/O’s. An embedded system would typically have a combination of different general-purpose input/output (GPIO) pins. These connectors typically operate with voltages of either 5V or 3.3V.\n\nLet us not forget that audio in/out jacks essentially are analog I/O connectors as well!\n\nPssst – Keep in mind that it can be possible to simulate extra analog outputs by applying Pulse Width Modulation (PWM) to a digital output (which has a state of either 0 or 1). You can read more about PWM here.\n\nBela\nBela is the embedded system of choice at SMC, because of it’s super low latency (0.5ms action-to-sound latency), and a generous amount of I/O pins. This is a complete system for creating an IMS, where Pure Data is one of the programming languages supported.\n\nI/O\n\n  2x Audio Inputs\n  2x Audio Outputs\n  8x 16-bit Analog Inputs\n  8x 16-bit Analog Outputs\n  16x Digital I/O\n  2x 1W 8ohm Speaker Outputs\n\n\nThe Bela is capable of software PWM on all 16 digital outputs (at once). But since the Bela’s internal “clock” is set to 44.1 kHz, this would work best for control voltage (CV) signals.\n\n  If you want 8-bit resolution you would need to allow 256 samples per PWM period, for a PWM frequency of roughly 172 Hz.\n\n  – ryjobil\n\n\nSampling Rates and Bit Depth\nBelow is a table of the sampling rates and bit depths of the Bela’s different I/O pins.\n\n\n  \n    \n      I/O\n      Sampling Rate\n      Bit Depth\n    \n  \n  \n    \n      Audio\n      44.1 kHz\n      16-bit\n    \n    \n      Analog\n      22.05 kHz\n      16-bit\n    \n    \n      Digital\n      44.1 kHz\n      n/a\n    \n  \n\n\nVoltage Levels\nIt is extremely important to keep voltage ranges in mind, and know the voltage tolerances of the different connector types, as to not fry anything!\n\nIn plain words: Do not connect 5V directly to a digital input!\n\n\n  \n    \n      Signal\n      Voltage Range\n      Voltage Tolerance\n    \n  \n  \n    \n      Digital I/O\n      0-3.3V\n      3.3V\n    \n    \n      Analog Input\n      0-4.096V\n      5V\n    \n    \n      Analog Output\n      0-5V\n      5V\n    \n  \n\n\nOther Systems\nThere are alternatives to Bela, in which I’ll mention a few embedded systems with GPIO pins for inter-connectivity.\n\nArduino Uno\n\n  14x Digital I/O (of wich 6 provide PWM output)\n  6x Analog Input\n\n\nThe Arduino Uno operates within 5V.\n\nRaspberry Pi 4\n\n  26x Digital I/O (of which 4 provide PWM output)\nThere are two channels of PWM which also can provide audio output\n\n\nThe Raspberry Pi 4 operates within 3.3V.\n\nFlipper Zero\n\n  8x Digital I/O\n    \n      of which 6 inputs can be assigned to a 12-bit ADC channel\n      of which 8 outputs provide PWM\n    \n  \n\n\nThe Flipper Zero operates within 3.3V.\n\nExternal Synthesizers\n\nSynthesizers use Control Voltage (CV) between it’s internal modules to operate – a perfect match for our embedded systems! Let’s look into the specifications of a few pocket-sized (give or take) semi-modular synthesizers which uses jumper cables for patching.\n\nKorg Volca Modular\n\nThe Korg Volca Modular is a West Coast (Buchla) style analog synthesizer. To connect with Eurorack-style synthesizers, it supports two CV inputs through a 3.5mm TRS with the following channels:\n\n1) Left: Clipped to +/-5V and converted to +/-3.3V\n2) Right: Expects a 1V/octave signal (0-6V) converted to internal pitch offset voltage control\n\nHowever, we’ll just bypass that limited and connect straight to one of the 50 patch points.\n\nThe Korg Volga Modular operates within 3.3V.\n\n\n   \n   Korg Volca Modular\n\n\nMoog Werkstatt-01\n\nThe Moog Werkstatt-01 is an traditional East Coast style analog semi-modular synthesizer that comes shipped as a kit. This instrument uses jumper cables for patching, although it usually ships with a mountable CV expander, who’ll transform the 20x pin I/O interface to one with 12x 3.5mm TS jack connectivity. Of course we’ll keep all 20 of them, not just “12 of the most useful inputs”.\n\nFor the adventurous, there’s an easter egg under the hood:\n\n  Test Points, Jumpers, and a mini 16 x 6 Experimenter’s pad are provided on the Werkstatt Printed Circuit Board.\n\n\nThe Moog Werkstatt-01 operates within 5V.\n\n\n   \n   Moog Werkstatt-01\n\n\nBastl Kastle &amp; Bastl Kastle Drum\n\nThe Bastl Kastle &amp; Bastl Kastle Drum essentially share the same internal hardware, but with different firmware and exterior design. These are strange, noisy, glitchy, experimental digital modular synthesizers made for exploration. They’re controlled by two Attiny 85’s, one acting as a VCO (for sound generation) and the other as a LFO (for sound modulation). These can be reprogrammed with an Arduino.\n\nFor traditional CV sharing, they sport a 3.5mm TRS jack I/O which can be freely routed. For our purpose, we’ll make use of the 36 pin I/O’s (of which two are linked to the jack I/O).\n\nThe Kastl’s operate within 5V.\n\n\n   \n   Bastl Kastle\n\n\nStart Connecting\n\nNow you have some ideas for “out-of-the-box” sound generation in your IMS. In an upcoming blogpost for my IMS project, I will present how I made use of a Bela and a Bastl Kastle in beautiful(?) harmony!\n\nDisclaimer\nDon’t try this at home! Or, well, please do. But play with this at your own discretion.\n\nThe information and specifications presented in this blogpost is correct to the best of the authors knowledge, but the they take no responsibility for any fried Bela-boards or any other damage caused by applying this information to the real synthesized world. Respect voltage tolerances!\n\nHappy patching!\n",
        "url": "/interactive-music/2022/12/07/kriswent-external-sounds-for-ims.html"
      },
    
      {
        "title": "Creating Complex Filters in Pure Data with Biquad~",
        "author": "\n",
        "excerpt": "One approach to building/rebuilding complex filters in Pure Data, with a little help from Python.\n",
        "content": "An Brief Introduction to Filters\n\nCoffee filters and audio filters work on much the same principle. A coffee filter lets your lovely hot coffee through while preventing bits of bean from ending up in your oat milk flat white. Meanwhile digital filters let through some frequencies (the coffee) while holding back others (the coffee grounds). Audio filters are one of the key components of the computer musician’s toolbox. As musicians, we most commonly think of filtering as a component of mixing or sound design. However, a filter can be applied to any time series data, for example control signals in Max/MSP or Pure Data, physical sensor data, or even geological data or tidal data.\n\nThe most used filter types in musical contexts are low-pass, high-pass, band-pass, and band-stop, which work largely as the names suggest. However, one can also specify more complex filter shapes. For example, suppose we want to correct or ‘flatten’ the natural frequency response of a room or a pair of headphones. A simple low-pass filter is not going to be enough, as there are numerous frequencies that are either boosted or attenuated to different extents. As a result, we will need to employ a filter with a complex response to neutralise the peaks and valleys.\n\n\n   \n   An example of a complex filter response in Sonarworks software, which aims to flatten the frequency response of my headphones.\n\n\nDesigning Complex Filters in Python &amp; Pure Data\n\nDesigning a complex filter in Python is made relatively straightforward using the Signal component in the SciPy library. We simply call an appropriate function, specify our requirements, and the rest is taken care of under the hood. Meanwhile, the Pure Data graphical programming language offers some user-friendly objects for the common filter types mentioned above as well as some more daunting algorithms. However, it does not have a one-stop solution for building complex filters of the sort we can specify in Python. Instead we can ‘cascade’ several simpler filter shapes together to recreate a more complex response. For example, we can use low-pass and high-pass filters together to create a sufficient, if fairly imprecise, band-pass filter. For more precise filter requirements in Pure Data however, we can turn again to SciPy.\n\nRecreating a Python Filter in Pure Data\n\nSciPy’s handy tf2sos() function allows us to break down an IIR filter designed in Python using a function such as  into a series of simpler sections. Each of these second order sections, of which there can be many, can then be interpreted by a biquad~ filter in Pure Data to recreate our complex filter.\n\nThe tf2sos() function returns a list of lists of filter coefficients that define the frequency response of each section. Below is one approach to recreating in Pure data an IIR filter built with SciPy. It uses Python to run a terminal command which opens Pure Data and sends lists of filter coefficients to Pure Data receive objects. These lists which are the interpreted by multiple biquad filters to recreate the original complex filter. This approach allows for filters of IIR filters up to order 16 to be accurately recreated in Pure Data. A small amount of precision is lost in translation due to the difference in accuracy of floating point numbers between Python and Pure Data. However this is negligible for our audio examples below.\n\nIn Python I piece together a terminal command to open the Pure Data patch and pass the filter coefficients as lists using the -send command line option. This option allows me to send directly to ‘receive’ objects in Pure Data by simply providing the name of the receive and the data I want to send to it. In this case, lists of numbers are passed via receive objects to several biquad~ objects to recreate the filter, and the resulting sound is written to the specified audio file.\n\nThe Python code builds the following command:\n\nC:/Program Files/Pd/bin/pd.exe -open tf2sos_biquad.pd -r 48000 -send \"; sec1\n0.7259912605703875 -0.0 0.03806324928680216 0.03806324928680211 0.0; sec2\n1.7126604844375657 -0.7597960137277255 1.0 -1.8856873367725067 0.9999999999999819;\nsec3 1.9117563444113768 -0.9417048830803751 1.0 -1.9555513841911991\n1.000000000000015; input_file ./test_audio/to_be_filtered.wav; output_file\n./test_audio/filtered_pd.wav\"\n\n\nThe code that builds this command is:\n\ndef __pd_build_command(sos_scaled, input_file, output_file):\n    \"\"\"Builds the command given the Numpy array of coefficients and\n    the input and output filenames.\"\"\"\n    \n    # Pd install directory\n    pd_executable = \"C:/Program Files/Pd/bin/pd.exe\"\n    pd_patch = \"tf2sos_biquad.pd\" # The Pd patch to open\n    \n    # Initialises empty list which will contains strings of coefficients\n    section_strings = []\n\n    # Initialises string containing all sends\n    sends = ' -send \"' \n\n    coeff_order = [4,5,0,1,2] # List of new order for coefficients\n\n    # Loops through the second order sections, which are all stored in a Numpy array\n    for sec_i, section in enumerate(sos_scaled):\n        coeff_string = \"\" # Initialising/resetting the string of coefficients\n        for i in coeff_order: # iterates through section in required order\n            # Appends together the coefficients into a string\n            coeff_string = coeff_string + str(section[i]) + \" \"\n        \n        # Appends together Pd message for each section\n        section_string = \"; sec\" + str(sec_i + 1) + \" \" + coeff_string[:-1] \n\n        sends = sends + section_string # Builds up sends string from section_string\n\n    # Finishes sends string by adding input and output filenames\n    sends = sends + \"; input_file \" + input_file + \"; output_file \" + output_file + '\"'\n\n    # Assembles the final command\n    command = pd_executable + \" -open \" + pd_patch + \" -r \" + str(sr) + sends\n\n\nThe command first opens the relevant Pure Data patch, sets the sample rate of the patch (48000, the same as I am working with in Python), and then sends lists of coefficients grouped into their sections. These are each received by their equivalent receive objects in Pure Data – the first section is received in ‘receive sec1’ in Pure Data, and so on. Lastly, I pass the path to the file to be filtered and where to save the filtered result. I can also add -batch -nogui to the command, which allows Pure Data to run offline i.e., not in real time and without opening the interface.\n\nWhile this example only recreates a low-pass filter, this can be theoretically be replicated with any shape of IIR filter designed in SciPy Signal.\n\n\n   \n   The Pure Data patch which receives filter coefficients, filters the audio, and writes the output to a file.\n\n\nOne useful feature of this implementation is its ability to handle IIR filters of various different orders up to 16, which in most cases is a reasonable upper limit for IIR filter order. I included eight second order biquad filters in the Pure Data patch as a 16th order IIR filter was the highest I have encountered so far in Python. However, the tf2sos() function can decompose IIR filters of any order and likewise my Python function which builds the command can similarly handle any number of second order sections. The limiting factor here is the Pure Data patch, although this can be easily expanded to handle larger numbers of sections.\n\nThe audio files and spectrograms below demonstrate how both filtering methods produce perceptually identical results.\n\nAudio before filtering\n\n\n  \n    \n    Alternate Text\n  \n  The audio sample before filtering.\n\n\n   \n\n\nAudio filtered in Python using SciPy Signal\n\n\n  \n    \n    Alternate Text\n  \n  The audio sample filtered using SciPy Signal.\n\n\n   \n\n\nAudio filtered in Pure Data using several biquad~ filter in cascade\n\n\n  \n    \n    Alternate Text\n  \n  The audio sample filtered by the same filter recreated in Pure Data.\n\n\n   \n\n\nDownloads\n\nDownload the Jupyter Notebook, Pure Data patch, and audio files discussed above from GitHub here.\n\n",
        "url": "/sound-programming/2022/12/07/jackeh-python_pure_data_biquad_filters.html"
      },
    
      {
        "title": "A Christmas tale of cookie boxes and soldering",
        "author": "\n",
        "excerpt": "How I got the cookie box drum I never knew I wanted for Christmas\n",
        "content": "The Cookie Box Drum\n\nThe final submission of the SMC Interactive Music Systems course was to create our own Interactive Music System (IMS), or more specifically create some sort of instrument using the Bela, at least three sensors and some funky Pure data code to control it all.\n\nTrying to come up with an idea for my IMS project, I envisioned a hand held electronic drum capable of playing lots of different drum sounds just by pressure control on the skin and movement of the drum.\n\nThis post won’t go into details about how the instrument turned out in the end (You can check it out in action on this video (link coming 12.12.2022)). I thought it might be more interesting to describe how a guy with ten thumbs and the practical sense of an ostrich went about the process of building the physical parts of the IMS.\n\n\n   \n   Spoiler: it got messy. This is just me doing some final tweaks. My family can't wait to have their kitchen table back.\n\n\nPlanning the build\n\nIn my mind’s eye I envisioned this black drum frame, strobe lights blinking, and some super cool logo in pink on the skin.\n\nBut what kind of material to choose? Should I build it from scratch or find some fitting frame and assemble something? I pondered this for a few days, spending time watching brilliant people construct all kinds of crazy stuff on Youtube.\n\nSomehow I finally realized that I had to find something easy to start with, so I could start testing my IMS. The complex construction would be postponed until later.\n\nSo where to go? To the kitchen. Like a kid I took out some different kitchen utensils and started banging anything resembling a drum. And I found my perfect pre-prototype candidate. A Christmas cookie box from last year.\n\n\n   \n   My dear cookie box\n\n\nI taped up my sensors, hooked it up to the breadboard and started hitting it with a drumstick. And it was booming. My piezo sensor was ringing like a church bell. And even though my first ever drumming as a kid happened on a cookie box, I had to admit that it wasn’t very pleasant to hit on bare metal with a drum stick. So I needed padding.\n\n\n   \n   Me 3 years old back in 1983 with my very first instrument, after banging on cookie boxes for a long time. \n\n\nIn a shelf in a CO store I found the perfect material. A black mat, thick yet soft enough to be my drum padding. Only problem was it didn’t have a price tag, and it was quite dirty. When trying to purchase it, the guy behind the desk looked at me like I had some serious issues, and informed me that they didn’t sell that. It was a shelf mat. I pleaded and almost faked some tears telling him I really needed it, and to avoid scandal the clerk told me to take it under my arm and pretend this never happened. So with a perfect drum padding and slightly injured self esteem I went home ready for the next stage.\n\nSoldering\n\nI had soldered some guitar mics and pedals back in the early 00’s, so I thought this part of the build would be like a walk in the park.\n\nTwo days later, three circuit boards in the graveyard and an amount of norwegian swear words not heard since Oddvar Brå broke his ski pole1 I was about to give up. Really.\n\nThe SMC master program has been pretty challenging to say the least, but I’ve never actually considered quitting a course. But facing the challenge of soldering very small components on a tiny board almost broke me. I had to call out for help.\n\nA former collegue of my brother had a father (I knew neither of them) that accepted a zoom call to give me some advice. His first reaction when seeing my board was “Norwegian swearword how did you do that? How shaky are your hands?” He was witnessing a soldering massacre.\n\nBut in the end he gave me some good advice and told me the most important sentence, sort of the moral of this story. “You’ll fix this. It’ll be fine. Just start over again.” And so I did.\n\nOr so I told him. Instead I made a devious plan to unsolder my mistakes and fix my board (so I didn’t have to start from the beginning), and spent the night doing just that. I decided not to test it until the next morning, as then maybe I could get some sleep in ignorant bliss.\n\nIt didn’t work the next morning. I had made such a stupid mistake (which took yet 4-5 hours of precious time to figure out) that I wont even describe it here, and yet another board had to go the graveyard.\n\nBut I sold(i)ered on (sorry), and somehow all this trial and mainly error had improved both my soldering and electronics skills. The final attempt went like a breeze.\n\n\n   \n   My three last boards. The one to the left finally worked\n\n\nFinal build\n\nNovember was a flurry of writing some long papers in other courses, desperately trying to finish all my assignments. I had a working pre-prototype (which was pretty fun to play with), but my vision of the cool pink logo and blinking lights was now just a distant dream. Sleep deprivation started to kick in, and I started to think that a cookie box drum was a pretty cool idea after all. And December came, the Christmas lights were lit in the trees, there were even some snow covering the three square meters of ground I could see out of my workspace window. The Christmas box drum was not just a good idea anymore, it was my only chance.\n\nAnd assembling my board and the bela inside the drum? My drawings of metal tiles, screws and drilled holes in the box?\nDon’t even ask.\n\nNow I’m planning to enjoy the holidays, and whenever I eat a cookie I will have a newfound respect for the box they came from. And I might tap it a bit too hard with my fingers to hear how it sounds. Is there a better cookie box out there? Probably.\n\n\n  If you’re struggling with soldering as me, I found this video helpful. Or ask around, people with such skills are usually nice people eager to share their knowledge. My only advice? Go slow, expect to redo and start from scratch, and don’t give up.\n\n\n\n  \n    \n      A famous national trauma, during a skiing world championship in 1982. &#8617;\n    \n  \n\n",
        "url": "/interactive-music/2022/12/07/arvidf-cookie-box-drum.html"
      },
    
      {
        "title": "Clean code",
        "author": "\n",
        "excerpt": "Any fool can write code that a computer can understand. Good programmers write code that humans can understand.\n",
        "content": "“Any fool can write code that a computer can understand. Good programmers write code that humans can understand.” \n–Martin Fowler-\n\nWhat is clean code?\n\nCoding is part of all scientific work nowadays. Sometimes we need to write short scripts, sometimes we end up having a huge application with many files of code. We usually focus on getting the correct result or having the correct logic in our code but what about readability?\n\nWe all at least read someone else’s code or shared our code with others before. Especially for developers like us who are developing solutions which can be reused in scientific or artistic contexts, it is even more crucial to share the code we develop or reuse a module someone else developed.\n\nDo you recall having difficulty while reading someone else’s code? Every developer spends way more time reading code then writing the code. It means spending a bit more time to write a more readable code saves a lot of time at the end! It even saves time for the code owner because many of us also struggle reading our own code which we wrote just a while ago!\n\nSo writing understandable and readable code is a very big responsibility for us, maybe as important as writing a working code.\n\n\n   \n   From a clean code lesson of Uncle Bob, Robert Martin.\n\n\nThen what should we be careful while writing code? Lets talk about some small things that can change a lot!\n\nName wisely\n\nHow we name our variable, functions and classes are the most important things to pay attention. So you should be understanding a code and what each variable &amp; function does by only reading it.\n\nWe mostly find ourselves adding lots of comments to our code but ideally a code should be self explanatory. If you need to add lots of comments, then probably you have some room for improvement.\n\n“A name should tell you why it exists, what it does, and how it is used. If a name requires a comment, then the name does not reveal its intent.”        \n–Uncle Bob Lessons-\n\nMost SMC students are using a jupyter notebook to develop. I can imagine most of us running cells in a different order and noticing weird behaviours because of having many variables with the same names in different cells like x or y. However if we give variables very clear names about what they are, why they are different, this wouldn’t be an issue.\n\nAvoid disinformation\n\nOnly use specific words if they apply to your case. For example, only have a variable named signalArray if the variable is an “array” or signalNp if the variable is a “numpy array”. If you do otherwise, you will create a wrong impression and it will be harder to follow the code.\n\nKeep your functions &amp; classes short\n\nHaving a very huge class or function will make everyone’s life very miserable. Instead, be councious about having functions that only does one single thing. So that you can also reuse them easier and you wouldnt need to read every line of code in order to understand what a class or function do.\n\n“The first rule of functions is that they should be small. The second rule of functions is that they should be smaller than that.” \n—Uncle Bob, Robert C. Martin-\n\nNot everything has to be in a class. If a function doesn’t need to know anything from a class then keep it outside. Instead of having very big classes, decide the responsibility of each recipe and create multiple. Divide them into files, classes and functions as if you are composing music.\n\n\n   \n   Clean Code Book of Robert Martin, strongly recommended if you are interested!\n\n\nIf you are interested to hear more and like youtube more than reading, then you can also try Uncle Bob’s youtube videos about clean code principles.\n\nIn summary,\nKeep in mind these checkpoints for your code:\n\n  Easy to read\n  Easy to change\n  Well tested\n  Every function does what you expect them to do\n  The Boy Scout Rule: Always leave the code better than you found\n\n\n",
        "url": "/sound-programming/2022/12/08/aysimab-clean-code.html"
      },
    
      {
        "title": "Shimmerion - A String Synthesizer played with Light",
        "author": "\n",
        "excerpt": "Use your phone’s flashlight to make music!\n",
        "content": "\n\nInspiration\nShimmerion was conceived as a combination of two instruments: the Mellotron, an analog tape-replay keyboard used to imitate string ensembles, and the very popular Theremin. I wanted to make a string synthesizer that is controlled primarily without physical contact by the performer. The additional key feature I aimed for was a tremolo effect (AM synthesis), that would make the sound exotic, leaving space for a more experimental performance/experience\n\n\n   \n   Mellotron &amp; Theremin\n\n\nDesign\nShimmerion is designed to be played in the dark, with its pitch controlled by the incoming light from the performer’s smartphone. There are two versions of the main signal, one with and one without an amplitude modulator (dry/ wet). The pitch of the modulator and the amplitude of the “wet” signal are controlled by the distance between the user’s hand and an IR sensor. The “dry” signal’s amplitude is controlled via a knob.\n\nSensors &amp; Controller\nI’m using 3 different sensors to drive the system:\n\n  LDR: controlling the frequency of the “dry” signal/carrier, starting from lower and getting to higher frequencies (100–5000 Hz) as incoming light increases.\n  Rotary Potentiometer: controlling the amplitude of the “dry” signal\n  IR: controlling the frequency of the modulator and the amplitude of the modulated signal, starting from lower and getting to higher frequencies (0.35–10 Hz) &amp; amplitudes as the performer brings their hand closer to the sensor. Both values stay at 0 until the performer’s hand gets into the proximity of the sensor. Then the IR starts outputting to an LED that lights up more as the hand gets closer.\n\n\nThe last integral part required for the interaction is a smartphone with a flashlight, used as the instrument’s controller. Anyone that wants to perform with Shimmerion can use their own smartphone’s flashlight to feed light into the LDR.\n\n\n   \n   Interaction with Shimmerion\n\n\nFabrication\nFor the case that all the components would be placed I decided to use a small paper mailbox, since it can be very easily painted and drilled. I painted the box black and ran the cables through little holes that I drilled with a pair of scissors, placing the sensors in its outer surfaces. At the core of the system is a Bela board, built on a BeagleBone Black computer. I built my own circuit on a solder board attached on top of the Bela to retrieve analog data from my sensors.\n\n\n   \n   Shimmerion's Interior View\n\n\nSound Synthesis\nThe sound synthesis of the instrument was done in Pure Data. Specifically, I used “Synth Strings” from the series of “Rich Synthesis Tutorials” by “Really Useful Plugins”. I tweaked the patch to fit my needs and work with my selection of analog sensors instead of a MIDI keyboard that was the original controller. For the mapping I used Stefano Fasciani’s “scalesiglin~” external object from his external library “sfalib”.\n\n\n   \n   Shimmerion's Patch\n\n\nInteraction Tips\nIdeally, Shimmerion should be played in a dark or at least dimly lit room, with the only light source for the LDR being the performer’s phone. Thus, the user will have complete control of the instrument’s full chromatic range. Additionally, it is suggested that the performer downloads an application to their phone (e.g., simple flashlight), that allows for more efficient control of the light, while also providing additional features such as SOS that can be utilized creatively. Lastly, a white glove on the hand used on the IR can provide better range, since the sensor reflects more efficiently on white surfaces.\n\n\n  \n    \n  \n  Shimmerion Performance\n\n",
        "url": "/interactive-music/2022/12/08/iosifaragiannis-shimmerion.html"
      },
    
      {
        "title": "Concussion Percussion: A Discussion",
        "author": "\n",
        "excerpt": "Whether it’s riding a bike or building an handpan-esque interactive music system, always remember to wear a helmet\n",
        "content": "\n   \n   This poor mannequin's in for a world of hurt, luckily it has a helmet on!\n\n\nIntroduction\n\nIf you use TikTok enough, you’re bound to come across a veritable boatload of videos featuring the steel drum instrument known as a handpan. Known mainly for its calming and aesthetically pleasing sounds, these giant chunks of metal are understandably super expensive. Luckily, through the use of some household objects, sensors, code, and a little bit of pixie dust, we can recreate the experience of playing a handpan AND throw in even more bonus features. Thus I present the bike helmet handpan, more formally known as Concussion Percussion.\n\nHow it Works\n\nHitting the Helmet\n\nSimilar to a handpan, Concussion Percussion allows the user to hit an ordinary bike helmet and play back one of four handpan samples that correspond to where the user hit on the helmet. Users can mix between two different samples, choosing to play either one at a time or a mix of the two. I chose to use piezo discs, which provide the ability to sense when the user has hit on or near the disc, as well as how hard the user hit.\n\n\n\t\n      \n    \n   Handpan Audio Example\n\n\nBending the Flex Sensor\n\nA flex sensor, which is a sensor that returns values  is used to control two different parameters. The first is the pitch of a reverberant drone that plays constantly.\n\n\n\t\n      \n    \n   Drone Audio Example\n\n\nThe second is controlling some background noise when a piezo registers a hit, creating a twinkling effect if you move the sensor fast enough.\n\n\n\t\n      \n    \n   Flex Sensor + Piezo Hit Audio Example\n\n\nTwisting the Knob\n\nThe rotary encoder (or “knob” for the uninitiated) also serves two key functions. The first is adding a delay to the handpan.\n\n\n\t\n      \n    \n   Handpan Delay Audio Example\n\n\nThe second is adding a bit of a harsher texture to the drone. The faster the user turns the rotary encoder, the more bitcrushed the drone becomes.\n\n\n\t\n      \n    \n   Rotary Encoder Bitcrusher Audio Example\n\n\nConstruction\n\nThe positions of the piezo discs on the helmet are arbitrary, since they are quite sensitive and can register a hit from anywhere on the helmet. I positioned them on my helmet so that it feels the most comfortable when playing.\n\n\n   \n   Position of the piezo discs on the helmet\n\n\nThe flex sensor is velcroed to the outside of a glove with the fingertips cut out, which allows the user to still play with that hand without the dampening effect of hitting the helmet with a glove on.\n\n\n   \n   Position of the flex sensor on the glove\n\n\nFinally, I mounted the rotary encoder on the side of the helmet to allow easy access to the user.\n\n\n   \n   Position of the rotary encoder on the helmet\n\n\nVideo Demonstration\n\n\n  \n    \n  \n\n\nSources and Special Thanks\n\nThe handpan samples I used for this project were provided by freesound.org user tarane468.\n\nArt for the comic book explosions can be found here.\n\nThe pitchshifter~ object used for the flex sensor can be found here.\n\nThank you to my mom for letting me borrow the bike helmet :)\n\nFiles\n\nYou can view the Pure Data file for this project here.\n\n",
        "url": "/interactive-music/2022/12/08/josephcl-concussion-percussion.html"
      },
    
      {
        "title": "Cellular Automata - Implementation in Pure Data",
        "author": "\n",
        "excerpt": "Check out the concept of Cellular Automata and my implementation in Pure Data.\n",
        "content": "\n  “If you couldn’t predict what it did, then probably that’s because it was capable of doing anything.”\n – John Conway on his 2-D grid Cellular Automaton called “The Game of Life”\n\n\n\n\nYears ago I stumbled upon an idea in mathematics which caught my attention: Cellular Automata - simple,\neasy to understand systems which are observed to produce complex behavior. I researched on it, had some\nfun with its gamy applications on the web but soon put it in the category ‘filed for memory’. The day came, and\n while I was programming my synthesizer for the course in Pure Data, I remembered. Could this be applied in PD?\n At the end of this blog you can see my Pure data patch in action.\n But wait, what are Cellular Automata\n (CA)?\n\nCellular Automata Explained\nStanisław Ulam and John Von Neumann conceived the idea. In an effort to formulate a theoretical model to describe self-replicating robots(!!!), von Neumann looked for a concept that would work. The  model wasn’t complete until his colleague Ulam suggested using a cell-based approach - Cellular Automata were born. I’ll go over the fundamentals with you in the following.\n\nConsider the idea of a theoretical robot that is a collection of cells on an n-dimensional grid.\nEach cell can be in a finite number of states and has a specific number of neighbours.\nThe state of a cell is determined by the state of its neighbours. If we apply discrete-time steps\n(like generations) each cell state will change in relation to its neighbour states.\n\nLet’s try to imagine the simplest form of a Cellular Automaton.\n\nA one-dimensional grid with cells\n\n\n  \n    \n       \n       \n       \n       \n       \n       \n       \n    \n  \n\n\na cell can either be dead (1) or alive (0)\n\n\n  \n    \n      1\n      0\n      0\n      1\n      1\n      0\n      1\n    \n  \n\n\neach cell’s state depends on its two neighboring cells\n\n\n   \n   \n\n\nLet’s summarize: a CA in its simplest form is a one-dimensional grid of cells that evolves over time.\nEach cell has a state, 0 or 1, dead or alive, and a neighborhood consisting of two other cells.\nStephen Wolfram initially described this most simplest of CA’s in his gigantic groundbreaking book ‘A New Kind of Science’ ,which you can access online for free if you dare.\n\nBut, depending on the neighbour cells, how do we decide a cell’s state? If we look closely, we see that three cells represent a 3-digit number in binary, so 2³ = 8 possible combinations.\n\n\n   \n   \n\n\nNow, we need to define a ruleset to determine the middle cell’s state for the next generation.\nWe use each combination from above once and determine an 8-bit number.\n\n   \n   \n\n\nTurns out there are 256 combinations - 2⁸ – and Stephen Wolfram systematized them by their corresponding binary number.\n\n\n   \n   Excerpt of the Wolfram Code numbering system\n\n\nIf we now add time to the equation and iterate over the grid and change each cell’s state with a new generation we get this. The starting condition is a single cell with a 1 - a seed.\n\n\n   \n   20 generation of Rule 90 in one row\n\n\nWe could also add a new row for each new generation and we get the following for rule 90.\n\n\n   \n   40 generations of Rule 90 \n\n\nThe grid evolves a pattern over time! It is an example of a set of simple rules that can produce\ncomplex behavior and I think the idea is beautiful.\n\n\n   \n   Excerpt of Wolfram's code over 16 generations\n\n \nConway’s game of life\n\n\nI want to introduce an implementation – certainly the most famous one- of the idea on a 2 -dimensional grid – the Game of Life (listen to John Conway’s retrospective on his creation.)\nIt is fun watching it evolve over time, and most importantly,  it is Turing complete which allows it to do this. With a balanced but simple set of rules it is capable of universal computation!\n\nThe following table shows Stephen Wolfram’s behaviour classification for the elementary rule sets according to their outcome over time. The same classification can be applied to 2-D Cellular Automata.\n\n\n  \n    \n       \n      Class1 : Stable\n      Class 2: Repetitive or Stable\n      Class 3: Random\n      Class 4: Complex\n    \n  \n  \n    \n      1 D\n      Rule 222 \n      Rule 90 \n      Rule 30 \n      Rule 110 \n    \n    \n      2 D\n      Aircraft Carrier \n      Gosper Glidergun\n       \n      Switch Engine (initial state)\n    \n  \n\n\nThe concept of Cellular Automata has extensive usecases, for example in pattern recognition,\ntheoretical modelling, encoding (Rule 30 as a pseudorandom number generator) or – Music.\n\nCellular Automata represent a type of computational thinking I can really relate to.\nMy Pure Data patch is a prototype for a final module I like to use for composition. It can only produce rule 90 at the moment, which is a Class 2 CA so it’s repetitive, and consists of 16 cells, but I aim at making it generic depending on input and give it more cells. Anyway, the current state is great for defining rhythmic patterns or melody lines without having too much randomness in there. And it blinks quite nice. Give it a listen down below.\n\n\n   \n   My CA module alone in the wild\n\n\nCA module and parts of other patches play together. Check my github for the file.\n\n\n  \n    \n  \n\n\nReferences\nhttps://mathworld.wolfram.com/CellularAutomaton.html\n\nGame of Life online\n",
        "url": "/sound-programming/2022/12/08/kristeic-cellular-automata.html"
      },
    
      {
        "title": "The Feedback Mop Cello: Making Music with Feedback - Part 2",
        "author": "\n",
        "excerpt": "Using a mop to play the feedback\n",
        "content": "In part 1 of this blog post, the idea of using microphones and loudspeakers was presented with focus placed on the musical use of acoustic feedback. Based upon these principles, I developed two projects over the course of the last few months; The Feedbackquencer, presented in the previous post, and the Feedback Mop Cello, which is presented here.\n\nThe Feedback Mop Cello\n\n\n   \n   The Feedback Mop Cello \n   Left - Bow, Centre - Body, Right - Speaker\n\n\nThe Feedback Mop Cello builds upon the functionality of the Feedbackquencer. While still keeping the central interaction of moving one component of a loudspeaker and microphone combination, the Feedback Mop Cello aims to offer additional expressive capabilities and control intimacy. The musician draws a ‘bow’, which is pointed towards a loudspeaker and upon which is mounted a microphone and accelerometer, back and forth across the mop (which represents the body of the cello). The changes in direction and proximity of the microphone in relation to the loudspeaker determine several aspects of the feedback generated. The motion of the bow (acquired through the accelerometer) is mapped to several parameters, including the loudness of the feedback. But on top of this, the interaction of the bow with the mop influences the sound of the feedback.\n\n\n   \n   The bow. Visible are the microphone and accelerometer mounted on the end\n\n\n\n   \n   A block diagram of the various hardware components and their position on the bow and the mop\n\n\nHowever, there is still the question of pitch. Although the pitch of the feedback can be altered through careful placement of the microphone as well as through the choice of loudspeaker, it can be quite difficult to play precise pitches in a reliable manner. That’s why the feedback signal is passed to a pitchshifter, similar to the Feedbackquencer. The pitchshift is controlled through a ‘neck’, intended to be analogous to the neck of the cello. By placing a finger higher or lower on this neck, the pitch is raised and lowered. This was a custom design, making use of 15 capacitive sensors with a pitch range of two and a half octaves, run from a Trill Craft breakout board. It even allows the use of techniques such as vibrato!\n\n\n   \n   Left - The capacitive sensors on the neck which control pitch, Right - The Trill Craft breakout board which processes the capacitive sensors\n\n\n\n   \n   A block diagram of the software processing of the sensor input\n\n\nBelow is a video of the Feedback Mop Cello in action.\n\n\n\n\n\nJoachim demoing the Feedback Mop Cello\n\n\n\nLike the Feedbackquencer, the Feedback Mop Cello runs on a Bela with the software written in Pure Data. If you want to try constructing the system and playing around with it yourself, all the software and design documents can be found here.\n\nWhy the Cello?\n\n\n   \n   Why use this instrument as a control interface for manipulating acoustic feedback? \n   From: www.freepik.com\n\n\nAlthough the Feedback Mop Cello offers an approach to an instrument based around the use of acoustic feedback, an immediate question is why present it in the form of a cello? The reason for this is also related to the goals of offering additional expressive capabilities and control intimacy. The cello and the physical gestures required to produce musical sound with it are a known quantity. By basing the input gestures of the Feedback Mop Cello on these, the goal was to gain a head start on achieving these goals.\n\nThis is also related to the concept of transparency in Digital Musical Instrument (DMI) design. Transparency can be thought of as an understanding of the distance between the gestural input and sound output of a DMI [1].\n\n\n   \n   The components of a DMI, showing the mapping \n   Reproduced from [2]\n\n\nAs instrument designers, we can arbitrarily choose how these inputs and outputs are linked (this is referred to as the mapping). To take a simple example, I could decide whether the mapping of finger position to pitch on the Feedback Mop Cello maps a higher finger position to a higher or a lower pitch. If the distance between the input and output of the instrument are not intuitively easy to understand (i.e. the mapping is opaque instead of transparent), it is difficult for the musician to intuitively express their musical ideas on the instrument. Moreover, it becomes very difficult for audiences to understand how the instrument works. What is the physical gesture which is creating this sound, or is the musician even creating the sound through their gesture at all?\n\nA solution to the difficulty in creating transparent mappings is to rely on commonly understood metaphors within the wider culture. For example, we often use one dimensional spatial metaphor for pitch, such as conceptualising lower pitches to the left and higher pitches to the right. All of the various metaphors we have add up to a cultural literature [1], a shared understanding that members of the culture have. If we base the mappings in this cultural literature, we can increase the transparency as there is a shared understanding by the musician and the audience of how the instrument should and will create sound based upon the musician’s input gestures. Therefore, by basing the design of the Feedback Mop Cello on the actual cello the aim is to create an instrument which presents interaction with acoustic feedback in a manner more transparent than the sole use of a single microphone and loudspeaker. If you understand how a cello works, you should understand how the Feedback Mop Cello works. Take a look at the video of a performance below and decide how successful this was!\n\nVideo Coming Soon!\n\nReferences\n\n[1]  S. Fels, A. Gadd, and A. Mulder, ‘Mapping transparency through metaphor: towards more expressive musical instruments’, Organised Sound, vol. 7, no. 2, pp. 109–126, 2002, doi: 10.1017/S1355771802002042.\n\n[2] G. Emerson and H. Egermann, ‘Exploring the motivations for building new digital musical instruments’, Musicae Scientiae, vol. 24, no. 3, pp. 313–329, Sep. 2020, doi: 10.1177/1029864918802983.\n\n",
        "url": "/interactive-music/2022/12/08/hughav-feedback-mop-cello.html"
      },
    
      {
        "title": "How to make your screen time a natural experience",
        "author": "\n",
        "excerpt": "Learn how playing with mud could equate to playing with computer.\n",
        "content": "For every year that passes new digital gadgets are introduced into our lives. Some sources indicate that a third of Norwegian adults spend between 3-6 hours just on their phone every day. Add onto it computer time, and the result is a seriously large portion of our day pushing and swiping on screens and buttons. Simultaneously we are told that we should spend more time in nature. What if you could combine both?\n\nIntroducing the Mud Tub\n\n\n\nIn the creator Tom Gerhardt’s own words, the Mud Tub “is an experimental organic interface that allows people to control a computer while playing in the mud.” Yes, you read that right. By sloshing around in wet mud, you may control your computer. Thus bridging the gap between nature and digital technology.\n\nHow it works\n\nThe bottom of the mud-filled tub is lined with pressure sensors. When you move around on the mud or apply weight on it, the program interprets this information and translates it into computer directions. It could be used to control video games, move a cursor around, and click virtual buttons. It could even be made into a musical instrument by hooking on some sounds! Granted, this would take some effort to achieve. However, the design in itself is simple enough and could be replicated by anyone with some effort. The Mud Tub is programmed in Processing, a programming language often used by newcomers to the world of coding. So why not make this a fun project?\n\nSome inspiration\n\nCheck out Tom Gerhardt’s own video on his creation:\n\n",
        "url": "/interactive-music/2022/12/08/olethan-Mud-Tub.html"
      },
    
      {
        "title": "SR-01",
        "author": "\n",
        "excerpt": "The climate aware synthesizer. Based on using few components while reacting to changes in light and temperature around it, causing it to sound different today than in a changed climate.\n",
        "content": "\n   \n   SR-01\n\n\nThe SR-01 (SurRound-01) is a synthesizer that uses a capacitive bar (Trill Bar) to control parameters. This allows for a one-to-many mapping strategy that makes it possible to control a set of parameters using one sensor instead of many knobs and switches seen in conventional synthesizer design. In addition to this, SR-01 uses a light sensor and a temperature sensor to capture the environment in which it is being played. The idea is to introduce some degree of unexpectancy which challenges the performer to react to the output given by the instrument, as well as afford exploration of sound.\n\nThe Sound Engine\nThe sound engine is duophonic with separate amplitude envelopes. It has two modes for oscillation, one that is based on west coast synthesis combining wavefolding and FM inspired by the Buchla Music Easel, the other one based on a classic east coast synthesis inspired by the Moog synthesizers.\n\nVideo Demonstration\n\n\n  \n    \n  \n\nSR-01 demonstration video performing Sonata No.1 in G Minor by J. S. Bach.\n\n\n   \n   Signal flow of the SR-01.\n\n\nTrill Bar Touchpoint Mapping\nThe Trill Bar can detect up to five touch points as well as touch pressure.\n\n\n  \n    \n      Touch Points\n      Parameter\n      Pressure Parameter\n    \n  \n  \n    \n      1\n      LPF Cutoff\n      Filter Envelope Amount\n    \n    \n      2\n      Osc Mod 1\n      Osc Mod 2\n    \n    \n      3\n      Attack - Release/Decay\n      Sustain\n    \n    \n      4\n      Finger Spread Pan\n      -\n    \n    \n      5\n      Reverb Time\n      Mix\n    \n  \n\n\nA total of ten parameters can be accessed from the Trill Bar. An issue is the sensitivity and size of the touch strip. Sometimes it glitches and jumps rapidly back and forth between parameters, or even senses touch points that aren’t there.\n\nAnalog Cassette Modulation\nI recorded a sine wave at 110 Hz onto a cassette through a Tascam 414 mkii, cut it to a loop and cooked it at eighty degrees celsius for one hour. I used a dictaphone cassette recorded to play it back through a pitch tracker patch using sigmund~. I offset the data so that the pitch was zeroed out and the only thing left was the changes in pitch info done in the heat treatment. This was recorded to a file which is being used as a modulation source to create analog modulation on pitch. For me this was a way of applying cassette tape as a sensor for capturing heat exposure over time. This can also be seen and heard live in the FAM audio demonstration.\n\n\n   \n   Cassette cooked for one hour at eighty degrees celsius.\n\n\nThe Environmental Sensors\nLDR and TMP36 sense the surroundings and are mapped to sound parameters. The mean of the two control which style of synthesis is being used. If it is warm and bright it uses west coast inspired synthesis and for cold and dark it uses east coast inspired synthesis. Temperature is captured between -4C and 36C, and light is captured from pitch black to the equivalent of direct sunlight. The temperature controls the amount of recorded analog cassette modulation is applied to the pitch and reverb. The warmer it gets the greater the motion of atoms, which is linked to sound in the SR-01 via modulation. Light controls the speed of this modulation linked to the energy in brightness of the instruments’ surroundings.\n\nMotion Recorder\nInstead of including a set of LFOs to free the user from constantly accessing the desired parameters, the SR-01 has a way to record the motion carried out on the selected parameter. After a motion is recorded the motion data gets looped over recorded time, only interrupted by either accessing the parameter manually via the touch strip for contemporary control, or double pressing on the record button while a parameter is accessed through touch points which disables the recorded motion for that parameter. My experience so far is that this is a very efficient way of doing LFO mapping, which I think can be applied to more conventional synthesizer designs as well.\n\nThe Reverb\nThe reverb is based on a set of stereo delays that have unique factors of the global delay time. These delays get modulated by the cassette modulation signal to blur the repeating delays. This is an alternative way to mimic the way sound waves travel when reflected in a room without doing vector calculations of the wave in time and space in a delay mesh. The result is a quite lush and dreamy reverb in the fashion of Valhalla’s Super Massive. The quality of the reverb resolution can be set by adjusting the instance number of delay lines. The two forms of synthesis have their own reverb settings which are morphed by changing temperature.\n\nSave/Load State\nThrough user testing the SR-01 received feedback that it would be nice to be able to save the state of the settings and then load it back at a later point. This draws parallels to Elektron’s way of saving and loading the settings on the Digitakt and Digitone instruments, which introduces both exploratory and performative features. For implementation it was possible to add this feature using the already implemented button. When double pressing the button with no touch points on the Trill Bar, the state is saved, whereas a single press will load the previous saved state. The integrated button LED blinks twice when saved and one long blink when loaded.\n\nFuture Improvement\nSince the Trill Bar is somewhat small in size, using something like the Trill Square could be interesting. This also opens up for an XY styled parameter control and corner based mixing allowing to mix between four different sources using one motion. The current mappings of the environmental sensors offer one approach to how the sound is affected by the surroundings. This concept can be explored further for different mapping styles.\n\nAudio Demonstration with Light and Temperature\nThe piece Velocity of Love by Suzanne Ciani performed on the SR-01 in dark and cold (Part 1), bright and cold (Part 2), bright and warm as well as dark and warm (Part 3).\n\nFirst without reverb.\n\n  \n    \n  \n  Part 1 00:00 - 0:23, Part 2 00:23 - 00:46, Part 3 00:46 - 01:48\n\n\nThen with reverb which is also affected by temperature.\n\n  \n    \n  \n  Part 1 00:00 - 00:20, Part 2 00:20 - 00:46, Part 3 00:46 - 01:58\n\n\nYou can download the SR-01 here.\nThank you.\n",
        "url": "/interactive-music/2022/12/08/henrikhs-sr01.html"
      },
    
      {
        "title": "Three Takeaways as a musicologist",
        "author": "\n",
        "excerpt": "Recounting the experience of making an instrument from scratch for the first time.\n",
        "content": "What if you could design novel sounds with a flashlight and your bare hand? This is an idea I decided to pursue with my first ever DIY digital instrument. I decided  to baptize it the “Ferelumen”, a word containing both FM (its sound synthesis method) and lumen (a popular measurement of light). Furthermore, it vaguely sounds like “Theremin”, a well known modern instrument that initially inspired the gestural control of my instrument.\n\nBeing an outsider (musicology rather than SMC master student) looking in, this course has been immensely challenging, as well as immensely rewarding. I decided to hop on this course as an extension to my research area of interest, interactive composition for video games and other virtual/reality-intersecting spaces.\n\nPlanning, designing, sketching, programming, and then actually soldering all the necessary parts into a coherent piece of equipment, is no small feat. Let alone in a few months. The amount of “real-life” programming and circuitry experience I’ve amassed through keeping up with this course has been worth its weight in gold.\n\nThree takeaways!\n\n  Take the amount of time you think you need making something and triple it.\n  Be humble in the vicinity of electronic circuits. You won’t always understand what went wrong. And things will go wrong.\n  There is no better feeling than finally getting your stubborn creation to work. And it makes all the struggle worth it.\n\n\nSo, what is my thing?\nIt’s a cardboard box with some sensors attached and a small computer inside. So simple, yet so severely difficult. However, it’s also a handcrafted FM synthesizer controlled with light and motion. By shining a flashlight on the instrument from different angles and distances, different timbres and textures emerge out of the speakers. By waving your hand up and down, some other parameters influence the timbral qualities further. Some parameters are also influenced by both motion and light simultaneously, hopefully making the experience somewhat exploratory. Lastly, a pressure sensor on the flashlight determines amplitude ranging from silence all the way to gnarly wavefolding distortion. Thus, amplitudal dynamics are present in a tactile manner.\n\nPictures and video of the instrument will be inserted here promptly\n\nThe result is gnarly-sounding synthesizer good for creating otherworldly sounds and abstract performance pieces. It could probably work great in audiovisual applications and as an accompaniment to experimental dance or theater.\n",
        "url": "/interactive-music/2022/12/08/olethan-Three-Takeaways.html"
      },
    
      {
        "title": "Pringles, I love You (not)",
        "author": "\n",
        "excerpt": "No, that is not true. I do not like Pringles. But I like the tube it comes with! That’s why I invited a friend over to eat the chips, so I could use the tube for my 4054 Interactive Music Systems project.\n",
        "content": "\n   \n\n\nSo what did I actually make? The creation is called WoVi, meaning wind-driven violin. It functions by blowing into a microphone for amplitude, and controlling pitch with a  soft membrane potentiometer. Not unlike how EWI’s work, but with a simplified design.\n\n\n   \n\n\nConstruction\nThe following parts were used:\n\n\n  \n    \n      Part\n      Use\n    \n  \n  \n    \n      Bela Board\n      Core of operations\n    \n    \n      Empty pringles box\n      Housing of all electronics\n    \n    \n      Electret Microphone\n      Amplitude controls\n    \n    \n      Soft Membrane Potentiometer\n      Pitch controls\n    \n    \n      Push button #1\n      Not in use\n    \n    \n      Push button #2\n      Key Selector\n    \n    \n      Push button #3\n      Reset/rest\n    \n    \n      Triple Axis Accelerometer Module (X axis)\n      Vibrato\n    \n    \n      Triple Axis Accelerometer Module (Y axis)\n      Not in use\n    \n    \n      Triple Axis Accelerometer Module (Z axis)\n      Not in use\n    \n  \n\n\nA Pringles tube has a inner dimensions of 70mm. The Bela board is 55mm wide, leaving little room for all of the electronics. An extra board had to be constructed so it could facilitate all of the sensor connections.\n\nAll of the sensors in the table above were implemented. To connect them, cables and more breakout board were created.\n\nIt helps to plan and prototype how everything fits together. This makes soldering quite simple. A total of one day was used. Luckily, few errors were made due to good planning.\n\n\n   \n   The board in its final form factor\n\n\n\n   \n   The board with room for the Triple Axis Accelerometer\n\n\n\n   \n   Wiring diagram used\n\n\nThe original idea had the buttons on top of the tube. This was not feasible as the first prototype was broken when drilled into. So a second tube of Pringles had to be eaten!\n\n\n   \n   Original sketch\n\n\nMapping and controls\nThe microphone controls amplitude of the pitch. It sends control data to the master level, so when the microphone does not register any input, the instrument is quiet. When blown or scratched, amplitude goes up so the pitch can pass.\n\nTo control the pitch, there are 15 scales installed in the instrument; 12 keys so its possible to play in major and minor in all keys, and 3 blues scales. The second button cycles through all keys, letting you know what key is selected. The third button is a bypass button, so the instrument can rest.\n\nAn accelerometer has been added, so when the instrument is lifted upwards, a vibrato is applied. It does not affect the signal when in resting position horizontally.\n\n\n   \n   ChatGPT's take at the problem... Not so far from reality! (https://openai.com/blog/chatgpt/) \n\n\nHow does it work in practice?\nHere you can see a video of it in use. The first key is C major. But to get to the Blues, I have to scroll through each other key, because its at the end. When pitching the instrument up in the air, a vibrato is added. A voice is added to indicate what key you are in (for those who does not have absolute pitch).\n\nA soft membrane potentiometer is not reccomended to use if you want fidelity and fine control of your instrument. As you can see in the video, it drifts a lot from the intended playing note. That is why it is a good idea to define what key you want to play in, so you can have a chance to play correctly.\n\nThe amplitude control is not perfect either. Does not only react to wind, but all other noises. A low-pass filter and other methods were tried out. If you want to see the signal processing in more detail, check out the documentation here!\n\n\n  \n    \n  \n\n\nExtra documentation\nThe projects software and instructions can be downloaded on Github.\n",
        "url": "/interactive-music/2022/12/08/jakobhoydal-PringelsWovi.html"
      },
    
      {
        "title": "The sound of rain",
        "author": "\n",
        "excerpt": "How we sonified the rainfall data.\n",
        "content": "\n\nData driven sonification is bringing us huge opportunities regarding experimenting sound of “things”. Things can be an unemployment rate as well as the number of deaths everyday. It can be anything.\n\nAs SMC Master students, we wanted to sonify rainfall timeseries data for our python project and apperantly rainfall has a good understanding of harmony!\nFor our project, we used rainfall timeseries data from kaggle which was collected from Power Data Access Viewer.\n\nAs it is described in kaggle website: \nThe POWER Meteorological data is prediction or observation given by NASA’s GMAO MERRA-2 assimilation model.\nThe data collected is monthly frequency data for a particular latitude and longitude in Mumbai for the period 2000 – 2020. The data consists of the following variables:\n\n  Specific Humidity\n  Relative Humidity\n  Temperature\n  Precipitation (The data consist of precipitation as monthly sum of rainfall )\n\n\n\n   \n   Do you hear rain singing?\n\n\nSound corpus\n\nFirstly we created sound corpus which had three synthesis. Two synthesis were two different musical lines in A Minor and one was the combination of chords responding to the same musical line. As first job for sonification, we calculated the duration of each note from the corresponding midi file by checking start and end time of the first note. Later we used this duration in order to slice the wav file of musical line into multiple wav files of each single note.\n\nAt this point, we had 7 wav files for each synthesis with the note names A, B, C, D , E, F and G.\n\nMapping rainfall data\n\nAs second step, we extracted some arrays from our rain data like temperature, humidity and perceived temperature which was a multiplication of specific humidity and temperature. Afterwards we took these values as frequency and mapped them into midi numbers. By doing that, we were able to use “note_number_to_name” function of “pretty midi” library, in order to get note names for each array item.\n\nAt this point, we had multiple rain data which were mapped to note numbers and three different corpus data of wav files with note names.\n\nSonification process\n\nAs a final step of our sonification, we created three musical lines based on three synthesis and array data. For each corpus and rainfall array, we started iterating on the array and appending the sound of that note. One sonification was created by “temperature” data while the other two were created by “relative humidity” and “preceived temperature” data and all the data was ordered by time.\n\nAnd there was our sonification! We summed three output signals and had one sound which was generated by rainfall data! We built conductors from rainfall data features and they played a song for us together!\n\nWe had some cracks between sounds which we couldnt fix yet, but we hope you enjoy listening it!\n\n  \n  Your browser does not support the audio tag.\n\n",
        "url": "/sound-programming/2022/12/09/aysimab-sonification-over-rain-data.html"
      },
    
      {
        "title": "Interactive Music Systems, Communication and Emotion",
        "author": "\n",
        "excerpt": "Talking about IMSs, expressing and even inducing emotions.\n",
        "content": "A definition for Interactive music systems (IMSs) is “those [systems] whose behavior changes in response to musical input” [1]. The more technology advances and becomes more accessible, the easier it is for everyone to get involved and create new ways to compose and interact with music. As artists do with every other art form: creating to communicate.\n\n\n   \n\n\nCommunication is an important aspect I like to contemplate when considering the possibilities of building new instruments. Specially the communication of emotions. Part of what defines a IMS as such, is the ability to be expressive and/or to amplify expression. The expression of creativity and the expression of emotion.\nIf we look at IMSs from this perspective of emotion, we can apply psychology to IMS design in order to more effectively create a system that will express or even induce certain emotions purposefully.\nIn fact, there are already some IMSs that have been created for this same purpose: inducing specific emotions.\n\n\n   \n\n\nOne example of this is the SiMS, a situated interactive music system created to explore the relationship between physiology, brain activity, musical features and emotion [2]. In order to do this, the authors proposed a software framework “based on solid psychophysiological and psychoacoustical principles” [2].\nTo put some examples, they used the heart rate data to measure for attention and arousal (a dimension of affect). Apparently, attention evokes the deceleration of heart rate, while arousal evokes the opposite effect. Other interesting examples are:\n\n  Increases in tempo or volume are correlated with valence and arousal.\n  Shortening of articulations correlates with increased valence.\n  Brightness and noisiness (or lack thereof) correlate with arousal.\n\n\nSo the authors took all of this knowledge and created a psychologically based IMS that generates music to induce specific emotions. If they wanted to induce a calm and relaxed emotion (which would equal high valence and low arousal) they would modify the already mentioned elements so that the music generated met the required criteria. This translates into the music generator being set to low brightness and noisiness, low tempo and volume and long articulations.\n\n \nApart from the SiMS, there are other similar IMSs such as the EmoteControl [3] or the Emotion Box [4] that are, as well, based on psychology to be as effective as possible with expression of emotions.\n\n\n   \n\n\n \nBeing a singer, I use music as a means to express my emotions, whether they be positive or negative. That is one of the main reasons why music is so important to me. It allows me to express feelings that, otherwise, I wouldn’t know how to express. Which is why being able to interact with instruments/systems that enhance my ability to be expressive is so appealing to me.\nHowever, and as the authors mention in their paper [2], these emotion-centered IMS can be more useful than just that. These systems can become efficient tools in music therapy and help people. Because, in the end, creating is always for the sake of people. Whether it helps its creator or the people it affects as an audience, art is always human-centered.\n\n \n\n \n\n \n\n \n\nReferences\n[1] R. Rowe, “Chapter 1: Interactive Music Systems” in Interactive Music Systems, 1993. https://wp.nyu.edu/robert_rowe/text/interactive-music-systems-1993/ [Accessed Dec. 09, 2022]\n\n[2] S. Le Groux and P. Verschure, “Situated Interactive Music System: Connecting Mind and Body Through Musical Interaction” in Proceedings of the 2009 International Computer Music Conference, ICMC, 2009, San Francisco, CA, USA. https://www.researchgate.net/publication/209435990_Situated_Interactive_Music_System_Connecting_Mind_and_Body_Through_Musical_Interaction/stats\n\n[3] A.M. Grimaud and T. Eerola, “EmoteControl: an interactive system for real-time control of emotional expression in music.” Pers Ubiquit Comput 25, 677–689 (2021). https://doi.org/10.1007/s00779-020-01390-7\n\n[4] K. Zheng, R. Meng, C. Zheng, X. Li, J. Sang, J. Cai, J. Wang and X. Wang, “EmotionBox: A music-element-driven emotional music generation system based on music psychology”,  Front. Psychol. 13:841926. doi: 10.3389/fpsyg.2022.841926\n",
        "url": "/interactive-music/2022/12/09/sofiagon-emotion-IMS.html"
      },
    
      {
        "title": "Expressive Voice: an IMS for singing",
        "author": "\n",
        "excerpt": "Take a peak at my IMS and the reasoning behind its design.\n",
        "content": "Some years ago now, I went to a concert in which the singer used a megaphone in front of their microphone to sing a part of the song. I remember thinking back then that it was such a cool concept to get a different sound from your voice…As a teenage singer, I had never thought of modifying my voice live. And now, years later, I got to implement that concept in a IMS.\n\n\n   \n\n\nI designed this IMS with the goal of giving more control to the singer (in this case, me) to enhance the expressiveness of their voice, (the reason why I chose its name). By allowing singers to add effects in real time, I wanted to expand what they can control from their performance to augment the emotions they want to transmit. But, more importantly, I wanted to expand what I could do in my performances. Yes. This IMS was somewhat of a self-indulgent project.\n\n\n   \n\nThis is what Expressive Voice looks like. Even though it’s not the most aesthetically pleasing design you’ll ever see, it fulfilled its purpose.\n\nTo better understand what it does, I’ll present the sensors and their functions:\n\n\n  The rotary potentiometer, more commonly known as a knob that regulates a distortion effect.\n  The sliding potentiometer, a.k.a. slider which regulates the amount of reverberation.\n  And the force resistor sensor (FRS) or what I usually call pressure sensor, which regulates a delay effect.\n\n\nAs for the effects, I chose the ones I thought were more common and that I considered myself capable of implementing (not without tutorials, that is). On the other hand, the reason why I chose these sensors among the different and more complex ones I had available was mainly because\n*singing is hard *.\n\nSinging in itself, like playing any other instrument, is already a complex task. But I, personally, need to concentrate a lot for singing. In the past, I was diagnosed with chronic pharyngitis (inflammation of the pharynx). Thankfully, after going to singing lessons, I stopped suffering pharyngitis, yay! However, I still struggle with my throat and need to think a lot about how I position it to avoid hurting it. Even when I talk. In fact, it’s been a few years now since I started playing guitar (at a basic level), yet I’m still unable to sing while I’m playing for long before I notice that my throat is sore.\n\nThe point I’m trying to get to (after all this over-sharing) is that singers, in general, and me, more specifically, need to be able to focus their concentration on singing. That is a big part of why I went with the knob and the slider, which are pretty common and straightforward sensors. As for the pressure sensor, it is a more interesting sensor and it does take a little more of attention than the other two. It is a little bit sensitive, so you need to apply just the right amount of pressure depending on how much intensity you want, which can take some thinking and practicing.\n\n\n   \n\n\nSo, basically, since I wanted to work on a project that really motivated me, I went for a design specific to my needs as a singer. Even though this kind of extended-microphone IMS is not a new concept at all [1]. But, as I mentioned in my previous post about IMSs and emotions [2], it is very important that IMSs become more accessible and that people create more of them. Someone might build one that caters to specific needs (whether they are aesthetic or for, for example, differently-able people). Someone might create one that aids people through music therapy. Or, simply, because someone will be able to carry out an idea that occurred to them in a concert, which they thought was very cool.\n\n \n\n \n\n \n\n \n\nReferences\n\n[1] Y. Park, H. Heo, K. Lee, “VOICON: An Interactive Gestural Microphone For Vocal Performance” in NIME, 2012. https://www.nime.org/proceedings/2012/nime2012_199.pdf\n\n[2] S. González, “Interactive Music Systems”, in The SMC Blog, 2022. https://SMC-master.github.io/interactive-music/2022/12/09/sofiagon-emotion-IMS.html [Accessed, Dec. 09, 2022]\n",
        "url": "/interactive-music/2022/12/09/sofiagon-expressive-voice-ims.html"
      },
    
      {
        "title": "Towards a Claptrap-Speaking Kastle Maus",
        "author": "\n",
        "excerpt": "Once upon a time, there was a maus living in a kastle..\n",
        "content": "Introduction\n\nIt’s the most beautiful time of the year! The time of the year when SMC projects are due and exams keep competing for your attention. As for the beautiful part of it, this means that the SMC perimeters will showcase telematic performances and instruments you didn’t knew you wanted to experience.\n\nThis is the story of my project in Interactive Music Systems, trough it’s early stages and aaaaalmost to it’s final iteration. I hope you enjoy it!\n\n\n   \n   Say cheese to Maus.\n\n\nThe Maus\n\nMeet Maus! Maus is a puppet. He competed against other puppets in demanding its owner’s attention. Some of it’s good qualities are:\n\n\n  two flexible arms\n  a mouth who can open and close (completely)\n  a head that can nod, with a secret compartment for an accelerometer\n  a talkative, claptrap-speaking mood\n\n\nMaus’ mouth is my personal favorite. I could lay for hours (if I didn’t have exams upcoming) and listen to his non-soothing, non-comforting, algorithmic - yet slightly randomized - babbling. His mouth also houses a light sensor. This is essential to my well-being, as this allows me to filter out some of his foulest words and worst ramblings, using a low-pass filter in Pure data of course. The arms are filled with one flex sensor each. As Maus is a profound gesticulateer, this will alter the jargon and dialect of his non-verbal output. As he is not the biggest fan of word-shaping, this will instead affect the wave-shaping, and also timbre, of his voice. As for his head, this is more of a mystery to me. One thing I know is that it can be manipulated in space, but I haven’t figured out how to use this to my advantage yet. I’m sure I’ll get there. I also know that his brain is just a mere accelerometer. This might be controlling the speed of his flapdoodling, but it might also just be an big empty void (which in case might prove excellent for spacious reverbs).\n\nExcept from his knack to speaking loud, (don’t get him started on his opinions on modular synthesis or he’ll never stop,) he is a great Maus!\n\nThe Kastle\n\nThe Kastle is where the Maus resides. It originates from the Czech Republic, but is now located next to the SMC Headquarters. To Maus, this is more than just a home, it’s a place of refuge. It’s a place where he can be himself, without being judged. It’s a place where he can be free. It’s a place where he can be Maus. The Kastle is magical, and the true inspiration for all of Maus’ gibberish.\n\n\n   \n   The (Bastl) Kastle.\n\n\nBesides being a place for refuge, the Kastle is also a place for music. It’s an analog synthesizer accounting for the essence of what Maus is communicating, and is interconnected with a Bela board. A few other sounds are generated and modulated in the Bela through a Pure data patch. This patch is the true puppet master, taking care of all the sensor data and mapping it either internally or to the Kastl. The randomized sequencer in the Kastl is also sent back to Pure data to control some secret magic only Maus will know about. As the sequencer will have to take responsibility for the randomness of Maus’ nonsensical sentence construction, I will briefly list it’s three states which is controlled by Maus’ waist bag:\n\n\n  Semi-random 8-step pattern (Ground)\n  Semi-random 16-step pattern (Floating voltage)\n  Ever-changing random pattern (5V)\n\n\nThis works in the way that by connecting the sequencer to ground or floating voltage, the randomly generated sequence will be “captured” in a 8-step or 16-step pattern. The output of the sequencer is really just 8 different voltages which is mapped to the synthesizers pitch, and also routed back to Bela for some aforementioned secrets. According to Bastl, “the STEPPED generator is inspired by the Rungler circuit by Rob Hordijk.”\n\n\n   \n   The actual Castle Maus (Burg Maus). Photography by Felix König.\n\n\nStatus Quo\n\nToday is Friday the 9th of December and there is still 3 days until deadline. My mood is toggling between stressed and composed, just like Maus’ waist bag is doing to him. Everything is prototyped and soldered, all sensors work, and the Bela is running. I’ve got bidirectional connection between the Kastle and Bela, while Maus himself have been sitting patiently on the sideline. It’s not long until I’ll look back to these moments of quiet serenity with rejoice, as I won’t be able to silence his mumbo jumbo anymore, though..  Below is a video documenting the work-in-progress this week, with raw unfiltered audio and temporary LFOs in pure data modulating the Kastle instead of Maus’ sensors.\n\n\n  \n    \n  \n  Work-in-Progress after the soldering was done, and everything (except sensors) spoke gobbledygook fluently to each other.\n\n\nThe next step for me now is to securely attach the sensors inside of Maus without any broken parts inducing “gråt og tenners gnissel”, as we are prone say in Norwegian. (Which would translate to something like crying and gnashing of teeth.) I’ll also have to work on the software working behind the scenes, to make this experience as pleasant as possible. There’s a thought gnawing like a mouse in the back of my head though, curios of transforming the Maus into a beat-boxer instead of claptrap-speaker by switching to the Bastl Kastle Drum. If there had been a comment section below, I’d ask for your opinion. In any case, I can’t wait to hear what Maus has to say!\n\nThis blogpost was sponsored by a few suggested sentences by GitHub copilot too fitting to be discarded.\n",
        "url": "/interactive-music/2022/12/09/kriswent-towards-a-claptrap-speaking-kastle-maus.html"
      },
    
      {
        "title": "Sonifying the Northern Lights: Two Approaches in Python & Pure Data",
        "author": "\n",
        "excerpt": "Making ‘music’ from the Aurora Borealis.\n",
        "content": "\n   \n   A fun image the DALL-E AI gave us with the prompt 'dido in space'.\n\n\nIntroduction\n\nThe weekend of 31 October 2003 passed like any other Halloween, unless you happened to be in Spain or Texas. If you did, you would have seen something you might have never seen in person before: the Aurora Borealis. Halloween 2003 is the period of highest intensity of solar winds, which cause the Northern Lights, since modern records began. Your three authors have spent two weeks deeply immersed in the solar winds data from the week of Halloween 2003, which saw possibly the biggest solar event since 1859.\n\nOur final two group assignments for SMC4001 were to sonify a dataset using both Python and Pure Data. Here is a deep dive into how and why we sonified this data in the ways we did.\n\nAbout Our Dataset\n\n\n   \n   Our raw data as plotted by the Space Weather Prediction Center.\n\n\nWe pulled our data from the Space Weather Prediction Center’s online dataset which has data going back to 1998. The SPCC, which is operated by the US National Oceanic and Atmospheric Administration, collects solar wind data from two different measuring satellites positioned around 1,500,000 miles / 2,400,000 km forward of Earth, as well as various Earth-based measurements. Their website allows us to get readings from every minute for the last 24 years, so we pulled the data from the period of 27 October - 2 November 2003, a period of 7 days, and then performed data cleaning that includes removing erroneous measurements and interpolating to fill the gaps. Finally, we computed 5-minute averages, which resulted in 2016 individual readings across the period. Each reading contains 10 different data points, of which we used 7 in each sonification.\n\nThe data we used is shown in the table below:\n\n\n\n  \n    Data Type\n    Data Point\n    Units\n    Measured from\n    What does it mean?\n  \n\n\n  \n    Physical\n    Proton density\n    /cm3\n    ACE satellite\n    The density of the protons in the ionised gas that makes up the solar winds.\n  \n  \n    Ion Temperature\n    Kelvin\n    ACE satellite\n    The temperature of the ions in the solar winds.\n  \n  \n    Bulk speed\n    km/s\n    ACE satellite\n    The speed of the solar winds.\n  \n  \n    Magnetic\n    Bz\n    nanoTeslas\n    ACE satellite\n    The strength of magnetic field of the solar winds (called the interplanetary magnetic field or IMF) in the z-axis i.e. North/South.\n  \n  \n    Bt\n    nanoTeslas\n    ACE satellite\n    The overall strength of the IMF.\n  \n  \n    Phi angle\n    Degrees\n    ACE satellite\n    The resulting angle from the horizontal Bx and By components of the IMF’s magnetic field.\n  \n  \n    Synthetic\n    Kp-index\n    No units (0-9)\n    Earth\n    A weighted average of the K-index at 13 different locations around the planet. Measures the disturbance in the horizontal component of Earth’s magnetic field.\n  \n\n\n\nSonifying in Python\n\nFor our assignment we were required to use a corpus of pre-recorded music to sonify our dataset in Python. We eventually settled on using the song at number one in the charts in Norway over the Halloween weekend 2003; Dido’s White Flag. Inspired by the cloud of ionised gas that makes up the solar winds, we decided to sonify the data using clouds of sound with granular synthesis.\n\nWe divided the song into ‘grains’ in two ways: by dividing the song equally by the length of the dataset (we call these the data point grains), and by using a MIDI representation of the song to split it into pieces each one 32nd note in length and quantized to the beat (we call these the MIDI grains). The first method resulted in grains around 109 ms in length, while the second resulted in grains around 88 ms in length.\n\nMapping with Feature Extraction\n\nMapping data to musical processes and parameters is an essential part of any sonification, and we mapped in two distinct ways. Firstly, we developed rules based on the individual data points for choosing what grains should be played at any point in the sonification. Secondly, we mapped the remaining data points to different audio effects and transformations.\n\nTo map individual grains to individual data points, we performed three types of feature extraction on each grain using the Librosa library in Python:\n\n\n  Spectral bandwidth, which measures the deviation from centre of mass of a signal and is an indicator of the timbre of a sound\n  Mel-frequency cepstrum, which provides coefficients for the power of a sound in different frequency bands. From these coefficients we took the largest coefficient value.\n  RMS, which measures the average power of a signal\n\n\nEach of these features resulted in a single number value per feature per grain. We used these numbers to rank the grains by each of their features. We also ranked our dataset by proton density, ion temperature, and Bz values. We then matched lowest to lowest between grains and values in the dataset, such that the grain with the smallest value in a certain feature corresponds to the smallest value in a particular datapoint. This mapping went as follows:\n\n\n  Proton density mapped to spectral bandwidth\n  Ion temperature mapped mel-frequency cepstrum\n  Bt values mapped to RMS\n\n\nThis resulted in each of the 2016 readings in our dataset corresponding to 3 different grains from the song, one for each feature-reading mapping.\n\nMapping with Audio Processing\n\nHaving now decided which fragments of our audio corresponded to which readings in the dataset, we mapped the rest of our variables using different types of audio processing:\n\nBulk speed determined the playback speed of each grain, within a range from half to double the original speed\nPhi angle determined the playback direction, such that if the value was below a threshold, the grain played in reverse\nBz mapped to the type and cutoff frequency of a filter. A low-pass filter was used for values above 0 and a high-pass filter for values below 0, while the absolute value of the reading was scaled as the cutoff\nKp-index mapped to the dry/wet parameter of a convolution reverb\n\nBuilding the Sonification\n\nHaving now mapped every value in our data to a grain or audio processing parameter, we assembled our sonification by moving through our dataset in chronological order. We pulled the three corresponding grains for each reading, layered them on top of each other, and then applied the processing to this summed grain. We repeated this process for both of the grain types. You can listen to the results below:\n\n\n  \n    \n    Alternate Text\n  \n  Our Python sonification using the data point segmentation.\n\n\n\n  \n    \n    Alternate Text\n  \n  Our Python sonification using the MIDI segmentation.\n\n\nReflections\n\nAfter hearing the result of our Python sonification, we decided to produce a scatter plot for each grain type to show which grains were mapped to which points in the data. We expected grains that were close to each other in the original song to provide similar values from feature extraction, and therefore that we would see some clustering when these were mapped to the data. There were a few instances of clustering, but far fewer than we expected. Plotting the average grain ID for each reading in the dataset, we see that while there is some deviation, the average grain ID is quite normally distributed.\n\nAs seen in the figures below, our segmentation approaches resulted in slightly different selection of grains. However, neither shows any strong relationship between the location in the dataset and location in the original song from which the corresponding grains are taken.\n\n\n   \n   The average grain distribution of the data point grains method.\n\n\n\n   \n   The average grain distribution of the MIDI grains method.\n\n\nWe also produced a dynamic visualisation of our dataset time-synced to the audio produced by both grain types. You can see the results of this for the data point grains below:\n\n\n  \n    \n  \n\n\nSonifying in Pure Data\n\nSonification in Pure Data was an entirely different challenge to doing it in Python. Pure Data is built as a real-time, often interactive audio and music programming language. Unlike Python, where we can use the Pandas library to conveniently handle large datasets in an Excel-like format, Pure Data forced us to handle our data in a different way.\n\nAccessing Our Dataset in Pure Data\n\nWe found the easiest way to store a dataset in Pure Data was in arrays. Arrays in Pure Data work much like lists in a coding-based programming language like Python. We provide a specific index, or position in the list, and get back the value stored there. For example, if we provide the number three, we will get back the third value stored in that array. Attaching this behaviour to a metronome which increments by 1 at regular intervals, we can move through our data in chronological order at a tempo we set. We then use the values we get back to affect parameters of other things we make in Pure Data like synthesisers or audio effects.\n\nThis logic forms the basis of our sonification in Pure Data. We exported each of our 7 data points into text files, and then loaded those into Pure Data arrays. A ticking counter then requests the value from the same position in each of the arrays. We then have the 7 values for a particular reading in the dataset, which we can scale and transform to adjust a vast number of parameters in the Pure Data patch.\n\nMapping\n\nIn Python we used a pre-existing song as the basis for our sonification. However in Pure Data we sonified the dataset from scratch using a variety of drum machines, sequencers, synthesisers, audio effects and other modules that we built during the semester.\n\n\n   \n   Kristian and Jack deep in the data looking for mapping inspiration.\n\n\nWe use our data to control the resulting music in two different ways:\n\n\n  Direct mappings - there is a one-to-one relationship between data and parameter. For example, density is mapped to the pitch of the lead and bass synths. If the value in the data increases, the parameter increases. This relationship is sometimes inverted, but the mapping is still considered direct.\n  Generative mappings - the data controls the behaviour of other algorithms which in turn control parameters. For example, the bulk speed data controls the amount of hits in the kick drum pattern. The data does not specify specifically when the kick should be triggered, only the frequency with which it should be triggered.\n\n\nThe full table of our mappings is below:\n\n\n\n  \n    Data Point\n    Mapped To\n    Mapping Type\n    Notes\n  \n\n\n  \n    Proton density\n    Lead synth pitch\n    Direct\n    Sometimes quantized to MIDI notes, based on bulk speed mapping below. Adjustable tempo i.e, the user can change how often a reading from the dataset generates a note.\n  \n  \n    Bass synth pitch\n    Direct\n    Quantized to MIDI notes. Adjustable tempo i.e, the user can change how often a reading from the dataset generates a note. Inverted from lead synth pitches.\n  \n  \n    Lead synth note probability\n    Generative\n    The chance that the lead synth plays a note when it receives a new frequency.\n  \n  \n    Ion temperature\n    Lead synth spread parameter\n    Direct\n    Spread is the degree of variation in pitch between the different oscillators in each synth voice.\n  \n  \n    Snare pattern density\n    Generative\n    Changes the probability of triggers in the snare pattern, therefore controlling the density of the pattern.\n  \n  \n    Bulk speed\n    Lead synth chorus effect parameters\n    Direct\n    Affects all three parameters of the first chorus effect equally.\n  \n  \n    Lead synth midi quantization\n    Direct\n    Tells the synth to use MIDI quantized frequencies below a certain threshold, and use the raw unquantized value above the threshold.\n  \n  \n    Kick pattern density\n    Generative\n    Changes the probability of triggers in the kick pattern, therefore controlling the density of the pattern.\n  \n  \n    LFO speed\n    Direct\n    The LFO in turn modulates several parameters of the bass synth and its effects.\n  \n  \n    Bz\n    Lead synth chorus effect parameters\n    Direct\n    Affects all three parameters of the second chorus effect equally.\n  \n  \n    Bt\n    Lead synth filter cutoff frequency\n    Direct\n    Affects the peak of the filter envelope.\n  \n  \n    Closed high hat pattern density\n    Generative\n    Changes the probability of triggers in the closed high hat pattern, therefore controlling the density of the pattern.\n  \n  \n    Phi angle\n    Lead synth delay parameters\n    Direct\n    Affects the delay time, dry/wet, stereo effect and feedback level.\n  \n  \n    Lead synth envelope attack &amp; release\n    Direct\n    Values below 225 set attack time in ms, values above 225 set release time in ms.\n  \n  \n    Bass synth envelope attack &amp; release\n    Direct\n    Values below 225 set attack time in ms, values above 225 set release time in ms.\n  \n  \n    Open high hat pattern density\n    Generative\n    Changes the probability of triggers in the open high hat pattern, therefore controlling the density of the pattern.\n  \n  \n    Kp-index\n    Bass synth reverb amount\n    Direct\n    Controls the dry/wet parameter.\n  \n\n\n\nBuilding the Sonification\n\nOnce our mappings were set, the process of sonifying our data was as simple as pressing play on the counter and letting it run. However, we also added some extra some extra functionality to our counter to allow the user to set variable start and end points, toggle looping and going in reverse through the dataset, and increment and decrement in individual steps for finding precise points in the data. As the control mechanism is essentially a metronome, we can also change the tempo and the subdivisions of the beat for each of the lead, bass and drum lines. The user can also change some parameters during the sonification, such as locking the drum patterns for a period of time, which can help to ‘tame the beast’ that is our patch.\n\nWe added a simple recording module to the end of the patch to write the output to a file, and with that, our Pure Data sonification was complete.\n\nYou can listen to the entire thing here:\n\n\n  \n    \n  \n  Our Pure Data sonification.\n\n\nOr watch a shorter video showing part of it here:\n\n\n  \n    \n  \n  A clip of our Pure Data sonification showing the user interface.\n\n\nReflections\n\nBecause the sounds used in our Pure Data sonification are built from scratch rather than sampled from a pre-existing song, the relationship between the data is generally more noticeable to us than in our Python sonification. However, we are now very familiar with the shape of our data. To a casual listener, the shape of the data is likely not identifiable apart from at a few key moments when the data changes rapidly and therefore so does the sonification.\n\nWe are generally happy with the result of our Pure Data patch. The sound is very chaotic, but this is to be expected when almost every parameter beside tempo is constantly changing. When combined with several user controls to lock the drum patterns and affect synth behaviour, the result is quite performable.\n\nDownloads &amp; Links\n\n\n  GitHub repo for our Python sonification here.\n  White Flag by Dido.\n  We downloaded our dataset from Space Weather Prediction Center here.\n\n",
        "url": "/sound-programming/2022/12/09/kristeic-jackeh-ahmetem-sonification.html"
      },
    
      {
        "title": "SMC Blog Sonified!",
        "author": "\n",
        "excerpt": "Making sound out of this blog.\n",
        "content": "\n   \n\n\nThis blog has been held by students and alumni of the SMC program for the last four years. Today, it contains 370+ posts about topics ranging from audio programming to musical interaction, concerts, and experiences. According to wordstotime.com, to read this beautiful blog entirely, you would spend 286 hours. But don’t worry, you can listen to it!\n\nBefore we dive into this, here are some fun facts about our blog!\n\n  Amongst 2334573 words typed in the blog so far, the three most frequent words were audio, sound, and music.\n  The word We was used 1.43 times more than the word I.\n  The longest word in the SMC blog is multiinstrumentalist with 20 characters - if we don’t count the word reverberaaatioooooooon with 22 :)\n\n\nTo make a sound out of data, it should be in the shape we want it to be. Cleaning, arranging, and scaling are crucial, especially in appropriately mapping the data to specific parameters. Then, we can create sound based on the numbers at hand. For this data, I started off by excluding some parts of the texts, removing unwanted words and filtering. Then, I scaled it before mapping it to specific sound parameters. Finally, these numbers turned into sound. Diagram below shows this sonification process.\n\n\n   \n\n\nGathering the Data\nThe SMC Blog runs on GitHub Pages with a GitHub repo in the background, which allowed me to get each blog as a seperate file in markdown format. Each post had the same structure: a frontmatter with meta information (e.g., date, tags, authors), blog post content in markdown format with links to images, videos, and embedded elements. This structure made the data cleaning relatively more manageable.\n\nI started cleaning the data by excluding the frontmatter from each file, which basically contains repetitive out-of-context words. Next, I scraped out markdown signs using BeautifulSoup. I cleaned the data further using a set of regular expressions, including lowercasing, removing punctuations, and cleaning white spaces.\n\npath = './_posts'\nfiles = os.listdir(path)\n\n# iterate files, remove frontmatter and parse to get text\nall = ''\nfor i, file in enumerate([s for s in files if s.endswith('.md')]):\n    text = ''\n    with open(os.path.join(path, file), encoding = 'utf-8') as f:\n        md = f.read()\n        md = re.sub(\"\\n\", \" \", md)\n        md = re.sub(r\"---.*---\", '', md)\n        html = markdown.markdown(md)\n        soup = BeautifulSoup(html, features='html.parser')\n        text = soup.get_text()\n        all += text\n\n# clean text\ncleaned = all.lower()\ncleaned = re.sub(\"(\\[.*\\])\", \"\", cleaned)\ncleaned = re.sub(\"(http[s]?\\://\\S+)\", \"\", cleaned)\ncleaned = re.sub(\"[0-9]\", \"\", cleaned)\ncleaned = re.sub(\"\\S*@\\S*\\s?\", \"\", cleaned)\ncleaned = re.sub(\"/\\S+/\\S+\", \"\", cleaned)\ncleaned = re.sub(\"\\n\", \" \", cleaned)\ncleaned = re.sub(r\"[^\\w\\s]\", \"\", cleaned)\ncleaned = cleaned.translate(str.maketrans('', '', string.punctuation))\ncleaned = re.sub(\"\\s+\", \" \", cleaned)\n\n# save all words\nwith open('all_words.txt', 'w', encoding='utf-8') as f:\n    f.write(cleaned)\n\nwords = cleaned.split(' ')\ndf_words = pd.DataFrame(words, columns = ['words'])\n\n\nThe steps above resulted in having an array of cleaned words. Next, I cleaned the outliers, words such as “all”, “and”, “to”, words that are too long, and words that are too much or less frequent.\n\n# remove unwanted words, remove extremes\nunwanted_words = ['', 'the', 'to', 'and', 'of', 'a', 'in', 'is', 'for', 'with', 'that']\ndf_words = df_words[~df_words['words'].isin(unwanted_words)]\ndf_words['w_len'] = df_words['words'].str.len()\ndf_words = df_words[(df_words['w_len'] &gt; 1) &amp; (df_words['w_len'] &lt; 20)]\n\n# calculate frequencies, remove less frequent\ncounts = Counter(df_words['words'])\ndf_freq = pd.DataFrame(counts.most_common(), columns=['words','freq'])\ndf_freq = df_freq[(df_freq['freq'] &gt; 10) &amp; (df_freq['freq'] &lt; 1500)]\n\ndf = df_words.merge(df_freq, how='left')\ndf = df.dropna()\n\n\nData Preprocessing and Mapping\nAfter the first cleaning part, words’ frequency of appearing in the entire blog gave me values ranging from 11 to 1323. I simply scaled it up by 10. The word lengths ranged from 1 to 15. I multiplied it by 2, to use as length (ms) of each audio piece. Also, this can be scaled globally in the final step using the speed variable.\n\n# scaling\ndf['freq'] = df['freq']*10-90\ndf['w_len'] = df['w_len']*5\n\n\nSonification\nFor every word, I generated short audio samples, which come together to create a musical piece. I decided to map each word’s frequency (i.e., how many times each appeared on the website) to the leading pitch. I used each word’s length as the duration of each audio sample. This way, I expected to obtain a sound pattern that reflects the characteristics of the text (i.e., more frequent words sound high-pitched, and longer words make longer sound).\n\nI ended up having 5.6 ms long audio pieces generated for each word on average. I applied hamming window to each, and combined them one after another, to have a piece of music! Code below does the job.\n\n# sonify\n\nsr = 48000\nsr_ms = int(sr/1000)\nsignal = np.array([])\nl = len(df.index)\nspeed = 0.5\nfor i, row in df[:2000].iterrows():\n    freq = row['freq']\n    note_dur = row['w_len']/speed #ms\n    start_sample = i*note_dur*sr_ms\n    dur_sample = note_dur*sr_ms\n\n    # sin wave\n    f = freq\n    A = 1\n    t = np.arange(0,note_dur/1000,1/sr)\n    s = A*np.sin(2*np.pi*f*t)\n\n    # window\n    window = scipy.signal.windows.general_hamming(len(s), alpha=0.5)\n\n    # append\n    signal = np.concatenate((signal, s*window))\n\nsf.write('SMC_blog_sonified.wav', signal, sr)\n\n\nHere is the result!\n\n\n  \n    \n    SMC Blog Sonified!\n  \n  \n\n\n\n\n\n",
        "url": "/sound-programming/2022/12/09/ahmetem-mct-blog-sonified.html"
      },
    
      {
        "title": "Make The Stocks Sound",
        "author": "\n",
        "excerpt": "Make The Stocks Sound\n",
        "content": "PD Sonification Project\n\nThis project investigates generative music ,as well as sonification. However, the focus of this post is on the sonification aspect of the investigation.\nAs part of the group final assignment for the Puredata course, we are sonifying the changes in the stock market index of Turkey and the S&amp;P500 over the course of 500 days, beginning on the same day and time.\n\nDue to the fact that changes in the S&amp;P index have an effect on stock markets around the world, it was necessary for us to be aware of the correlations that exist between the various pieces of data.This required proper scaling and normalisation as the fluctuations can be relatively negligible within days. The temporal data of Turkey stock index fluctuations are used to control the pitch of the phase-modulation polyphonic synthesiser that is being developed for this project. If the randomization machine that is built into the patch is turned on while the instrument is playing, it will randomly regenerate the pitch and timbral characteristics of each of the six modules that forms the synthesisers.\n\n\n   \n   patch\n\n\nA polyphonic granular synthesizer is also derived from the S&amp;P index. This synthesizer allows the S&amp;P index to regulate the start point and playback speed of the grains. In addition, the density and presence of a stock market sound recording increases when the market is bullish, and the opposite is true when the market is bearish. Additionally, an oscillator-based panning module that is controlled by the same data is also included. When the market is expanding, it moves further toward the stereotypical picture, and when it is contracting, it moves in the opposite direction.\n\nResults:\n\nPlease listen to an exerpt of it!cheers!\n\n\n\n",
        "url": "/sound-programming/2022/12/10/ninojak-masoudn-make-the-stock-sound.html"
      },
    
      {
        "title": "Markov Chain Core in PD",
        "author": "\n",
        "excerpt": "Markov Chain Core in PD\n",
        "content": "Markov chain\nAA stochastic model called a Markov chain or Markov process describes a series of potential occurrences in which each event’s probability solely depends on the state obtained in the preceding event. A discrete-time Markov chain is created from a countably infinite sequence where the chain changes state at discrete time steps (DTMC). A continuous-time Markov chain is a term for a continuous-time process (CTMC). It bears Andrey’s name, a Russian mathematician, Markov(Wikipedia).\n\nIn this case, Midi data should be given into the analyzer by reading a midi file and providing its pitch input.\nBased on the current state and a set of probabilities, Markov Chains determine the following state. If this were mapped to pitch, selecting our next note would include taking into account both our present note as well as a list of potential notes and their probability.\n\nMarkov Chains choose the next state based on the current state and a set of probabilities. As an example, here are the notes of ‘Happy Birthday’:\n\n\n  C4 C4 D4 C4 F4 E4\n  C4 C4 D4 C4 G4 F4\n  C4 C4 C5 A4 F4 E4 D4\n  Bb4 Bb4 A4 F4 G4 F4\n\n\nThe next note in a First Order Markov Chain is determined by the current note and a set of probabilities. We can learn about the likelihood of one note following another by analyzing Happy Birthday. This table’s first column displays all of the notes in the song, followed by a list of numbers representing all of the notes that followed that pitch.\n\n\n  C4, C4 D4 F4 C4 D4 G4 C4 C5;\n  D4, C4 C4 Bb4;\n  E4, C4 D4;\n  F4, E4 C4 E4 G4;\n  G4, F4 F4;\n  A4, F4 F4;\n  Bb4, Bb4 A4;\n  C5, A4;\n\n\nAs a result, C5 was only followed by A4. Probabilities between 0 and 1 can be measured. The likelihood of A4 being followed by C5 is one, as is the probability of A4 being followed by F4. Typically, this is represented by a table displaying the following probabilities:\n\n\n   \n   transition matrix\n\n\nThis table is known as a transition matrix, a state transition matrix (STM), or a transition table.\n\nOnce we have this information, we can develop fresh material based on these probabilities. The STM can be generated by manually or by analyzing existing music or data. For example, you could load up all of Mozart’s works and construct algorithmic compositions using the same set of note probabilities.\n\nIn this post, we’ll look at how to use PureData to automatically analyze a MIDI file and build a first order STM, which we’ll then use to construct some simple algorithmic music compositions. We’ll only use pitch in this example, but you can easily extend it to include rhythm, dynamics, or other musical components.\n\nBulding Markov Chain Analysis core\n\nOur new matrix is saved in a data object known as a coll. When you refer to a coll with the same name (in this case, pitchMatrix), it retrieves the same data.\n\n\n   \n   markov analysis\n\n\nInside pd pair\n\n   \n   This additional subpatch chunks up the notes into pairs here.\n\n\nPitch generator\nNow we can generate pitches based on transition values.\nBeginning with a single note from our input melody, we use the length object to determine the length of the list of possible future notes. This is used as input to the random object, which selects a number at random and then selects one of the following notes from our own pitchMatrix object. In our STM MIDI, for example, if note 60 is followed by 64 62 64 65, there is. There is a 5% probability of picking 64 and a.25 chance of picking 62 or 65.\n\n\n   \n   .\n   \n   \n   \n   \n   pitch generator\n   \n\n# Sources:\n[source](http://www.algorithmiccomposer.com/2010/05/algorithmic-composition-tutorial-markov.html)\n\n# Another approach:\n[Markov](https://github.com/simesky/puredata-markov-chains.html) \n\n# Attention :\nSome versions of PD do not include coll, please consider using text object.\n\n\n  \n  \n\n",
        "url": "/sound-programming/2022/12/10/ninojak-masoudn-markovchain_pd_masoudn_ninoJ.html"
      },
    
      {
        "title": "Designing DFA and LAA Network Music Performances",
        "author": "\n",
        "excerpt": "Music Performances in High Latency\n",
        "content": "Two Configurations for Network Music Performances in High Latency Environments\n\nMuch time, effort and research in the field of NMPs goes into the reduction of latency.  For seamless musical performances over a network, latency needs to be imperceptible to musicians.  Due to the nature of many remote solutions, especially those available at a consumer level, this is often not possible and latency can be at levels making synchronised music performance unworkable.\n\nIn their paper linked below, Alexander Carôt and Christian Werner propose several configurations for working with situations where latency is unavoidably high.  These revolve around, rather than attempting to reduce latency, to accept its presence and potentially use it to form part of the music or synchronisation.  These are often based on one location being termed the “master” for the sake synchronisation, with the second being the “slave”.  Here we will instead use the terms primary and secondary nodes to denote this.  Our groups attempted to implement the following two configurations for musical performance:\n\nDelayed Feedback Approach involves delaying the primary node’s signal relevant to the roundtrip latency as to sound the same as how the secondary node will experience it.\n\nLatency Accepting Approach revolves around not attempting to improve or optimise latency in any way and instead absorb it into the creative content of the performance.\n\nDelayed Feedback Approach (DFA)\n\nFor this designed performance we used a drum machine at the primary node playing sequenced drum patterns and a synthesizer at the secondary node played by a human musician.\n\n\n   \n    \n\n\nThe design of this configuration was that the drum machine would be transmitted across the network in its latency-rich state to the secondary node.  The performer at the secondary node would play along with the drum machine, transmitting the output from the synthesizer back to the primary node, again with unaffected high latency.\n\nWith the synthesizer signal being returned to the primary with latency, we added a sample delay to the monitor path of the drum machine which was equal to half of to the total latency in the system.  As such, as the synthesizer was only travelling one way across the network, when the secondary node’s signal and the delayed drum machine monitor signal played together, they sounded in time.\n\n\n   \n   Roundtrip latency was measured by sending a click through the system and recording its return.  The distance between the two was then measured in milliseconds and samples.\n\n\n\n   \n   Taking the average lengths between the two clicks gave the total roundtrip latency.  This was 109ms or 5245 samples.\n\n\n\n   \n   Using a sample delay set to half the roundtrip latency in order to delay to drum machine monitorr signal\n\n\n\nHere is the performance using the DFA configuration.  The audio is recorded from the mixer at the primary node with a mix of the returned synthesizer signal and the delayed drum machine monitor signal.  As you can hopefully see, this audio lines up perfectly with the footage from the secondary node, implying that the delayed monitor path was configured correctly.\n\nLatency Accepting Approach\nFor the LAA (Latency Accepting Approach) Fabian and Henrik used SonoBus over a wifi connection to create a latency-rich environment. Roundtrip latency averaged at just above 200ms, giving a good foundation to try LAA.\n\n\n   \n   Sonobus settings\n\n\nAs noted by Carôt and Werner, a latency accepting approach, lends itself best to ambient music with slow movements giving performers time to react to each other.  As such, this is what was attempted here.  A video of the performance is  below:\n\n\n\nConclusion\n\nDFA and LAA lend themselves well to different forms of telematic musiking in latency-rich environments.  The more percussive music style was appropriate for the DFA while LAA was better suited to a beatless ambient approach.\n\nFor the DFA performance, while this approach worked well with the use of the drum machine, the monitoring delay would have made live playing difficult.  This approach would be more feasible if all participants were playing to a ‘delayed feedback’ metronome, allowing all participants to, in effect, play to a single, synchronised timing source.\n\nIn practice, despite  very high latency and having a lot of audio packet drops, LAA felt fairly seamless for the performers to play ambient music. We did not attempt to play rhythmically-based music with this approach as the high-latency would have rendered it impractical.\n\nWhile neither of these approaches represent a generally applicable solution, they do offer technical and creative routes to allow for effect music performance in latency-rich environments.  While DFA represents a route for more conventional, rhythmic music, LAA forces the musicians to work in an unconventional manner.  This has the potential to open them up to greater creativity and different, new modes to work within its limitations.\n\nReferences\n\n[1] A. Carôt and C. Werner, ‘Fundamentals and principles of musical telepresence’, Journal of Science and Technology of the Arts, pp. 26–37, Jan. 2009, doi: 10.7559/CITARJ.V1I1.6.\n",
        "url": "/networked-music/2023/02/16/DFA-and-LAA-NMP.html"
      },
    
      {
        "title": "Testing Two Approaches to Performing with Latency",
        "author": "\n",
        "excerpt": "We tested two approaches to dealing with latency in network music. Read all about it!\n",
        "content": "Introduction\n\n\n   \n   The Master-Slave Approach (left) and Laid Back Approach (right) as illustrated by Carôt and Werner.\n\n\nThis semester in Physical-Virtual Communication and Music 2, we have been both designing networked music performances and reading about the experiments of others - the challenges they have faced and the lessons they have learned. We set out to investigate the various strategies to work with latency in Network Music Performances presented by Alexander Carôt and Christian Werner in Fundamentals and Principles of Music Telepresence. In this blog we will pass on some of our insights about using the Master-Slave Approach (MSA) and Laid Back Approach (LBA) for network musical performances in high-latency situations.\n\nIn the Master-Slave Approach, one performer/location is the ‘master’ while the second location is the ‘slave’. The master performs without regard for the slave and therefore has control of the tempo. The slave then simply follows the master as they hear it. This results in the illusion of perfect sync for the slave, while the master receives the slave’s audio delayed from their original performance by the RTT latency of the system.\n\nIn the Laid-Back Approach, a more collaborative relationship is established between performers/locations. In this approach the performers are to listen to and follow each other, thus maintaining tempo is the equal responsibility of both parties.\n\nSetting Up\n\nThe Challenge of Stable Latency\n\nOur main problem was maintaining stable latency. We initially planned to use SonoBus for our performance, but preliminary latency testing using the software gave wildly varying results for the latency in the system. In our initial measurements, the round trip latency was 41 ms. However, from there we saw a approximately linear increase over the course of 30 minutes. When we manually added delay using the Portal’s Midas M32 mixer, the latency measurements did not increase as we expected. When we then removed this extra delay, the inherent latency of our system had increased by several milliseconds from our previous measurement - far beyond the expected amount of network jitter.\n\n\n  \n    \n      No Delay\n      10ms Delay on Send\n      10ms Delay on Receive\n      10ms Delay on Both\n    \n  \n  \n    \n      44ms\n      44ms\n      57ms\n      57ms\n    \n    \n      50ms\n      50ms\n      63ms\n      63ms\n    \n    \n      53ms\n      56ms\n      68ms\n      68ms\n    \n  \n\n\nTable 1: Our calculated latency amounts in milliseconds while adding and removing artifical delay. Note the ‘No Delay’ numbers increasing over time, which was the main source of our initial confusion.\n\nAfter these confusing results, we decided to conduct our performance with the LOLA software on the Portal and Videoroom LOLA computers instead.* Then we were able to obtain a stable latency of 21 ms - a good baseline from which to manually add latency in order to test the MSA and LBA approaches.\n\n* We experienced these issues when using SonoBus to record latency measurements despite entirely manual settings, therefore we cannot recommend it for that specific purpose. However, both ourselves and other SMC students have found SonoBus ideal for quickly setting up high-quality network rehearsals, performances, and recordings, therefore we can easily recommend SonoBus as a user-friendly solution for those purposes.\n\n\n   \n   Kristian and Jack performing together with increased latency between the Portal and the Videoroom.\n\n\nAudio Routing Diagram\n\nOnce we decided to switch our testing from SonoBus to the LOLA software, things moved along more smoothly. The Portal and Videoroom LOLA PC’s were connected to the LOLA network for audio and video streaming. An external laptop running Ableton Live and an sound card were used to a) record audio of our performances, and b) provide the click track for Kristian to play with for the Master Slave Approach.\n\n\n   \n   Our audio routing in the Portal and the Videoroom.\n\n\nTesting the Approaches\n\nWe decided to test the suitability of each approach at multiple levels of latency and playing at different speeds. We tested each approach four times as follows:\n\n  Playing to a half-time beat at 120 BPM with 50 ms round trip latency (RTT), or 25ms one-way\n  Playing at regular beat at 120 BPM with 50 ms RTT/25 one-way\n  Playing at a half-time beat at 120 BPM with 100 ms RTT/50 one-way\n  Playing at regular beat at 120 BPM with 100 ms RTT/50 one-way\n\n\nFor our tests Kristian played the e-drums in the Portal while Jack played violin in the Videoroom. Kristian and Jack could see each other on video through LOLA and the Ximea cameras. Unlike the audio over LOLA, this video was never subject to any artificial latency.\n\nMaster Slave Approach\n\nWe found the master-slave approach to work well with both 50 and 100ms of RTT latency with the half-time beat. For Kristian on drums, it initially took some adjusting to focus just on the click track and mentally block out the delayed signal from Jack. However after this adjustment it was relatively easy to perform with the latency. This approach was very straightforward for Jack in the Videoroom as he simply had to play in time with the audio he received from Kristian.\n\nPlaying with the regular beat at 120BPM with 50ms of RTT latency was also fairly easy using the MSA. Kristian was able to adapt to the latency similarly to the half-time beat, and it felt the same as the half-time beat for Jack as he was not directly dealing with latency. It was slightly more difficult with 100ms of latency, but still playable.\n\nLaid Back Approach\n\nWe found it fairly straightforward to keep in time with each other while performing the half-time beat using the LBA with 50ms of RTT latency. However, as we performed this approach without using a click track, it did have a tendency to slow down over time. This is a common pitfall of performing with latency, so it is unsurprising that we also found it to be an issue here. This problem was further amplified when we tried performing the half-time beat with 100ms of latency.\n\nThe laid back approach was less effective when performing the regular beat at 120BPM with 50ms of RTT latency. The faster beat increased the cognitive load for both Kristian and Jack significantly. We required several takes before we were able to hold together for more than a few bars, and the tempo also slowed down as it did with the half-time beat. This may have been partly due to the fact that neither are regularly performing currently, so our internal tempi are not what they once were. It is likely that professional session musicians would be able to adapt to this latency much better than we could.\n\nThe regular beat with 100ms of latency proved even trickier to manage. Once again, it took several takes for us to adjust and the music slowed down over the course of the performance.\n\nWatch the Video\n\nHere is the audio-video recording of each test, along with our comments on our personal experience during the process.\n\n\nConclusion\n\nCarôt and Werner present several approaches for dealing with high-latency systems for network musical performance. We set out to test two of these, the Master-Slave Approach and the Laid Back Approach, with different levels of latency and at different speeds. While the MSA required some initial adjustment by the master, we found it to be usable from a performance perspective at varying levels of latency, and to a lesser extent with both slow and fast music.\n\nMeanwhile, although the LBA worked well at with the slower beat and with lower latency, we found it difficult to manage in faster music and with high latency.\n\nWhile both approaches have their place within NMP settings, we feel that they are more effective for music which doesn’t require precise synchronisation, such as ambient/experimental or relaxed jazz styles, or for slower music.\n\nReferences\n\n[1] A. Carôt and C. Werner, ‘Fundamentals and principles of musical telepresence’, Journal of Science and Technology of the Arts, pp. 26–37, Jan. 2009, doi: 10.7559/CITARJ.V1I1.6.\n",
        "url": "/networked-music/2023/02/16/aysimab-latency-assignment.html"
      },
    
      {
        "title": "Jazz Over the Network at 184,000km/h",
        "author": "\n",
        "excerpt": "We worked with local high school students to put together a jazz concert over the LOLA network. Here’s a retrospective from Team RITMO.\n",
        "content": "Introduction\n\n\n   \n   Musicians from Edvard Munch Vgs in soundcheck with Alex riding the faders.\n\n\nFor our third network musical performance as SMC students, we teamed up with students from Edvard Munch High School in Oslo to put together a jazz concert between Salen at the Department of Musicology and the motion capture lab at RITMO on Tuesday 7 March 2023. Read below for a look behind the scenes and some reflections on the event from the RITMO performers and technicians.\n\nFirst though, some fun facts and figures. Salen and RITMO are roughly 870 metres apart. Sound travels at approximately 343m/s so it would take sound around 2.5 seconds to travel that distance. However, using the LOLA network we are able to send sound and video from one location to the other and back again in 17 milliseconds. In other words, we played jazz at 149 times the speed of sound, or 184,000km/h.\n\nThe ensemble from Edvard Munch High School consists of 3 singers, 2 trumpets, piano, electric guitar, bass guitar, and drum kit. We split the ensemble between our two locations, with the trumpets and guitars performing at RITMO.\n\nGetting Technical\n\nAs with our previous concerts, we used the NMP kits as the core of our setup, which were connected together between Salen and RITMO through the purpose-built LOLA network. We recorded each of the four RITMO musicians on separate tracks to the Yamaha mixer included in the kit. We used Shure SM57 microphones for the trumpets, a Sennheiser e609 microphone for the electric guitar amplifier, and we used a DI box for the bass guitar. This setup worked well and resulted in a great-sounding mix, both at the front of house location in Salen and through the headphones we used for monitoring at RITMO.\n\nWe were lucky to be able to make use of a mounted 4k TV from RITMO for the performers. We arranged this screen such that the performers could see both themselves and the Salen performers throughout the concert. This worked well, although the performers experiences some difficulties in communicating with their bandmates remotely. See the Student Reflections section below for more details on this issue.\n\nThe LOLA network worked flawlessly during both the soundcheck day on Monday and the concert on Tuesday. We performed round-trip (RTT) latency measurements on both days and recorded 15ms and 17ms of latency respectively with a sample rate of 48kHz and a buffer size of 64 samples on both ends. This worked well for our use case as we aim to achieve a RTT of under 30ms, above which performability can be negatively affected. The performers confirmed that this latency worked well. They could see some slight latency in the camera feeds in front of them, but this did not affect their ability to perform with the system.\n\nYou can find a more detailed technical specification of the setup we used on the wiki page here.\n\nWatch the Performance\n\nWatch the full performance on the SMC YouTube channel here:\n\n\n\nStudent Reflections\n\nAfter the performance, we spoke to the RITMO performers to understand their experience of playing telematic music. This was their first experience of network musical performance for all the musicians, therefore we were excited to learn more about their perspectives on the technology. The performers all found the experience to be rewarding. However, they also identified several issues which largely fall into two categories: those inherent to telematic performance, and flaws with our specific setup.\n\nAll the RITMO performers found that they missed the physical aspect of performing together in the same room. Specifically, they found it difficult to properly feel the groove as this relies as much on feeling physical movements together as it does listening to bandmates. This could be rectified partly through changes to the setup in each location. The Salen camera was placed facing towards the performers which the RITMO performers said made them feel like they were playing to, rather than with, their counterparts. Placing the Salen camera amongst or behind the performers could have therefore helped to alleviate this issue. Relatedly, the RITMO performers were seeing the Salen musicians on a fairly large TV. A larger view, such as the large projector screens included in the NMP kit, could have helped to make the RITMO performers feel like they were part of the action.\n\nThe RITMO performers also identified that performing using headphones felt unnatural and took away from the live feeling of the performance, instead feeling more like a studio or practice session. We chose to use headphones for two main reasons: to minimise bleed into the trumpet and guitar microphones, and because the speakers at our disposal were not large enough for monitoring over loud brass instruments. It is therefore possible that we could have relied on monitoring through wedge speakers, but this might have caused issues with bleed and feedback.\n\nSummary\n\nAccording to some reflections, the capability of NMP performance is still unknown, even among younger generations. Furthermore, many conceptions are the result of cultivated expectations. For example, our young performers generally agreed that rhythmic music with repetitive patterns is easier to perform telematically. However, we are aware that this is not always the case. This may indicate that some kind of familiarization and education is required. Perhaps physical and virtual communication courses will be offered in schools in the near future!\n",
        "url": "/networked-music/2023/03/12/jackeh-ritmo-mid-semester-concert.html"
      },
    
      {
        "title": "Playing Jazz Over Network",
        "author": "\n",
        "excerpt": "A live performance by Edvard Munch High School students with collaborative networked music technology.\n",
        "content": "\n   \n   Concert Poster\n\n\nIntroduction\nFor our third telematic performance we had the pleasure of teaming up with students from Edvard Munch High School. They brought a varied repertoire and a very talented group of musicians who were great to work with! This post will give you an insight into the work we did as technicians in Salen which was the front of house in this concert.\n\n\n   \n   Explaining the setup to performers.\n\n\nTechnical Setup\nFor setting up the connection between two locations, we relied on our previous experiences and used Lola Software and two NMP kits which were proven to provide low latency connection.\n\nOne NMP kit was set up in Salen, our main performance space located at the Aalborg University (UiO) while the other one was set up at RITMO, the Centre for Interdisciplinary Studies in Rhythm, Time and Motion at UiO. In Salen, behind the audience, we set up a technical area for the Lola computer, live streaming monitors and mixer so that all the team was positioned in a way to see everything and act when needed. Our field-tested technical setup approach worked again well this time.\n\n\n   \n   The NMP kit setup.\n\n\nAudio and Monitoring\nAll audio in Salen was run through the Midas M32 mixer which also received the network audio directly from the Behringer ADAT card. By simply routing the incoming audio in TotalMix to output five to eight we avoided unplugging fixed cables in the NMP kit. We used wedge monitors instead of headphones to preserve a realistic ‘concert look’ in Salen where the audience was located. This worked well and we encountered no feedback issues, although we had to connect the two vocal monitors together due to connection shortage on the stagebox.\n\n\n   \n   Four monotracks (25-28) received directly to the Midas mixer from RITMO.\n\n\nVideo\nFor the video setup, we chose Salen as the main location, as the audience were invited and expected in Salen. For a natural look, we used a projector and positioned the video of the RITMO performers in the middle of the Salen stage on a black curtain. The video was positioned in a way that every performer was visible with their expressions.\n\nWe knew from the previous experiences that the performers also need to communicate through the video, so we placed one high speed monitor, with the other performers’ video, on the stage towards the performers so that especially the drummer and singers could see RITMO performers and be able to communicate. This is beneficial for the performers’ musical cues and visual communication.\n\nThere was also one monitor connected to the NPM for us to monitor the Lola connection and the video from our stage.\n\nWe promoted the concert by distributing posters around the university before the event and live-streamed it on YouTube from the Salen streaming station. Watch the full performance on the SMC YouTube channel here:\n\n\n\nFeedback from the Musicians\n\nWe conducted short interviews with the performers after the concert since all of our team members were technicians this time and feedback on the setup is always helpful.\nIn short, the musicians at Salen gave us positive feedback. They really enjoyed playing in this kind of setup and appreciated the novelty and smoothness (since we had ideal latency conditions this day) of it. They said not being physically together in one room was a challenge, but a good one. The monitor setup also gave them a natural feeling and added to immersion. The pianist said she sometimes forgot that it was a network setup.\nThe only issue that came up was with communication between musicians. They had to rely on more obvious and stronger gestural cues to signal transitions which was, in hindsight for them, no hindrance for playing correctly. They also mentioned that the canvas and monitor setup felt quite natural since they normally have to look back or to the side to communicate with their fellow musicians in a traditional setup.\n\nThe idea to separate the rhythm section (drum kit and bass guitar) luckily did not backfire. The drummer mentioned that he felt sufficiently connected with the bassist which makes rhythm section separation a valid alternative setup.\n\n\n   \n   Thank you.\n\n",
        "url": "/networked-music/2023/03/12/aysimab-salen-mid-semester-concert.html"
      },
    
      {
        "title": "MIDI music generation, the hassle of representing MIDI",
        "author": "\n",
        "excerpt": "A brief guide of the troubles with MIDI representation for generative AI\n",
        "content": "\n   \n   \n\n\nIntroduction\n\nAs there has been advances in generative AI, there has also been a lot of research in generative music using Neural Networks. But if you want to generate music, you need to represent music. The two main methods are using symbolic music representation or audio representation. In audio representation, you represent sound as frequencies. This can be challenging due to the fact that audio is sampled using frequencies at several thousand hertz. This requires a huge amount of computational power to be able to train on a big amount of data. The other option is to represent music symbolically. Here, a large amount of music can be represented with a lot less data. Simple, right? No..\n\nMIDI: The bohemyth\n\nMIDI (Musical Instrument Digital Interface) was made in the early 1980s as a way to connect electronic musical instruments and computers. The ease of use made it so that it became the leading way of symbolic music representation. So why isn’t this then fed to the Neural Network? Well, one of the problems is that MIDI both has a lot of redundant information, it is noisy and complex. Neural Networks also experience a lot of trouble with keeping track of sequences with the way MIDI uses note-on events, and a corresponding note-off event to keep track of playing notes. It becomes a 1D representation to represent 2D information. This is shown to be challenging for Neural Networks. So instead of representing symbolic music as raw MIDI, we need to preprocess it, doing some feature extraction.\n\nLet’s clean up MIDI\n\nSince MIDI is too noisy and complex, a solution can be to strip it down a bit. A lot of redundant information about the track can be removed, and the information can be stripped down to just the note events that MIDI represents. TensorFlow teaches this in their generative MIDI guide (source). They also remove the note-on and note-off events and instead use step and duration to represent the time between notes, and duration of a note being played. This is due to the fact that every note-on event needs to be closed by a note-off event. This can cause trouble since a note-on event without a corresponding note-off event will cause an error. This method works, and works better than raw MIDI data, but it is still considered a 1D representation, where the values of step and duration dictate the timing information. So it suffers from the same challenges that raw MIDI data does.\n\nAdding time dimension with piano roll\n\nTo solve the problem of having 1D representation, we need to convert the 1D midi data to 2D data. The most common method to do this is piano roll. In piano roll, instead of representing each note by a group of values, it quantifies time into time steps and represents each time step as a vector. Each of these vectors will normally have up to 128 numbers, where each represents one pitch. These numbers are often ‘1’ for active notes and ‘0’ for inactive notes. This quantification is done in fixed intervals in contrast to 1D representations which only samples on notes. When doing this over several time steps, we get a matrix where each column represents the time axis, and each row represents the pitch played at each time step. Now we have a 2D representation. It now has a more structured sequence representation, due to the constant time interval between two successive columns.\n\n\n   \n   MIDI notes represented as a piano roll\n\n\nPiano roll, the perfect choice?\n\nSo this is it, we now have the perfect way to represent MIDI? Of course not.. There are several problems with piano roll as well. Some have workarounds like the fact that there is no way to represent silence, or that there is no difference between a sustained note and two rapidly played notes. Silence can be represented by adding a row that is active when no note is played, and sustained notes can be represented by another symbol like ‘_’ in the matrix instead of ‘1’.\n\nBut some problems are not solvable with quick fixes, like the fact that piano roll has very high memory consumption. This leads to a need for powerful equipment needed to train models using large amounts of data. In my research I use a low sampling rate when sampling the piano roll. But still for every number 1D representation uses, standard piano roll uses over 330 numbers.\n\nWhat other 2D representations are there?\n\nRelative pitch\n\nSince piano roll is imperfect, a lot of clever ways to reduce memory consumption have been introduced. One of them is often called relative pitch. With relative pitch the goal is to reduce the size of the piano roll. It relies on transition data. You can simply make a piano roll that only looks at the transition in pitch between the current note and the previous. If no transition is made, the middle row is active. If the transition is 4 half steps, the row at index (middle row + 4) is active. If the transition is down 3 half steps, the row at index (middle row - 3) is active.\n\n\n   \n   MIDI notes represented as a relative pitch\n\n\nNo transition is normally not bigger than one octave up or down, and if it is it is kept in range with a modulus 12 operation. Therefore it only needs 12 + 12 + 1 = 25 values to represent an octave up, an octave down and remaining at the same pitch. This is less than 20% of what a full piano roll uses.\n\nCharacter based\n\nThe reason why both piano roll and relative pitch uses a vector of numbers either ‘0’ or ‘1’ is because Neural Networks learn using numbers. For instance learning using ‘0’ to show inactive ones and ‘1’ to show activeness in a vector of length 128 (called one hot encoding) has been found to be a lot easier for a Neural Network than learning to predict a number between 1 and 128. But there are some networks called character-based networks that can learn characters. Using them, you can represent advanced musical structures, using characters. This is an entirely different field, deserving a blog post in itself, but it has also become popular. Instead of needing a vector for each timestep, you can use letters.\n\nConclusion\n\nTo conclude, sequence music representation sounds simple, but it is not. When we want to represent complex data like sound, using as little memory as possible, we often lose something. The key is to find a compromise between being what we get and what we lose.\n\n",
        "url": "/machine-learning/2023/04/19/tryb-midi-hassle-copy.html"
      },
    
      {
        "title": "Audio Engineering for NMPs: Part 2",
        "author": "\n",
        "excerpt": "A deeper dive into mixer work for NMPs\n",
        "content": "Building on the previous blog post, which offers a high level overview of the extra considerations for audio engineers working with Network Music Performances (NMPs), this post will take a detailed look at examples for audio routing, with particular focus on the mixer and ADC/DAC.\n\nVideo Tutorial\n\n\n\nThis video tutorial covers the details and considerations for two key methods of audio routing for NMPs.  These are summarized below:\n\nSending Via Auxiliary Sends\nUsing auxiliary sends for audio routing represents a simple and efficient method for sending audio over a network.  Using one or a pair of aux sends results in a mono or stereo summed signal, akin to sending a monitor/foldback mix to performers.\n\nReceiving this signal requires only a single mono or stereo input on the receiving mixing desk, which can be bussed to FOH or foldback as needed.\n\nBeing only a mono or stereo audio feed, its load on the network connection is minimal.  This does however, come at the expense of flexibility and control for the engineer receiving it.  They cannot control the relative volumes of instruments in the feed, meaning that if changes to the mix are need they must contact the remote engineer to alter it.\n\nSending Via Sub-Groups\nSending via sub-groups on the other hand, offers greater flexibility for the corresponding audio engineer at the expense of greater network load.  In the video demonstration, four channels of audio are sent over the network, doubling the amount of audio information compared to the first example. Whether this is a viable approach will depend on the network system being used, as well as its stability and latency, which is outside the scope of this tutorial.\n\nThe advantage of this method is that the receiving engineer is given separate instrument feeds which can be patched into the mixer and processed in a way similar to any of the other local sources present.  This does however, also necessitate a mixer with a channel count able to accommodate both the local and remote sound sources.\n\nConclusion and Further Considerations\nThis tutorial and blog post has looked at the specifics of working with the mixer and ADC/DAC in the context of NMPs, demonstrating two key methods and concepts for sending and receiving audio over a network.\n\nAs stated, the two methods shown do not have to be used in isolation.  They can be used asynchronously across the two locations for a “best of both worlds” scenario taking the techniques most appropriate for each engineer’s needs.  For more complex sessions they can also be combined to provide control over a greater number  of channels.\n\nAn alternative for more advanced users, would be to split the incoming signals. One feed would be sent to the mixer but would only be used for FOH mixing and foldback. The second feed would be sent directly to the ADC, bypassing the mixer and any need to send network audio via it at all.  This represents a setup with the greatest amount of flexibility and complexity.  It essentially builds on the concept of having separate FOH and foldback engineers in that it adds another separate stream of audio which would ideally be handled by another dedicated member of personnel.  Again, this method would only be recommended where ensembles are large enough that the added channel count is necessary, the network system used is robust enough to carry the higher number of audio streams, and where engineers are suitably familiar with the previously covered techniques for audio engineering for NMPs.\n",
        "url": "/networked-music/2023/04/20/alexanjw-NMP-AE-2.html"
      },
    
      {
        "title": "Pair Programming Over Network",
        "author": "\n",
        "excerpt": "Pair Programming from different locations, is it possible?\n",
        "content": "As collabration proved its impact on productivity, programming also started to be done by teams instead of individuals only. Programming culture has brought the habits like mob programming and pair programming which enables developers to work on one station and one code base together in real time.\n\nWorking alone on a project for a long time started to be less charming and less educational compared to working together and brainstorming with teammates.\n\nWhat is Pair Programming?\n\nPair programming means two developers writing code together at one station while mob programming means more than two developers such as a whole team writing code at one station. Those sessions are part of developers regular development cycle and missing them definitely would mean losing interactivity in the teams.\n\n\n   \n   \n\n\nProblems after hybrid working\n\nEspecially after the pandemic, we all started to work from home or from different locations; however remote working not only means meeting online and ensuring audio or video quality. It also means introducing solutions for teams to collaborate together.\n\nAt the first times of the pandemic, many of us have tried explaining a code block to eachother or showing eachother how to make a change in a code piece over an online meeting.\nIt usually ends up someone repeating lines like that and drives everyone just crazy:\n\n\n   \n\n\nSo, it was very urgent for developers who likes to collaborate on the code to find a solution, yet almost all the known compilers started developing extentions or plugins for creating collaborative sessions over network on the code files.\n\nHow does pair programming over network work?\n\nThere are many software plugins, extentions or applications which allows us to have smooth experiences while working on the same code in real time.\n\nFor example IntelliJ has Code with Me plugin which allows us to have a live session on one station and edit the same code together. Visual Studio Live Share​ is on the other hand an extention on Visual Studio Code which does the same job and lets developers to start a live share and easily copy a share invite to others with any intended access rights.\n\nHere you can take a look at the video I created in order to explain how pair programming over network works and how you can use Visual Studio Live Share for it:\n\n\n  \n    \n  \n  Video of \"How to do Pair Programming Over Network\"\n\n\nThere are also many applications which focuses on collabrative programming only. Tuple​, CodePen​, CodeAnywhere and many others can be used for working together on a code base in real time without dealing with plugins.\n\nIn summary, as everything, interacting on the same code or any file is also possible over network! It definitely makes up for the one big thing hybrid working has been missing and the current software solutions tries to make the hybrid experience almost as good.\n\nTry it out and see for yourself!\nHere are some links to download these solutions:\n\n\n  Visual Studio Live Share\n  Code With Me\n  Tuple for Mac and Linux\n  CodePen\n  CodeAnywhere\n\n",
        "url": "/networked-music/2023/04/21/akarcaal-pairprogramming.html"
      },
    
      {
        "title": "Integrating JackTrip and Sonobus in a DAW",
        "author": "\n",
        "excerpt": "How you can integrate both JackTrip and Sonobus into your DAW\n",
        "content": "Introduction\n\nJackTrip and Sonobus are both free open-source software applications designed for live, high-quality audio streaming and communication. They allow musicians, and other creative professionals to collaborate in real-time, almost regardless of their geographic location. This makes them ideal for producers wanting to collaborate in real-time.\n\n## JackTrip DAW Integration\n\nTo setup JackTrip in your Daw you will need to download JACK, JackTrip and QjackCTL. Start by opening Qjack, JackTrip and your DAW. In my case I will be using Live. First, start your Qjack-server. Go back to your DAW, choose ASIO and your driver type and JackRouter as your audio device.\n\n\n   \n   \n\n\nIf you are getting an error, it might be because you have forgotten to start your Qjack server first. If successful you will see Live within the Qjack graph window.\n\nThe next step is to start a JackTrip server. As soon as a connection is established you will see JackTrip pop up in the Qjack graph. To route incoming audio to my DAW I connect the receive output from JackTrip into one of my Live inputs. I will also remove the connection between JackTrip and my system playback since I can monitor the signal within Live. Also send the output from Live back into JackTrip so that on the other side they will able to hear the metronome while recording.\n\n\n   \n   \n\n\nYou should now see a signal incoming in your DAW and you’re ready to record! If the signal is only being heard on one ear try to connect both receives to both playbacks on the other end.\n\nSonobus DAW Integration\n\nUnlike JackTrip you will only need to download the Sonobus software to use it in a DAW.  After installing you should be able to find Sonobus in your DAW. To use Sonobus in your DAW simply add it to one of your channels as an effect.\n\nTo start a session, click on the connect button. If you want to create a session, simply give your group a name and password and click connect to group. Other will be able to join your session by simply typing in the same group name and password.\n\n\n   \n   \n\n\nSince Sonobus is running as a VST it means we’re able to run multiple instances of it on different channels. This means that we can record multiple people playing at once or send multiple channels from our DAW over the network. Let’s add another instance of Sonobus to channel 2 where we have the drums playing. If we want to record the incoming signal, we can do this by creating a return track and another audio track. I set the audio track to resampling mode and to only receive from sends. Within my first instance of Sonobus I solo the track I want to record and then send the signal to the return track. The final step is to start recording.\n\n\n   \n   \n\n\nI hope this short guide has been helpful!\n",
        "url": "/networked-music/2023/04/23/fabianst-jacktrip_sonobus_daw.html"
      },
    
      {
        "title": "SonoBus setup - Standalone App and Reaper Plugin",
        "author": "\n",
        "excerpt": "Check out my tutorial on how to use SonoBus for your networked performance.\n",
        "content": "SonoBus is a quite versatile tool for all kinds of networked performance environments, especially since you can use it both as a standalone app and as a VST. Let’s have a look together at how you setup your basic SonoBus session, choose the right audio send quality and options for handling latency. After that, I show you how to send multichannel audio over the network from within Reaper with the help of the SonoBus plugin.\n\nVideo Tutorial\n\n\n  \n     \n  \n\n\nTimestamps:\n\n  00:00 - Introduction\n  00:27 - Audio Setup\n  01:27 - Connecting to a network\n  02:38 - Audio send quality settings\n  03:56 - Latency monitoring\n  \n    04:23 - Latency settings -Jitter buffer and latency match\n  \n  07:04 - Reaper session setup\n  08:55 Sonobus VST3 setup\n\n\nSonoBus as a standalone app\n\nBefore connecting\n\nA quick list of tips before starting to ensure a stable baseline:\n\n  no matter the network, definitely use an ethernet cable for connecting to your router, WiFi only adds an extra layer of packet sequencing and network jitter.\n  test your bandwidth and network latency beforehand so you have a basic understanding of what you are going to be able to achieve quality-wise\n  use an external audio interface instead of your internal soundcard to reduce hardware latency\n\n\nAudio settings\nChoose your interface, input and output routing and your sample rate. All participants in your session can work with different sample rates since audio gets automatically resampled in SonoBus if necessary. With your sample buffer size, start low and go up only if you need to, for example if you hear crackling in your audio processing.\n\nPrivate network group connection\nYou can either use SonoBus’s own connection server (obviously does not work with a local network) or create your own group session by using the app on its own. If you are the one creating the group you need to enter your IP and a specific port in the format IP:Port. Port 10999 is the only port that works with this method.\n\nSend formats\nWhat kind of audio format you use really depends on the bandwidth of your network and your specific use case. There is a trade-off you have to keep in mind. Compressed audio is less demanding in terms of bandwidth but adds an additional 2.5ms of latency on either side of the connection. Uncompressed audio uses about 10 times more bandwidth but adds no additional latency.\n\nLatency settings/ options\n\n  There are three main sources of latency:\n    \n      Software (audio buffer, apps) and hardware (audio interface)\n      Delay in the network connections between you and the other participants\n      Latency added by buffering network jitter\n    \n  \n\n\nWhat is network jitter? It is essentially a variable latency of incoming data because of dropped or re-routed packets of data. We may hear audible clicks if packets are dropped or arrive late, which is why we need to use buffering. SonoBus offers three modes for buffering jitter and I explain those in detail in the tutorial. There is also viable and quick solution called ‘latency match’ if your priority is maintaining a stable performance environment.\n\nSonoBus as a VST in Reaper\nLet’s say you want to provide backing tracks for your band mates during a concert in Oslo while you are being comfortable in Berlin, or you just want to share your new composition in a listening session. SonoBus as a plugin makes this quite an easy feat. It is as simple as setting up Reaper as you are used to and then creating a bus on which your plugin will be. Send the channels you want to share to the bus, start the plugin and connect to the network - et voilà - you are ready to go. Important side fact: you will not be able to run a local connection server in the plugins. This is only possible in the standalone app. The straightforward solution is to start a standalone application on one machine and just let it sit there, functioning as a hub to which you connect from within Reaper.\n\nSources\nSonoBus userguide\n",
        "url": "/networked-music/2023/04/23/kristeic-SonoBus-standalone-and-vst.html"
      },
    
      {
        "title": "Spatial Audio with Max-Msp and Sonobus",
        "author": "\n",
        "excerpt": "This post’s video tutorial aims to introduce readers to the many uses for which the Sonobus can be implemented as a VST in Max-Msp.\n",
        "content": "Multi-channel Audio Over the Network\n\nThis post’s video tutorial aims to introduce readers to the many uses for which the Sonobus can be implemented as a VST in Max-Msp. Thanks to Max by Cyclying74’s modular and object-based design, we can easily incorporate over-the-network multichannel solutions like Sonobus as a VST into a variety of spatial audio scenarios. Hope you Enjoy!\n\n\n\nApproch:\n\nWe used the ICST Ambisonics package for this video tutorial. The package is cost-free and is easily installable using the Max-Msp package manager. The approach in this experiment is to encode a single mono file source to higher order ambisonics and then decode the audio back into separate 8 decoded channels since it is assumed that we are not playing back audio in ambisonic format, so that we send the decoded audio over the network by Sonobus.\n\nAs the heart of the audio chain we have the mc.ambiencode~ and, mc.ambidecode~:\n\n\n   \n   Encoder and Decoder\n\n\nThe encoder is given the spherical coordinates as the position of the sounding source in space. This is accomplished by initializing and utilizing the object’s costume attributes to provide us with the features we require for control:\n\n\n   \n   Ambimonitor object\n\n\nAlso the same approach applies for the speakers relative position where it can easily be given to the decoder:\n\n\n   \n   Ambimonitor object\n\n\nSimply you can bring the Sonobus in the game by loading the VST3 version in the VST~ object and give it proper number of inputs you need to be sent to the network!\n\n\n   \n   \n\n\nTakeaways:\n\n\n  For fine-tuning the latency, the buffer size or signal vector size in Max should be set within the audio status. Max controls the Sonobus as the VST inside the patcher.\n  To send multiple channels while you are actively positioning the sound source in space, you require a noticeably strong lan internet connection.\n  You can further investigate the documentation and develope rather sopisticated setups to be sent over the network!\n\n",
        "url": "/networked-music/2023/04/23/masoudn-spatial-max-sonobus.html"
      },
    
      {
        "title": "NMP kit Tutorial",
        "author": "\n",
        "excerpt": "A practical tutorial on the NMP kits.\n",
        "content": "\n   \n   NMP Kit\n\n\nThis is a tutorial for getting started with the NMP (Network Music Performance) kits. It focuses on practical use of the kits and does not go into theoretical detail on what a NMP is and how the different softwares (LoLa, JackTrip, SonoBus etc.) works. Audio signal flow and solutions for routing audio in the system is covered, however - there are many solutions to this, so consider the ways shown in the video as a starting point for understanding audio routing in the NMP kit.\n\nTo start with, we need to take off the flight case lids and store them somewhere safe. The kit comes with an audio cable case and a computer cable case. From the computer cable case, grab the ethernet cable and connect to the ethernet PCI on the back of the computer. Do not connect to the motherboard.\nThere are two screens. One of them is an ultra fast gaming monitor for stage performers to see the network performers. The other one is for the NMP engineer to use with the rack computer. There is also an ultra fast projector to project network performers together with local performers. Connect to the GPU PCI on the back of the computer using HDMI or DisplayPort.\n\nBe careful when getting the Ximea camera from the rack drawer. Mount it on the Manfrotto camera stand and connect the camera USB to the camera and to the computer’s USB 3.0 PCI on the back.\nGrab the mouse and keyboard from the rack drawer and turn on the computer power supply (back) and on/off button (front right side). Log in.\nLaunch TotalMix and LoLa and adjust buffer settings and options to optimize your network connection.\n\nThe mixer can be used to mix down input sources to a stereo mix which can be sent over the network. It’s also possible to send multiple channels, by using group output 1-2 and 3-4. This gives a total of six channels. ADAT inputs can also be used directly, but this requires splitting the input signal to the mixer, or that these signals get mixed to the speakers connected to the mixer’s main outputs using TotalMix. There are many workarounds, so use the ones needed for your setup.\n\nNetwork audio 1-2 is received on channel 15-16 on the mixer. For receiving more channels, network audio 3-4 can be routed via the headphone amplifier, and ADAT outputs 5-8 are idle and ready to be used. Though it might not be the easiest setup to mix with, as the mixer has stereo channels from channel 9-16.\n\nCable management is important for maintaining the kit during use. Fold the cable by following the natural bend of the cable. Both audio and computer cables should be wrapped using the attached cable straps before putting them back in the designated cable cases.\n\nNMP Kit Tutorial - Getting Started\n\n  \n    \n  \n\n\nFor more and detailed information, please see the NMP Wiki.\n\nThank you.\n",
        "url": "/networked-music/2023/04/23/henrikhs-nmp_tut.html"
      },
    
      {
        "title": "Audio Codecs and the AI Revolution",
        "author": "\n",
        "excerpt": "I dove headfirst into the world of machine learning-enhanced audio codecs. Here’s what I found out.\n",
        "content": "\n   \n   A MUSHRA plot for the EnCodec ML-based codec. The X axis is bitrate and the Y axis is the MUSHRA score, a subjective measure of audio quality.\n\n\nIntroduction\n\nAudio codecs are something we experience nearly every day, even if we don’t know it. From the audio you stream from Spotify (other streaming services are available) to the voices of your classmates or colleagues that you hear on Zoom, it is all compressed using an audio codec. So, what is a codec?\n\nAn audio codec is essentially an algorithm for changing the representation of an audio signal. It is often tied up with compression to reduce the amount of storage required. All codecs must include an encoder, which turns a raw audio signal into an encoded format, and a decoder, which does the opposite.\n\nTraditional Codecs\n\nOne of the most popular audio codecs of the past two decades is MP3. You will probably recognise MP3 as an audio file type, but it also the name of a codec. The MP3 codec made waves (get it?) because it can compress audio to around 10% of its original size – roughly 1MB per minute of audio. In this case, the MP3 file is the encoded format, which is decoded during playback.\n\nAnother popular codec, albeit one with less brand recognition, is Opus. Opus is designed primarily for real-time applications. When you hold a voice call on WhatsApp or Discord, your microphone audio is encoded on your computer using Opus, sent over the internet to your friend, whose computer decodes it. Opus is specifically designed to encode at very low bitrates – that is, to compress audio so that it is represented by only a small number of bits per second, in turn using less of the available bandwidth on your home Wi-Fi or mobile network. It is also designed to be low-latency, meaning it introduces only a minimal amount of delay during the encoding and decoding processes.\n\nTraditional codecs like MP3 or Opus leverage psychoacoustics and fancy maths to compress audio. They start by removing anything the human ear won’t hear – frequencies above the range of human hearing or sounds masked by other sounds, and then use mathematical tricks like Fast Fourier Transform to represent the signal using less information.\n\nAudio Codecs &amp; The AI Revolution\n\nMachine learning is quickly making its mark in every arena of technology, and audio codecs are no different. The last three years have seen the release of several new audio codecs which use machine learning, rather than the traditional psychoacoustic and mathematical techniques, to compress audio. Prominent among them are SoundStream, Lyra, which is developed by Google, and EnCodec from Meta.\n\nEnCodec is particularly relevant to network musical performance. It can encode wide-band stereo audio signals to bitrates as low as 3kbps while maintaining significantly higher quality than Opus at the same bitrate. SoundStream and Lyra are similarly impressive, although they are intended for compressing speech rather than music. Listen to the audio clips below to hear Opus and EnCodec in action on a sample of a string quartet:\n\n\n  \n    \n    Alternate Text\n  \n  The uncompressed audio sample.\n\n\n\n  \n    \n    Alternate Text\n  \n  The sample compressed to 6kbps using Opus.\n\n\n\n  \n    \n    Alternate Text\n  \n  The sample compressed to 6kbps using EnCodec.\n\n\nML Codecs for Network Musical Performance\n\nWhat advantages do these new codecs offer for network music applications? Primarily, they may allow us to perform network music over a much wider range of networks. At SMC we are spoilt by the LOLA network, which is our gigabit local network used solely for music research. However, by encoding music to the small bitrates made possible by ML-enhanced codecs, we may be able to rehearse and even perform music over standard consumer Wi-Fi and even mobile networks like 5G.\n\nOf course, there are some problems a snazzy new codec won’t solve. We’re still ultimately limited in terms of latency by the speed of light – sending audio over fibre optic cables from Oslo to Sydney takes a minimum of 60ms simply due to the laws of physics. However, ML-enhanced codecs may help to democratise network music performance, bringing it outside of the classroom and into the hands of the average home musician. That is something we can all get behind.\n\nWatch the Video\n\n\n\nReferences\n\n\n  S. Hacker, MP3: the definitive guide, 1st ed. Sebastopol [Calif.]: O’Reilly, 2000.\n  ‘Opus Codec’. https://opus-codec.org/ (accessed Apr. 22, 2023).\n  Défossez, J. Copet, G. Synnaeve, and Y. Adi, ‘High Fidelity Neural Audio Compression’. arXiv, Oct. 24, 2022. Accessed: Apr. 08, 2023. [Online]. Available: http://arxiv.org/abs/2210.13438\n\n\n",
        "url": "/networked-music/2023/04/23/jackeh-audio-codecs-ml.html"
      },
    
      {
        "title": "Basics of Computer Networks in NMPs",
        "author": "\n",
        "excerpt": "Crash course on computer network used in Network Music Performances.\n",
        "content": "Are you curious about the magic that happens behind the scenes of network music performances? This blogpost briefly covers the fundamentals of computer networks in NMPs in extremely simple terms! It all starts with a network, which is like a city where computers are the houses and communication is like sending parcels between houses.\n\n\n   \n   A city or a network.\n\n\nIP Address and Port\nEach computer in a network has an identification number called an IP address, which is like the physical address of your house in a city. Remember that if you are in different places, you may need to do something called port forwarding to connect to each other.\n\n\n   \n   \n\n\n\n   \n   \n\n\nServers\nServers are dedicated devices that manage and provide services to other devices in the network, like central offices in a city that serve the residents.\n\nPing\nWhen you connect to a network, you can test your connection by sending a tiny package to a remote host and receiving it back.\n\nPacketization\nPacketization is a process of breaking up large blocks of data into smaller, more manageable chunks called packets. Think of packetization like in a trip, rather than trying to stuff all your belongings into one giant bag, it’s easier to organize your items into smaller, more manageable packages. Similarly, packetization allows data to be sent more efficiently over a network, reducing the chances of errors.\n\n\n   \n   \n\n\nJitter and Jitter Buffer\nNormally in networks, packages are sent regularly. But sometimes, there may be variation in the delay between package deliveries, which is called jitter. We solve this with a technique called jitter buffering which is temporarily storing incoming packages and releasing them at a steady rate. However, packing up enough amount of packages introduces latency. It is essential to find the balance between the latency and jitter in that case!\n\n\n   \n   \n\n\nCompression\nAnother technique to increase efficiency is compression, which reduces the data size while still retaining the essential information. Good, but this can also introduce latency as the compression algorithms take some time for theirselves.\n\nNetwork Capacity\nMost of the time, network’s capacity is measured by the amount of data that can be sent in a given amount of time, called bandwidth. Think of it like the width of roads in a city that determines how much traffic can flow through at once. Bandwidth is a theoretical measurement. When it comes to practice, the actual amount of data that is transmitted over a network is called the Throughput. Bandwidht is a measure of capacity, while throughput is a measure of performance.\n\n\n   \n   \n\n\nFirewall\nJust like a security guards standing at the entrance of your garden, firewalls are intelligent barriers that protect networks and computers by controlling incoming and outgoing traffic. However, it can be a nightmare when trying to establish a custom connection. So, it’s very common to bypass firewall when connecting for NMP.\n\n\n   \n   \n\n\nTCP/UDP\nWhen it comes to transmitting data, the two most common protocols in computer networks are TCP and UDP. TCP ensures that data is transmitted accurately by verifying that all packages have been received. UDP, on the other hand, sends data without confirming receipt or checking errors, making it faster but less reliable. Depending on the application, you may use one or the other. For example, UDP is more reliable for real-time, while TCP is preferred for critical applications.\n\nConnection Typologies\nTo set up a connection for a network music performance, there are different typologies to choose from, like hub-server and peer-to-peer setups. In a peer-to-peer setup, each participant is connected directly to each other, which is useful for smaller performances. In a hub-server setup, a central server controls the flow of data between participants, which is useful for larger performances with more participants. As always, there are trade-offs to consider when choosing the best setup for your needs.\n\nThere are various software available today, you can go ahead and check their websites or documentation to see which typology they use. Here are the common software and their type of connection.\n\n\n   \n   \n\n\nVideo\nAll these gathered in a 7-minute-long video lecture. Enjoy!\n\n\n\nIllustration elements: Freepik\n",
        "url": "/networked-music/2023/04/23/ahmetem-basics-of-computer-networks.html"
      },
    
      {
        "title": "Challenges with Midi",
        "author": "\n",
        "excerpt": "Is it easy to create chord progression from midi files?\n",
        "content": "Is it easy to create chord progression from midi files?\n\nWhen programming with audio, midi format can become very beneficial for several reasons. However, it may also be challenging depending on the task intented to be performed. In this post I am going to try to share my experience with midi analysis as part of my machine learning project.\n\nThe goal of analysis\n\nFor my project, I needed to extract a chord progression from each song which can be represented as an array of chord names. In order to do so, I needed to parse midi files and understand the chord played at each time step.\n\nChallenges\n\nUsing music21 library\n\nAs first challenge, after parsing the midi file by music21 library, I was only able to get the musical lines as divided to parts by using partitionByInstrument function. It meant that I had paralel musical lines that includes note, rest, chord values but includes them per instrument part not for overall song. As my project required, I needed to understand the chord that was played at each time spent so getting that information per instrument didnt help me to find a single chord played.\n\nIn addition, parts have a stream of the instrument elements which might be note, rest or chord but some of the notes were playing at the same time so it wasnt reliable in the sense to have an order within. I also needed to represent each notes played as chord as well so it was still challenging for my need.\n\nAnother challenge was with chordify function of music21 As stated in the guide of MIT, chordify function is defined as below:\n\n“Chordify is a madeup word that we created in music21 for the process of making chords out of non-chords. Chordify powerful tool for reducing a complex score with multiple parts to a succession of chords in one part that represent everything that is happening in the score.”\n\nAfter parsing midi data to music21 stream by music21 converter function, it is possible to chordify this stream. However this didnt work with any midi file I found. I am still not able to understand the root cause of the problem but the problem was fixed after downgrading the music21 library version to 6.7.1.\n\n\n   \n   The code converts and chordifies the midi file\n\n\n\n   \n   The exception I got from each file\n\n\nUsing pretty_midi\n\nPretty midi is one of the most used libraries in order to analyse midi data, but it required extra work in order to extract some simple informations. The functions like piano_roll gives a very good overview but they are also complex to work with some ML methods like regression which I intended to use.\n\nAlso it is easier to get musical line by instrument from midi and understand each note or chord and start end times but then we need to iterate over each note, analyse the paralel ones and come up with a chord represents all the paralelel chords. There is also mostly an overlap between instruments and what they play and it makes this even harder.\n\nConclusion\n\nAt the end, I ended up spending a lot of time in order to extract a one line chord progression from midi and I started to use other data formats like chord txt files. However with more effort I believe it might still be possible to run more advanced algorithyms on midi files and extract a chord that is independent from any instrument or any paralel musical line.\n\nBut as an answer to the first question:\n\n  No it is not always easy :)\n\n",
        "url": "/machine-learning/2023/04/24/akarcaal-midichallenges.html"
      },
    
      {
        "title": "Programming with OpenAI",
        "author": "\n",
        "excerpt": "How OpenAI solutions help us to program?\n",
        "content": "Artificial Intelligence solutions has been positioning itself in critical roles within the work force of ours for a while now. However with OpenAI solutions like ChatGPT, Github Co-Pilot (both of them were built on GPT3.5 then 4 next) we all experienced a big shock and it started lots of discussions again.\n\nAs a developer, I also have been using ChatGPT and observing developers around me trying it out. It is very surprising that how fast those tools became part of our daily life and we all got used to it very quickly but I believe it needs to be used very carefully.\n\n\n   \n   Copilot acts like a pair programming friend\n\n\nBe careful if you are getting lazier\n\nLets start with Github Copilot. As stated, GitHub Copilot turns natural language prompts into coding suggestions across dozens of languages. It means, you can write code in many language by just explaining your need in a spoken language. This sounds very charming if you dont feel confident with programming or if you are trying a new language out, or if you have a task that got very challenging. Just explain your need and copilot will develop your function in your preferred editor.\n\nIt is however not very helpful always. It is critical during the learning process to experience bugs and problems and try fixing them out by visiting several sources. If your intention is to continue developing yourself and being able to program independently, then asking every task to your AI friend may make you very lazy. You can of course try to understand each suggested code block and turning the pilot off after some point, I highly recommend to do it as early as possible!\n\n\n   \n   Dont let yourself to get lazy\n\n\nTodays amazing solutions made us not to tolorate a second of stopping and being hesitant but it is actually a very creative moment! We shold be stopping between steps and spend some time which we only try to think about the solution or find it from somewhere in our mind. I see it even myself that I used to try to one up with solution myself and read more before, now I find myself asking the question to an AI chat or co-pilot after the third second.\n\nDont ask everything to AI but try finding the solution yourself\n\nThis applies especially to ChatGPT. Even though we except the fact that ChatGPT can make mistakes or can be biased, it still is way easier to ask it compared to doing the research.  I started using CharGPT for a new thing I tried and after some time I noticed how much repetation I made in the project and how much time I wasted. I am of course not claiming that it is not a useful or helpful tool. It is amazing and it will be even more amazing in the future but it is very important to keep the control over our progress in the project and not copy paste a suggestion right after.\n\nIn addition, after each copy paste or delegation of responsibility, it gets harder for you to spot the bugs in the code and fix it. Do not delegate your learning to a chatbot and become a button pusher left with no improved skill.\n\nAI is helpful but be careful\n\nIn summary, there are very cool OpenAI based solutions exists and the new ones are coming. The new versions of them are even better and more precise. I totally think the progress is amazing and it will change a lot of things in the future but if you want to learn the basics or have control over your components, be careful. Take AI as your friend but be the one being in charge and not find yourself repeating everything those solutions recommend.\n",
        "url": "/machine-learning/2023/04/24/akarcaal-programmingwithchatgpt.html"
      },
    
      {
        "title": "Comparing MIDI Representations: The Battle Between Efficiency and Complexity",
        "author": "\n",
        "excerpt": "A comparing of different MIDI representations for generative machine learning\n",
        "content": "\n   \n   \n\n\nIn my previous blog post, I wrote about different ways to represent MIDI for a neural network with the intent of generating melodic, symbolic music. You can read my previous post here. In that post I mentioned some drawbacks and benefits of different representations, but how can this be observed in the music that is generated? I have implemented three of these methods, and generated some melodies based on the representations. The results stem from three similar neural networks, with similar configurations in order to compare only the representations.\n\nThe representations\n\nThe three different representations I compared are 1D event based, piano roll and relative pitch. The main differences between the representations are that event based is a 1D representation, while piano roll and relative pitch are 2D. 1D representation for a neural network is suboptimal, but in return it is far more memory efficient. Piano roll quantizes MIDI into timesteps and represents each time step as a one hot encoded vector. It is the most complex representation, but with a very high memory consumption. Relative pitch is a continuation of piano roll, but only represents the transition data. For more detail about the methods, please check out my previous blog post.\n\nLet’s start with training\nI have mentioned that the different representations have different memory consumptions and training speeds. For a machine learning problem, it can be important to make efficient models, in order to make it available for programmers without powerful machines.\n\nIn the following tests I trained the three different representations on a subset of the complete dataset I use, and gathered data on the memory usage and training time. Each model was trained locally using GPU on my M1 macbook pro.\n\nEvent based\nThis is the most efficient method. To represent the dataset takes 5298 inputs to the model, where each input is a vector of length 3, resulting in a total of 15.984 numbers fed to the model. This results in a peak memory consumption of 1,23 GB, making each epoch trained in 2 seconds.\n\nRelative pitch\nThis second most efficient method is relative pitch, and it is much heavier than event based. To represent the dataset takes 31.496 inputs to the model, where each input is a vector of length 25, resulting in a total of 787.400 numbers fed to the model, resulting in a peak memory consumption of 1,51 GB. This method traines an epoch in 22 seconds. This is more than 10 times the time it took to train the model using event based representation.\n\nPiano roll\nFinally is piano roll. To represent the dataset takes, like in relative pitch 31.496 inputs to the model. But here each input is a vector of length 129, resulting in a total of 24.062.984 numbers fed to the model, resulting in a peak memory consumption of 2.71 GB. An epoch ends up taking 24s seconds to train.\n\n\n   \n   Comparison of the different models\n\n\nWhat about the music?\nWhile reading about memory consumption and training time for the different representations is interesting, we need to take a dive into the generated music. How does the music sound, and what are the characteristics of each model?\n\nA disclaimer: In my research, I have put a limited effort into fine tuning the neural networks, since it is a comparative study. So while the melodies generated are of sub optimal quality, I think it still works as grounds for a comparison of the different models to see strengths and weaknesses to each representation. The network used is an LSTM, which is no longer state of the art for music generation, music generated using it has been described as sounding like a pianist playing without any intention or goal. This can be heard in the generated music.\n\nResults\n\nEvent based\nWhile it is super efficient at storing music data in a compact way, it generates very poor results, with little musical quality. One of the first things you notice is that every note has the same duration and with no silence between any notes. It also sounds somewhat random, like a four year old playing the piano with one finger. It also often plays sharp-notes, although it is trained on music in C major. You can listen to an example here. Only one is needed, because there is little variation in what it generates.\n\n\n\t\n      \n    \n   Event based example\n\n\nPiano roll\nBeing the most computational heavy seems to be paying off in terms of results. This method has managed to generate the most melodic melodies, it seems to be the most on beat, and often uses silence to make cool beats or dynamic sounding melodies. But it is still a varying model, and does often make vague music. It very often only wants to pick either silence or the same note as the previous note. Therefore I had to implement some randomness to choose whether it should pick its best note, or if it should choose a note that it deems the next or third best. Another problem is that it often sounds like it does not have any goal, while it often picks notes that are somewhat musically reasonable, it can struggle making music that feels intentional. I believe this is in part the result of using a LSTM.\n\n\n\t\n      \n    \n   An example of an on beat, dynamic and melodic melody.\n\n\n\n\t\n      \n    \n   An example of the lack of intention.\n\n\nRelative pitch\nBeing a compressed version of piano manifests itself in the output generated using this representation. You can find some of the benefits of piano roll like managing to use silence to make more dynamic melodies than event based representation did. The same drawbacks are also apparent, but to a larger degree. Relative pitch struggles with making non repetitive melodies because it almost always wants to generate a note that is either silence, the same note, two halftones up or two halftones down. This makes it repetitive and unmelodic. It also struggles with staying in one key.\n\n\n\t\n      \n    \n   An example of not staying in one key.\n\n\n\n\t\n      \n    \n   An example of repeating notes, only going up or down a fixed interval\n\n\nConclusion\nFrom my own experience listening to what my models generated, it is clear that in these three representations, the quality of the output can be ranked in the same order as the memory consumption, and the clear king is piano roll. It is able to represent melodic structures in a way that the other two do not come close to. It is important to note that this is just my experiences exploring the different methods, and a different model, dataset or hyperparameters could show other results.\n\n",
        "url": "/machine-learning/2023/04/25/tryb-comparing-representations.html"
      },
    
      {
        "title": "A Rhythmic Sequencer Driven by a Stacked Autoencoder",
        "author": "\n",
        "excerpt": "Sometimes you need to leave room for the musician\n",
        "content": "A Partially-Automated, Autoencoder-Driven, Generative Step-Sequencer\n\nFollowing on from the previous blog posts on autoencoders, this post will take a look at implementing a step sequencer for melodic musical parts but with its rhythmic aspect driven by a generative “stacked autoencoder” model.  Beyond the technicalities, the project also aimed to explore methods whereby generative AI in music could be made more accessible to music practitioners.  This meant that firstly, the training data pipeline needed to be simple and accessible for the average music producer to use. Secondly, it necessitated a structure which was computationally light enough that training and inference could be run on a consumer computer and was fast enough to not interfere with the creative process.\n\nStacked Autoencoder\n\nSimply put, a “stacked” autoencoder can be thought of as a traditional autoencoder architecture with a greater number of hidden layers.  This means that the data goes through several stages of encoding before it reaches the latent layer and a corresponding greater number of decoding layers until it reaches the output.\n\n\n   \n   Stacked Autoencoder from Briot(2020) \n\n\nBriot (2020), explains the advantage of this for music generation as,\n\n“The chain of encoders will increasingly compress data and extract higher-level features… They are also useful for music generation… This is because the innermost hidden layer, sometimes named the bottleneck hidden layer, provides a compact and high-level encoding (embedding) as a seed for generation (by the chain of decoders)”\n\nThis technique of feeding new, user input-able information into the “bottleneck layer” is the basis for the generative aspect of this project.\n\nTraining\nThe training data pipeline for this project needed to represent a methodology which could be made accessible to on-the-ground music producers.  Most electronic music practioners will cite having extensive libraries of audio samples and loops.  There are also huge amounts free and commercially products of royalty-free loops available to practioners.\n\nTraining a neural network on audio presents challenges however.  Feeding a neural network with raw digital audio is inappropriate as the computational load is too large.  Furthermore extracting meaningful, musical and useful features from audio is a task requiring expertise beyond the expectation of the average music practioner.\n\nAs the sole concern of the model is rhythm, the training data was processed to extract rhythmic transient/onset information.  This was then encoded as a symbolic representation of two musical bars with a resolution of 16th notes. This resulted in binary lists representing 32-step sequences where a 1 denoted a rhythmic event and a 0 denoted a “rest”.\n\nAn example from my dataset of this:\n[1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n\nWe usually imagine step-sequences moving horizontally left-to-right.  Returning to Briot’s diagram above of the stacked autoencoder, if you imagine the sequence turned vertically and each step fed in as a separate input to that structure, then you have an idea of how the model is given this representation of the data.\n\nThe result of this is two-fold.  Firstly, a format of training data which is freely accessible to music practioners can be used to train the model.  Secondly, the architecture of the autoencoder is computationally very light and can be trained and utilised very quickly.\n\nThe Human Aspect\nIn opposition to systems employing AI for the creation of more and more complete systems for automatic music generation, this project aimed to instead purposely leave room for the human musician.\n\nAs such, this sequencer offers only a “partially-automated” functionality.  While the AI model is responsible for generating rhythmic information, pitch information is left for the human, as are several ways of altering how the inferred data is used.  Below is the current user-interface for the sequencer and an explanation of the various controls.\n\n\n   \n   Sequencer UI in Pure Data \n\n\nX, Y\nThese are the primary AI-focused user controls. These are for the values to be passed into the “bottleneck layer” and thus generate the rhythmic information.  This layer was purposely kept to 2-dimensions as it presents an intuitive representation to the user.  Another term for the ‘bottleneck layer” is “latent space”, connoting ideas of a physical space of possibility.  This concept is also used in the now-discontinued but open source Mutable Instruments Grids  Eurorack module, where it is referred to as “Topographical” sequencing.\n\nThresh\nWhen generating new rhythmic patterns, the model outputs values between 0 and 1.  Therefore, the output has a threshold value for what is and is not considered a rhythmic event.  By default, this is 0.5 but by opening the parameter to the user, they are able a further level of control over the sequence.\n\nPitches &amp; Num Steps\nThese represent controls often found on conventional step-sequencers.  The user inputs the musical pitches to be stepped through as well as the length in steps of the repeated sequence.\n\nFuture Work\nIn order for this process to be truly accessible and implementable for the wider world of electronic music practioners, further work is required.  Currently the model’s training and inference happen in Python via Jupyter Notebook, while the UI exists in Pure Data with OSC being used to communicate between the two.  Ideally, the entire process would exist within a single environment with the addition of a user interface for the training process.\n\nThis project has however achieved many of the primary goals set out for it.  Furthermore, the need for lightweight, rapidly trainable and accessible models will likely continue as generative AI becomes more commonplace and more musicians become open to the idea of “AI assistants” being involved in the creative process.\n\nReferences\nBriot, J.-P., Hadjeres, G., &amp; Pachet, F.-D. (2020). Deep Learning Techniques for Music Generation (1st ed. 2020., p. 1 online resource (XXVIII, 284 p. 143 illu, 91 illu in color.)). Springer International Publishing. Available from: https://link.springer.com/book/10.1007/978-3-319-70163-9\n",
        "url": "/machine-learning/2023/04/25/alexanjw-stacked-autoencoder.html"
      },
    
      {
        "title": "Pytorch GPU Setup Guide",
        "author": "\n",
        "excerpt": "Having trouble getting Pytorch to recognize your GPU? Try this!\n",
        "content": "Getting Pytorch to recognize my GPU is something I’ve spent more time on than should be necessary. Here are some things I learned while pulling my hair out.\n\n\n   \n\n\nSetup\n\nI’ll be using conda for both environment and package management, and I’m setting up a Windows 10 PC. If you install Pytorch through your command line interface (CLI) like so…\n\n\n  \n    conda install torch\n  \n\n\n…a CPU compiled version of pytorch will be installed. A CPU is much slower than a GPU, and can speed up both training and inference tenfold, so wouldn’t it be nice to be able to use a GPU instead? This is how:\n\n\n  Create a new environment with conda create -n “YOUR_ENVIRONMENT_NAME” python=”DESIRED_PYTHON_VERSION”\n    \n      Note: Pytorch requires python=3.7 and above.\n    \n  \n  Move into the new environment using conda activate “YOUR_ENVIRONMENT_NAME”\n  Go to the Pytorch website, enter your platform and package manager, and copy the resulting command.\n    \n      Note: The conda command will look something like: conda install pytorch torchvision torchaudio pytorch-cuda=[CUDA_VERSION] -c pytorch -c nvidia\n      Note: The pip command will look something like: pip3 install torch torchvision torchaudio –index-url https://download.pytorch.org/whl/cu[CUDA_VERSION]\n    \n  \n  Copy the command to your command line interface, and you should be good to go.\n\n\n“Now hold up a minute”, I hear you saying. “How do I know which CUDA_VERSION I need?” A web search will take you down a rabbit hole, so keep on reading and I will save you the trouble.\n\nCUDA Versions\n\nCUDA has both a driver API and a runtime API, and their API versions can be entirely different. This CLI command:\n\n\n  \n    nvcc --version\n  \n\n\nwill tell you the runtime API version, while\n\n\n  \n  nvidia-smi\n  \n\n\nwill tell you the driver API version. This command points to the GPU driver, and it’s this CUDA version you need when installing Pytorch. This means the original command from the Pytorch website works just fine for most cases. Just double check with the command above if you’re running into issues.\n\nTroubleshooting\n\nTo check if Pytorch can find your GPU, use the following:\n\n\n  \n    import torch\n    torch.cuda.is_available()\n  \n\n\nThis will return True if a GPU is found, False otherwise. If your GPU cannot be found, it would be helpful to get some more feedback. Try sending something to the GPU. It will fail, and give you the reason:\n\n\n  \n    torch.zeros(1).cuda()\n  \n\n\nShould you want to start over because Pytorch is still not communicating with your GPU, you can remove your current environment and packages through your command line interface like so:\n\n\n  \n    conda activate base\n    conda remove -n \"YOUR_ENVIRONMENT_NAME\" --all\n  \n\n\nYou could start trying to downgrade one package and update another until you get the right configuration, but I think it’s just easier to start from a clean slate, and I had to do so a few times.\n\nAdditional notes\n\nIf any GPU is recognized, you can now get more info about them or even decide which tensors and operations should go on which GPU.\n\n\n  \n    torch.cuda.current_device()     # The ID of the current GPU.\n    torch.cuda.get_device_name(id)  # The name of the specified GPU, where id is an integer.\n    torch.cuda.device(id)           # The memory address of the specified GPU, where id is an integer.\n    torch.cuda.device_count()       # The amount of GPUs that are accessible.\n  \n\n\nHopefully, that will resolve some issues for you. Happy hacking!\n",
        "url": "/machine-learning/2023/04/25/olivegr-pytorch-gpu.html"
      },
    
      {
        "title": "Generating Video Game SFX with AI",
        "author": "\n",
        "excerpt": "A first look at text-to-audio sound effect generation for video games.\n",
        "content": "Early 2023, Hugging Face released a pipeline for AudioLDM. What this means is that audio generation with AI became readily accessible to everyone, with low setup times. Text-to-audio is, at this very moment, right at our fingertips! Curious about its usage in game audio, I set out to explore its current capabilities and limitations.\n\nAI SFX Generation Setup Guide\nGetting up and running took a bit more effort than I anticipated due to Python packaging conflicts and GPU issues. If you are having trouble with pytorch and GPUs, you can follow my setup guide. If you still have issues using CUDA, do like me and switch from full precision (torch.float32) to half precision (torch.float16) like so:\n\n\n  \n    from diffusers import AudioLDMPipeline\n    pipe = AudioLDMPipeline.from_pretrained(\"cvssp/audioldm\", torch_dtype=torch.float16)\n    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n    pipe = pipe.to(device)\n  \n\n\nIf you’re wondering: yes, this can negatively influence your results, but not enough for it to be a real issue.\n\nGenerating SFX With AI\nThe prompts used were widely different for the sake of exploring the versatility of the AI. The authors suggest sticking to sounds from real life, but—lazy as I am—I asked directly for video game sounds as well. 20 sounds were generated for my first experiment, and here are the parameters I used:\n\n\n  \n  audio = pipe(prompt,\n               num_inference_steps=15,      # This is the amount of denoising steps. Higher is cleaner, but makes generating take longer.\n               audio_length_in_s=4.0        # The length of the generated audio. Default is 5, but for a one-shot sfx, you don’t need more than 3-4 seconds. This will also speed up generation on CPUs.\n               num_waveforms_per_prompt=1,  # How many files to generate.\n               guidance_scale = 3,          # How much to conform to your prompt. Higher is more accurate, but introduces more noise.\n               #negative_prompt=\"music\"     # Elements you want to avoid in the resulting sound.\n              ).audios[0]\n  \n\n\nResults were mixed. For a sound to be usable as is in a video game, it will more often than not need to be isolated, meaning no background noise or overlapping sounds. Some prompts gave me multiple sounds of the same type layered on top of each other:\n\n\n  \n    \n    Alternate Text\n  \n  Prompt: \"hammering a nail into wood\"\n\n\nOthers contained nothing but noise:\n\n\n  \n    \n    Alternate Text\n  \n  Prompt: \"high quality construction site sfx, slight compression\"\n\n\nThis was mostly the case for “imaginary” sounds with no real-world references, like a dragon. I can imagine the data set for dragon sounds is comparatively quite small.\n\n\n  \n    \n    Alternate Text\n  \n  Prompt: \"a big dragon's roar\"\n\n\nOn the other hand, many of the foley-like sounds were as clean as I could ever have hoped for. Each sound took 3-4 minutes to generate on the CPU, and mere seconds on my GPU after additional testing.\n\n\n  \n    \n    Alternate Text\n  \n  Prompt: \"a nail being hammered into wood\"\n\n\nAdding the word “cinematic” to the prompt generated similar sounds, but with more bass.\n\n\n  \n    \n    Alternate Text\n  \n  Prompt: \"cinematic hammering a nail into wood\"\n\n\nThe model seems either somewhat biased toward music, or infer some musical meaning from certain adjectives. By this I mean that no prompts for music generated sound effects, but 2-3% of prompts for non-musical sounds generated synthesized chords, even with a negative “music” prompt.\n\n\n  \n    \n    Alternate Text\n  \n  Prompt: \"footstep in wet sand\"\n\n\nMore can probably be done to avoid generating music-like content, but I figured it wasn’t worth spending time on. I simply discarded any files with musical content, but who knows, maybe you discover some new sounds you like.\n\nAsking for 90s video game sounds provided some interesting results. I specified 90s, because the style at the time was limited by hardware but a lot of pre-processing could be done to make sounds more expressive, like crazy pitch envelopes. AudioLDM is capable of making abstract sound effects equally expressive.\n\n\n  \n    \n    Alternate Text\n  \n  Prompt: \"90s video game sound effect\"\n\n\nSacred Gems\nOne of my current projects is designing sound and music for a 2D bullet hell game called Sacred Gems. Aesthetically, it takes a lot of inspiration from 80’s anime and we wanted similar elements to be featured in the soundscape. Since some of the sounds can be a bit noisy, and as far as I can tell the pipeline is fixed to a sample rate of 16kHz, it is likely that many sounds generated using this technique can be used as source material for the game.\n\nTo speed up the process, I generated 50 text prompts with ChatGPT which were in turn used to generate audio. The prompts followed our internal sound direction guidelines. This process took about 5 minutes on a GPU, saving a lot of time I would otherwise have to spend getting source material.\n\n2 of the files contained musical elements and were discarded. 3 files contained excessive noise and were also discarded.\n\n\n  \n    \n    Alternate Text\n  \n  \n    \n    Alternate Text\n  \n  \n    \n    Alternate Text\n  \n  AI Generated Water Effects\n\n\nTo hear the final results of this process, follow the development of Sacred Gems on the official development Discord server and wishlist the game on Steam!\n\n\n   \n\n\nWill AI Take Your Sound Design Gig?\nAs it stands, I will occasionally be using AudioLDM to generate source material for designs. While text-to-audio is not quite ready to replace current workflows, it is only a matter of time. That said, these tools can still save you time. If you know what kind of frequency content you need for a design, but are not picky about what the sound containing those frequencies actually is, then this is a solid method to get what you need quickly.\n\nThe implication of being able to generate sounds quickly is that your soundscapes will never become repetitive. New files could be generated upon loading the game, or even a new level. While this thought is currently a novel one, it is the direction we are heading in. As audio directors, composers, and sound designers, we should prepare to work with emerging tools and find ways to control them to provide better services for our clients faster.\n\nVideo Exploration\n\n\n\nIf you want to see more exploration of the model in real time, check out the video above or follow this link!\n",
        "url": "/machine-learning/2023/04/25/olivegr-text-to-audio.html"
      },
    
      {
        "title": "Chroma Representations of MIDI for Chord Generation",
        "author": "\n",
        "excerpt": "Understanding two ways of representing and generating chords in machine learning.\n",
        "content": "\n   \n   An example of a chroma histogram.\n\n\nIntroduction\n\nUsing AI to generate new music, or art of any sort for that matter, can instil wonder in some and disdain in others. For some, it’s an exciting new frontier of technology while for others, we are simply giving away some of our best ways of making sense of the world around us to computers which can never truly appreciate the world, only predict it.\n\nI can’t solve that ethical dilemma in this post, but I can at least highlight one way in which we can use machine learning to augment, rather than supplant, our musical creativity.\n\nWhen I set out to conceptualise my machine learning project this semester, I started from a project I made several years ago in which a patch in Max/MSP synthesises a harmony in real time to a violinist playing a melody. In this project the harmony generated was quite static, working with largely fixed musical intervals. What if this project could be expanded with machine learning? What if, instead of using a deterministic rules-based system for generating harmony, I could leverage machine learning to generate harmony that was constantly surprising to both the performer and the audience?\n\nThe What &amp; Why of Chroma Representations\n\nMany researchers have developed machine learning-based systems for generating or suggesting chords to accompany melodies. Most of these system work by analysing incoming music as harmony and melody, and then suggesting a suitable next chord in the sequence. In machine learning we call this classification algorithms, because it is determining the most suitable option from a set of discrete chords. Most of these systems select from a small subset of possible chords, such as just the 24 major and minor chords.\n\nThis works well for offline systems such as composition aids, in which the composer can take the suggestion and play around voicings, chord extensions, and instrumentation. What about real-time systems, where we need complete harmony as quickly as possible?\n\nTo tackle this issue, in my project I work with different method of representing harmony to a machine learning algorithm, called chroma histograms. Chroma representations of music are those which remove octave information. For example, in a chroma representation, all C notes are counted as the chroma pitch C, regardless of whether they are high or low. A chroma histogram is therefore the ratios of each of the 12 chroma pitch in a given set of notes, displayed as a list of numbers which add up to 1.\n\nChord label representation:\n\n‘C major’\n\nChroma histogram representation:\n\n\n  \n    \n      C\n      C#\n      D\n      D#\n      E\n      F\n      F#\n      G\n      G#\n      A\n      A#\n      B\n    \n  \n  \n    \n      0.33\n      0.00\n      0.00\n      0.00\n      0.33\n      0.00\n      0.00\n      0.33\n      0.00\n      0.00\n      0.00\n      0.00\n    \n  \n\n\nInterpreting Chroma Histograms\n\nWhen training my machine learning model, I show it chords and ask it to generate chords as chroma histograms. As a result, the output is not a single chord such as ‘C major’, but instead a set of probabilities that describe what notes should and shouldn’t work in the current context. I can then use this information to work out which, and how many, notes to include in the chord.\n\nThe idea behind choosing chroma histogram representations is that while they can represent a C major, such as in the example above, they can also represent any collection of notes which do or do not form part of a standard chord.\n\nI propose this is preferable for a real-time scenario such as the one I describe above because it should allow my machine learning model to generate diverse and interesting harmony that isn’t limited to a small selection of possible chords.\n\nConclusion\n\nOf course, this is all well and good as conjecture. I’m currently in the process of finalising my machine learning model which puts chroma histograms into action. This section will be updated with some listening samples and concrete results soon!\n\nThe real-time implementation of this algorithm is also beyond the scope of this project. However, it takes only around 30-50 milliseconds to generate a suitable chroma histogram in my testing with a preliminary version of the model. I am therefore optimistic that it will be applicable for deployment as part of a real-time harmony generation system soon.\n",
        "url": "/machine-learning/2023/04/25/jackeh-chord-generation-chroma-representations.html"
      },
    
      {
        "title": "The whistle of the autoencoder",
        "author": "\n",
        "excerpt": "How I used autoencoders to create whistling.\n",
        "content": "I trained variational autoencoders to generate a range of novel and authentic human whistling sounds. Surprisingly, while the capability of human whistling has been widely studied, it has not been focused that much using the power of machine learning until now. I was eager to see if this technology could capture the essence of whistling and create something truly unique. And the results did not disappoint! Join me in exploring the primitive beauty of whistling through the eyes of machine learning!\n\n\n   \n   Example whistling spectrogram from the dataset.\n\n\nData\nI used MLend’s Hums and Whistles dataset, which contains recordings of people whistling different songs. I preprocessed the the dataset to get ready for the training.\n\nFirst things first, I min-max normalized the amplitude of the recordings. Then removed the silent portions that are over 500 ms, using the pydub library. To ensure smooth transitions, I added 50 samples fade in-out at the joining points. Next, I chopped each recordings into 65280-sample-long audio pieces, and disregarded the remaining slice. This awkward number is to make sure that resultant dataset has a power-of-two shape. I extracted spectrogram of each sample using librosa, on an fft window of 512 sliding 256 samples. Then, converted them to magnitude values.\n\n\n   \n   \n\n\n\n\n   \n   \n\n\nNext, I applied frequency filtering to the spectrograms to reduce their dimentionality. To do that, I started with analyzing the spectral density of the dataset to have an idea of the overall frequency behaviour of the whistlings. In that analysis, above 5400 Hz indicated no significant power and I decided to filter the frequencies that are not significantly used when whistling. The mentioned point luckily is very close to the mid-point of the frequency range, and I put a global threshold at the middle point (5469.4) to make sure things are in powers of two. Above I plot the spectral average and the spectral power distribution of the dataset, along with the filtering threshold.\n\nThese steps resulted in ~12000 spectrogram representations with a shape of (128, 256). I splitted them into training and validation sets (80:20), and I was ready for training! See below the examples from the preprocessed dataset to have an idea of their shape and spectral behaviour.\n\n\n   \n   Preprocessed whistling samples.\n\n\nModel and Training\nAfter several experiments with different shapes and hyperparameters, here is the details of my final model desing.\n\nEncoder and Decoder\nEncoder part has 5 convoltuional layers with 512, …, 32 filters. Encoder encodes the input spectrograms into a 64-dimensional latent space, by learning the characteristics of the spectrograms and mapping them as a probability distribution.\n\nThe latent space that encoder creates is a 64-dimensional array with spectrogram encryptions are scattered here and there. See below the UMAP latent space visualisation after the training.\n\nDecoder part is a mirrored encoder, taking the encoded representations to reconstruct a spectrogram. Last layer of the decoder has a sigmoid function that produces a spectrogram output with values between 0 and 1.\n\n\n   \n   Model diagram\n\n\nLoss Functions\nI used a combintaion of Kullback-Leibler Divergence (KL) and Mean Squared Error (MSE). MSE aimed to reduce the reconstruction error between the input and the output, while KL aimed to establish optimal probability distribution. I weighted the KL by $10^-6$ to balance their importance in the loss function.\n\nTraining\nWith all these, I trained the model with 10000 samples for 178 epochs with a batch size of 64. I performed the training on Google Colab, and it took around 3 hours with premium GPUs.\n\nGenerating New Samples\nAfter training the model, I used the decoder part to generate new samples simply by randomly picking a sample from the latent space. Then, the decoder reconstructed a new spectrogram based on whatever it has in his mind(!) about the selected point. And that’s the final result, ALMOST!\n\nPostprocessing\nFirst I had to put the filtered spectrogram part back - I simply filled the missing parts with zeros. Secondly, I denormalized the amplitudes back to the normal range. Then I converted the spectrograms into audio using the Griffin-Lim algorithm. It is now a final result!\n\nSo what?\nIt went well! I think things worked and the resultant spectrograms are very close to human-made ones. Altough some noise in the background and some artifacts here and there, I could barely distinguish them from the originals. Also, worth mentioning that not all the generated spectrograms are that good - some were even terrible, sounding like an alien whistling, or someone practicing whistling while shaving.\n\nHere are some generated whistling samples.\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\nI will also conduct a user study with a pool of model-generated and original whistling samples. I will ask participants to rate the whistlings by their realism (in terms of audio-quality and expressiveness) to compare the model whistlings with the real ones. This way I hope to find out how unreal the model-generated whistles really are. I will add the results here when ready!\n\nKeep on whistling and stay tuned!\n",
        "url": "/machine-learning/2023/04/25/ahmetem-whistling-autoencoders.html"
      },
    
      {
        "title": "What's wrong with singing voice synthesis",
        "author": "\n",
        "excerpt": "Dead can sing\n",
        "content": "Overview\n\nAmong the many AI-driven innovations in the music industry, Singing Voice Synthesis (SVS) technology is generating significant interest but as with everything else, it’s not all sunshine and roses - some ethical dilemmas are giving folks a headache.\n\nSVS has been around for more than two decades. The goal of it can be to learn human-like singing voices given input conditions, like speech, lyrics and melodies. Here we have many threads of focus like replicating existing vocals, refining singing expressions to improve performances or converting speech to voice. However, since the projects often involve the works of existing artists, debates have arisen around the unclear standards and practices regarding the ownership problems. Interestingly, public awareness about this topic is currently quite limited indicating a lack of clear methods to handle them.\n\n\n   \n\n\nConcerns\n\nWhile we may still doubt the quality of AI’s musicality, there are still growing concerns that automation could drive music production costs to zero, thereby undermining artists’ value. Additional issues pop up when voice samples are combined with other samples to create new voices, resulting in the original voice data provider not benefiting monetarily. Consequently, people often worry about the future value of human musicians if AI takes over, rather than simply being a collaborative tool.\n\nThe potential misuse of this technology is another significant concern. The synthesized singing voices are sounding increasingly realistic and almost indistinguishable from human voices. Referencing deepfakes or potential scams(e.g., voice phishing) it may be necessary to think about the development of technologies that will counteract such misuse.\n\nAlso, AI replicating the voice of a deceased person happened to cause some public concerns. How right is it to use the personal records of a deceased person without their consent and profit from it? Although the family or close associates of the deceased artist may hold the rights, they are not the artists themselves, and this distinction raises further morality issues. Additionally, the authenticity of the work and respect for the artist’s legacy comes into question when AI-generated vocals are used in new productions.\n\nSunny side\n\nDesigners behind machine learning algorithms, primarily considered the collaboration between humans and AI as a big area of opportunity, and none believed that AI was a replacement for humans, partly because of the value attributed to human skills. Their perspective was focused on seeing AI’s role as augmenting human abilities, similar to how Autotune is commonly used to manipulate a singer’s voice.\n\nWhile this synergy between AI and humans is often presented as ideal, it is still limited to encapsulating ethical concerns and considerations of the asymmetric relationship between machines and humans. Despite these challenges, AI can help improve singing expressions like converting speech to singing voices, allowing singers to explore new creative tools for their work and making performances more engaging and enjoyable for audiences.\n\nSummary\n\nTo conclude, Singing Voice Synthesis offers both opportunities and challenges in music. While it has the potential to enhance performances and open up new avenues for artistic expression, simultaneously raises ethical dilemmas, intellectual property issues, the value of human musicians in an increasingly automated world, and potential misuse. To ensure a responsible and ethical integration of AI, it would be nice to think about some guidelines and methods for balancing creativity, collaboration, and ethics.\n\n“One Note Samba” by SKYGGE using artificial intelligence tool Flow-Machines\n",
        "url": "/machine-learning/2023/04/25/ninojak-SVS.html"
      },
    
      {
        "title": "Clustering audio features",
        "author": "\n",
        "excerpt": "Music information retrieval(MIR)\n",
        "content": "Overview\n\nThe following blog post is about music information retrieval(MIR) and identifying similar patterns in my music library. I was curious to use unsupervised learning methods to analyze and cluster musical tracks based on their similar audio features. Generally, this approach can be quite helpful to increase the workflow of organizing musical libraries.\n\nFeatures\n\nFor this example, the dataset consists of 50 wav audio files of the same durations, channel and sample rate. Usually, before we start the pattern recognition task and grouping sound files based on their content similarity, we first need to find a way to go from the low-level and voluminous audio data samples to higher-level representations of the audio content. This is the purpose of feature extraction (FE), the most common and important task in all machine learning applications. In my case, I aimed to extract the following features:\n\n\n  \n    Spectral bandwidth - represents the difference between the upper and lower frequencies and has a direct correlation with the perceived timbre of the sound.\n  \n  \n    RMS - measures the average loudness or the energy of the audio\n  \n  \n    Spectral flatness - helps to differentiate between noise-like and tone-like sounds\n  \n  \n    Zero-crossing rate - classifies percussive sounds\n  \n  \n    Chroma stft - identifies pitch classes and melodic and harmonic content\n  \n  \n    Mel-spectrogram - approximates human hearing, as it emphasizes the \nlower frequencies more than the higher ones and provides the representation of the audio’s spectral content over time\n  \n  \n    MFCC - carries information about the sound’s timbre, phonetic content, and other characteristics\n  \n  \n    Spectral contrast - the difference between the peaks and valleys in an audio signal’s spectrum.\n  \n\n\nClustering\n\nAfter extracting features it is important to further preprocess data before feeding them to the clustering algorithm. One important step is feature scaling, which ensures that all features contribute equally to the distance calculations. This improves clustering results and accuracy. Also, it is especially important during k-means clustering, since it can be biased towards the variables that have larger values.\n\nThen, I initialize k-means clustering algorithm with four clusters and a random state to fit the algorithm to the scaled features and generate integer labels for each data point. After selecting a specific cluster and retrieving the indices of the corresponding audio files we can display and playback the clustered audio files, showing their similarities.\n\n\n   \n   feature clusters\n\n\nEvaluation\n\nDuring the evaluation of the optimal number of clusters in the dataset, Utilising elbow method and the silhouette score indicated that the system requires further refinement to improve the separation of the data points.\n\nThe elbow method revealed that the optimal number of clusters may be poorly estimated as the point of the curve is not diminishing steeply.\nSilhouette’s score of 0.4 suggests that data points are somewhat well-separated within their clusters, but there may be some overlap.\n\nV Measure Score: 0.40947638495245836\nHomogeneity Score: 0.43588570440829166\nCompleteness Score: 0.3860844120745334\n\nAll three metrics show that clusters do not contain data points that belong only to a single class and the number of similar data points that are put together by the algorithm varies.\n\nSources\n\nMIR\n\nFeature extraction and Clustering\n",
        "url": "/machine-learning/2023/04/25/ninojak-clustering-audio.html"
      },
    
      {
        "title": "A caveman's way of making art",
        "author": "\n",
        "excerpt": "…and art in the age of complexity.\n",
        "content": "\n   \n   Kakadu Rock Paintings, Australia.\n   Painted around 20.000 years ago.\n\n\nIt’s fascinating to think about the cavemen who painted on the walls of the caves. Did they worry about the technical aspects of painting, or were they more focused on creating something that is expressive? Perhaps they didn’t have the luxury to think about the tools they used, as their main goal was to communicate their ideas. They had a direct connection with their medium, using simple tools like rocks and their hands to leave their mark on the walls. It was a pure expression of their creativity, without the interference of complicated tools or techniques.\n\nFast forward to today, where we have access to advanced tools like machine learning to create art. While these tools have opened up new possibilities for us, they also come with a cost. In my opinion, the complexity of these tools can instead sometimes create a gap between us and the result we’re trying to achieve. It takes a lot of effort and time to create something truly unique and reflective of ourselves. As a result, it’s easy to become lost in the process and lose sight of our original intention.\n\nTo make such a claim, we first need to establish what we mean by the complexity of a tool in the context of creating art. I view complexity in this context as a subjective term that depends on the user’s ability to utilize the tool. We know that the builders of the pyramids, for example, had a deep understanding of the advanced math required to construct such marvels, and this knowledge was ingrained in their ways of thinking naturally. Moreover, their creations often served religious or cultural purposes, imbuing them with an artistic quality.\n\nIn contrast, we approach learning and creating with these tools from a different perspective. We often learn these skills later in life, and we have limited opportunities to embed them deeply into our minds. We may struggle to think artistically throughout the entire creative process, unlike our ancestors who may have had a more integrated approach to artistic thinking. And it can be difficult to tap into artistic thinking when the technical challenges and tools feel so overwhelming. From my own experience working on machine learning projects this semester, I’ve noticed that my moments of artistic thinking only occur when I listen to the final result. Only at that point, did I switch gears from a logical, numerical mindset to a more artistic one, evaluating the output based on aesthetics and creativity.\n\n\n   \n   Kristian and Jack looking at numbers for a creative project.\n\n\nIn addition, as someone with a technical background, I have often struggled with finding my own voice in the realm of art. The rigid logic and structured thinking of my education made it challenging to break out of the box and truly create something unique. When I dived into the Music and Machine Learning world, I realized more about the potential for technology to assist in the artistic process. But even then, I found myself grappling with the question of how much of the output was truly my own creation, and how much was the result of the algorithm.\n\nThat being said, I don’t think this means we should abandon these advanced tools altogether. Instead, we need to find ways to incorporate them into our artistic processes in a way that allows us to express our unique perspectives and creative visions. This may mean taking a step back from the technical details and focusing on the emotions and meanings we want to convey through our art. It may also mean developing new tools or adapting existing ones in ways that better suit our artistic needs. We may need to explore unconventional paths and embrace the unknown, letting go of the safety of repetition and comfort.\n\nAll in all, while advanced tools like machine learning have expanded our possibilities for creating art, I think that they may also present challenges that can hinder our artistic expression. The complexity of these tools can create a gap between ourselves and the final result. We should try to find our ways to integrate them into our own creative process - to have it as a genuine reflection of ourselves and our ideas, rather than just another output of technology.\n",
        "url": "/machine-learning/2023/04/26/ahmetem-caveman-art.html"
      },
    
      {
        "title": "Music AI, a brief history",
        "author": "\n",
        "excerpt": "Chronicling the field of AI-generated music’s start, where it went from there, and what you can expect from musical AI right now.\n",
        "content": "AI generated music is a field that’s evolved a lot, especially the last few years. In this post we’ll be looking at where it all started, where it went from there, and what you can expect from musical AI right now. \nIt could be argued that the first procedurally generated piece of music was created in the early 1950s, where composer John Cage used the ancient Chinese divination text I Ching to generate instruction sets for music. The first proper AI music composition was however the Illiac Suite in 1956. Programmers Lejaren Hiller and Leonard Isaacson used the ILLIAC computer, the first ever computer to be built, to create a score that was later played by humans. Hiller would define a set of rules, for example only using notes within an octave and harmonies that tended towards the major and the minor with no dissonance, along with some other rules. Then, by providing some input, the computer would generate a score. This is one of the resulting scores played by humans. \n\n\nThe 1980s was where a lot of fundamental breakdowns in the field were made. Among others, the composer David Cope developed a general algorithm that focused on the main points: deconstructing and analyzing input music, looking for commonalities and signatures that signifies style, and recombining musical elements into new works. This work is used as the foundation of many future AI models.\nIn 2002, a paper written by François Pachet proposed the program called The Continuator. A machine learning model that would listen to musicians play, and then continue playing, following the original style. This is one of the first programs to use ML in real-time to generate original music, and it marked a significant milestone in the field.\nIn the years between 2010 and now, developments pick up in speed and several advancements are made. Iamus is a computer cluster that composes classical music in it’s own style. It is one of the first models that uses evolutionary algorithms with supervised learning to generate music.\nOpenAI’s Jukebox is a recent model, released in 2020, that uses neural networks to generate raw audio data. This is significant as most models before this only generated MIDI data or musical scores. \nLooking at the current state of the art, we find a lot of different programs fulfilling numerous roles within the field of musical AI, but one of the most notable ones is Google’s MusicLM. This is a machine learning model that can process text-based prompts and generate music based on the prompts. This model features impressive accuracy, and the ability to parse prompts detailing instruments used, purpose of the music, feelings involved, experience level of the player, and more.\n\n",
        "url": "/machine-learning/2023/04/26/noorfo-music-ai-a-brief-history.html"
      },
    
      {
        "title": "Spotlight: AutoEncoders and Variational AutoEncoders",
        "author": "\n",
        "excerpt": "A simple generative algorithm\n",
        "content": "Spotlight: AutoEncoders and Variational AutoEncoders\nSince we all prayed to the Autoencoder god this semester we thought about giving you an introduction to the architecture and capabilities of autoencoders and their sibling the variational autoencoder. Especially VAE’s have become extremely popular over the last years, capable of creating entirely non-existing human faces and purely synthetic music.\n\nTo generate or not to generate\nGenerative deep learning models are a type of artificial neural network that can learn to generate new data samples similar to the training data provided. These models are designed to learn the underlying patterns and structure of the data, which can then be used to create new samples. If you dive a little bit deeper into the matter you will find the information on the internet that the vanilla autoencoder cannot be used for generative purposes. If we define “generative” as the ability to create data then it is absolutely true that an autoencoder can be used for generative purposes. The thing is that it is just does not work very well with creating continuous data. That’s were VAE’s come in.\n\nAutoencoders\nAn autoencoder is a neural network structure with an equal number of inputs and outputs and at least one hidden layer which is smaller in size. It’s two main purposes are signal or image denoising and dimensionality reduction.\n\n   \n   Autoencoder architecture\n\n\nThe structure of the AE effectively acts as a form of “compression” and “decompression” of the data.  Importantly, these stages mirror each other.  Many purposes use these to map one value to another. In a musical context, this can be used for accompaniment, mapping inputs of musical parts to desired outputs of accompanying parts, such as in the case of MiniBach.\n\nThis can also be used for purely generative purposes, if the desired output of the system is set to be the input.  Here the system would essentially “learn” how to construct the input data via its encoder/decoder stages.  In this way, an autoencoder is able to “learn” the rules of a dataset.\n\nThe smallest layer in the system, is often referred to as the “latent layer” or “bottleneck layer”.  After training, we can use only the decoder layer to generate material by giving values into the bottleneck layer and having them fed-forward through the decoder stage generating new material based on the training data.\n\nVariational Autoencoders\nA variational autoencoder is very similar to a traditional autoencoder but with an important twist. Traditional autoencoder map the encoded data to a fixed point in the latent space which, when reconstructed by the decoder, will ideally produce a 1-to-1 reconstruction of the input data. That’s why autoencoders solely optimize reconstruction loss.\n\nIn a variational autoencoder the encoded data does not get mapped to a fixed point but to a probability distribution. This is were the true generative capability of a VAE lies. When this distribution is sampled and ran through the decoder it will generate a new sample resembling the input data, or anything you specify that lies in-between. Simplified, if you have a dataset that contains flute and guitar recordings, with sampling the latent space correctly you could essentially create an instrument that sounds a bit like a flute and a bit like a guitar.\n\nTo do this the model needs it’s own specialized loss function. The most common way of calculating loss in a VAE is by combining the reconstruction loss and Kullback-Leibler divergence. This divergence, which is also called KL divergence, essentially computes the “divergence” between two probability distributions (i.e., how much they look not like each other).\n\nConclusion\nIn summary, while both AEs and VAEs are neural networks that can be used for data compression and dimensionality reduction, VAEs go one step further by explicitly modeling the probability distribution of the latent space. This allows VAEs to generate new data samples from the learned distribution, making them particularly useful for generative tasks.\nSources\n\nDhinakaran, Aparna. 2023. “Understanding KL Divergence.” Medium. March 22, 2023. https://towardsdatascience.com/understanding-kl-divergence-f3ddc8dff254.\n",
        "url": "/machine-learning/2023/04/26/fabianst-autoencoders-and-variationalautoencoders.html"
      },
    
      {
        "title": "Breakbeat Science",
        "author": "\n",
        "excerpt": "AI-Generated amen breakbeats\n",
        "content": "Introduction\n\nGenerating audio with deep learning techniques can be quite a daunting task. To generate a single second of audio at a sample rate of 48kHz means you need to generate 48 000 separate samples. This quickly becomes computationally expensive, even if sample rate is lowered.\n\nWhat if you instead of using raw audio, you used a visual representation of it? This is what I did in my project titled “VAmEn”. I used images of mel-spectrograms to generate audio samples of breakbeats using a Variational AutoEncoder (VAE). This is a simple but powerful machine learning architecture.\n\nWhat is a Mel Spectrogram?\n\n\n   \n   Spectrogram extracted from the song \"Look\" by Venetian Snares \n\n\nI mentioned I trained my model using mel spectrograms, but what it is? A mel spectrogram is a visual representation of sound that shows the energy at different frequencies over time.\n\nIt uses a non-linear mel scale to better represent how humans hear sound and the Fast Fourier Transform (FFT) to calculate the energy at each frequency. The resulting 2D image shows time on the horizontal axis, frequency on the vertical axis, and energy as color.\n\nThe final result would then be an image made out of pixels which is a lot less data than a raw audio file making it much easier for the machine to learn.\n\n## Variational AutoEncoders\n\n\n   \n   Simple VAE architecture\n\n\nVariational AutoEncoders (VAEs) are a type of artificial neural network that can learn to represent high-dimensional data in a lower-dimensional space. They work by encoding input data into a lower-dimensional latent space, and then decoding it back into the original space.\n\nVAEs use a probabilistic approach to encoding and decoding, allowing them to generate new data that resembles the input data. This makes them ideal for generative purposes. If you want to learn more about AEs and VAEs check out this post!\n\nVAmEn\n\nMy goal was to create a model with the ability to generate mel spectrograms of breakbeats. I did this by training my model on a dataset I created consisting of almost 10 000 randomized amen break samples!\n\nI extracted the mel spectrogram for each sample and gave these to my model. Since I used large 512x512 pixel images and almost 10k samples training took quite a while, even though I lowered the sample rate to 10kHz. I am lucky enough to own a decently powerful powerful GPU which sped up the training process. If you do not have GPU access I would recommend looking into Google Colab. Here you can get access to a remote GPU from Google to train your model.\n\nResults\n\n\n   \n   AI-Generated spectrogram created by my model\n\n\nOnce finished training the model was able to create quite good looking spectrograms by sampling the latent space, which sounded great. To turn my spectrogram into audio I used an algorithm called the Griffin-Lim algorithm which produced surprisingly good results. Take a listen!\n\n\n  \n    \n    Alternate Text\n  \n  AI-Generated Breakbeat\n\n\nSources\nFrenzel, Max. 2018. “The Variational Autoencoder as a Two-Player Game — Part I.” Medium. April 23, 2018. https://towardsdatascience.com/the-variational-autoencoder-as-a-two-player-game-part-i-4c3737f0987b.\n\nReducible, dir. 2020. The Fast Fourier Transform (FFT): Most Ingenious Algorithm Ever? https://www.youtube.com/watch?v=h7apO7q16V0.\n\nRoberts, Leland. 2022. “Understanding the Mel Spectrogram.” Analytics Vidhya (blog). August 17, 2022. https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53.\n\nBastwood, “The Aphex Face .” n.d. Accessed April 24, 2023. http://www.bastwood.com/?page_id=10.\n\n“Variational Autoencoder.” 2023. In Wikipedia. https://en.wikipedia.org/w/index.php?title=Variational_autoencoder&amp;oldid=1148033201.\n",
        "url": "/machine-learning/2023/04/26/fabianst-project-blogpost.html"
      },
    
      {
        "title": "Recognizing Key Signatures with Machine Learning",
        "author": "\n",
        "excerpt": "The first rule of machine learning? Understand your data! A look into how music theory came to my rescue for classifying key signatures.\n",
        "content": "Introduction\n\nMy machine learning project this semester is all about generating chords to a melody. For a model to learn how to generate chords, we need lots of chords to show it – and I mean a lot of them.\n\nMy model is trained on around 3 million chords, all of which are extracted from MIDI files. I use a subset of the Lakh MIDI dataset, which consists of around 17,000 MIDI files of popular, classical, and jazz music. Like many large datasets you can find online, the Lakh dataset is not a ‘clean’ dataset. It essentially consists of MIDI files pulled from across the internet, which means it has all the errors, corrupt files, and formatting inconsistencies that we might expect. One of the most challenging inconsistencies I had to overcome in the dataset was that only around 20% of the files contain key signature information.\n\nThe solution to this issue required a second machine learning model, a good understanding of the data, and a solid grasp of basic music theory. As such, this post will assume you have a basic understanding of classification machine learning models and music theory.\n\nMusic Theory Meets Machine Theory\n\nWhy do we need key signatures for determining chords using machine learning? The reason has a lot to do with the music theory concept of functional harmony.\n\nThe goal with my final chord generation model is to generate tonal and coherent progressions to accompany a melody. Chord order, and the relationship between subsequent chords, is essential for the generated progressions to be musically coherent. We can consider it in terms of functional harmony i.e., does the progression go where we expect it to go? V chords often lead to I chords, for example, and it is important for my model to learn these types of relationships.\n\nIn order to represent my dataset to the model in terms of functional harmony, I first decided to transpose every MIDI file in the dataset to C major/A minor. To transpose a song by the right amount, we first need to know its original key. This is where machine learning comes in.\n\nI therefore used the files from my dataset which do contain key signatures to help me recognize they keys of those that do not. For each file, I needed two pieces of information: a) the key signature of the song, and b) a chroma histogram of the song. A chroma histogram is simply a list of 12 numbers which describes the percentage of each type of note in a given set of notes. In this list, the first number refers to the percentage of C’s, the second the percentage of C#’s, etc. The percentage of each type of note is closely related to the key signature. For example, we’d expect to see a lot of C’s but few F#’s in a song in C major. This trend in the data mean that I can train a classification machine learning model on this data, feeding it the histograms as the input, and asking it to predict the key signature as the output.\n\nKey Confusion\n\nAs with all machine learning tasks, it rarely works well the first time out. I plugged the data into a Support Vector Machine model in SciKitLearn, and it predicted keys with an accuracy of… 68%. Not bad, but we can do better.\n\nWe can understand the mistakes our model is making by plotting a confusion matrix like the one below.\n\n\n   \n   The confusion matrix of the model trained to recognise all the major and minor keys.\n\n\nThe X axis shows the key as predicted by our model, while the Y axis shows the actual key. The dark diagonal line shows that our model is getting it right most of the time. However, we can also see some fainter lines illustrating common errors. For example, C minor is often being confused with D#/Eb major. Why is this?\n\nLet’s turn back to music theory. Eb major and C minor are said to be ‘relative’ keys to each other as they contain the same set of notes, they just start the scale from a different place in that set. Our model is therefore being confused by this major/minor distinction as it has two possible labels for chroma histograms that look very similar.\n\nI therefore decided to remove this distinction by converting all minor keys to their relative major. This meant I now had only 12 keys, rather than the original 24. I re-trained the model with this newly adapted data and the accuracy jumped immediately to 84%. Much better! See the confusion matrix of this new version below.\n\n\n   \n   The confusion matrix of the model trained to recognise just the major keys.\n\n\nConclusion\n\nWith my key classification model accuracy up to 84%, I was then able to deploy it for recognizing the key signatures of those MIDI files which didn’t contain one already. Despite taking place entirely behind the scenes, this is a vital component of my project which allows me to present the chords in my main dataset to the final model in a way that captures how they relate to and interact with the chords around them, rather than simply by the raw chroma pitches involved.\n\nWhen I first started working with machine learning this semester, a common refrain was ‘know your data’. It can be easy to think that work with MIDI or other symbolic representations of music should be easy. It’s just a list of instructions after all - how hard can they be to interpret? As my challenges in building an effective key signature classifier show, working with MIDI throws up its own set of challenges that require you to really ‘know your data’.\n",
        "url": "/machine-learning/2023/04/26/jackeh-key-signature-classified.html"
      },
    
      {
        "title": "Isn't Bach deep enough?",
        "author": "\n",
        "excerpt": "Deep Bach is an artificial intelligence that composes like Bach.\n",
        "content": "Deep Bach\n\nIn this post we will try to introduce the Deep Bach! Hope you enjoy!\n\nIntroduction\n\nDeepBach is an artificial intelligence that composes like Bach.\nDeepBach, a deep learning system based on recurrent neural network (RNN) architecture, has the unique capacity to produce music in the style of famed Baroque composer Johann Sebastian Bach. The system employs a graphical model to represent polyphonic music, with a particular emphasis on choral compositions. Graphical model developed to model polyphonic music,  Using  pseudo-Gibbs sampling and a customized representation of musical data, DeepBach does well.\n\nThe model is taught to recognize and mimic the intricate patterns and structures in Bach’s chorale harmonizations. DeepBach, once trained, can make extraordinarily convincing chorales in the style of Bach, a feat that sets it apart from other automatic music creation approaches, which typically function sequentially.\n\nThe level of control provided by DeepBach is one of its most notable characteristics. Users can impose restrictions on the created score, such as notes, rhythms, and cadences. This level of personalization enables a one-of-a-kind combination of human curiosity and AI efficiency.\n\nIn a blind test, roughly half of the participants thought a DeepBach-generated harmony was produced by Bach himself. This outcome is noticeably better than music generated by other algorithms.\n\nBelow is a sample of DeepBach’s work:\n\n\n\nRemarks\nDeep learning algorithms, such as DeepBach, have shown great potential in the realm of music generation. They take a fresh look at composition, fusing technology and art in an intriguing and novel way.\n\nHowever, there are still obstacles to be conquered. One major difficulty is the creation of algorithms capable of producing really original music rather than simply copying existing compositions. Another problem is ensuring that the generated music is emotionally resonant and appealing to listeners as well as technically sound.\n\nDespite these difficulties, the potential for this technology to transform the music industry is unquestionably fascinating. As the sector develops and improves, we may witness a new era of music production and consumption, fueled by AI’s unique talents.\n\nSources:\n\n  Sturm, B. L. (2016). “DeepBach: a Steerable Model for Bach chorales generation.” arXiv preprint arXiv:1612.01010.\n  Eck, D., &amp; Schmidhuber, J. (2002). “Learning representations by back-propagating errors.” Cognitive computation.\n  https://www.sonycsl.co.jp/news/3882/\n\n\n",
        "url": "/machine-learning/2023/04/26/masoudn-deep-bach.html"
      },
    
      {
        "title": "Persian classical instruments recognition and classification",
        "author": "\n",
        "excerpt": "This blog post will go over various feature extraction techniques used to identify Persian classical music instruments.\n",
        "content": "\n   \n   \n\n\nIntroduction\n\nThis blog post will go over various feature extraction techniques used to identify Persian classical music instruments. The similarity in timbre and tonality between various instruments makes this task difficult. We will investigate the efficacy of a number of features, including Chroma, Spectral Centroid, Spectral Flatness, and Zero Crossing Rate in order to address this challenge. After that, we will classify the instruments based on these features and evaluate their performance using a Support Vector Machine (SVM) model.\n\nProcess\n\nData preparation: lets begin by making a list of the instruments used in Persian classical music and listening to the corresponding audio files for each one. We take into account Santur, Setar, Ud, Ney, Tar, and Kamancheh as our instruments.\n\nFeatures listed below:\n\nMFCC: Represents the audio signal’s power spectrum and is frequently used in tasks involving music and speech recognition.\nChroma: Chroma is a useful concept for analyzing tonality and harmony in music as it depicts the distribution of energy across various pitch classes.\nThe spectral centroid, which represents the power spectrum’s center of mass, can shed light on the timbre of an audio signal.\nMeasures the power spectrum’s flatness and can offer information about the timbre of the audio signal.\nZero Crossing Rate: This metric illustrates the speed at which the audio signal crosses the zero amplitude and can shed light on the audio signal’s texture.\nData organization: For later use, we create dictionaries out of the extracted features and store them in pickled files.\n\nEvaluation of extracted features: Using a k-fold cross-validated SVM model, we combine the extracted features for all instruments and assess each one’s performance. Then, we evaluate each feature’s accuracy and contrast how well it performs.\n\nFindings\n\nHere are the findings of the experiment:\n\n  The accuracy of the SVM model using MFCC features is: 88.88%\n  The accuracy of the SVM model using Chroma features is: 62.12%\n  The accuracy of the SVM model using Spectral Centroid features is: 41.91%\n  The accuracy of the SVM model using Spectral Flatness features is: 24.40%\n  The accuracy of the SVM model using Zero Crossing Rate features is: 42.45%\n  The accuracy of the SVM model using MFCC concatonated features is: 89.8%\n\n\n\n   \n   Findings\n\n\nThe SVM model’s accuracy when employing MFCC features is 88.88%.\nUsing Chroma features, the SVM model achieves a 62.12% accuracy rate.\nWith Spectral Centroid features, the SVM model achieves a 41.91% accuracy rate.\nWhen Spectral Flatness features are used in an SVM model, the resulting accuracy is 24.40 percent.\nUsing features based on the zero crossing rate, the SVM model achieves a 42.45% accuracy rate.\nConclusion\n\nThe data  collected shows that the MFCC features do well at identifying the instruments used in Persian classical music, with an accuracy of 88.88%. This makes sense, as MFCC features have previously been shown to capture crucial aspects of the audio signal for applications like speech and music recognition.\n\nWith an accuracy of 62.12%, the Chroma features are a close second. Chroma features still provide a respectable level of accuracy for identifying instruments in Persian classical music, despite being less effective than MFCC features in this particular application.\n\nLower precision is achieved by employing the Spectral Centroid, Spectral Flatness, and Zero Crossing Rate features. These features are not as discriminative as the MFCC and Chroma features for the task of instrument recognition in Persian classical music, or in general instrument recognition, but they can provide some insight into the timbre and texture of the audio signal.\n\nAs the MFCC and Chroma feature represent the first two accurate feature vectore we concatonated them to increase the accuracy and as the results show it increased it by roughly 1 percent.\n\nAcknowledgements:\nDataset:\nMousavi, Seyed Muhammad Hossein, and VB Surya Prasath. “Persian Classical Music Instrument Recognition (PCMIR) Using a Novel Persian Music Database.” 2019 9th International Conference on Computer and Knowledge Engineering (ICCKE). IEEE, 2019.\n\n",
        "url": "/machine-learning/2023/04/26/masoudn-persian-classification.html"
      },
    
      {
        "title": "Generating music with an evolutionary algorithm",
        "author": "\n",
        "excerpt": "Looking at a theoretical and general implementation of an evolutionary algorithm to generate music.\n",
        "content": "Evolutionary algorithm generating music\nIn this blog post we will be looking at how a theoritcal implementation of an evolutionary algorithm (EA) to generate music would work.\nAn evolutionary algorithm is a type of optimization algorithm inspired by biological evolution. It works by repeatedly generating and evaluating candidate solutions to a problem, and then applying genetic operators such as mutation and crossover to create new solutions. The fitness of each candidate solution is evaluated using a fitness function, which determines how well it meets the objectives of the problem. Over time, the algorithm converges towards a set of optimal solutions that meet the objectives of the problem. \nThe first step is to decide on the data representation method. This could for example be MIDI-data, or raw audio data. \nNext up is deciding a fitness function. This is one of the biggest challenges in AI generated music, as knowing what is “good” or “bad” music is very subjective, and it’s hard to define rulesets used to evaluate music resulting from our model. Although none of this is perfect, these are some approaches that can be used to define a fitness function. \n\nHarmonic complexity: One approach is to evaluate the harmonic complexity of the generated music. This could involve measuring the number and type of chords used in the music, and rewarding solutions that use a diverse range of harmonies.\nMelodic interest: Another approach is to evaluate the melodic interest of the generated music. This could involve measuring the variety of melodic contours, rhythm, and intervals used in the music, and rewarding solutions that are more varied and interesting.\nRhythmic stability: A fitness function could also evaluate the rhythmic stability of the generated music. This could involve measuring the consistency and predictability of the rhythm, and rewarding solutions that have a stable, easy-to-follow rhythm.\nmotional impact: A fitness function could also evaluate the emotional impact of the generated music. This could involve measuring the emotional response of listeners to the music, and rewarding solutions that evoke a strong emotional response.\n\nThe fitness function will serve as a judge that can evaluate and score the music. \nWe also need to define some hyperparameters such as population size and mutation rate. \nThen we need to generate the initial population, a set of random solutions that are used as the starting point for the evolutionary algorithm. \nAfter having done this we can start the training. First, we’ll apply genetic operators to create new solutions from the existing ones. The two main genetic operators used in evolutionary algorithms are mutation and crossover. Mutation involves making small random changes to a solution, while crossover involves combining two or more solutions to create a new one. In the case of generating music, mutation might involve changing a few notes in a melody or rhythm, while crossover might involve combining the melody from one solution with the chords from another. \nThen the model will evaluate the fitness for each generation’s solutions, using the selected fitness function. \nFinally, the model selects parents through a choice of selection techniques, such as fitness proportionate selection, tournament selection, or rank-based selection. It’s important to choose an appropriate selection technique, as they vary in how much they will explore or exploit the results. The parents are then used to generate new children with our genetic operators. This whole process is then repeated until the model reaches a certain fitness threshold or enough time has passed, and we have our result.\n",
        "url": "/machine-learning/2023/04/26/noorfo-generating-music-with-evolutionary-algorithm.html"
      },
    
      {
        "title": "Scream Machine: Voice Conversion with an Artifical Neural Network",
        "author": "\n",
        "excerpt": "Using a VAE to transform one voice to another.\n",
        "content": "The human voice is one of the most versatile and arguably the most important instrument that we have at our disposal. Humans are capable of creating the wildest sounds, like polyphonic overtones or false cord activation. Especially the ladder is one of the most important techniques in metal vocals. But trying to find your entry into metal voice singing is hard. You have to learn completely new techniques and activate parts of your throat that you never thought existed. I thought: if I am not yet there, can I teach a machine to learn how to do it?\nSo I set sail this semester to find out whether I can emulate metal vocals with a machine learning model.\n\nThe idea and the model\n\nLet’s create a machine learning model that is trained on paired input data of a clean voice and the metal vocal version of it. The trained model would then generate the metal vocal equivalent if given the clean voice as new input. \nI decided to go with a Variational Autoencoder framework because of it’s reconstruction capabilities, relatively lightweight architecture and the possibility to feed it images of spectrograms which drastically reduces training and inference time. The idea is simple: feed my clean voice into the model and define the metal vocals as target, so that the autoencoder learns to reconstruct the metal vocals from my voice.\n\nThe dataset\n\nThe hardest part of this project was to find the right dataset. In order to be trained effectively, your model needs lots of data. First, I thought to just record myself speaking over two hours and send the recording through a distortion effect. While this would have yielded enough data I would have ended up with only teaching the model to learn to recreated the distortion effect and not all the intricacies and different timbres metal vocals have to offer. So I decided to go the hard way and re-record the lyrics of an entire album with my own voice. This is very ineffective since I have to manually align both tracks, otherwise the model would not get the correlations between the two voices. \nI chose to record one of my favourite albums of all times, Fear Factory’s ‘Soul of a New Machine’. After the preprocessing stage, which includes generating melspectrogram images of the inputs and targets, I started to explore a fitting model architecture.\n\n\n  \n    \n    Alternate Text\n  \n  Metal vocals from the album\n\n\n\n  \n    \n    Alternate Text\n  \n  My clean voice rendition of the lyrics\n\n\nThe outcome\n\nSince this was an exploration towards the limit of what is possible with a very small dataset and a lightweight architecture, I am pretty happy with the results. You can see that the model was able to learn from my data and transpose some of my input and target properties to the reconstruction. There is much room for improvement though which could include additionally passing Melspectrogram Cepstral Coefficients (MFCC’s) to the input layer and obviously, expanding my dataset.\n\nNow, listening to the outcome of the image-to-audio algorithm (the librosa version of the Griffin-Lim algorithm) reveals another field for improvement: data loss. Although the visual reconstruction is quite acceptable, the audio reconstruction lacks the phase component of the original signal as well as form and clarity. Hear for yourself:\n\nNevertheless, I think the results are promising and I am looking forward to implementing another iteration of my idea.\n\nInput to input reconstruction:\n\n\n   \n   Reconstructed spectrograms from input to input modelling \n\n\n\n  \n    \n  \n  Input audio\n\n\n\n  \n    \n  \n  Reconstructed audio\n\n\nInput to target reconstruction:\n\n\n   \n   Rendering of input to target modelling \n\n\n\n   \n   Clean to metal voice spectrogram render \n\n\n\n  \n    \n  \n  Input audio\n\n\n\n  \n    \n  \n  Reconstructed metal audio\n\n",
        "url": "/machine-learning/2023/04/26/kristeic-ML-project-scream-machine.html"
      },
    
      {
        "title": "Motion Controlled Sampler in Ableton",
        "author": "\n",
        "excerpt": "A fun and creative way of sampling\n",
        "content": "Introduction\n\nIn this post I will show how I created a sampler controlled by IMU data from a phone using Max For Live in Ableton Live. It’s a simple but powerfull system with intuitive controls.\n\nIMUs and Phones\n\n\n   \n\n\nAn IMU is an electronic device that measures and reports a body’s force, angular rate, and orientation of the body by using accelerometers and gyroscopes. They are inertial since these sensors only measure their own movements\n\nWhat makes IMUs so ideal in music technology is the ability to capture motion data they give. The constant stream of data they provide can be mapped in several ways depending on use case. In my case I used acceleration and orientation data specifically, but also other sensor data like the phone’s light sensor.\n\nTo get access to this data and to send it to my computer I used an open source application called Sensors2OSC for Android. This application let me choose what sensors I wanted to capture and then send the data over the network as Open Sound Control (OSC) messages.\n\nSampler Modules\n\nSequencer\n\nThe main sequencer is controlled using orientation data. There are two tables which represents how many steps there are in a sequence and which segment of the sample is being played. These tables are controlled by the phones rotation on the x- and y-axis\n\n\n   \n\n\nLFO\n\nI also added an LFO which is controlled by acceleration data and can be assigned to various parameters. You can use the LFO for either controlling the sequencer or the filter cutoff.\n\n\n   \n\n\nFilter\n\nNext, I added a standard filter module where you can choose what type of filter you want to use. The filter cutoff was then mappen to the phone’s light sensor. This meant that a brighter environment would open up the filter while a dark one would close it.\n\n\n   \n\n\nSpectral Freeze\n\nThe final module was a spectral freeze effect. This effect takes a snapshot of the signal and calculates the FFT and creates a small spectrogram. This spectrogram is then stored and freezed in place. When finished recording all the spectrograms they are combined and resynthesized to create a unique sound depending on the playback speed and FFT window used. The volume of the effect can also be controlled using the pitch of the phone.\n\n\n   \n\n\nConclusion\n\nThis is how the final patch looks. If you’re interesed in trying it out it’s available on my Github page I’ve included a README which throughly explains all functions and how they’re used.\n\n\n   \n\n",
        "url": "/motion-capture/2023/05/08/fabianst-motion-sampler.html"
      },
    
      {
        "title": "Xyborg: Wearable Control Interface and Motion Capture System for Manipulating Sound",
        "author": "\n",
        "excerpt": "Witness my transition from human to machine - with piezo discs\n",
        "content": "In this semester’s Motion Capture class I wanted to find a way to literally take a step back from my MIDI keyboards and explore the space around my body. The idea to control sound with a wearable device has been sitting in my head for a while, so the opportunity was ripe to explore the concept.\n\nYou can buy wearable devices such as the genki ring but I wanted to see if I can come up with an inexpensive customizable version. Check out my final concept in the picture below.\n\n\n   \n   Current state of the device fixed to my forearm and hand.\n\n\nThe idea\n\nPiezoelectric discs function as sensors and send an electric signal when pressed. They are soldered together and connected to an Arduino MEGA R3. The plan was to be able to use different thumb-finger combinations and touching multiple sensors at once to send control signals. I created 4 different functionalities with a processing pipeline in Max: on/off (thresholded continuous signal), continuous, continuous~ and double tap. So far so good!\n\nI used a MYO armband for capturing arm motion. You can do a lot with it since you get 9-axis IMU data, data from the EMG sensors and classified gestures. The final Max patch includes the control display for the wearable device and MYO data as well as a phase-modulated synthesizer and several effects. But how do you maintain control over the continuous data stream so it does not constantly modulate your parameters? That’s where my xyborg hand comes in. With pressing the right sensor you open up a gate that allows you to modulate an effect parameter like the cutoff of the lowpass filter. MYO gestures were used to decide whether an effect is on or off.\n\nResults\n\nI was happy with the first test run since I could perform and modulate effect parameters quite reliably. After continuous use though, the piezo discs started to give up on me and introduced a lot of noise and signal peaks into the system making it impossible to not trigger stuff randomly. The MYO also proved to be somewhat of a nuisance since gesture classifications are pretty much unsuable when you want to use EMG data and change parameters in relation to muscle tension simply because gestures randomly trigger when tensing muscles.\n\nWell, no system is perfect from the beginning. That’s why I sat down again and impoved the design. I won’t use the MYO again but incorporate other sensors, possible movesense or bare MPU’s, and place them differently to capture hand motion and a wider angle of the roll-axis. Piezo discs will be replaced with strain gauges and fixed to a ring aound the finger. Check out my renderings down below.\n\n\n   \n   Rendering of a future concept\n\n\n\n   \n   Inner side of the hand\n\n\nI am happy with the outcome so far since I see the potential. Next iteration with music soon to come!\n\n",
        "url": "/motion-capture/2023/05/08/kristeic-xyborg-MoCap-project.html"
      },
    
      {
        "title": "Simple yet unique way of playing music",
        "author": "\n",
        "excerpt": "Gestures can be more intuitive to play around with\n",
        "content": "Overview\n\nIt may seem weird to imagine, but movement can actually make a musical performance richer, more natural, and even more unique than other traditional methods.\nI’ve recently stumbled upon an exciting approach to interactive music performance. The system can be easily implemented using accelerometer data from a phone to control audio samples in real time within the Max/MSP environment.\n\n\n  \n  The OpenSoundControl help patch\n  Source: Cycling 74\n\n\nImplementation\n\nThe following workflow uses a custom Max/MSP patch that processes incoming accelerometer data from a smartphone, mapping it to the playback of pre-produced audio samples. The given accelerometer sensor has a maximum range of 78.45 and a resolution of 0.0012, allowing precise and responsive control over the pieces.\n\nThe Sensors2OSC app sends the X, Y, and Z-axis data via OSC (Open Sound Control) messages, ensuring low latency and high-precision interaction between the phone and the Max/MSP patch. Upon receiving OSC messages, the patch applies various transformations to detect significant changes in hand movements and trigger sound.\n\nThe playback of the samples is managed using the “groove~” objects in Max, allowing for continuous looping and amplitude control. Additionally, users can replace samples during the performance, adding versatility and flexibility to the system.\n\nUser Feedback\n\nWith regard to the system’s efficacy and potential, a user study was conducted with three participants, two musicians and one non-musician. According to the feedback, the project is intuitive and engaging, offering an exciting musical experience. The ease of setup and minimal learning curve highlights the potential for the application to be used by diverse users, regardless of their musical background or technical expertise.\n\nIntegrating accelerometer data augmented the experience, as movements enabled more articulate control over the sound than traditional programming methods. However, participants also identified limitations regarding the choice of audio samples. They suggested that percussive or longer ambient sounds would work better as foundational elements.\n\nFuture Work\n\nUsers also mentioned several directions for future exploration, such as capturing additional gesture data combined with camera tracking and audiovisual elements. These suggestions open new ways for the design, expanding its capabilities and offering users to have more creative outcomes. For instance, they mentioned that adding biometric data from wearables or integrating multiple data streams could enable more nuanced and expressive control over the sound, creating a richer and more personal musical experience.\n\nQuestions about the system’s adaptability to different genres or musical styles were also raised. Future developments could involve implementing new features or controls that allow users to tailor the application according to their needs and creative goals.\n\nConclusion\n\nThe smartphone-based interactive music system may offer a surprisingly exciting and accessible approach to controlling sound through gestures and movements and stimulating improvisational musical experiences. Despite some limitations, the system’s flexibility allows for expansion and customization, making it suitable for various musical contexts.\nPotential enhancements include:\n\n  Integrating different gesture recognition algorithms.\n  Advanced mapping techniques.\n  Incorporating additional sensors or input modalities.\n\n\nAesthetically, integrating Max/MSP’s Jitter objects could create unique effects modulated by accelerometer or gyroscope data, resulting in a multi-sensory experience that blends audio and visual elements. Critically reflecting on the system’s benefits, limitations, and potential improvements can lay the foundation for further advancements in interactive music technology at a faster pace.\n\n",
        "url": "/motion-capture/2023/05/09/ninojak-looping-with-hand-movements.html"
      },
    
      {
        "title": "Generative Music with IMU data",
        "author": "\n",
        "excerpt": "Eight routes to meta-control\n",
        "content": "For  this project I wanted to explore the use of IMU motion data for techniques associated with generative music practices, such as those often found in Eurorack and audio programming.  Brian Eno defines this practice generally as the creation of a system which automatically creates musical parts based on rules and sensibilities set by the musician.  The musician the therefore designer and curator over the system, rather than being directly responsible for creating the musical ideas.\n\nHere, eight processes/techniques were emulated in Pure Data and their parameters opened to IMU motion  data.  These are listed below.   There is also a performance video of myself using all eight processes in a complete system.\n\n\n   \n   Some of the motion controlled \"modules\" in the performance system \n\n\nFor control, the IMU sensor used was the one built into my smartphone.  It was then attached to my wrist using an armband normally intended for runners.  I chose to attach the sensor to the  wrist due to high range of potential movement afforded by that placement.\n\n\n\nStep Sequencer\nPitch - Mapped to MIDI note number on the selected step\nRoll - Chooses the step to be edited by pitch\nYaw - Number of notes in the connected Euclidean sequencer, allowing for a unified sequencing environment.\n\nRandom Sequencer\nPitch - Mapped to “lvl”, control over the highest possible  MIDI value generated\nRoll - Mapped to “loop length”\nYaw - Mapped to “offset”, control over the lowest possible MIDI value generated\n\nClock Division Mixer\nPitch - Mapped to 1/8 clock division\nRoll - Mapped to 1/4 clock division\nYaw - Mapped to 1/6 clock division\nX acceleration  - Mapped to 1/2 clock division\nY acceleration - Mapped to 1/3 clock division\nZ acceleration - Mapped to 1/1 clock division\n\nEuclidean Sequencer\nPitch - Mapped to the number of notes/triggers\nRoll - Mapped to “rotation” or timing offset\nYaw - Mapped to total number of steps in thee sequence\n\nEuclidean Drum Sequencer\nPitch - Mapped to number of kick drum triggers and the length of sequence.\nRoll - Mapped to number of closed hi-hat triggers and the rotation of the kick triggers.\nYaw - Mapped to number off open hi-hat triggers, closed hi-hat rotation and closed hat sequence length.\n\nArpeggiator\nPitch - Mapped to the number of notes in the arpeggio\nRoll - Mapped to the MIDI note number of the arpeggio’s root note.\nYaw - Mapped to arpeggio “direction”, up, down or up and down.\n\nMotion Generative Wavetable\nPitch - Mapped to select which value in the wavetable to change.  This value changed also triggers normalisation of the wavetable.\nRoll, Yaw, X, Y, Z acceleration - All fed in to write to the wavetable array.\n\n",
        "url": "/motion-capture/2023/05/09/alexanjw-generative-motion-control.html"
      },
    
      {
        "title": "A Body Instrument: Exploring the Intersection of Voice and Motion",
        "author": "\n",
        "excerpt": "Manipulate your voice with your body\n",
        "content": "\n   \n\n\nIn this post I am exploring the exciting intersection of vocals and motion by developing a system that allows performers to translate their physical movements into sonic processes. I’ve designed a performance system that allows performers to control and manipulate their voice  in real time using their body movements. By using their bodies as instruments, performers are able to create music that is both highly personalized and deeply expressive.\n\nMotion Tracking\nTo capture the motion of the body, I used the OptiTrack system in the Portal and used Motive to capture the position of markers placed on the body. I then sent the positions of the markers and rigid bodies to my computer using NatNet, which a network protocol for streaming motion capture data. I used 21 markers, 18 of them making up 6 rigid bodies on OptiTrack’s plastic rigid bodies. I placed the rigid bodies on the head, hands and feet, and 3 standalone markers along the spine.\n\nOne of the challenges in using marker-based motion capture systems is the need to identify each marker uniquely. In some systems, markers come with preassigned IDs, which makes tracking them easier. However, in other systems, the markers do not come with IDs, and the tracking software has to assign unique IDs to each marker. This can be challenging because the tracking software needs to distinguish between markers that are very close together or even touching. In some cases, the markers can move in and out of each other’s range, making it difficult to assign a consistent ID to each marker.\n\nTo overcome this challenge, we can use various methods to assign IDs to markers. One method is to place the markers in specific patterns that the tracking software can recognize, such as placing four markers in a square shape or three markers in a triangle shape. This is how I use rigid bodies. They are automatically detected and their centroids are calculated and sent over the network with unique IDs.\n\nHowever in Motive, standalone markers didn’t come with preassigned IDs so it was a challenge to track the spine markers. To address this, I developed a custom algorithm in Python that is, in every packet reveiced, calculating the distance between each marker and the previous positions. Based on this calculation, I obtained a matrix of distances. I matched each marker to the closest one in this matrix to accurately track the movement of each marker and use the data for my research as long as I launch the system standing in T pose.\n\n\n   \n   Marker placement on the body\n\n\nAudio Processing\n\nHere are the motions that I used and the correspioinding audio processing operation.\n\n\n  \n    \n      Motion\n      Audio   processing\n    \n  \n  \n    \n      Angle of   the head\n      Stereo panning\n    \n    \n      Hand   distance (left hand higher)\n      Tremolo modulation frequency and magnitude\n    \n    \n      Hand   distance (right hand higher)\n      Delay Feedback\n    \n    \n      Spine   curvature\n      Comb. of Reverb, low-pass filter, band-pass filter\n    \n    \n      Neck   curvature\n      Comb. of Reverb, low-pass filter, band-pass filter\n    \n    \n      Left foot   lift/press\n      Boolean control for clearing the loop\n    \n    \n      Right   foot lift/press\n      Boolean control for writing in the loop\n    \n  \n\n\nThe resultant stereo audio is routed to 6-speaker array forming a 225°-sphere suspended from the ceiling, with the user’s sitting position being the center of the sphere. The following shows how the speakers are placed around the user.\n\n\n   \n   Speaker positions\n\n\nEvaluation\nI sat down with two talented classmates and a close friend to put my system to the test. I wanted to hear firsthand to what extent the implemented system was succesful? The group consisted of a singer, a violinist, and a user experience researcher. They individually tested the system after my general introduction to the genreal purpose and how to use.\n\n\n   \n    There was a display in front of the participants showing\n   the waveform of the looping channels with a vertical line showing the time.\n\n\nOverall, everyone liked the system. They all found it creative, expressive and fun to play with. They found stereo panning with head and the delay effect the most natural. They also liked the looping functionality as it creates a room for various possibilities of music creation.\n\nHowever, there were also points that they mentioned to be improved. One was the latency that is introduced by the wireless microphone, audio interface and the processing time of the effects. Secondly, some of the participants found the pedal motion not natural or something that would take time to get comfortable with. Some participants said that they wanted to see more tracks to record on, loop and cycle for more possibilities.\n\nOverall, it seemed that the system worked in terms of introducing new ways of creating music with voice, and interaction with body motion. It was very exciting to create such an interactive system as it opens up new avenues for musical expression and creativity.\n",
        "url": "/motion-capture/2023/05/09/ahmetem-vocal-instrument-motion.html"
      },
    
      {
        "title": "Music Between Salen and the World",
        "author": "\n",
        "excerpt": "We played in a global NMP concert. Check out our experiences.\n",
        "content": "For our final network music performance Dispersionology and Other Tales, we performed with musicians in several countries in a performance conducted by composer Douglas Van Nort. The concert on our side happened in Salen, our favourite in-house auditorium.\n\nConcept\n\nFor this telematic experience, as SMC Students, we contributed an improvisational performance conducted in the United States and performed in more than four countries.\n\nDouglas Van Nort describes the performance as:\n\n  “In the world of physics, dispersion describes a phenomenon in which the rate of propagation of a wave in a medium, its phase velocity, is dependent on its frequency. This can be seen in light, sound, gravity waves, etc. Its a property of telecommunications signals, including the pulses of light in optical fibre cables, describing how the signal broadens and spreads out as it moves across the channel. Dispersion therefore is inherent in the medium that more-and-more binds us these days, in the movements of light pulses that transports our attention, and our listening, around the globe. A beautiful consequence of dispersion is a change in the angle of refraction of different frequencies, leading to a prismatic opening up of a full colour spectrum from incoming light. This ability to broaden out as signals propagate through the network reflects a much wider expansion of distributed listening and sounding that is made possible in the context of telematic musicking. It occurred to me recently that, as of early 2023 I’ve engaged this medium now for 20 years, with an ear towards exploring the myriad ways that the shared real/virtual and nowhere/everywhere site of performance can act as both a point of convergence towards a singular locus of performative attention – yet also a dispersive prism, reflecting individual voices and the preservation of creative agencies of every performer.”\n\n\nPieces\nTuning Meditation by Pauline Oliveros An improvisational piece where every performer picks a pitch not represented at that point and plays that pitch with sustain of a breath time.\n\nDispersionology by Doug Van Nort\nAn improvisational piece whereby all locations and performers were directed/conducted/cued by Doug Van Nort.  This was either via a system of gestures which referred to a location or grouping of musicians (layers), their volume, tone vs. noise, which musical “palette”  they should respond to, as well as “sound painting”, whereby musicians freely interpreted Van Norts physical actions.\n\nAnd Other Tales by Doug Van Nort\nAn improvisational piece whereby all musicians responded to a variety of randomised on-screen prompts and images.\n\nPreparation\n\nWe started planning 3 weeks in advance to get ready for this big event.\n\nIn our first planning session we tried to draw an outline for our part in the concert, and our composition. We roughly decided on the performers and the instruments. We also discussed the various compositional concepts for our own piece we wanted to perform, with room for other musicians. Sadly, since we were significantly less people in the end, we decided to not to include a composition and only perform in the concert with other people.\n\nWe were also involved in a rehearsal/technical test with the other participants of the event on the Sunday evening prior, where we were able to troubleshoot audio and video routing, as well as learn in greater detail the artistic intensions, the gestures involved, and the musical palettes of the performance.\n\nPerformers and Instruments\n\nWe performed with variety of instruments and techniques covering a wide spectral range in addition to the balance between tone and noise.\n\n\n  \n    \n      Performer\n      Instrument\n    \n  \n  \n    \n      Fabian\n      No-Input Mixing\n    \n    \n      Aysima\n      Accordion\n    \n    \n      Emin\n      Ney Flute\n    \n    \n      Alex\n      Electronics\n    \n    \n      Nino\n      Vocal with effects\n    \n    \n      Kristian\n      Experimental Acoustic Guitar\n    \n    \n      Kristin Nordeval\n      Vocals\n    \n  \n\n\nFabian’s No-input mixer is a standard mixer where the outputs are connected to the inputs creating a feedback loop which is then controlled by the mixer’s controls and several guitar pedals.\n\nAccordion can play both melody and bass section in a composition. For the parts lead by Douglas, accordion had different roles. While playing a musical composition like phone ringing, accordion contributed with the keyboard part and played notes in sequence. For some other parts it was better for accordion to bring sustain sounds with high or low pitches depending on the character of the composition.\n\nNey is a fretless wooden flute capable of creating both noisy and tonal sound, which enables an experimental space for its performer. With his Ney, Emin played melodic lines and noise effects during the concert.\n\nAlex contributed Electronics playing a small Eurorack modular synthesiser through delay and reverb pedals.\n\nNino experimented with improvisational vocal performance. She processed the sound with various modulation effects with a kaleidoscopic vibrato that created strong pitch variations.\n\nKristin as a soprano soloist, improvised with the group in both classical and avant-garde styles.\n\nKristian explored unorthodox techniques for acoustic Guitar, laying the instrument flat, using a bottleneck to slide along the strings, tap the body of the instrument etc.  This was run through an effects chain in Ableton Live.\n\n\n   \n   Us discussing the different sound palettes that were a conceptual characteristic of Douglas Van Nort's composition\n\n\nGetting Technical\n\nIn this concert, we transmitted audio using a network music performance software, JackTrip. We connected to a jacktrip server set up by the team around Douglas Van Nort with one of the SMC’s portable LoLa kits.\n\nAudio Engineering\nAudio routing for this project meant allowing for a FOH stereo mix for our local audience, three foldback mixes for Oslo-based musicians, as well as sending a stereo sub-mix of our instruments to the Jacktrip server. The plot below shows the routing used.\n\n\n   \n   Audio Routing in Salen\n\n\nVideo Connection\n\nDuring the concert, all the participants met in a Zoom call to follow Douglas’ conducting and interact with other musicians. We used the Logitech PTZ meeting camera in Salen to share the video of the entire stage.\n\nTo follow the conductor and the live score, we placed two monitors on the stage in front of us: one for Douglas’ video, one for the live score page. These two screens were connected to a laptop. We also projected both on the projection screen on the stage for the audience, which was connected to the stationary computer in the Salen.\n\n\n   \n   For the third piece of the performance, we setup a live sheet on the stage and followed it\n\n\nLatency Measurements\n\nMeasurements have been done beforehand on two different jacktrip servers with a loopback. We used the same setup we had been utilizing for other concerts. Server 1 had a Roundtrip latency time of 188ms whereas server 2 came in on 261ms RTT. High latency times were expected and did not really affect performance since the concert and musical pieces were planned to be a latency-accepting. The final concert happened over server 1 with a global sampling rate of 44.1 kHz and a buffersize of 256 ms.\n\nReflections\n\nDespite this mode of improvisational performance being outside the experience of many in the class, it was both technically and musically executed to a standard we were pleased with. Our participation in the event went smoothly without any technical mishaps.\n\nWatch the performance on YouTube here:\n\n\n",
        "url": "/networked-music/2023/05/18/ahmetem-final-nmp-salen.html"
      },
    
      {
        "title": "Unsupervised Meta-Embedding for Bird Songs Clustering in Soundscape Recordings",
        "author": "\n",
        "excerpt": "A case study on nocturnal and crepuscular tropical bird songs.\n",
        "content": "\n  Code available on GitHub\n\n\nAbstract\n\nAmazonian forests are threatened by numerous anthropogenic pressures not visible by satellite imagery, such as over-hunting or undercover forest degradation. Knowledge of the effects of these degradations is essential for an effective local conservation policy. However, these effects can only be assessed using quantitative methods for monitoring biodiversity in the field. In recent years, ecoacoustics has offered an alternative to traditional techniques with the development of Passive Acoustic Monitoring (PAM) systems allowing, among other things, to automatically monitor species that are difficult to identify by observers, such as crepuscular and nocturnal tropical birds. Although the use of such systems makes it possible to acquire large sets of data collected in the field, it is often difficult to process these data because they generally represent several thousand hours of recordings that need to be annotated and validated manually by an expert with in-depth knowledge of the phenology and behavior of the species studied. The objective of this thesis is to develop a new method to facilitate the work of ecoacousticians in managing large unlabeled acoustic datasets and to improve the identification of potential new taxa. Based on the advancement of Meta-Learning methods and unsupervised learning techniques integrated into the Deep Learning (DL) framework, the Meta Embedded Clustering (MEC) method is proposed to progressively discover and improve the inherent structure of unlabeled data.\n\nBackground\n\nEcoacoustics\n\nThis thesis mainly relies on a recent discipline called ecoacoustics that studies sound along a broad range of spatial and temporal scales in order to tackle biodiversity and other ecological questions. This discipline has been introduced in 2015 in [1].\n\nIn this thesis, soundscape recordings are used in order to analyze sounds produced by animals (e.g. biophony). Soundscape are generally defined by three components:\n\n\n  Geophony (i.e. sounds generated by physical events such as waves, earthquakes, or rain)\n  Biophony (i.e. sounds produced by animals)\n  Anthropophony (i.e. sounds associated with human activity)\n\n\n\n  \n  A soundscape recording with Regions Of Interest (ROIs) (source: https://scikit-maad.github.io)\n\n\nObjectives\nThis thesis was carried out in the Muséum National d’Histoire Naturelle (MNHN) de Paris with the EcoAcoustics Research (EAR) team. The objective was to develop a framework useful for a better understanding and visualization of highly dynamic and complex sound scenes in tropical environments in order to tackle the following problems:\n\n\n  National biodiversity inventories not carried out yet (e.g. in developing countries)\n  Rare species are targeted (e.g. nocturnal and crepuscular bird species)\n  Absence of experts\n  Large amount of unlabeled data (i.e. many hours of recordings)\n\n\nTherefore, the main problem is how to get around the problem of lack of large labeled datasets in challenging acoustic environments?\n\nMeta-Embedded Clustering (MEC)\nFor this purpose, the Meta-Embedded Clustering (MEC) method was proposed to facilitate the issues of discovering and gradually improving the inherent structure of unlabeled data. This method is mainly based on Meta-Learning algorithms and more specifically on Unsupervised Meta-Learning (UML) techniques [2] that have the advantage to improve few-shot image classification by learning features into clustering space.\n\n\n  \n  Unsupervised Meta-Learning (UML). From “Unsupervised few-shot image classification by learning features into clustering space”, by S. Li et. al, Conference, Tel Aviv, Israel, October 23, 2022, Springer.\n\n\nThe MEC method is performed in five successive steps where (i) the data is passed through the initialized model, (ii) initial estimate of the non-linear mappings are computed to avoid the curse of dimensionality, (iii) a clustering algorithm (HDBSCAN) is performed on the latent space, (iv) a pseudo-labeled dataset is built from the clustering algorithm’s predictions and (v) the model is fine-tuned on the pseudo-labeled dataset for n episodic tasks.\n\n\n  \n  Meta Embedded Clustering (MEC)\n\n\nResults &amp; Discussion\nThree research questions were asked in this thesis:\n\n\n  Q1: How well does episodic training improve the performance of a Meta-Learning algorithm compared to classical training?\n  Q2: To what extent can Meta-Learning algorithms fine-tuned on pseudo-labeled data classify classes that were not used during training?\n  Q3: To what extent Meta-embeddings can improve the clustering quality of unlabeled data?\n\n\nIn this blogpost, particular attention is paid to the research question Q3 because it is at the core of this thesis and is closely related to the MEC method. When performing clustering on the latent space, results presented in the following table show that Meta-embeddings compared to baseline embeddings (ResNet18) improve the quality of the clustering.\n\n\n  \n    \n      Embedding\n      Number of clusters\n      Accuracy\n      NMI\n      ARI\n      DBCV\n    \n  \n  \n    \n      Baseline\n      4\n      30.58%\n      0.1201\n      0.0212\n      -0.0949\n    \n    \n      Meta\n      13\n      67.48%\n      0.8142\n      0.5813\n      -0.2029\n    \n  \n\n\nThe goal in this thesis was to find a ground truth number of 21 clusters. Results presented in the following table show that performing iterative clustering on the Meta-embeddings can help refine the clusters and further improve the clustering quality (from 17 to 19 clusters with 69.10% to 76.60% accuracy).\n\n\n  \n    \n      Iteration\n      Number of clusters\n      Accuracy\n      NMI\n      ACI\n      DBCV\n    \n  \n  \n    \n      0\n      17\n      69.10%\n      0.8460\n      0.5650\n      -0.3547\n    \n    \n      16\n      19\n      76.60%\n      0.8681\n      0.6842\n      -0.0920\n    \n  \n\n\nConclusion\n\nThe global objective of this thesis has been to facilitate the work of ecoacousticians in their management of acoustic data and identification of potential new taxa, by discovering and gradually improving the inherent structure of unlabeled data. Based on nsupervised clustering-based methods, the Meta Embedded Clustering (MEC) method turned out to progressively improve the inherent structure of unlabeled data. This method has eventually allowed to further improve the accuracy of the data clustering (69.10% vs. 76.60%) and, in this way, contribute to determine a number of clusters closer to the actual number of clusters expected. In conclusion, the use of unsupervised Meta-embedding has proven to be an effective solution for improving the clustering of bird songs in soundscape recordings. These technological methods can therefore bring forward novel research in developing countries that can facilitate the identification of species as well as the detection of potential new rare bird species.\n\nReferences\n\n[1]  Sueur, et. al (2015). Ecoacoustics: the ecological investigation and interpretation of environmental sound. Biosemiotics, Springer. https://link.springer.com/article/10.1007/s12304-015-9248-x\n\n[2]  S. Li et. al (2022). Unsupervised few-shot image classification by learning features into clustering space. BConference, Tel Aviv, Israel, October 23, Springer. https://link.springer.com/chapter/10.1007/978-3-031-19821-2_24\n\n",
        "url": "/masters-thesis/2023/06/12/joachipo-master-thesis.html"
      },
    
      {
        "title": "Bandwidth to Band Together",
        "author": "\n",
        "excerpt": "A Study on Approaches for Remote Music Collaboration\n",
        "content": "Abstract\n\nThe thesis evaluates approaches of remote collaboration in a contextualised setting of traditional Digital Audio Workstations (DAWs) ability to facilitate collaboration, and contemporary solutions for Remote Music Collaboration Systems (RMCS). With a review of three approaches to remote collaboration, they have been evaluated by opportunities and constraints in a collaborative songwriting and mixing setting. By using a framework for categorizing DAWs utilization and usage, existing DAWs have been evaluated and contextualized with how they can transition into approaches for remote collaboration. The research has been conducted by examination of existing platforms, review of literature and previous research, personal experiences, and an experiment where approaches of remote music production have been tested with a following group interview. The results present an overview of contemporary solutions, possibilities and obstacles when conducting remote music production.\n\nBackground\n\nIn this blogpost, a brief overview of the background and results are presented. Telematic communication and collaboration have experienced a drastic increase in the last ten years have been further accelerated by the Covid-19 pandemic (Vitagliano, 2021). Broader high-speed internet coverage enables professional broadcasters and consumers to use the same infrastructure and tools for distance production and collaboration. Before wide spread internet coverage, the transfer of audio and video for broadcasters has relied on expensive solutions such as satellites or telephone lines, sacrificing latency when using satellites, and quality when using telephone infrastructure. A popular term used in the broadcasting and film industry is green production, a practice aiming to reduce the production’s environmental impact. In practice, implying less movement of people or goods to a location either to send it back to a centralized location, or part of the production solved in a more environment-friendly way. Transfer of audio and video leads to new obstacles in terms of what’s the most efficient way of solving latency, data transfer, and storage. In an organized environment such as broadcast, there is little room for creative collaboration, as most actions are planned and executed. Broadcast operations contrasts how Remote Music Collaboration Software (RMCS) operates, which relies on feedback from other collaborators, often in a unorganized environment. Still, RMCSs provide a valuable testing ground to understanding remote production principles, as they exhibit the same issues with latency and data volume as professional broadcasting solutions. The thesis investigation were done with two research questions:\n\n\n  \n    Research Question 1:What are the opportunities and constraints of three distinct collaboration approaches for users in a collective songwriting or mixing setting?\nHypothesis: Each approach can be used to facilitate remote collaboration in its way, but it depends on the situation, users, task, and product desired. More than one approach is required to fulfill a end-to-end production, and several approaches must be used to obtain a finalized product.\n  \n  \n    Research Question 2: What affordance are there in the most used Digital Audio Workstations, and how does this affect the facilitation of remote collaboration?\nHypothesis: Existing platforms are designed based on who uses the platform, and not all platforms are utilized the same way. Affordance in DAW design affects how remote collaboration can be facilitated, as producers, artists and technicians have different requirements for a platform.\n  \n\n\nThe study proposes to identify approaches used in remote collaboration of music production and songwriting as follows:\n\nAsynchronous Approach (AA):\nThe process of transferring files asynchronously, with limited real-time communication. It is widely used due to its flexibility, but it can face challenges with cross-platform compatibility and large file transfers. Signal-chains can be bounced together with audio, making revisions more difficult. Bandwidth limitations can also impact efficiency in projects requiring frequent revisions.\n\nObserver Controller Approach (AC):\nThe process of one collaborator in control over the session while the other participant(s) observe and comment in real-time. One participant (the Controller) controls the platform, while the other participant (the Observer) provides instant feedback and input. This approach is beneficial in producer-talent scenarios and can work with any platform that supports audio transmission and screen-sharing.\n\nController-Controller Approach (CC):\nThe process of participants having equal control over the session and communicating in real-time. Mltiple participants have equal control over the session simultaneously. This synchronous real-time approach eliminates the need for file exchanges and allows all participants to see and hear changes in real-time.\n\n\n  \n  Approaches towards remote collaboration\n\n\nTogether with RMCS playing an increased role in how music is created and collaborated, the DAW market offers many software alternatives, ranging from user-friendly to complex software with advanced capabilities. Some DAWs specialize in specific tasks, while others aim to provide as many features and functions as the user can imagine. Although most DAWs share the same basic layout and functionality, their tools, features, workflow, and GUI differ. According to Strachan, Cubase and Logic were initially MIDI sequencers that later added audio support in the 1990s. In contrast, Pro Tools started as a hard-drive recorder and did not implement MIDI until 1999. When Ableton and FL Studio were released in 2001, there was an expectation that a multi-functional DAW that was not reliant on hardware should include everything in the box (Strachan, p. 75-79, 2017). Pro Tools continues to follow a “retro-imitation” or “skeuomorphic” design philo- sophy based on analog hardware, given its historical foundation as a recorder (D’Errico, 2022). Pyramix, a DAW primarily used for recording and mixing in ultra-high sample rates, follows a similar process-oriented design philosophy that prioritizes functionality over aesthetics.\n\nThe study proposes to divide workstations into three categories:\n\n\n  \n    Amateur Centric workstations are categorized as workstations with limited functionality, focused on the beginner/amateur musician or producer. They can do basic recording, editing, and producing but cannot expand into the higher level complexity needed for professional work, limiting the toolset available. For example, Bandlab offers simple creative tools for songwriting but limits the virtual instrument libraries and the number of tracks available. While these DAWs may have a different functionality than professional workstations, they can still be effective tools for those who are just starting out and learning the basics of music production. They can help beginners develop their skills and gain confidence before moving on to more complex and advanced software.\n  \n  \n    Artist Centric workstations are based on the creativity of the artist or producer. They often disregard the recording and mixing part of a project, and favours the production or performance process. For example, FL Studio’s sample-based workflow is centered around the sampler and sequencer, allowing for easy creation of loops and patterns. The interface is designed to be visually appealing and intuitive for creative experimentation, with a focus on step-sequencing and automation. However, FL Studio has limited recording capabilities compared to other DAWs and may not be the best choice for more traditional audio editing and mixing tasks.\n  \n  \n    Mix Centric workstations usually do not have any reasonable limitation in the number of tracks, busses, or plugins that the creator can apply and are generally designed to facilitate audio recording, editing, and mixing tasks, with a focus on creating polished, professional-sounding mixes. Some examples of Mix-Centric DAWs are Pro Tools, Logic Pro, and Cubase. Their primary function is to be an endpoint in the creative process - releasing recorded music or audio straight from the DAW to the desired platform or format. In general, Mix-Centric DAWs prioritize audio quality and editing/mixing capabilities over creative experimentation and linear workflows.\n  \n\n\nDependencies in Virtual Environments\n\nBoth Olson and Olson (2000) and Spilker (2012) suggests that virtual environments does not replace a physical meetup. The participants of my study suggest that the Asynchronous and Controller-Observer approaches do not constitute as large of a difference in their workflow compared to how they would have done it physically in the same room. This aligns with what Hracs et al. (2016) discusses, with a location-agnostic approach to music production making it more accessible. Reliving the boundaries between a virtual environment and a physical environment, can provoke new modes of hybrid platforms such as Bandlab; by not emulate or imitate workflows seen in desktop platforms for music creation. If the platformization of workflows into online environments also can support a collaborative DAW, it would create a new dichotomy of how music is collaborated by supplying a collaborative environment regardless of the platform actually being used for this purpose. The collaborative function can be seen more as an add-on, rather than the primary function, breaking barriers between virtual and physical environments.\nThe informal interactions was not measured in this study, as the participants were free to interact in-between sessions. But its clear from this study that the participants found it just as enjoyable working remotely as physical presence. This can also be of affect from their knowledge and confidence in remote environments, but it can also be the nature of the Controller-Controller approach, which incorporates both individual and collective presence. The Asynchronous- and Controller-Observer approaches does not display the same level of collaborative emergence as the Controller-Controller approach, which offers a more democratic environment than the other approaches. Roles has to be more defined to effectively collaborate in a delayed-feedback collaboration. It is clear that all approaches function as pre-distribution networks, before the finalization of a project. Both the Asynchronous and Controller-Observer approach function best before, and after the main part of the project to either initialize ideas, or to finalize them. However, to create the actual content of a project, a Controller-Controller environment is best suited, given equal participation from both collaborators is needed. The motivation to initiate remote collaboration, is determined by how engaging contributes are willing to be in the project. As shown both from the experiment and the contemporary state of collaboration platforms, they function on separate terms where it may be easier for some creators to neglect the real-time and synchronous aspect of the collaboration, to rather collaborate in a delayed-feedback environment. This can potentially ease the scariness of begive themselves into a remote collaboration. Bandlab is one example of a platform that both supports Asynchronous collaboration tools in their social network, where the same material later can be edited in a Controller-Controller environment. This hybrid may function as a ice-breaker for collaborators that would rather collaborate in a physical environments.\n\nCollaboration Approaches vs. DAW Affordance\nThe study highlights an interesting dichotomy within DAWs, where certain DAWs are better suited to facilitate specific modes of collaboration than others. This finding can be attributed to the inherent design and functionality of the different types of DAWs. The Controller-Controller platforms presented in the study position themselves as Amateur- and Artist-Centric types. They are opposed to Mix-Centric workstations which can be easily adapted for Controller-Observer or Asynchronous collaboration approaches. Mix-Centric DAWs are characterized by a greater emphasis on ensuring reliability, consistency, and standardization. These dependencies, rather than enhancing the creative process, contrast Amateur-  and Artist-Centric DAWs, which are designed to a more userfriendly, flexible, and conducive to experimentation and improvisation, which is also the feedback seen in the Controller-Controller approach.\nEven though there is a lack of Mix-Centric DAWs that support Controller-Controller approaches, it is essential to note that the transition to a Controller-Controller environment may not be an easy or immediate one. The reliance on reliability and consistency in Mix-Centric DAWs means that it will take time to establish the necessary dependencies to create a reliable system for Controller-Controller collaboration. However, as more musicians and music producers begin to recognize the benefits of this mode of collaboration, we may expect to see a gradual shift towards more userfriendly and flexible DAWs better suited to facilitating new ways of collaborating. Nevertheless, the need for a Controller-Controller environment in Mix-Centric DAWs must also be discussed, as the dependencies in those platforms are not the same as Artist-Centric or Amateur-Centric workstations. A mix- or mastering engineer may not need the same modes of collaboration, with equal participation from others in the process. Therefor a “poducer-artist” setting would be more suiting to compare with, as the “producer” in this setting is the mix- or mastering engineer, and the “artist” is the client, defined in this study as Controller-Observer approach to remote collaboration.\n\nReferences\n\n[1]  Vitagliano, J. (2021). Bandlab founder meng ru kuok talks reaching 30 million users, outselling garageband and empowering music- makers around the world. Retrieved April 12, 2023, from  https://americansongwriter.com/bandlab-founder-meng-ru-kuok-talks-reaching-30-millionusers-outselling-garageband-and-empowering-music-makersaround-the-world/\n\n\n[2]  Olson, G. M., &amp; Olson, J. S. (2000). Distance matters. Human–computer interaction, 15(2-3), 139–178.\n\n[3]  Spilker, H. S. (2012). Piracy cultures| the network studio revisited: Becoming an artist in the age of\" piracy cultures\". International Journal of Communication, 6, 22.\n\n[4]  Strachan, R. (2017). Sonic technologies: Popular music, digital culture and the creative process. Bloomsbury Publishing USA.\n\n\n[5]  D’Errico, M. (2022). Push: Software design and the cultural politics of music production. Oxford University Press.\n\n\n[6]  Hracs, B. J., Seman, M., &amp; Virani, T. E. (2016). The production and consumption of music in the digital age. Routledge New York, NY.\n\n\n",
        "url": "/masters-thesis/2023/06/15/jakhoy-master-thesis.html"
      },
    
      {
        "title": "Say Hello to Team U",
        "author": "\n",
        "excerpt": "Meet SMC’s three new first year students. Coming from the United Kingdom and the United States they have a wealth of diverse musical, professional and technological backgrounds.\n",
        "content": "\n\nTeam U\nSMC’s 2023 students, Tom, Karenina and Juliana, have all had strong backgrounds in music from an early age. Playing cello, singing and being a part of various music groups have defined large portions of their lives.\n\nWithin the past five years each of the three members have gravitated towards various aspects of music technology. Working in live sound, with Max and experimenting with electronic music composition and performance have drawn them each to SMC to explore music technology more deeply. You can meet each of them below:\n\n\nKarenina Juarez\n\nHi there! My name is Karenina and I am from Los Angeles, CA, USA. For my Bachelor’s degree I attended Berklee College of Music in Boston, Massachusetts, USA. There I discovered my passion for the convergence of music, technology, and visuals. I graduated in 2021 with a Bachelor of Music majoring in Electronic Production and sound design with a minor in Music Technology. During the pandemic, I interned at Tesla where is was taught valuable AV skills that helped me in my AV career. I have since worked at Harvard and MIT as an AV supervisor and AV and Events Associate respectively.\n\nIn this program, I want to sharpen my skills in programming and go more into depth of my studies of machine learning and music computing.\n\nOutside of music I like to swim, crochet, travel, read, and meet awesome new people!\n\nYou can follow me on Instagram, Soundcloud, and Github to see what I’m up to!\n\n\nJuliana Bigelow\n\nJuliana Bigelow, is forging a path for herself as a woman in audio and media. In June of 2021 she graduated with Bachelor’s of Science in Sonic Arts and Music Production from Portland State University. In addition to her studies, she has worked in digital marketing, journalism, and audio editing.\n\nShe is passionate about accessibility and hopes the SMC program will allow her to create accessible music technology and installations.\n\nYou can see more of her work at julianabigelow.com and listen to her music on SoundCloud.\n\n\nTom Oldfield\n\nHello! My name is Tom Oldfield and I’m a musician from the UK. I studied classical cello and use this alongside hardware and modular synthesizers to make ambient, experimental and dance music. I like building, experimenting, improvising and finding new ways to interact with technology. I am excited to begin my journey in SMC as part of team U! Check out what I’m working on my \nwebsite or follow me on Instagram.\n",
        "url": "/people/2023/08/18/thomaseo-Team-U-first-post.html"
      },
    
      {
        "title": "Controling Guitar Signals Using a Pick?",
        "author": "\n",
        "excerpt": "A deeper dive into the Magpick\n",
        "content": "Controling Guitar Signals Using a Pick?\n\nIntroduction\n\nThere are many ways of playing and expressing yourself using the guitar. Either through the use of extended techniques, effect pedals, or even augmented guitars. If you want to control and manipulate the signal coming from the guitar you’re generally forced to  use your feet to control pedals. But this introduces some problems in regards to adjusting pedal parameters while playing since humans generally don’t possess the ability to operate and fine-tune switches and dials with their feet, especially not when wearing shoes.\n\n\n  \n  \n    A pedal board can help a guitarist express themselves. \n    Photo by Kelly Sikkema.\n  \n\n\nHow to Create New Control Surfaces\nThere are many ways to go about creating new control surfaces, either for an existing instrument or a new system you’ve developed. In 2001 Perry R. Cook introduced 13 principles for designing computer music controllers, and in 2009 he added 3 more.\n\nThese principles are artistic, technological and more philosophical and are meant to serve as guidelines for instrument designers. He notes that these principles are not “universal, but are rather a set of opinions formed as part of the process of making many musical interfaces”.\n\nThere’s also the question of difficulty when designing a new controller. Should playing this instrument be easy? James McDermott et al. makes a case for why music interaction should be difficult in some cases. One of the main arguments for difficulty is long-term engagement. If a musician can master an instrument without much hassle they will quickly get bored and move on.\nThe Magpick\n\nThe Magpick introduced at NIME 2019 is an augmented guitar plectrum which enables the user to control pre-defined effects using magnetic signals emitted from the guitar’s pickups. This pick promises low latency and highly nuanced control over the sound by using the Bela platform. The idea behind the project was to use ancillary gestures, which are the physical movements made by musicians while playing their instruments or singing, to control sound production. The Magpick specifically utilizes the pre- and post-pluck movements of the picking hand to actuate the sensors in the plectrum.\n\nThe system works by taking a hollowed out pick and embedding it with loops of wires which then reacts to the magnetic signal sent out from the guitars pickups. So, voltage is generated based on how fast you are playing. The signal then gets sent to an amplifier before it reaches Bela to be further processed.\n\n\n  \n  \n    The Magpick system. \n    Photo by bela.\n  \n\n\nEvaluating the System\nIf analyzed through Perry Cook’s principles for designing a computer music controller. Does it hold up to scrutiny? These principles aren’t hard rules you have to follow when designing an instrument interface, but more of a design philosophy. The principles most applicable to this project are:\n\n  Copying an instrument is dumb, leveraging expert technique is smart\n  Existing instruments suggest new controllers\n\n\nLet’s have a look at how the Magpick approaches these principles.\n\nPrinciple 1:\nCopying an instrument is dumb, leveraging expert technique is smart\nBoth the second and third principle are closely linked in this project. What Perry means by saying copying is dumb is that instead of re-inventing the wheel you can invent something new wheel-inspired, since the best wheel is still the wheel. The Magpick is not trying to re-invent the guitar pick but rather it uses the guitar pick design to create it’s own control interface.\n\nThe Magpick is used very similarly to a traditional pick. A lot of the gestures utilized will be familiar to guitarists. Although it does add some new gestures to the repertoire, like vertical strumming above the strings demonstrated here.\n\n\n\nPrinciple 2:\nExisting instruments suggest new controllers\nNow the last principle is quite interesting, because it’s true in some ways and untrue in others. The guitar pick is decidedly not a new controller. But the Magpick is a new way of using the controller. As mentioned in their paper, the Magpick is not the first guitar pick controller, there have been others like the PLXTRUM, but it is the first one which uses sensors to detect changes in the magnetic field.\n\n\n  \n  \n    Vase depicting a subject holding a lyre and plectrum from the late archaic period. \n    Photo by Egisto Sani.\n  \n\n\nWhat About Difficulty?\nThe Magpick is interesting when it comes to the question of difficulty, since it is meant to be used by guitarists whom already possess previous knowledge on how to play the guitar and use a traditional pick. They mention that guitarists were able to seamlessly integrate the Magpick into their playing quite quickly. This would suggest that the device is not particularly hard to use, so how can it inspire long-term engagement? Through subtlety and precision. Due to the sensors used and the Bela platform which offers extremely low latency players are able to achieve a level of precision not possible with traditional devices such as pedals.\n\n\n\nConclusion\nThe Magpick offers an interesting control interface for the guitar which is easy to integrate and use for players. Analyzing the controller through Perry Cook’s principles one can also see that it avoids many common pitfalls new music controllers often fall into.\n\n",
        "url": "/interactive-music/2023/09/19/fabianst-ims-review.html"
      },
    
      {
        "title": "Exploring Breath-based DMIs: A Review of the KeyWI",
        "author": "\n",
        "excerpt": "The relationship between musician and instrument can be an extremely personal and intimate one\n",
        "content": "Introduction\nThe relationship between musician and instrument can be an extremely personal and intimate one.  Contrastingly the nature of interacting with machines is often characterised as being the opposite; removed, cold, impersonal.  Electronic instruments however, must always straddle this. Of interest to me currently is the use of breath as a control-interface for Digital Musical Instruments (DMIs). Its innate connotations of intimacy implying potentials for deep connection with an instrument.\n\nThe KeyWI was shown at the NIME 2020 conference here.  As set out in the paper, the KeyWI is intended as an accessible breath-based DMI modelled on the melodica. A more detailed breakdown of the instrument is provided on its website here as well as the open-source code and hardware on the Github repository  A demo video of the instrument from its maker is shown below.\n\n\n\nTechnical and Self-Evaluation\nAs detailed on project website and paper, the system was evaluated at a technical level to assess how it compared to other breath-based DMIs and acoustic instruments. This looked at performance in dynamic range, attacks/transients, variance and expressiveness and pitch control interfaces. These results illustrate that in terms of technical hardware performance, the KeyWI’s breath interface performs more on-par with the response of acoustic breath-based instruments than comparative commercial DMIs.\n\nIn terms of the “accessibilty” of the instrument, as well as being a fully open-source project, the system has a keyboard for pitch information. This is justified as being a more familiar, standardised interface for musicians than the numerous equivilents in acoustic instruments.  Finally, the KeyWI prides itself on its simple code-base, with the Faust DSP code for its melodica sound engine being a mere twenty-five lines long.\n\nCritical Evaluation\nExpressivity and accessibility are the key elements of this instrument’s self-identity.  As such, in order to evaluate the system further I will draw on ideas of expressivity, engagement, virtuosity, immediacy and autonomy.\n\nBeginning with “expressivity”, the key affordance for this is via the breath sensor.  Examining the code across the various sound engines, this is mapped to amplitude and, where applicable across the sound engines; cutoff frequency of a low-pass filter, a modulation LFO and interplay between tone and noise.  Though this demonstrates a one-to-many mapping seen as good practice, in actuality it is seemingly only ever mapped to amplitude and one other timbre parameter.  This does correlate well with Wanderley et al.’s guideline for DMIs to, “require energy for amplitude”, though.  With such mapping, many expressive techniques are open to the musician involving control over dynamics including, theoretically, tremolo.\n\n\n   \n   KeyWI's breath sensor \n\n\nAs discussed, several design decisions were made specifically for the instrument to be easy to pick up and play.  Blowing into the breath sensor appears intuitive and many musicians will be familiar with the keyboard interface.  Furthermore, there are only controls for volume and switching between fixed preset sound engines.  Seeing as the musician need only power the instrument and connect it to a speaker, there are relatively few obstacles to its usage. As such, the instrument has a very immediate feel to it.\n\nSuch decisions for the sake of immediacy may however impact the instrument’s potential for long-term engagement.  Of relevance here are virtuosity, autonomy, as well as further dimensions in expressivity.  Taken with these considerations in mind, the key bottlenecks are the chosen keyboard interface and the preset sound engines.  While the keyboard interface does allow for immediacy, its chosen form is limiting by a keyboard’s standards. The keys are smaller than normal, and it covers only two and a half octaves. In other digital instruments, smaller keybeds will have controls for switching octaves up and down but that does not appear to be present here.  Furthermore, requiring one hand to hold the instrument, makes two-handed playing impossible. Wanderley et al.’s guidelines state that two handed operation is ideal.\n\nRegarding the KeyWI’s sound engine, again while this approach allows for immediacy, the simplicity of the sound engines and controls stands in opposition to the allowances digital synthesis offers over timbral properties and their opportunities for greater expressivity.  What is notable however, is that these are also notably the limitations present in the original melodica.  Indeed, one of KeyWI’s sound engines (present in the demo video) is an emulation of the melodica itself.  This calls into question justifications for modelling DMIs on existing instruments to such a degree.  Why build an emulation of something when the thing itself is so readily (and cheapily) available?\n\n\n   \n   KeyWI: is it too similiar to what it's modelled on?\n\n\n   \n   \n\n\nThough the demo video definitely displays musical competence, it is not hard to imagine an experienced musician reaching the instruments limitations relatively quickly. This can impede on the perceived autonomy of the musician, whereby the human’s capability outstrips that of the instrument.  Furthermore if an instrument feels limiting to a musician in this way, can it be said to be truly expressive?   Taken together, these can notably impact the likelihood of an instrument’s potential long-term usage.\n\nConclusions\nHaving outlined the trade-offs present which may limit the KeyWI’s long-term usage, it is important to note that this is not stated as a goal of the instrument by its maker. Future work could be justified to allow for a greater range of pitch and timbre control as well as a more sophisticated sound engine.  Though, if the instrument’s desired goal is immediacy and accessibility over longevity, that too is valid.\n\nWhere the KeyWI is most notable is in the potential of its breath sensor.  That tests of the sensor show it performing on-par with acoustic wind-based instruments implies great potential for usage in other instruments.\n\nReferences\n\nCaren, M., Michon, R., &amp; Wright, M. (2020). The KeyWI: An Expressive and Accessible Electronic Wind Instrument.\n\nDobrian, C., &amp; Koppelman, D. (2006). The ‘E’ in NIME: Musical Expression with New Computer Interfaces.\n\nMalloch, J., Garcia, J., Wanderley, M. M., Mackay, W. E., Beaudouin-Lafon, M., &amp; Huot, S. (2019). A Design Workbench for Interactive Music Systems. In S. Holland, T. Mudd, K. Wilkie-McKenna, A. McPherson, &amp; M. M. Wanderley (Eds.), New Directions in Music and Human-Computer Interaction (pp. 23–40). Springer International Publishing.\n\nWallis, I., Ingalls, T., Campana, E., &amp; Vuong, C. (2013). Amateur Musicians, Long-Term Engagement, and HCI. In S. Holland, K. Wilkie, P. Mulholland, &amp; A. Seago (Eds.), Music and Human-Computer Interaction (pp. 49–66). Springer London.\n\n",
        "url": "/interactive-music/2023/09/21/alexanjw-keywi.html"
      },
    
      {
        "title": "How to break out of the comping loop?",
        "author": "\n",
        "excerpt": "A critical review of the Reflexive Looper.\n",
        "content": "“Comping” is a term coined by jazz musicians, meaning accompaniment. Traditionally, this term is associated with musicians in the rhythm section, such as the pianist, bassist, guitarist or drummer, whose role is to accompany the soloist when the latter plays the theme of a jazz composition or standard, or improvises a solo. It is therefore necessary for jazz musicians to develop skills in different formations (duo, trio, quartet, etc.) in order to improve their ability to accompany a solo musician or to interact with the musicians of the rhythm section. This defines the very essence of jazz improvisation, which consists in spontaneously inventing solo melodic lines, or accompaniment parts, as part of a jazz performance.\n\n\n   \n   A robot making canned music with musical instruments (November 3, 1930 Syracuse Herald)\n\n\nGenerally speaking, it’s not always possible for jazz musicians to rehearse as a group (i.e. difficulty in finding a rehearsal venue, difficulty in finding a schedule that suits everyone) and often means having to resort to alternative means of rehearsing alone. Loop pedals can be used to record a riff or chord sequence to create a “loop”. This enables musicians to play several tracks of music simultaneously. Nevertheless, this type of practice has the disadvantage of always playing the same content and generating the so-called “canned (boringly repetitive and unresponsive) music effect”, due to the systematic repetition of the recorded loop without any variation (Pachet et al., 2013).\n\nAnother possibility is to use “minus-one” recordings (i.e. Jamey Aebersold Play-A-Longs) to play a piece with a recorded professional rhythm section. However, this does not take into account the style of the soloist, which may differ considerably from that of the recorded musicians, and this does not allow for interaction as the recording is totally unresponsive.\n\n\n  \n    \n    Alternate Text\n  \n  Jamey Aebersold Jazz® Play-A-Longs\n\n\nThe Reflexive Looper\n\nBased on these observations, Pachet et al. (2013) proposed the concept of “reflexive interactions” in a system called the Reflexive Looper (RLR). This aims to create “copies of musical performances” in order to tackle the limitations of traditional musical systems, such as generating stylistically consistent material or enabling Human-Computer Interaction (HCI). RLR consists of a system for playing “copies of past musical performances” of oneself (Pachet et al., 2013a). In particular, this allows the system to develop a semblance of creative sense in order to generate an interaction between the musician and the system, based on the material produced by the musician. The aim here is to create enough of a transformation not to resemble direct imitation (Gifford et al., 2018).\n\n\n   \n      \n  \n\n\nMapping\n\nIn order to allow the correspondence between parameter control and sound synthesis parameters, several types of mapping can be used (Hunt et al. 2000). On the one hand, the use of systems producing a mapping strategy through internal adaptations of the system thanks to prior training, and on the other, the use of mapping strategies defining relationships explicitly. In the case of the RLR, the mapping strategy is based on the supervised learning of a multi-modal classification model (i.e. Support Vector Machine). Here, the model is trained on MIDI signals produced by a “guitarist improvising with a Godin MIDI guitar on eight jazz standards with different tempos and rhythmical feels (Bluesette, Lady Bird, Nardis, Ornithology, Solar, Summer Samba, The Days of Wine and Roses, and Tune up)”.\n\n\n   \n   Mapping of performer actions to synthesis parameters (Hunt et al. 2000)\n\n\nThus, from the system’s internal adaptations, it is possible to generate two principles:\n\n  the principle of interaction based on feature extraction\n  the principle of “other members”\n\n\nIn the first case, the system can extract audio features specific to the musician (i.e. RMS, peaks count, and spectral centroid) and re-use them to generate an accompaniment according to the style of the musician’s playing. In the second case, the system can differentiate the musician’s playing mode and interact accordingly. For example, if the musician improvises a solo, the RLR interacts by playing the rhythm section (bass and chords). And conversely, if the musician plays chords, the RLR interacts by playing a solo with a bass line.\n\nScore Following\n\nThe RLR is based on embedded knowledge of the tempo and overall structure of the jazz standards chord grid. This requires the system to interact with the musician while taking into account the correct chord of the grid. This functionality refers to the concept of score following (Dannenberg 1984; Vercoe 1984), in which a system follows the progress of a live musician through a predetermined score, and “reacts’’ accordingly. According to Drummond (2009), this has the disadvantage of making the system more reactive than interactive, given that the system is initially programmed to follow the musician in all circumstances. So, although RLR can extract the features of the musician in real-time while applying an algorithmic transformation to generate “interactive” content, score following systems have been criticized as being “a perfect example for intelligent but zero interactive music systems” (Jordà 2005: 85).\n\nControl and Expression\n\nFinally, previous work has suggested that “the expressivity of an instrument is dependent on the transparency of the mapping for both the player and the audience” (Fels et al., 2002). The RLR has the advantage of being fully automatic and having no physical controllers (e.g. pedals). This eliminates the need for additional mental attention on the part of the musician, and allows greater availability for the expression necessary for jazz improvisation. Although the addition of physical controllers could eventually give more control over the audio generated by the RLR (Pachet et al., 2013), the intelligent and automatic interpretation of the material produced by the musician nevertheless enhances its expressiveness by ensuring that the mapping between input and audio generation is located at the appropriate level of structural detail (Dobrian et al., 2006).\n\nReferences\n\n\nPachet, F., Roy, P., Moreira, J., &amp; d'Inverno, M. (2013, April). Reflexive loopers for solo musical improvisation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 2205-2208).\n\nGifford, T., Knotts, S., McCormack, J., Kalonaris, S., Yee-King, M., &amp; d’Inverno, M. (2018). Computational systems for music improvisation. Digital Creativity, 29(1), 19-36.\n\nHunt, A., Wanderley, M. M., &amp; Kirk, R. (2000, September). Towards a model for instrumental mapping in expert musical interaction. In ICMC.\n\nDannenberg, R. B. (1984, October). An on-line algorithm for real-time accompaniment. In ICMC (Vol. 84, pp. 193-198).\n\nVercoe, B. 1984. The Synthetic Performer in the Context of Live Performance. Proceedings of the 1984 International Computer Music Conference (ICMC84). Paris, France: International Computer Music Association, 199–200.\n\nDrummond, J. (2009). Understanding interactive systems. Organised Sound, 14(2), 124-133.\n\nJordà, S. 2005. Digital Lutherie: Crafting Musical Computers for New Musics’ Performance and Improvisation\n\nFels, S., Gadd, A., &amp; Mulder, A. (2002). Mapping transparency through metaphor: towards more expressive musical instruments. Organised Sound, 7(2), 109-126.\n\nDobrian, C., &amp; Koppelman, D. (2006). The ‘E’in NIME: musical expression with new computer interfaces.\n\n\n",
        "url": "/interactive-music/2023/09/22/joachipo-reflexive-looper.html"
      },
    
      {
        "title": "Review of On Board Call: A Gestural Wildlife Imitation Machine",
        "author": "\n",
        "excerpt": "Critical Review of On Board Call: A Gestural Wildlife Imitation Machine\n",
        "content": "Review of On Board Call: A Gestural Wildlife Imitation Machine\n\nThe On Board Call introduced in NIME  2022 is a handheld musical device designed to mimic and engage with wildlife sounds, such as bird or animal calls. Instead of just playing pre-recorded sounds, it uses microprocessor-based synthesis and sensors like an accelerometer and force sensor to allow users to interactively modify and perform with the sounds in real-time. Developed as part of the PLACE art-science project at Griffith University, its goal is to enhance appreciation for the eco-acoustic diversity of nature. The device is cost-effective and easily assembled from off-the-shelf components, making it ideal for community workshops focused on active listening and connecting with natural environments.\n\n\n   \n   On Board call System(Photo from NIME website)\n\n\nHardware:\nThe musical interface uses an ESP8266microprocessor and an Adafruit MAX98357A audio board, powered by a battery case. The 6050-accelerometer detects gestures like pitch and yaw, communicating via the 12C protocol. Accelerometer data adjusts audio’s frequency and timbre, while a force sensor regulates volume. A rotary encoder with a switch allows volume and algorithm adjustments. The Adafruit board links to a loudspeaker for sound output. Components are mounted on a compact  PCB, designed for durability and potential future adaptations.\n\n\n   \n   On Board Call PCB\n\n\n\n\nSoftware and Mappings:\nThe software allows for the replication of calls with multiple sound components by duplicating the architecture. Presets define synthesis parameters for specific animals, but real-time pitch and timbre adjustments are performer-driven. Some presets automate rapid envelope repetitions to mimic certain call effects. The software, crafted in the Arduino IDE, utilizes the Mozzi library for sound generation.\nThe software design began by analyzing calls of different animals from the Oxley Creek Common site. Spectral analysis highlighted the variations in pitch and timbre. This examination revealed that these sounds could be emulated using basic frequency modulation methods.\n\nIn the context of the Call software for the interactive music system, the mapping relationships appear to be oriented towards optimizing user interaction and simplifying synthesis architecture(Drummond, 2009).According to Hunt and Krik, and Miranda and wanderley. (Hunt and Kirk 2000; Miranda and Wanderley 2006), we can evaluate the Call software’s mapping as follows:\n\n\n  \n    One-to-One Mappings: This is evident when specific gestures or positions of the device correspond directly to certain synthesis parameters. For instance, the device’s resting state has established values for pitch and timbre. Similarly, the rotary encoder’s default function is to control the master volume.\n  \n  \n    One-to-Many Mappings: The accelerometer’s x and y axes controlling both pitch and timbre can be considered an example of this. A single tilt or movement impacts multiple synthesis parameters simultaneously, enriching the resultant sound. Such a design decision helps in minimizing complexities that might arise with individual controls for each parameter.\n  \n  \n    Many-to-One Mappings: Loudness control in the software is an example, as it takes input both from the device’s position (orientation) and the force sensor, i.e., pressure applied by the user.\n  \n  \n    Many-to-Many Mappings: While not explicitly stated, the rotary encoder’s multifunctionality (adjusting synthesis parameters, selecting algorithm presets, etc.) suggests that its rotations and presses can influence a variety of parameters, making it a many-to-many mapping interface.\n  \n\n\n\n   \n   On Board Call interaction diagram with a simple two-operator FM synthesis architecture (Photo from NIME website)\n\n\nThe use of sigmoid curves to provide stability near the device’s resting state indicates a deliberate effort to smoothen real-time mapping. According to Wanderley gesture data aquisation is “direct” and “alternate” in this case where sensors monitor the performer’s actions, capturing specific gesture features like pressure, displacement, and acceleration, with each variable typically detected by a distinct sensor, and alternate in a manner that it does not resemble any instrument (Miranda et al,.2006).\n\nTrials and Evaluation:\n\nAs outlined in the project paper, the design was refined through numerous iterations and prototypes in both software and hardware. Field trials were also conducted with professional musicians in natural settings, primarily in ensemble setups alongside acoustic instruments during imitation and listening sessions. The focus of these trials was primarily on the design’s durability. However, the paper does not address the feedback received or any interaction with the musician group.\n\nFurthermore, the paper notes consultations with birding communities. These sessions encompassed playing, comparisons, discussions, and analysis. The authors contend that birders, as expert wildlife listeners, offered invaluable insights that influenced the synthesis and gestural control aspects of the device’s design.\n\n\n\nUsabilty and Engagement:\nIn contrast to prior systems that utilize fixed spectromorphologies, parameters, and envelopes, the approach discussed here ventures into the realm of dynamic spectral interaction. The On Call device, leveraging its microprocessor PCB audio system for sound synthesis, places a premium on user-friendliness and ease of use, even if it means compromising on imitative accuracy. Holland et al. explored the question of whether musical interaction should necessarily be simple (Holland et al., 2013). Emulation, as defined by imitative accuracy in this project, is brought into focus by Kesilar, who examines the challenge of addressing a well-known issue which in this case is imatation of wildlife tones(Keisler, 2001). This prompts an inquiry: Does this design approach risk diminishing user engagement? McDermott and peers suggest that long-lasting engagement often stems from activities that present initial challenges to novices(McDermott et al., 2013).\n\nSources\n\n\n  \n    Brown, A. R. (Ed.). (2022). On Board Call: A Gestural Wildlife Imitation Machine. NIME 2022. https://doi.org/10.21428/92fbeb44.71a5a0ba\n  \n  \n    Drummond, J. (2009). Understanding Interactive Systems. Organised Sound, 14(2), 124-133. https://doi.org/10.1017/S1355771809000235\n  \n  \n    Holland, S., Wilkie, K., Mulholland, P., &amp; Seago, A. (2013). Music Interaction: Understanding Music and Human-Computer Interaction. In S. Holland, K. Wilkie, P. Mulholland, &amp; A. Seago (Eds.), Music and Human-Computer Interaction (Springer Series on Cultural Computing). Springer, London. https://doi.org/10.1007/978-1-4471-2990-5_1\n  \n  \n    Hunt, A., &amp; Kirk, R. (2000). Mapping Strategies for Musical Performance. In M. M. Wanderley &amp; M. Battier (Eds.), Trends in Gestural Control of Music. IRCAM–Centre Pompidou.\n  \n  \n    Keislar, D. (2011). A Historical View of Computer Music Technology. In R. T. Dean (Ed.), The Oxford Handbook of Computer Music (Oxford Handbooks online edition). Oxford Academic. https://doi.org/10.1093/oxfordhb/9780199792030.013.0002\n  \n  \n    McDermott, J., Gifford, T., Bouwer, A., &amp; Wagy, M. (2013). Should Music Interaction Be Easy?. In S. Holland, K. Wilkie, P. Mulholland, &amp; A. Seago (Eds.), Music and Human-Computer Interaction (Springer Series on Cultural Computing). Springer, London. https://doi.org/10.1007/978-1-4471-2990-5_2\n  \n  \n    Miranda, E. R., &amp; Wanderley, M. (2006). New Digital Musical Instruments: Control and Interaction Beyond the Keyboard. A-R Editions.\n  \n\n\n",
        "url": "/interactive-music/2023/09/22/masoudn-onboardcall.html"
      },
    
      {
        "title": "The Tickle Tactile Controller - Review",
        "author": "\n",
        "excerpt": "Like many digital instruments I have come across, the instrument design takes its initial inspiration from the piano, a fixed-key instrument.\n",
        "content": "The interactive music system I have decided to review is the Tickle tactile controller. The developers generously published two papers on it in 2019 [1, 2] and made their Pure Data patches available in a GitHub repo. One of their papers mentioned wanting to use the Bela board to create an embedded system under the Future Work section [1].\n\nLike many digital instruments I have come across, the instrument design takes its initial inspiration from the piano, a fixed-key instrument. However, what struck me most was how different the prototypes were. They experimented with force-sensing (using a kitchen weighing scale!) before finally settling on capacitive sensing.\n\n\n\n\nFirst prototype\n\n&nbsp;\n\n\nSecond prototype\n\n\n\n\nThird prototype\n\n\n\nFourth prototype\n\n\n\n\nFifth prototype\n\n\nSixth (and final) product\n\n\n\n\nThis IMS was meant to bridge the gap between the analog and digital worlds. In a YouTube interview, one of the developers mentioned that they wanted to move away from playing knobs or keys to “get a bit closer (to the sound-producing action)”. They have drastically updated their DO, FEEL, and KNOW ideas of interaction design [3]. As of the latest version, there is a piezo disc under the surface, which you excite by tapping and scratching with your fingers - or by bowing, and it lets you generate different resonances.\n\nIn the video, they demonstrate using the Tickle tactile controller with a separate new hardware component called the Waveguide. This stereo analog delay Eurorack module does Karplus-Strong synthesis - a method of physical modeling synthesis for plucked strings and percussive sounds. It is possible to re-trigger the sound before the initial sound has decayed completely, making it possible to achieve a somewhat polyphonic output.\n\n\nEurorack module - WAVEGUIDE (Image Copyright: synthanatomy.com)\n\nWithout the Eurorack module, they use synthesis algorithms in their open-source Pd patches. By interacting with the touch controller, the user excites the surface, sending a noise burst into a filtered delay line. They preferred to use hard, textured material on the surface to enhance the noise signal from a user interacting with the touch surface.\n\nThe approach to mapping differs from typical electroacoustic instruments because the sound control is now directly linked to physical actions instead of relying on abstractions of other technical parameters. In this sense, there is a direct mapping between the physical output from the touch surface of the Tickle and the input of the sound-producing model.\n\nAs Paine says, new performance practices arise with new instruments, and it is still crucial to have a notion of “liveness” [4]. The Tickle fits in very well with the physicality idea of actions leading to sounds through the secret sauce of control mapping. The performer has agency over when and how to make what sound, even though this agency might be less pronounced/evident than in an acoustic instrument such as the guitar.\n\nSince the sound synthesis part is done with a Eurorack modular system, it is limited to one-to-one or one-to-many mappings, with many-to-many mappings offering challenges due to electrical reasons. However, simple Eurorack synthesis integrated with flexible control over mappings with the Tickle controller is a combination that can unlock lots of new possibilities.\n\nFrom what I observed in another performance video, the Tickle tactile controller exudes several one-to-one mappings. Each hexagon on the touch surface triggers a specific sound. BUT, since this is a controller compatible with Pure Data patches, it is possible to create one-to-many, many-to-one, and many-to-many mappings as well. In addition to the touch surface, there are three knobs: volume, decay, and timbre. Volume is reasonably straightforward. Decay controls the decay time. Timbre, which is the rightmost knob, excites different overtones. The controller has excellent cross-compatibility. It works with audio and MIDI in a DAW (Ableton Live), Max/MSP, and Pd. They are building support for SuperCollider, too.\n\nWhen tapped, the hexagons generate a discrete sound parameter; when the surface is rubbed, scratched, or bowed, it can be mapped to control a continuous parameter. Their Instagram page suggests you can also do wild stuff with it, like using a toothbrush to interact with the touch surface! That means infinite ways of interacting with it - so cool. It reminds me of the idea of Co-adaptation, where the user not only understands the system’s capabilities but learns how to modify it to their taste [5]. One should keep in mind though that the control parameters would still be fixed and these are what in turn enable (and also limit) the range of expressivity of the surface.\n\n\nStill image from the IG video\nOverall, this IMS allows for many natural and intimate interactions [6], with defined control over sound synthesis. The sounds created from this can be as familiar as the violin or percussion and also quite alien - still with a familiar analog touch.\n\nOne of the next steps for the developers of this IMS is to include real polyphony, so the ability to produce two sounds simultaneously (without the physical synthesis delay trick I explained earlier). They also want to incorporate pressure sensing and haptic feedback in their design. Currently, the primary feedback is visual, and the secondary feedback is sound. Haptic feedback would be a great addition since this is a tactile controller. This would make the Tickle feel more like an instrument, too. The devs also wanted to include analog circuitry inside the Tickle for sound synthesis. I don’t know if they still want to do this, though, because they did develop an entire Eurorack module to complement the Tickle… which in turn is a testament to backward compatibility in their design ideology (something Cook [7] mentioned as a design principle, too).\n\nIt is nice to see people make cool things and leave them as open source so others can learn from their process. Most interactive music systems don’t see the light of day after the prototyping stage, but it is inspiring that these guys from Chair Audio were able to develop an actual product with it. The fact that they continue to work on it shows that their IMS gained traction in the community!\n\n \n\nReferences\n\n[1] Wegener, C., &amp; Neupert, M. (2019, July). Excited Sounds Augmented by Gestural Control. In Proceedings of the 2019 International Computer Music Conference, New York, NY (p. 5).\n\n[2] Neupert, M., &amp; Wegener, C. (2019, March). Isochronous Control+ Audio Streams for Acoustic Interfaces. In Proceedings of the 17th Linux Audio Conference (LAC-19) (p. 5).\n\n[3] Verplank, B. (2009). Interaction design sketchbook. Online publication.\n\n[4] Paine, G. (2011). Gesture and Morphology in Laptop Music Performance. In The Oxford Handbook of Computer Music. Oxford University Press.\n\n[5] Holland, S., Mudd, T., Wilkie-McKenna, K., McPherson, A., &amp; Wanderley, M. M. (2019). Understanding Music Interaction, and Why It Matters. In New Directions in Music and Human-Computer Interaction (pp. 1–20). Springer International Publishing AG.\n\n[6] Wessel, D., &amp; Wright, M. (2002). Problems and Prospects for Intimate Musical Control of Computers. Computer Music Journal, 26(3), 11–22.\n\n[7] Cook, P. R. (2009). Re-Designing Principles for Computer Music Controllers: a Case Study of SqueezeVox Maggie. In Proceedings of New Interfaces for Musical Expression.\n",
        "url": "/interactive-music/2023/09/22/mahamr-TickleTactile.html"
      },
    
      {
        "title": "The Augmented Violin: Examining Musical Expression in a Bow-Controlled Hyper-Instrument",
        "author": "\n",
        "excerpt": "A brief look at the affordance for musical expression in a violin-based interactive music system.\n",
        "content": "Many interactive music systems involving existing instruments seek to extend the capabilities of that instrument by adding additional control surfaces for the performer to interact with. For example, a trumpet player typically only uses three of their fingers to play their instrument. To the intrepid IMS designer, this is an open invitation to give them something to do with the remaining seven. In IMS circles this is called exploiting the spare bandwidth of the performer.\n\nNow consider a bowed string instrument like the violin. In this example the fingers and hands are now all accounted for. We could instead use the feet to control other processes, for example through a foot pedal. Aside from the additional cognitive load, one downside of such a system is that the performer is required to dedicate signficant time to learning how to interact with the new input, and how that can best integrate this new input with the gestures already required to perform on their instrument.\n\nThe Augmented Violin\n\nHowever, there is another possible approach. What if, instead of adding an additional method of input, we reuse the gestures already being performed to play the existing instrument? Enter the Augmented Violin, a violin-based interactive music system developed by a team of researchers at IRCAM in Paris which interprets ‘gesture parameters’ from the way the violinist uses their bow as the way of interacting with the system.\n\n\n\nThere have been several iterations of the Augmented Violin since it first debuted in 2006. Here we will explore the initial version, which interprets ‘gesture parameters’ of bowing motion to control the pitch of a drone sound. The instrument is able to recognize for three different types of bow strokes (détaché, martelé, and spiccato for the string players in the audience) which are each mapped to different drone pitches. At the same time, the intensity of a given bowing gesture controls parameters of a granular synthesis which is mangling the sound of the violin in real time. As the amount of motion in the bowing gesture increases, so does the number of tiny ‘grains’ of sound as well as the range of pitches they cover.\n\nFrom Quantity to Gesture\n\nTo a string player, the bow strokes such as those being tracked by the Augmented Violin are an essential part of their expressive toolkit on their instrument. However, much to the chagrin of technologically-minded string players everywhere, there is sadly no such thing as a ‘bow stroke sensor’. Instead, bow strokes are complex gestures which are the result of careful manipulating several aspects of the bow usage that we can measure, including bow speed and acceleration perpendicular to the string, and the height of the bow above the string throughout the stroke.\n\n\n   \n   The initial version of the Augmented Violin uses an accelerometer attached to the heel of the bow\n\n\nIn order to quantify bow strokes, the creators of the Augmented Violin attach an accelerometer module to the heel of the violin bow, which allows them to measure the acceleration of the bow in all three dimensions. They then train a machine learning model on a dataset of acceleration data with the accompanying bowing gesture labels to classify the three different bow strokes from their acceleration characteristics in real time during a performance. Meanwhile, the intensity of a given bow stroke is calculated as range of acceleration values measured during the stroke.\n\nVirtuosity and the Augmented Violin\n\nA key consideration when designing or evaluating an IMS is that of the ‘expressivity’ afforded by the interface. But what is expressivity? Christopher Dobrian and Daniel Koppelman set out some helpful guidelines and best practices in 2006. To them, music expression is ‘the felicitous or vivid indication or depiction of mood or sentiment; the quality or fact of being expressive’. Examining the Augmented Violin through this lens then, how does it afford the performer the option of musical expression?\n\nOne core tenet of their argument is that it is mastery of an instrument is what opens to door to expressivity. A successful interactive system, or musical instrument of any kind for that matter, should, they say, afford the seasoned performer the ability to create the aforementioned ‘depiction of mood or sentiment’. However, true mastery can be hard to find in the IMS community, because designer-performers often use their creations for one performance before they are retired. Comparitively, the performer of the Augmented Violin can have precise control of the system quickly as they are already intimately familiar with instrument on top which it is built. In this way, then, the Augmented Violin makes a strong initial case as a truly ‘expressive’ music system.\n\nHow Expressive is The Augmented Violin?\n\nHowever, Dobrian et al. are keen to point out that ‘control ≠ expression’. A control interface with expressive potential can be undone by ineffective gesture-sound mapping. How does the Augmented Violin fair in this arena?\n\n\n  “I would assert that the gestural phrasing of bowings is the expression of the instrumental thought for strings instruments.”\n\n  Florence Baschet, composer of BogenLied which was composed for the original Augmented Violin\n\n\nMapping the intensity of the bowing gesture to parameters related to the intensity of the granular synthesis retains a physical relationship between input and output while also being logically complex and sonically engaging. Meanwhile, the logic in the mapping from bow stroke type to drone pitch is arguably less robust in this initial version, as bowing gestures have little to do with pitch control in purely acoustic violin playing apart from moving between strings. This mapping also limits the number of available drone pitches to three because that is how many types of bow strokes the machine learning model has be trained to recognize. As a result, the latently expressive bow strokes are arguably underutilised as a means of expression.\n\nAn alternative mapping could combine bow stroke and intensity to determine drone pitch. For example, bow stroke type might control the base pitch of the drone, which is then raised linearly in line with stroke intensity in a sonic result similar to a pitch bend. Such a mapping would increase the harmonic range of the IMS while affording greater expressive control of pitch.\n\nConclusion\n\nBy using extracted bowing gestures and bow stroke intensity rather than directly measured bowing parameters, the Augmented Violin is controlled using the gestural language of its performer, rather than the measurement oriented language of the technology being used under the hood. Despite some mapping that arguably don’t utilise the bowing gestures as effectively as they could, it nonetheless presents a compelling avenue for technologically extended string instruments.\n\n",
        "url": "/interactive-music/2023/09/22/jackeh-augmented-violin.html"
      },
    
      {
        "title": "Shadows As Sounds",
        "author": "\n",
        "excerpt": "4-step sequencer using seeds and corns\n",
        "content": "Background\n\nNIME (New Interfaces for Musical Expression) is the largest international gathering of digital luthiers interested in pushing the boundaries of instrument design, exploring how artistic expression, ergonomic design and novel technologies interact during performance with new musical instruments.\n\nIn 2022, Patricia Cadavid an artist and a researcher from Colombia received the NIME Best Music Category prize for creating an open-source interactive electronic instrument that generates rhythms and experimental live sound inspired by the Andean yupana, a hand-controlled physical calculator that was used for the placement of seeds on divided boards to perform basic arithmetic operations.\n\n\nbela.io/nime-2022/\n\nIntroduction\n\n\n   \n   Kanchay_Yupana//: Rhythm sequencer inspired by ancestral Andean technologies\n\n\nMany artists today have been inspired by ancestral technologies of their territories of origin to claim them in artistic expression. Kanchay_Yupana_// (Yupana of Light in Quechua) can be seen as an interface that belongs to one of them as well. The instrument is a part of the designer’s short genealogy of interfaces that builds upon the memory of ancestral technological tools and reuses them in new artistic ways.\n\nKanchay_Yupana_// functions similarly to an ancient Andean physical calculator. The arrangement of seeds on board creates patterns that can be transformed into rhythms and other sonorities. By taking this foundational concept of seed positioning to produce a concrete rhythmic action, adapted the interaction to a step sequencer.\n\nTechnical Implementation\n\n\n\nThe interface includes a wooden board with sixteen boxes and holes, LDR digital modules, potentiometers, and a Teensy 2.0 microcontroller. The holes are arranged in three rows consisting of four steps and an additional box for the general control. The board has a 24 x 25 cm design that enables low-cost manufacturing and easy transportability.\n\nThe wooden material is also simple to prototype and has historical context as well. The interface is built as a 3-instrument drum machine based on the structure of the one-seed-per-box yupana where the placement of one seed in each box represents a particular cipher. In this fashion, the core section of the instrument preserves yupana’s original four-column arrangement.\n\nIn addition, at the peripheries of the interface, there are an extra four slot rows of different sizes. The controls for each instrument are given in this row. To play or pause the rhythm, the same holes and photoresistors are used along with the potentiometer to control the loudness of each track. The total loudness and tempo are additionally manipulated by a higher box with two potentiometers, and a separate hole determines the device’s on/off state.\n\nEach box has a Light Sensing Module LDR to detect the shadow cast by placing the seed in the box. Potentiometers control the volume and tempo of each instrument as well. Modules were selected over traditional LDR components due to their affordability, dependability, easier programming and configuration.\n\nAlso, using a micro potentiometer to determine each component’s on/off state by having a threshold of light, significantly facilitated the electronic building of the interface. The Kanchay_Yupana// uses native Andean corn and huayruro seeds to produce shadows. These seeds have historical significance and are optimal for interaction.\n\nA Teensy 2.0 microcontroller is integrated for its affordability, compactness, and ease of programming. Pure data patch receives MIDI messages produced by recognizing the shadow on photoresistors. The instrument enables real-time rhythm creation. Users are free to interact by adding or removing seeds, and manipulating the rhythm’s steps and tempo.\n\nReuse of tools for creative porpuses\n\n\n  \n    \n      \n      \n    \n  \n  \n    \n      The Representation of khipu and yupana\n      Representation of number 1999 in yupana\n    \n  \n\n\nKanchay_Yupana// can be viewed as a supplementary instrument for another Electronic_Khipu_ made by Patricia Cadavid. This interface is also based on the ancient device that was used for ritual purposes and as a calculating tool. Electronic_Khipu_ produces sound by knotting its sensitive conductive rubber strings.\n\nSince The yupana and the khipu were not used originally to make music, the artist demonstrates an ability to repurpose machinery and technology for expressive musical ends (Tanaka, 2012). The performance with both interfaces seems like a speculative exercise of imagining sounds in an Andean musical context.\n\nPiece, not an instrument\n\nThe Electronic_Khipu_ is programmed to generate a variety of experimental sounds that are explicitly linked to conductivity and the performer-instrument relationship, whereas the Kanchay_Yupana// contributes to generating a rhythm that can easily change during the performance, indicating different moments of intensity or calm. Together they transform the performance into a ritual act where the artist connects with the audience in a unique way.\n\nOne could argue that in this instance, the instrument no longer stays as a separate tool but becomes an artwork itself. It responds well to Perry Cook’s 5th principle for designing computer music controllers: “Make a piece, not an instrument or controller”. Also, in this context, technology is viewed not merely as a material-oriented channel but as a transparent medium for communicating ideas. (Mudd, T. Chapter 8: Material-Oriented Musical Interactions).\n\nGood for beginners\n\nTechnically, using both instruments live is simple and intuitive. The artist was able to observe the interface’s accessibility for individuals as well, resulting in an immediate understanding and a fast learning curve. This so-called, low “entry fee” in getting started with a computer-based instrument could be beneficial for novices as being relatively easy and less intimidating to try out. However, sometimes many simple-to-use computer interfaces appear to have a toy-like character after even a limited period of use and do not encourage continuous musical evolution (Wessel, D., &amp; Wright, M. Problems and prospects for intimate musical control of computers).\n\nHopefully, this won’t be the case here as the artist plans to improve and maximize the instrument’s usability by adding LEDs that show the step in real time, larger control boxes for better ergonomics and integrating extra holes for creating further steps to construct more complicated rhythms. however, often this approach can easily fall into the “curse of programmability” where designers redefine and remap indefinitely without ever finishing artworks or creative projects (Cook, P. Principles for designing computer music controllers).\n\nFinally, I believe Kanchay_Yupana// can establish quite an intimate and delicate connection with both the performer and the audience as well. It is definitely worth checking!\n\n\n\nReferences:\n\n\n  \n    Cook, P. Principles for designing computer music controllers. Link\n  \n  \n    Mudd, T. Material-oriented musical interactions. Link\n  \n  \n    Wilkie, K., Holland, S., &amp; Mulholland, P. Towards a participatory approach for interaction design based on conceptual metaphor theory: A case study from music interaction. Link\n  \n  \n    Wessel, D., &amp; Wright, M. Problems and prospects for intimate musical control of computers.\n  \n  \n    Tanaka, A. (2012). Sensor-based musical instruments and interactive music. In The Oxford Handbook of Computer Music. Oxford University Press. Link\n  \n\n\n",
        "url": "/interactive-music/2023/09/23/ninojak-shadows-as-sounds.html"
      },
    
      {
        "title": "The Daïs: Critical Review of a Haptically Enabled NIME",
        "author": "\n",
        "excerpt": "Is this a violin?\n",
        "content": "The Daïs is an interesting approach to designing an instrument that does not look like a string instrument, but sounds like one. Let’s look at it together from two perspectives: design principles and expressive capability. Check out the full paper on the instrument here.\n\n\n   \n   Ask yourself: What kind of sound does this instrument make/ how is it played?\n\n\nThe Da ̈ıs -Technicalities and Mapping Movement to Sound\n\nApart from its somewhat cryptic name (“da ̈ıs” refers to a platform in a room where royalty or other dignified individuals would be seated), the system is relatively simple in design. The instrument’s creator, Pelle J. Christensen, used iterative design and made the project open source by sharing code, CAD models, and build instructions. He also made an interface that can be used to control physical modeling synthesis algorithms. We like that! First, let’s look at the technical setup and design process. Next, we’ll see how these goals show up in the finished instrument.\n\nThe Da ̈ıs [1] is made of MDF and has an elastic string that holds up a plate. There is an infrared proximity sensor in the body that checks how far away the plate is from the sensor. To find out how tilted the plate is, an inertial measurement unit is used. A haptic actuator gives vibrotactile feedback. So far, so good, but how does it keep the algorithm under control?\n\nWhen you press down on the plate, the distance between it and the sensor gets shorter. This is mapped to the algorithmic expression of pressing the bow onto the string. Tilting the disc is equivalent to a bowing stroke. Moving the disc to the other side doesn’t change the direction of the stroke because the tilt input doesn’t work both ways. Instead, it stops the sound completely. If you’re wondering why there’s a MIDI keyboard in the setup, it’s because the frequency is mapped to MIDI note inputs - a simple but questionable choice for a bowed string simulation. The frequency is also linked to the disk’s side-to-side tilt, which lets you change the pitch and vibrato. Now you are familiar with the mechanisms of the Da ̈ıs and we will have a deeper look into the design next. But before that, check out the creator of the instrument explaining and playing it down below.\n\n\n\nReflections on the Design Approach\n\nPerry Cook is an icon within the digital musical instrument design scene and established many design principles over the years. “Build a (new) copy, don’t trash the original” is one of those principles, and the Da ̈ıs is a good example of this.The first cardboard iteration was used to test sensors and the overall effectiveness of the design. Instead of designing it entirely on paper, this method makes it simple and quick to validate or discard sensors and features. And if something goes horribly wrong, you can always start over with an earlier version.\nJoseph Malloch and other authors give further advice on how to be effective when designing a digital musical instrument. They state, for example, that amplitude requires energy. Simply put, you need to constantly do something in order for the instrument to make sound. Think of all traditional instruments. There isn’t one that keeps making noise even after you stop putting energy into it. In that case, the Da ̈ıs is good because you have to press down on the plate a certain distance to hear it. Then, Malloch says that two hands are better. This is kind of true for our plate-based string instrument, but only because it takes pitch input from a MIDI keyboard. It’s not really a win. Lastly, you should use complex mappings. When you change one parameter, it should affect other parameters. Again, this is largely untrue for the Da ̈ıs, since almost all mappings are one-to-one. This has an effect on expressivity, as we will see.\n\nHow expressive is the Da ̈ıs?\n\nChristensen says that his instrument is “designed to provide intuitive control of multiple simultaneous parameters.” Does performing with it live up to these design promise? \nYou can find the papers I used to find out how to make a digital musical instrument more expressive here and here. There are several factors to consider that influence an instrument’s expressive capability.\nThe ability to accurately capture input gestures, which includes finding an appropriate combination of sensors, musical functions, and feedback, is one factor and minimal requirement. Although the Da ̈ıs includes tactile feedback, which is useful in theory because it allows you to ‘feel’ where you are musically, its benefit is difficult to quantify here. This is due to the fact that it is not stated whether the tilt or depression of the disc is mapped to the intensity of the feedback or if it is simply “press-to-vibrate”.\n\nAnother critical factor is the right mapping formula. At the appropriate level of structural detail, input and sonic output should be mapped to one another, which needs a complex enough mapping strategy. The concept of controlling algorithmic expressions with gestural input leaves plenty of room for structural detail, but it is also where the Da ̈ıs falls short. Except for the frequency, almost all parameters are mapped one-to-one. Because the disc does not need to be moved continuously, simply mapping the angle of the disc to the speed of the bow does not account for a continuous bowing gesture. The fact that negative angles do not correspond to a change in bowing direction but rather to an overall stop of the note does not let you replicate a real bi-directional bowing gesture. A complex mapping is recommended to have second- and third-order analyses of the input data. Christensen even suggests mapping the angular velocity of the disk to the speed of the bow, but neither the paper nor the video show this being done. Increasing the tilting angle and including input data derivatives can easily improve an expressive playstyle.\n\nSumming up\n\nIn the long run, I believe that the Da s, because it is all open source, provides a great opportunity to explore and understand how to translate sensor data into control gestures for manipulating a sound algorithm. It also exemplifies common pitfalls when trying to connect the physical and digital worlds. The approach to the design schedule and the idea of controlling a physical modeling algorithm are where it shines.\n",
        "url": "/interactive-music/2023/09/25/kristeic-ims-review.html"
      },
    
      {
        "title": "A Critical Look at Cléo Palacio-Quintin’s Hyper-Flute",
        "author": "\n",
        "excerpt": "A Boehm flute enhanced with sensors\n",
        "content": "\n  \n  \n    The Hyper-Flute - \n    Palacio-Quintin, C. (2008). Eight Years of Practice on the Hyper-Flute : Technological and Musical Perspectives. Proceedings of the International Conference on New Interfaces for Musical Expression, 293–298.\n  \n\n\n⏯ A video may be auto-playing down in this page - jump to video\n\nMany interactive music systems aim to enhance existing instruments by extending their controls. Cléo Palacio-Quintin’s Hyper-Flute is one of those interesting hyper-instruments that provide more functionalities to extending the creative possibilities. The Hyper-Flute is a standard Boehm flute extended via strategically placed electronic sensors to enable control of digital sound processing parameters in real-time. It opens the door to a highly expressive and enhanced way of interaction with the conventional flute.\n\nMany interactive music systems stay untouched after their implementation. Cléo Palacio-Quintin’s Hyper-Flute is unlike, as she kept playing it for years and after 8 and 15 years, she retrospectively reflected on the time passed, the instrument’s evolution and other things about this interesting instrument.\n\nDesign of the Hyper-Flute\n\nWhen designing The Hyper-Flute, she says she wanted to not further complicate the instrument or compromise its existing capability but integrate the electronics efficiently to resonate well with the conventional flute’s nature. She refers to Jonathan Impett’s approach when he was implementing a Meta Trumpet:\n\n\n  “As far as possible, this is implemented without compromising the richness of the instrument and its technique, or adding extraneous techniques for the performer – most of the actions already form part of conventional performance.” - Impett, J. 1994\n\n\n\n\nIn the core of her design, she placed various sensors at strategical positions on the flute to enable the unused bandwidth of the flute and the flute player. This approach, in my opinion, helps effectively achieve her objective of expanding the creative possibilities of the flute without compromising or overcomplicating its inherent nature. In her initial design, magnetic field sensors were placed at the G# and C# keys, which are the only levers providing a bit larger space for such component. The main points of contact on the flute surface were occupied with pressure sensors. Mercury tilt switches were used to detect tilting and rotations of the flute. A light sensor, responsive to ambient light on the flute, and on/off button switches accessible via the thumbs were also integrated. Furthermore, she implemented an ultrasound transducer to precisely track the flute’s distance from the computer. To convert sensor signals into MIDI messages, she employed a Microlab interface. This comprehensive sensor arrangement showcases a thoughtful approach to enhancing the flute’s capabilities while maintaining its integrity.\n\nReflecting on the mapping strategy of the instrument and its evolution over time, expressive control mechanisms, such as the Hyper-Flute, demand an integrated approach where all gestures harmoniously contribute to the musical output, as extensively discussed in the literature. It emphasizes the importance of considering the performer’s existing instrumental skills and the need for efficient techniques when designing an extended instrument like the Hyper-Flute. Initially, she mostly used one-to-one mapping where each sensor controls one parameter. Later, however, she changed to a complex web of mappings between a variety of sensors and sound processing parameters. By capturing the subtle motions of the flutist’s fingers, pressure, and even tilt, the Hyper-Flute translates these actions into a multiparametric control scheme for sound synthesis.\n\nHer approach of establishing such complex mapping aligns very well with Hunt and Kirk’s principle of “complex tasks may need complex interfaces”.\n\n\n  “Complex mappings cannot be learned instantaneously, but then again, we have never expected this from acoustic instruments. Complex mappings also appear to allow users to develop strategies for controlling complex parameter spaces.” - Hunt A. &amp; Kirk R., 2000\n\n\n\n\nPerry Cook’s Principles and the Hyper-Flute\n\nPerry R. Cook has articulated his opinions and philosophy on designing computer music controllers in multiple NIME papers, drawing from years of experience. His concise set of principles has served as a guiding framework for numerous digital luthiers, offering a valuable perspective on the intricate task of designing computer music controllers. In this section, we will explore two of these principles to critique the Hyper-Flute. We will touch upon two of the principles to criticize the Hyper-Flute.\n\nSimilar to other successful computer music controller designs, Cléo’s Hyper-Flute clearly falls under Cook’s category of “some players have spare bandwidth.” Much like its counterparts, in many instances, the flute and its player naturally have some bandwidth to spare during musical performance. Cléo strategically places sensors at points on the flute where the “spare” fingers can actively interact with during play.\n\n\n  “I am, above all, an instrumentalist. I wanted to find other possibilities for my instrument, and that opened the door to the world of electronics and the development of my hyper-flute,” Palacio-Quintin C, 2011\n\n\n\n\nPerry Cook’s idea is quite clear at this point: “make a piece, not an instrument or controller.” They stuck to this idea for over nine years, as we find out when they revisited their principles. Their idea clearly emphasizes the importance of letting musical considerations guide the design process. It is worth looking at Cléo’s enhanced flute from this point of view. She says she wanted to enhance the flute to seek novel sonorities, perfectly aligning with Cook’s principle. On the other hand, she also mentions that she used some of the sensors just because of that they were available at that time. While keeping the musical considerations in the center of this design, she was still depending on technical availability. All in all, her approach seems to have proven successful, as she expressed satisfaction with the core design and mapping of the instrument. This not only enabled her but other composers as well, to compose pieces for it and continually master its virtuosity.\n\n\n\nYears of performing with the Hyper-Flute\n\nAs mentioned before, many interactive music systems are left untouched to gather dust after the proof-of-concept stage. One distinguishing factor of this work from others is that she had continuously played the hyper-flute for 15 years. Flash forward to 2017, to the time where Cléo reflects on her musical journey with the Hyper-Flute, she pointed out the same:\n\n\n  “Nevertheless, there are very few performers who have played consistently on the same augmented instrument for as many years as I have, and there have been almost no publications concerning performance skills on such new instruments.” Palacio-Quintin, C. 2017\n\n\n\n\nCléo’s insightful critique on the Hyper-Flute instrument sheds light on the lasting success and reliability of its design and implementation. Despite technical advancements, such as the integration of enhanced sensors and an accelerometer, the core design remains unchanged. In her retrospective review from 2017, Cléo emphasizes the importance of preserving the original design which has proven to be successful over time. It is remarkable that even after years of immersive practice, Cléo still values the traditional and reliable core of the Hyper-Flute, even in an age of ever-evolving technology. This consistency highlights the continued success and longevity of the core design of the Hyper-Flute.\n\nShe obviously managed to achieve a good amount of complexity that leads to a good learning curve for other musicians attempting to learn the instrument. She says other musicians learning the flute would have to spent significant amount of time to learn the instrument. In terms of engagement, the stability and consistency of mappings over the years play a crucial role. Stability in mapping, even as the instrument and software evolve, is vital for developing performance skills in the long term. The paper recognizes the need for adaptability and fine-tuning of mappings for different sound processing methods, reinforcing the notion that effective mappings are context dependent. The fact that other composers compose pieces for the hyper-flute, and she is playing in public, in my opinion, proves the success of the instrument not only in terms of reliability but also from the audience’s perspective.\n\nCléo’s achievement in mastering the complex and unique Hyper-Flute design has resulted in a significant learning curve for other musicians attempting to learn, and also composers to create pieces specifically for the Hyper-Flute. It is noteworthy that Cléo regularly performs with the Hyper-Flute in public, providing further evidence of the instrument’s success, not only in terms of reliability but also from the audience’s perspective.\n\n\n\n\nCléo Palacio-Quintin performing with the Hyper-Flute.\n\nConclusion\n\nThe Hyper-Flute stands as a successful combination of technology and tradition, showcasing the potential of repurposing the “spare bandwidth” to control interactive systems. It aims for complex interaction, which requires complex mapping design. Cléo not only crafted an engaging complex mapping in the Hyper-Flute, but also managed to create a reliable instrument that remained engaging throughout the years, providing a learning curve toward virtuosity and offering a significant potential for public performances. From Perry Cook’s “make a piece, not an instrument or controller” perspective, although she started implementing the instrument based on the availability of components, she kept the musical considerations in the core of the design. Her own critical reflections on the Hyper-Flute after eight years and one and a half decade serves as a valuable lesson for IMS designers and reviewers, enlightening aspects often not shared in the works of interactive music systems. Finally, I believe the Hyper-Flute’s design and implementation underlines the significance of intuitive mappings, a deep understanding of performers’ skills, and the need for continued evolution and refinement to achieve a high-quality interactive music system.\n\n\n\nReferences\n\nPalacio-Quintin, C. (2008). Eight Years of Practice on the Hyper-Flute : Technological and Musical Perspectives. Proceedings of the International Conference on New Interfaces for Musical Expression, 293–298. doi:10.5281/zenodo.1179609\n\nPalacio-Quintin, C. (2003). The Hyper-Flute. Proceedings of the International Conference on New Interfaces for Musical Expression, 206–207. doi:10.5281/zenodo.1176549\n\nPalacio-Quintin, C. (2017). 2008: Eight Years of Practice on the Hyper-Flute: Technological and Musical Perspectives. In A. R. Jensenius &amp; M. J. Lyons (Eds.), A NIME Reader: Fifteen Years of New Interfaces for Musical Expression (pp. 335–351). doi:10.1007/978-3-319-47214-0_22\n\nSimas, R. (2011). Cléo Palacio-Quintin: Takes performance to the future on her hyper-flute. Musicworks, https://www.musicworks.ca/featured-article/profile/cl%C3%A9o-palacio-quintin\n\nCook, P. R. (2001). Principles for Designing Computer Music Controllers. Proceedings of the International Conference on New Interfaces for Musical Expression, 3–6. https://doi.org/10.5281/zenodo.1176358\n\nCook, P. R. (2009). Re-Designing Principles for Computer Music Controllers : a Case Study of SqueezeVox Maggie. Proceedings of the International Conference on New Interfaces for Musical Expression, 218–221. doi:10.5281/zenodo.1177493\n\nHunt, A., Wanderley, M. M., &amp; Kirk, R. (2000). Towards a Model for Instrumental Mapping in Expert Musical Interaction. International Conference on Mathematics and Computing. Retrieved from https://api.semanticscholar.org/CorpusID:29778041\n\nImpett, J. (1994, September). A Meta Trumpet (er). In Proceedings of the international computer music conference (pp. 147-147). International Computer Music Accociation.\n",
        "url": "/interactive-music/2023/11/11/ahmetem-hyper-flute-review.html"
      },
    
      {
        "title": "Developing for Muzziball",
        "author": "\n",
        "excerpt": "Check out what we worked on as a team in this year’s Applied Project.\n",
        "content": "Introduction\n\nFor our SMC Applied Project we have worked on several proof-of-concept development solutions for Muzziball™, an exciting interactive ball which is currently in the development and prototype phase. The prototype of the Muzziball platform is based around a Raspberry Pi and a SenseHat cape inside of a 3D printed sphere. The intention for the platform is to elicit movement from the user by having music and audio react to motion in real time. In order to explore further potential functionality and to provide short-to-medium term solutions for future rounds of prototypes, our team took on several areas of development for this platform, all of which are discussed below.\n\nIntegrated Speaker Solution\n\nThe project partners specified that for future iterations of the prototype they wanted a unified solution for powering the Raspberry Pi and speakers. The original arrangement consisted of a power bank, the RPi and a bluetooth speaker. We quickly came to the conclusion that designing and implementing an explicit and safe power circuit is above our skill level, so we focused on a power bank solution. After a period of research and exploration we found the following solution.\n\n\n  \n  \n    Component and signal flow overview\n  \n\n\n\n  \n  \n    The assembled setup\n  \n\n\nWe also tested three different kinds of speaker drivers. The largest-diameter speaker driver produced the best clarity as well as bass response comparable to the existing bluetooth speaker.\n\n\n  \n  \n    The range of speaker drivers we tested\n  \n\n\nAll tested hardware arrangements provide a significant increase in sound quality and clarity without compromising too much in the low-end, thanks to the class D amplifier board and high-quality speaker drivers. With the new power bank, the system can run twice as long.\n\nLED Mapping\n\nThe product included an array of RGB LED lights as part of the SenseHat on the Raspberry Pi. Some functionality, such as colour changing, was already implemented to make use of these lights. However, we identified an opportunity for creating other mappings while also building a more robust and flexible interface for controlling them.\n\nWe therefore created an API in Python for handling the behaviour of the LEDs. This allows users to change light modes by calling a simple function, and then have the lights change over time. Our new API also adds motion-reactive lights, allowing the lights to change colour depending on how much the ball is moving.\n\n\n  \n    \n  \n  The motion-reactive mode in the new lighting system.\n\n\nMapping Motion Data with ML\n\nWe worked to implement new possibilities of mapping the motion data using machine learning. In exploring solutions, we used two machine learning packages, Rebecca Fiebrink’s Wekinator and the Neuralnet external for Pure Data.\n\nWe decided to take orientation as our input data as we felt this was underused in the current implementation. The SenseHat API allows access to accelerometer, gyroscope and magnetometer data streams. In order to calculate real-time orientation we used a sensor fusion algorithm from the micropython-IMU repository micropython-fusion on Github.\n\n\n  \n  \n     Pitch, roll, and yaw axes of the Raspberry Pi/SenseHat\n  \n\n\nWe focused on using a regression ML model to map two motion inputs (pitch and roll) to four outputs. Using the regression model allowed for a smooth transition between orientation states and greater potential for musical mappings. We created 4 new presets to showcase the model:\n\n  Sine tones with evolving chords\n  Sawtooth waves through analogue-style filters\n  Choir - the four output values from the model control the volume of five choir samples.\n  Disco - the four output values control the volume of four different layered disco samples.\n\n\n\n  \n  \n     Summary of the motion data ML mapping workstream \n  \n\n\nOffline Motion Data Analysis\n\nWe aimed to provide tools to illustrate movement data and how it related to other controls in the system.  This meant obtaining and storing the motion data as well as creating a “tag” system.  Here, tags refer to each user created preset and therefore the type of audio being played.\n\nThe acceleration data is stored as time series together with the tag information. The tag information was collected by Python using the “socket” library. The tag was built in a way that it could be sent from different places in the system and present different control data.  It can therefore be used for other purposes later. This data is then stored in a csv file locally on the Raspberry Pi.\n\nAnalysis was then done in a Jupyter Notebook, reading the csv file and displaying plots to show movement data over time per tag. The acceleration at different axes was difficult to understand so we used magnitude of those three axes as the parameter to show the movement amount. As a result, the partners now have the tools to pick a time range and a tag and understand the movement elicited over this time.\n\n\n  \n  \n    Plot of the magnitude of the acceleration as movement, over time per tag\n  \n\n\nConclusion\n\nOur intention throughout this project has been to offer potential solutions to the project partner which they can use for future iterations of the product if they wish to do so. To that end we have been able to successfully implement our solutions, and we look forward to seeing where Muzziball goes in the future.\n",
        "url": "/applied-project/2023/11/26/alexanjw-applied-project.html"
      },
    
      {
        "title": "Intuitive Robotics",
        "author": "\n",
        "excerpt": "Controlling a robot arm with medical sensors\n",
        "content": "Introduction\nFor our project we collaborated with SiFi Labs to create and implement a system to showcase how you can use their BioPoint biosensors to control a robotic arm. This was quite an ambitious and daring project. As this project was a collaboration between us SMC students and the informatics department we wanted to work on a project which could combine our different fields of expertise.\n\n\n  \n  \n    Hard at work trying to figure out an optimal control interface\n  \n\n\nWhat will the robot arm do?\n\nWith the powerful combination of SiFi Labs’ BioPoint biosensors and the Universal Robots UR10e, we aimed to showcase the precision and versatility of our robotic system through a visually engaging task: painting 🎨. By harnessing the data from BioPoint’s IMU sensors, we can translate the movements captured by the user into the movement of the robot arm.\n\nThe BioPoint\nThe BioPoint is a powerful and lightweight device developed by SiFi labs featuring 6 different sensors capable of recording almost any biosignal imaginable. For our use case we were mostly interested in utilizing the Intertial Measurement Unit sensors (IMU) on the device. Multiple of our team members had previous experience using these sensors for artistic applications so we thought it would be fitting for our project.\n\n\n  \n  \n    The BioPoint Biosensor\n  \n\n\nIntroducing the Robot\nMeet the robot we worked on for this project. The Universal Robots UR10e. This is a medium sized industrial robot, and is also quite large as you can see! This big guy is known for its robust capabilities and ease of integration into various applications - it opens up a world of possibilities for automation in diverse industries.\n\n\n  \n  \n    The UR10e Robot Arm\n  \n\n\n\n    \n    \n   The first move matters! \n\n\nNavigating the Project: From Concept to Reality\n\nEmbarking on a three-month project, we knew time was of the essence, guiding us to keep our goals practical and achievable. Our chosen task, to get the robot to paint, offered a balanced challenge that aligned with our timeframe. To map out our journey, we utilized proven planning and project management tools.\n\nProject Network Diagram (PND)\n\nWith the Project Network Diagram (PND), we visualized the sequential tasks, always having a clear next step in sight. This dynamic tool provided a bird’s-eye view of our progress, ensuring we stayed on track and understood the needs and evolving aspects of our project.\n\n\n  \n  \n    Project Network Diagram\n  \n\n\nImplementation\n\nOur team recently embarked on a complex project involving the programming and maneuvering of a robot. We kicked off this endeavor by plunging deep into the mechanics of robotics control, learning the intricacies of tech pendant manipulation, and slowly mastering manual control using Joint and TCP actions. Although initially, we were tethered by local networks, later, we successfully tapped into the world of remote control through the Universal Robots ROS driver, which was quite a hurdle given our unfamiliarity with a ROS environment.\n\nWe persisted, exploring UR’s predefined protocols for communication over TCP/IP. We examined only two among the six communication interfaces available - the Primary/Secondary and RTDE interfaces. While the Primary/Secondary interface made it possible to transmit data to the robot and alter specific state changes, the weak link was its low frequency, manifesting in choppy motion. Thus, we advanced towards the RTDE interface with a higher frequency and, in turn, smoother motion. This improvement came with the added burden of managing data on the robot’s end, which was handled by developing a custom program to interpret and act upon the data.\n\nSimultaneously, we implemented other necessary software for handling data streaming and processing from the BioPoint devices. With rigorous research, planning, and trials, we created a Python program that established a seamless and efficient connection with the devices, streamed the IMU data, and processed it to extract purposeful parameters for robot control.\n\n\n  \n  \n    The Control Pipeline for the System\n  \n\n\nThe Robot Arm in Action\n\n\n    \n   Robot moving in harmony with the arm! \n\n\nConclusion\nLooking ahead, our project sets the stage for exciting future endeavors and advancements in the realm of intuitive robotics. As we refine our system based on real-world feedback and continue to address challenges, the door opens to explore applications beyond painting. Future work can involve further enhancing the precision of our robotic system, incorporating machine learning algorithms for adaptive control, and expanding the compatibility with additional sensors for a richer user experience. We originally indented to implement haptics for this project which we sadly did not have time for, but this is also a possiblity in regards to future work.\n",
        "url": "/applied-project/2023/11/27/fabianst-intuitive-robotics.html"
      },
    
      {
        "title": "The Saxelerophone: Demonstrating gestural virtuosity",
        "author": "\n",
        "excerpt": "A hyper-instrument tracking data from a 3-axis accelerometer and a contact microphone to create new interactive sounds for the saxophone.\n",
        "content": "“Knowing the game” is often necessary when it comes to evaluating live performances, especially if it turns out that the performer in front of you is playing one of those totally wacky Interactive Music Systems (IMS)! In the NIME community , it’s quite common for these IMS to reflect a “short-lived expression of individualism” rather than a design intended for a wider audience (Vasquez et al., 2017). In fact, this is essentially due to their very nature, as IMS have the unfortunate tendency to limit the demonstration of virtuosity associated with acoustic instruments, since most of these systems operate a brutal separation between human action (i.e. gesture) and the sound production system.\n\nWell, in the NIME community there’s even talk of a virtuosity crisis (Dobrian and Koppelman, 2006), and there’s actually a fair amount of concern about questions that deal with how IMS performances can be meaningful, perceptible and useful (Wessel and Wright, 2002, Schloss, 2003). It’s true that the concept of virtuosity in the field of IMS has often been associated with a large sonic palette, via the manipulation of synthesizers or the use of a whole host of samples (Cascone, 2002), that can be easily manipulated by the performer, without having to worry too much about the gestures involved (West et al., 2021). And that’s where the problem lies! Because, in order to recreate a relationship between human action and the sound production system, shouldn’t we first and foremost be interested in gesture as a sound-generating function?\n\nThe Saxelerophone\n\nWell, I think so, and that’s why I created the Saxelerophone. This IMS is designed to take account of the performer’s bandwidth, given that “some players have spare bandwidth, some do not” (Cook, 2017), but also of the visual relationship between gesture and sound in a live performance. In this sense, the Saxelerophone’s main objective is to improve the demonstration of virtuosity, which often tends to be misinterpreted by the public when it comes to new musical instruments (Vasquez et al., 2017).\n\n\n   \n   The Saxelerophone. Left: Soprano saxophone (Selmer Super Action 80 Series II) with sensors on the mouthpiece connected to a Bela board. Right (top): ADXL337 3-axis digital accelerometer sensor positioned above the contact microphone. Right (bottom): Contact microphone based on piezo material positioned on the saxophone’s ligature.\n\n\nTo achieve this, I conceived the Saxelerophone as a hyper-instrument (i.e. added additional sensors to the saxophone), “to give extra power and finesse to virtuosic performers” (Machover and Chung, 1989). On the one hand, it’s a simple, flexible system consisting of a contact microphone and a three-axis digital accelerometer, designed to be mounted on any reed instrument fitted with a ligature. On the other hand, it’s a system based on complex gestural control of sound synthesis, using robust machine-learning methods to learn static regression mappings, enabling the construction of a new expressive and creative sound space for developing gestural virtuosity.\n\nThe code, design files and technical documentation to replicate the system are available at the following address: https://github.com/joachimpoutaraud/saxelerophone.\n\nGestural control of sound synthesis\n\nTo saxophonists, it’s relatively easy to move their instrument around in space when playing. In fact, the smaller the saxophone, the easier it is, and that’s quite common to see this kind of demonstration at live performances.\n\n\n\nSo I thought it would be interesting to use the musical gestures of saxophonists, which have a certain capacity to visually accompany sound, in the sense that they are gestures in response to sound. These gestures have already been described as “sound-accompanying” (Jensenius et al., 2010) or “sound-tracing” (Godøy et al., 2006) gestures.\n\nThree main objectives were defined for demonstrating gestural virtuosity with the Saxelerophone.\n\n1. Audio sensing\n\nFor audio sensing, the aim was to collect acoustic information specific to the instrument with a contact microphone, so that it could be reused to generate new sounds. Here, I was interested in the fundamental frequency of the notes produced by the instrument, to be converted as the frequency of the carrier oscillator within the audio spectrum. Note that the contact microphone was also used to amplify the acoustic sound of the saxophone.\n\n2. Gesture sensing\n\nNext, a three-axis digital accelerometer was used to determine the saxophone’s orientation and changes in movement.\n\n\n   \n   ADXL337 3-axis digital accelerometer.\n\n\nIt was this sensor that first and foremost enabled me to define a physical relationship between the performer’s sound-accompanying gestures and sound synthesis control parameters (i.e. mapping). The mapping of gestures to sound synthesis parameters was then performed using machine learning. To do this, a regression algorithm was trained using an ANN framework for Pure Data called neuralnet to create a new sound space in which the performer could navigate to generate new interactive sounds. A schematic representation of the Saxelerophone’s mappings is shown in the diagram below.\n\n\n   \n   Saxelerophone's mappings.\n\n\n3. Motion sensing\n\nFinally, accelerometer data was integrated to estimate the instrument’s velocity. It is this last variable that allowed me to demonstrate a certain gestural virtuosity, since it enabled me to control the natural volume of the saxophone according to the volume of the synthesized sound. As the volume of the acoustic sound is inversely proportional to the synthesized sound, this means that the faster the instrument is moved in space, the more the sound will be synthesized, and vice versa. This allows the performer to play with two different musical palettes (acoustic vs. synthesized), depending on the speed of change of its position in relation to time.\n\nPersonal Reflections\n\nAs the designer of this instrument, I feel that the challenge of developing an instrument to demonstrate a new gestural virtuosity for the saxophone has on the whole been successful. The main technical challenges were to define the right frequency variation of the resistor-capacitor circuit (RC circuit) for use with the contact microphone, and to design a system that was flexible and adaptable to any reed instrument fitted with a ligature. After conducting a quick preliminary study involving 4 participants on the question of the audience perspective, the results obtained were largely positive. This confirmed my position on the subject, although the results were not compared to another IMS, nor to a different prototype version of the Saxelerophone.\n\nAs a performer of this instrument, I think first of all that a new approach to writing could be developed for this instrument, taking into account the performer’s gestures on the score. In addition, I think that in-depth work on the saxophone’s playing modes could be envisaged to deepen learning and engagement with the instrument. For the time being, the Saxelerophone relies on a single space for personal sound creation. Although it is possible to create new sound spaces ad infinitum, it would be beneficial to add new input variables related to the instrument’s own playing modes (percussive, blown, sung or multiphonic sounds) when training the model. This would give the performer greater expressiveness and a wider variety of sounds to match his or her virtuoso abilities.\n\nPerforming with the Saxelerophone\n\n\n\nReferences\n\n\nVasquez, J. C., Tahiroglu, K., &amp; Kildal, J. (2017). Idiomatic composition practices for new musical instruments: context, background and current applications. In NIME (pp. 174-179).\n\nDobrian, C., &amp; Koppelman, D. (2006). The ‘E’in NIME: musical expression with new computer interfaces.\n\nWessel, D., &amp; Wright, M. (2002). Problems and prospects for intimate musical control of computers. Computer music journal, 26(3), 11-22.\n\nSchloss, W. A. (2003). Using contemporary technology in live performance: The dilemma of the performer. Journal of New Music Research, 32(3), 239-242.\n\nCascone, K. (2002). Laptop music-counterfeiting aura in the age of infinite reproduction. Parachute, 52-59.\n\nWest, T., Caramiaux, B., Huot, S., &amp; Wanderley, M. M. (2021, May). Making mappings: Design criteria for live performance. In NIME 2021. PubPub.\n\nCook, P. (2017). 2001: Principles for designing computer music controllers. A NIME Reader: Fifteen years of new interfaces for musical expression, 1-13.\n\nMachover, T. &amp; Chung, J. (1989). Hyperinstruments: Musically Intelligent and Interactive Performance and Creativity Systems.\n\nA. R. Jensenius and M. M. Wanderley. Musical gestures: Concepts and methods in research. In Musical gestures, pages 24–47. Routledge, 2010.\n\nR. I. Godøy, E. Haga, and A. R. Jensenius. Exploring music-related gestures by sound-tracing: A preliminary study. 2006.\n\n",
        "url": "/interactive-music/2023/11/30/joachipo-saxelerophone.html"
      },
    
      {
        "title": "Sync your synths and jam over a network using Sonobus",
        "author": "\n",
        "excerpt": "A quick start guide to jamming over a network. Designed for instruments which can synchronize using an analog clock pulse.\n",
        "content": "Introduction\nIn this post we are going to use Sonobus to set up a simple jam session between two locations over the internet, designed for electronic instruments with a built in sequencer. The primary location (referred to as the ‘Hub’) will send audio alongside an analog clock pulse. The secondary location (the ‘Node’) will receive these and in turn, send back it’s own audio. There are multiple ways to achieve this effectively, here’s just one way to do it. This technique is referred to as Forward Synchronisation.\n\nEquipment/software checklist\nThe equipment/software both locations will need are as follows:\n\n\n  Network cable (plus USB adapter if necessary)\n  Sonobus software (with plug ins)\n  External sound card (with 3 or more outputs for the Node)\n\n\nThe Hub also needs Ableton Live or similar DAW that can delay individual tracks.\n\nYou may have to adapt this setup to match the instruments you want to use. At the Hub, I used a Roland TR8s drum machine connected to Ableton Live through USB midi. At the Node, a KORG Monologue which receives analog clock from the 3rd output an external sound card.\n\nSonobus is free software which enables peer to peer transmission of audio over the internet. You can download and read more about it here. Sonobus is also available as a plugin so you can run it from inside your DAW of choice.\n\nStep 1 - Connecting to a network\n\nAlthough it is possible to use Sonobus over wifi, it is highly recommended to use a network cable. From my testing of this setup, not using a cable increases the latency by around 80ms, enough to make playing any type of instrument feel uncomfortable. Once you have connected the cable, the Node should find their IP address by looking in the network settings on your computer.\n\nStep 2 - Starting a session in Sonobus\n\nThe node can now open Sonobus and click ‘connect’. This opens a menu where you can give your session a name, an optional password and choose a display name (this can be handy to label yourself ‘Node’). Under the ‘connect to group’ button there is a box called ‘Connection Server’. Here you can enter your IP address. The last thing we need to do is allocate a port number. For Sonobus, this is 10999. The IP address and port number should be separated by a colon like this:\n\n\n\nOnce this is entered, click ‘Connect to Group’. If it fails to connect, double check your IP address and consult the guide on the Sonobus website. Copy your IP address and node to send to the Hub because they will need to enter the same information.\n\nNow the Hub can also open Sonobus but this time as a plug in in Ableton Live. I recommend using Sonobus on a return track. Input the same information as the Node (except change the display name to ‘Hub’) and click connect.\n\nStep 3 - Sending audio\n\nThe Hub can make a new track, make sure you have signal coming in and sync with Ableton Live in which ever way suits your set up best. For me this is through USB Midi. For the purposes of setting up and adjusting the connection, make a simple sequence that it easy to rhythmically hear (like a metronome or 4 to the floor kick with a short decay time). Next make the track ‘sends only’ and send signal to the return track with Sonobus on it. If you open the plug in, you sound now see signal coming in, being sent to the Node and hear it from the return track.\n\nThe Node can also set up their instrument and send audio back. Check the correct sound card is selected and that uncompressed audio is being sent (PCM 16 bit) in the global settings. Clicking the ‘Input Mixer’ opens more settings to select the correct input channel.\n\n\n\nStep 4 - Sending clock\n\nIt is very important to remember that clock pulses are extremely loud so remember that everyone should remove headphones/mute outputs while we set up the clock. The Hub should make a new Midi track with a Drum Rack on. We are going to send a sample of an audio pulse every 16th note. This article from Ableton links to a free download of an audio clock pulse and has some further information on syncing modular gear with Live.\n\nMake a new return track with a fresh instance of Sonobus. Connect to the same session as before with a display name ‘Clock’ and mute the Hub and the Node. A mono signal is fine to send here (this can be changed in the top centre of the window) but make sure the clock is also being sent as uncompressed audio. Now follow the same audio routing as before except send to the second return track. Go back to the first instance of Sonobus and mute the clock.\n\nNow the Node should be receiving the clock signal. This can be routed out the 3rd output of the sound card and into the sync input of the instrument. Check that you are not hearing any clock in your headphones. Now test that the clock is reaching the instrument and that control of start/stop transport is taken over by the clock. You might have to adjust some settings on the instrument for this to function correctly (it is possible to skip this step and just start the sequencer manually on the first beat).\n\nStep 5 - Synchronizing latency\n\nFinally we need to synchronize the instruments. This can be a bit of trial and error because we are trying to find a balance. The ideal situation for the network is to have a large delay buffer, but the best thing for us playing the instruments is to have as small of a jitter buffer as possible. First the Node should go to the clock and set the latency to manual, a good number to start with is 20ms.\n\n\n\nNext the Hub also needs to delay the clock being sent to their instrument by the same amount. In my case, I can delay the Midi clock in Ableton’s settings.\n\n\n\nFinally both people need to adjust the latency of the audio they are receiving and the monitor audio of their own instruments. As before with the clock, set the jitter buffer to 20ms. Then delay their own monitoring by opening the input mixer and clicking the button M.FX. The Additional Monitoring Delay button should be highlighted blue when activated.\n\n\n\nTesting and tweaking\n\nTest out the connection by starting and stopping the clock, getting it running in time and waiting to see if it goes out of sync. If this happens, repeat the steps in this section with a higher number, remembering to change all settings to the same number. If you are hearing some latency but the timing seems consistent this could be down to latency within your system (all setups have some degree of latency) so you can manually adjust the Additional Monitoring Delay until the instruments synchronize. Underneath is a flow chat to help visualise the signal path. Good luck and happy jamming!\n\n\n",
        "url": "/networked-music/2023/11/30/thomaseo-sonobus_sync.html"
      },
    
      {
        "title": "Touch/Tap/Blow - Exploring Intimate Control for Musical Expression",
        "author": "\n",
        "excerpt": "Touch/Tap/Blow is, as its name suggests, an interactive music system which aims to combine three forms of intimate control over a digital musical instrument.  Notes and chords can be played via the touch interface while bass accompaniment can be driven by the player’s foot tapping. Below are the details of it’s main elements.\n",
        "content": "Touch/Tap/Blow is, as its name suggests, an interactive music system which aims to combine three forms of intimate control over a digital musical instrument.  Notes and chords can be played via the touch interface while bass accompaniment can be driven by the player’s foot tapping. Below are the details of it’s main elements.\n\n\n  \n  \n    Touch/Tap/Blow\n  \n\n\nTouch\n\nFirstly, there is a touch interface for note input and pressure sensitivity.  This is comprised of capacitive touch strips using Trill Craft.  These are are arranged in an isomorphic chromatic manner.  The layout is based on the layout of B-System chromatic button accordians.  There are three notes accessible on each strip arranged chromatically vertically.  This isomorphic layout allows for harmonically related notes to be close together and for major and minor triad chords to have the same hand “shape” when playing (like a barre chord on guitar)\n\nUnderneath the touch strips are four force sensitive resistors to capture pressure.  These have been hand-made using conductive tape and semi-conductive foam as detailed here\n\n\n  \n  \n    Touch strips and FSRs\n  \n\n\nThe force sensors are mapped to wavefolders on each of the pads’ voices as well as to amplitude modulation.  More details on the sound engine are below.\n\nTap\nAt the player’s feet is a foot tap block with a piezo pickup.  With this, the player can tap their foot to drive the tempo of a sequenced bass voice.  The notes of the bass are generated by a shift register which stores the notes that the player plays sequentially.  As more notes are added to the register, the oldest are pushed out.\n\nBlow\nFinally, in front of the player is the microphone which acts as a breath sensor.  This controls the frequency of the modulation oscillators for amplitude modulation as well as being combined with the force sensors for modulation amount.\n\n\n  \n  \n    Touch/Tap/Blow - note the blue microphone in front\n  \n\n\nSound Engine and Composition\nIn his Principals for Designing Computer Music Controllers, Perry Cook advises to “create a piece, not an instrument”.  Following this, the design of Touch/Tap/Blow’s sound engine was informed by another intimate relationship between musician and instrument, that of Suzanne Ciani and the Buchla modular synthesiser.  The synthesis itself is based on augmenting sine wave oscillators with amplitude modulation and wavefolding controlled by the pressure and breath sensors.  Furthermore, several other decisions as well asa the piece performed here are based on the performance ideas from Ciani’s Report to National Endowment, anecdotally referred to as the ‘Buchla Cookbook”.  One such example is the use of the live chromatic note input being fed into a shift register.  In my implementation, the shift register is being used for the bass sequence voice for an ever evolving accompaniment part.\n\nThis is a flow diagram illustrating the signal flow between sensors and sound engine.\n\n\n  \n  \n    Signal Flow diagram\n  \n\n\nPerforming with Touch/Tap/Blow\n\n\n\nReflections\n\nThis instrument has proved to be a fun and engaging interface for musical expression.  A notable strength has been the touch interface which offers something familiar to musicians accustomed to keyboard-based instruments but with a different layout.  The force sensors offered an intuitive interaction with their wavefolder mapping.\n\nThe tap interface adds an important dimension to the instrument, notably opening up possibilities beyond the touch interface live playing, making it more suited to solo performance.\n\nThe breath interface proved to be the most challenging aspect of the instrument.  Using a microphone for this purpose meant that it was prone to being affected by ambient noise.  Gating/thresholding this signal meant that only strong blowing signals could be used which limited the subtlety of performance possible.  My initial concept for this element was for amplitude, like a wind instrument, however I was not able to implement this in a musically pleasing manner.\n\nA key challenge in constructing the instrument was the sheer number of elements that comprise it.  The touch strips and force sensors are “hand-made” and as such were sometimes prone to needing troubleshooting and recalibration or even to outright failure. The touch strips for instance required individual and repeated calibration to track their notes successfully.  The force sensors where prone to failure when their wires were not completely secure. Many attempts were made to implement them in a neater way but nothing worked as reliably as crocodile clips.\n\nOne of my main intentions with Touch/Tap/Blow was to make something that still felt like a musical instrument with the balance of immediacy and open-endedness, with potential for both musical expression and failure.  Indeed you can hear me play wrong notes and make mistakes in the performance.  While it is possible to lock the note input to quantised musical scales, this potential for failure is, to me, an important part of musicality.\n\nWhen playing the instrument of extended periods I did find myself with pain in my hands and arms from pressed on the force sensors so much.  This is perhaps an innate problem with the ergonomics of how I implemented the instrument as a physical object.  Or it could be that, like other instruments, my intuitive technique for playing it was not optimal.\n\nOverall, Touch/Tap/Blow achieved what I set out for it, albeit with some difficulties and compromises along the way.\n",
        "url": "/interactive-music/2023/12/01/alexanjw-touchtapblow.html"
      },
    
      {
        "title": "The Paperback Singer",
        "author": "\n",
        "excerpt": "An interactive granular audio book\n",
        "content": "Introduction\nIn this blog post I will detail how I created my Paperback Singer. The Paperback Singer is an interactive audio book powered by a granular synthesis engine controlled by a paperback book. Inspired both by the concept or granularity, microsounds and short-term memory, the instrument is able to play back audio files and it tries to sonify the process of remembering what has just been read on a page by using audio buffers and IMU sensor readings.\n\nAll these actions are what the Paperback Singer is capturing and using as control signals for the audio engine. The goal was to make the user more aware of all the ways their body moves and subconciously interacts while reading. All movements are captured by an Inertial Measurement Sensor (IMU) and a Flex Sensor while a photoresistor is used for switching between playing modes. The instrument features two different modes of playing meant to represent the act of reading and remembering. Which a player can seamlessly choose between by opening and closing the book.\n\n\n  \n  \n    Front view of the Paperback Singer\n  \n\n\nWhat is Granular Synthesis?\n\nThe Paperback Singer uses granular synthesis which is an unique sound processing technique where you take a segment of audio and slice it into small “grains” between 10-100 milliseconds and then play them back. The technique was first proposed by physicist Dennis Gabor in 1947 but it wasn’t until 1959 a piece was made using his theory. Iannis Xenakis Anlogique A-B from 1959 is considered the first granular piece.\n\n\n\nToday granular synthesis is a lot more accessible and you can find graunlar plugins in many popular DAWs. The technique is even used by popular EDM artists such as Flume\n\n\n\nDesigning the Paperback Singer\n\nMetaphor 1: Word Grains\n\nWhen designing the Paperback Singer there were multiple methaphors I wanted to explore with the instrument since just creating a granular synth would not be too exciting. The first thing I tried to to was to transfer the concept of granularity to something different. That’s how I came up with the idea of using a book.\n\nIf you think about it then reading is a sort of granular activity. You can divide a book into chapters, sections, paragraphs, sentences, words and finally individual letters. In this way you end up with a sort of word grain you can play around with.\n\nMetaphor 2: Subconscious Interactions\nThe next step was to come up with a way of controling these word grains. I tried to think about in what way you interact with a book while reading and quickly realized that this is quite an interesting topic in itself. When reading you are usually fully immersed in the text in front of you, making you unnaware of all the ways you are interacting with the book itself. You are moving it around and adjusting it in various ways to enhance the reading experience. These are the movements I wanted to capture and use to control the granular synth.\n\n\n  \n  \n    Diagram of how sensor data is mapped in the system\n  \n\n\nCapturing Movements\n\nTo capture these movements I added an IMU-sensor to the book. An IMU sensor is a device that measures an object’s movement using accelerometers, gyroscopes, and magnetometers. It gives information about acceleration, rotation, and orientation. In my case I was interested in using acceleration and pitch, roll, yaw data which is obtained by combining the gyroscope and accelerometer data, this is called sensor fusion.\n\n\n  \n  \n    Side view of the instrument, the IMU sensor is on the top of the book to the left\n  \n\n\nMetaphor 3: Short-Term Memory\nThe final metaphor I wanted to explore was that of short-term memory. When reading you will often forget something you’ve just read. I wanted to explore this concept with my instrument and so I came up with two different modes of playing. I called these reading mode and memory mode. To switch between these modes you simply open or close the book.\n\nReading mode is engaged when opening up the book. A photoresistor is used to recognize this gesture. While in reading mode the instrument simply plays back one of the audio files loaded with some simple ways of controling the volume, pitch of the audio file and duration of each indivual grain. You’re also able to switch between audio files by bending the front cover, activating the flex sensor.\n\n\n  \n  \n    The instrument in reading mode with the photoresistor on the right\n  \n\n\nMemory mode is enganged when closing the book. When in memory mode the synth uses the last 10 seconds of audio as it’s input. In memory mode you have more extensive control over the granular synthesis parameters of the system. You’re able to scrub through the recorded sound and focus on specific parts of it.\n\nReflections\nDesigning and conceptualising this instrument was quite fun and challenging. Personally I was very happy with the metaphors I came up with and how they played a part in the system itself. I conducted a short user study with some of my fellow students and got some great feedback on the instrument. They quite liked how the instrument was played and all the possible sounds it could produce.\n\nThere were quite a few challenges along the way, the biggest one being processing power constraints. The system is quite CPU hungry and would often go above 100% so I had to work hard on optimising it before performing.\n\nFrom a performance perspecitve it is quite easy to explore different sounds and textures. I would love to add more functionalities to the system making it even more engaging to play. Like some effects or the possiblity of placing different grains in stereo.\n\nDemonstration\nI was lucky enough to play a short concert using the Paperback Singer. This was quite a fun experience to finally get to showcase what I’ve been working on all semester and hear the feedback from the audience.\n\n\n",
        "url": "/interactive-music/2023/12/01/fabianst-paperbacksinger.html"
      },
    
      {
        "title": "CordChord - controlling a digital string instrument with distance sensing and machine learning",
        "author": "\n",
        "excerpt": "How can we use sensors to control a digital string instrument? Here’s one idea.\n",
        "content": "\n   \n   CordChord: a two-voice digital string instrument. L-R: the whole instrument, back of the instrument, the circuitry, optical distance sensor placement\n\n\nIntroducing CordChord\n\nCordChord (cord as in string, chord as in two or more notes played together) is a two-voice digital string instrument that is part harp and part cello, and is built using the Bela platform. In order to understand the behaviour of two strings that neither vibrate nor have acoustic amplification, I made use of an intriguing methodology for string for tracking the movement of and pressure exerted through violin bows. Read on to learn how CordChord works and some of my reflections on the project.\n\nDesigning the Frame\n\nLike many of the finest string instruments from the likes of Stradivarius, CordChord is constructed out of maple and spruce wood for old-growth forests (discarded IKEA bed slats I found in a skip last winter) that I assembled with an all-natural glue (nailed together into long rectangle). I then fixed the two ‘strings’ (lengths of polyester rope) between the short ends of the structure using fine pegs of ebony wood (metal hooks from Clas Ohlson).\n\nDetermining the Positions of the Strings Using Sensors\n\n\n   \n   Optical proximity sensors are mounted on the neck facing towards the strings\n\n\nMost musicians have a fairly accurate mental model of how an acoustic string instrument works. By changing the effective length of the string by stopping it using our fingers, the string vibrates at a different frequency when plucked, hit, or bowed. However, unlike the guitar, cello, or violin, the performer of CordChord does not vibrate the strings at all, and there is no ‘body’ which acts as a resonating chamber to amplify these vibrations.\n\nInstead, I needed to find a way of interpreting the behaviour of the strings using sensors. An approach previously used for tracking the motion of and pressure exerted through violin bows came to my rescue. In 2013, Pardue et al. proposed a method for bow tracking using optical distance sensors.\n\nThese sensors are essentially two components in one housing: an infrared (IR) LED and an IR photoresistor/light-dependent resistor. The basic concept is this: the IR LED emits infrared light, which then hits and is reflected by a reflective object or surface. For an object directly infront of the sensor housing, this light will be reflected directly back towards the IR photoresistor. The closer the reflective surface is to the sensor, the more of the IR light that makes it back to the photoresistor. As a result, we can interpret the reading we get from the photoresistor as correlating to the distance between the reflective object and the sensor itself.\n\nBut how does this apply to tracking violin bows? Pardue et al. place four of these distance sensors under the bow stick facing towards the bow hair. As the bow is pressed down into the string, the bow hair deflects around the contact point by an amount which correlates to the downward force applied through the bow. As the hair deflects, the distance between the stick and the hair changes. By measuring this change at the points where the sensors are mounted, we can determine where the string contacted the bow and how much the hair moved. In fact, every combination of contact point and pressure results in a unique set of readings from the sensors.\n\nI decided to repurpose this approach to determine the position of the strings in CordChord. Attached to the vertical neck plank, which is equivalent to the bow stick in this analogy, are four distance sensors per string, pointed towards the string. As the strings (equivalent to the bow hair) are reflective to IR light, pulling a string towards the neck changes the readings from the sensors pointed towards it.\n\nI then trained a simple regression machine learning (ML) model using the Neuralnet external for Pure Data which, given the values received from the sensors pointed at each string, is able to predict the position along the neck at which the string is pressed (equivalent to the contact point in the bow example), and by how much it is displaced at that position (equivalent to the pressure). This position and displacement are then mapped to pitch and volume respectively; pressing the string low on the neck will result in low pitches and vice versa, while pulling the string only slightly will create a softer sound than pulling it all the way towards the neck.\n\nControlling Timbre with Capacitive Sensing\n\n\n   \n   Capacitive strips on the back of the neck allow the performer to control the timbre with their thumbs\n\n\nI settled on capacitive sensing to control the timbre of the sound engine. The performer can place their thumbs on two copper strips on the back of the neck, which increases the capacitance in the circuit. Different thumb positions result in different amounts of contact between the skin and the copper strips, which we can read in software as a number. This number is mapped to grain length of the granular synthesiser and a delay feedback parameter such that not touching the strips at all will result in a ‘grainy’ sound with little of the delay effect, whereas touching lots of skin to the strips will create a much more sustained sound with a noticable delay effect.\n\nSound Engine\n\nThe sound engine for CordChord is a two-voice (one per string) granular synthesiser which chops up a sample of a cello sustaining a single pitch. This sample is sped up or slowed down to create the pitch, which is determined by position of the performer’s finger that is pressing the string. As mentioned above, the grain length, or the length of each tiny ‘grain’ of sound that is taken from the original sample, is controlled by touching the capacitive copper strips on the back of the neck. Longer grain length results in more overlap between consecutive grains, in turn resulting in a richer timbre and a more sustained tone.\n\n\n   \n   The entire data flow for CordChord split into the sensor data, machine learning, and sound engine subsections \n\n\nReflections &amp; Future Improvements\n\nCordChord works well as a prototype. However, as with any prototype, there are a myriad of small issues and areas for improvement.\n\nTwo primary issues is the relationship between the dimensions of the frame and the range of the distance sensors. The distance sensors I used have a working sensing range of around 3cm, but this range does not scale linearly. For example, assuming a scaled sensor reading in range 0-1, 0-0.5 is covered by the first ~5mm of distance from the sensor, while the distance around 2-3cm from the sensor only covers around 0.9-1. As a result, the ML model can more accurately register the position of the string when they are pressed close towards the neck. The effect of this is twofold. Firstly, it results in an audible ‘squelching’ sound as the ML model struggles to determine the pitch when first pressing a string. Secondly, it also means that the quieter volumes are less accessible to the performer while also maintaining accurate control of pitch. This could be partially solved by reducing the distance between the neck and the string.\n\nThere is also simply the limited precision of my dataset collection method and of the frame itself. Because the sensors are quite sensitive to light, I collected the dataset in a windowless room in near complete darkness. This meant my hands did not block any ambient light reaching the sensors, but it also meant my measurements were never quite exact. This was compounded by some inaccuracies in the fabrication of the frame and the sensor placement; while I tried to be as precise as possible by measuring twice and sawing/gluing once, this being a handmade prototype so nothing is exactly square.\n\nThe result is that, while I intended for the pitch to be linearly mapped across the length of the string, it is more sensitive in some areas i.e., the middle, than it is at the ends. This means that pitches are found mostly by ear rather than by feel, as they are often not exactly where the performer expects them to be. In turn, this acts as a barrier to intuitive control of the instrument.\n\nTake a Listen\n\nNow you’ve read about it, take a look at the demo and performance video:\n\n\n",
        "url": "/interactive-music/2023/12/01/jackeh-cordchord.html"
      },
    
      {
        "title": "An Interactive Evening on Karl Johans Gate",
        "author": "\n",
        "excerpt": "What if everyday objects decide to kick it up a notch and embrace a life of their own?\n",
        "content": "What if everyday objects decide to kick it up a notch and embrace a life of their own? I’m talking about a magical realm where paintings aren’t just static canvases but interactive experiences that respond to the environment around them. We’ve all seen this idea in movies and cartoons, where art - whether it is statues of paintings, gains a new lease on life. We’re diving into that fantasy land where art meets tech, and it’s more than just Alexa playing your favorite tunes.\n\n\n   \n   The singing lady portrait from Harry Potter\n\n\nLet’s talk about sensors. They’re like the superheroes of the tech world, inspired by our human senses. Touch, sight, hearing – you name it, there’s a sensor mimicking it. Want to know the temperature? Thermometer’s got your back. Eyes for visuals? Cameras are the tech doppelgangers. Ears for hearing? Say hi to microphones. Taste and smell might be trickier, but pH strips can at least tell us if something is acidic or alkaline. The magic happens when these sensors team up to create seamless interactions. Think of your voice assistant – it hears you, processes your words, and then responds through a speaker. It’s a dance of sensing, processing, and responsive output.\n\nNow, entering into our project – an interactive version of Edvard Munch’s Aften på Karl Johan (tr: Evening on Karl Johans Gate). I am putting sensors on a canvas to make it react to the world around it. Imagine lights twinkling or sounds playing as you walk by.\n\n\n   \n\n\nThe image was reproduced on poster fabric commonly utilized for conference presentations. Using clamps, the printed poster fabric was carefully stretched and affixed to the canvas frame with a staple gun. During this process, it was crucial to avoid overstretching the print to prevent eventual tearing. Tactful folding of the fabric at the corners was necessary to prevent bunching, a phenomenon known as “donkey ears” in canvas stretching. Subsequently, the fabric print underwent a matte sealer treatment, employing white decoupage glue that dries clear. Multiple coats were applied with a paintbrush, allowing each layer to dry before adding the next. Matte sealer not only gives it an oil painting vibe but also makes it water-resistant, a handy feature when you’re transporting it through the rainy streets of Oslo.\n\nI am weaving a story into the mix, using sound design to skillfully convey a narrative that engages and immerses viewers. Taking a cue from the world of films and games, the project borrows techniques from both. It includes diegetic and non-diegetic sounds, as well as intermittent sounds, adding a touch of realism with random environmental noises.\n\nThe wind sounds in this project were crafted using a Pure Data patch sourced from Andy Farnell’s book, “Designing Sound,” providing a foundation for authentic synthesis. One-shot environmental sounds were sourced from audio databases on the internet. The background music, uniquely composed for this project, originated in Logic Pro X. It involved the use of a non-native synthesizer plugin, coupled with MIDI orchestration, incorporating a diverse array of sample libraries from third-party VST plugins to achieve a rich and dynamic auditory landscape.\n\nFor the background score, I used the principles of hyperorchestration and tweaked the timbre of the virtual music ensemble to match the vibe of the painting. It’s like adding reverb for that “all alone” protagonist scene. The painting becomes a storyteller, setting the mood with a tense aura in its current iteration.\n\nThe painting has a light-dependent resistor making LEDs light up when it gets dark, giving those windows in the painting a soft glow. Infrared sensors – one triggers random street sounds when you walk by, and the other toggles ambient sounds like wind and crowd murmurs on and off. The project was implemented on Bela and Pure Data (Pd). I learnt canvas stretching techniques, how to hammer nails perpendicularly, how to design a circuit, cut a perfboard, use a glue gun -  the list goes onnnn.\n\nAnd, by the way, hard work did pay off. Behold - my first ever perfboard!\n\n\n   \n\n\nI made sure there were no overlapping wires, that is was customizable (have left space for new inputs and outputs), and that everything was pressed against the board firmly. I was advised to solder parts for one sensor at a time and check if the circuit worked before moving on to the next. However, I soldered the whole thing all at once, then nervously connected this to my Bela and sensors… and it worked. :’) The pic below shows the top of the perfboard. There are no wires on this side.\n\n\n   \n\n\nMy key takeaways from this project were:\n\n  One must ensure that the addition of sound and lights complements the original artwork, and aligns with its style and mood - instead of overshadowing it.\n  User testing is a good way to gather feedback and observe how people respond to the interactivity. This helps analyze whether the addition of new modalities (sound and light) contributes to or takes away from the overall art experience.\n  Experimenting with varying intensity and interactivity levels is the way to go. It is perhaps the only way to find the right balance.\n  Sensor thresholds should be set according to the environment the interactive object is in.\n  It always helps to document the process, so when you are writing about it later, you remember the important details.&lt;/div&gt;\n\n\n\n  \n  Grab some snacks and enjoy the video\n\n\nList of Acknowledgments\nThis interactive journey has been a heartwarming collab and I’ve learned that creating something new truly takes a village. Here’s a shoutout to my amazing folks:\n\n  Anders from UiO Grafisk Senter: a painter turned graphic designer, who printed the painting on conference poster fabric and offered invaluable guidance on mounting it on a canvas frame\n  Mojtaba: showed me how to solder when I first joined RITMO over a year ago\n  Finn: lent me a hammer and nails\n  Björn, Bilge and Kayla: encouraged/recommended hacks for the assembly of the painting\n  Non-UiO friend: helped figure out how to use a staple gun and stretch fabric onto a canvas\n  Classmates: cheers to an inspiring semester filled with creative sparks\n  Friends at RITMO: their kind words, enthusiastic smiles and friendly waves in the blue room subroom brightened my days\n  Game audio and film scoring teachers: whose lessons continue to inspire me, even years later\n  Staff at Kreatima: paint brush recommendations and insights on deep edge canvases\n  Alexander, my supervisor: for always lending me an ear, in addition to the Designing Sound book and a staple gun for this project\n  Stefano, my teacher: for being an ocean of knowledge and generously sharing it\n  Mom: for contagious enthusiasm and brilliant story ideas. Bohat saara pyaar. :] &lt;3\n\n",
        "url": "/interactive-music/2023/12/01/mahamr-KarlJohansGate.html"
      },
    
      {
        "title": "The Hyper-Ney",
        "author": "\n",
        "excerpt": "Electrizing an ancient flute using capacitive and motion sensors\n",
        "content": "\n  \n  \n    The Hyper-Ney\n  \n\n\n⏯ A video may be auto-playing down in this page - jump to video\n\nIn this post, I’ll discuss about an interactive music system that I designed and built in the last couple of months. It’s called Hyper-Ney and as the name suggests it’s an enhanced ney flute.\n\nAbout the Ney Flute\nNey flute is an ancient wooden flute from middle east rooting 4500–5000 years back. It’s a very simple design with 7 holes on a wooden body, allowing it’s player to create noisy and ambient sound on a continuous pitch scale. You can have a look at this YouTube video for a traditional performance with the Ney.\n\nA feature that distinguishes it from similar instruments of other cultures is the flared mouthpiece or lip rest, called a bashpare, traditionally made of water buffalo horn, ivory, or ebony, but in modern times many are plastic or similar durable material.\n&lt;/br&gt;\nThe Turkish ney is played by pressing the bashpare against nearly-closed lips and angling the flute so that a narrow air stream can be blown from the center of the lips against the interior edge to the left or right, depending on whether the flute is left- or right-handed in construction.1\n\nDesigning the Hyper-Ney\nJonathan Impett, a researcher and trumpet player, has influenced the way I think about the Hyper-Ney’s design. In designing his Meta-Trumpet, he made sure to keep the traditional playing techniques intact. He innovated without losing what makes the trumpet, well, a trumpet. Please see Impett’s performance with his instrument with this design perspective in mind\n\n\n  \n  \n    Jonathan Impett\n  \n\n\n\nAs far as possible, this is implemented without compromising the richness of the instrument and its technique, or adding extraneous techniques for the performer - most of the actions already form part of conventional performance. In keeping with this idea, it proved possible for the trumpet at the heart of the system to remain inviolate. - Jonathan Impett, 1994\n\n\n\nInspired by his approach, when designing the Hyper-Ney, I hold onto the classic aspects of playing the ney flute. I tried to avoid altering the interaction dynamics with the ney instrument as far as possible.\n\nWhen discussing the design principles for interactive computer instruments, Perry Cook mentions the principle of spare bandwidth. This principle focuses on utilizing the unused capacity of the instrument for other purposes, which proves to be very interesting and successfull. I had this principle central when designing the Hyper-Ney.\n\nNey flute is similar to other flutes in terms of the spare bandwidth it offers. I built the Hyper-Ney on this availability, by adding extra techniques to synthesize and process audio. This takes us to the technical part where I will write about the implementation more in detail.\n\nDeep Into the Technical\nTo achieve my design goals, I integrated various sensors to a Mansur size ney (see here for size reference). I used a Bela for the sound processing and synthesis. Before I describe the Hyper-Ney part by part, please have a look at the final form of the components on the Hyper-Ney below.\n\n\n  \n  \n    Main components of The Hyper-Ney\n  \n\n\nMouthpiece\nI added a tiny piece of aluminium tape on the mouthpiece to build a capacitive sensor to capture the position of the lips. It needed a very precise work to be able to place the tape symmetrical to accurately capture the capacitance.\n\nHoles\nI added tiny solid wires around all 6 front holes of the ney. This allowed me to track the utilization of the holes as well as the amount of coverage of each hole. These capscitive sensor wires are routed along the ney’s body and connected to Trill Craft, which is a board for creating complex capacitive sensors with Bela.\n\nMotion of the Ney Flute’s Body\nAltough not very common when playing ney, motion of the instrument proved very successfull in terms of interaction and creativity. I added an accelerometer at the end of the ney to use the acceleration in two axes in musical mapping.\n\nAudio Input\nTo capture the original sound of the instrument, I attached a contact microphone close to the mouthpiece and routed it’s hack cable along the ney to come out with other cables.\n\nBoards\nBela and BeagleBone Black boards are stacked on top of each other. I created a connection board (perfoboard) to be able to connect to Bela pins in a roboust way. These three boards, input and output jacks and a battery are attached to my lower arm using velcro strips.\n\nHere take a look at two diagrams. One showing the high-level design of the system, second showing the main components of the system and connections, and second showing\n\n\n  \n  \n    System design\n  \n\n\n\n\n\n  \n  \n    Detailed connection diagram\n  \n\n\n\n\nHere is the final look of Hyper Ney.\n\n\n  \n  \n    A closer look at the Hyper-Ney while being played\n  \n\n\nMapping and Sound Processing\nAn interesting aspect of my work is that the spare bandwidth I utilized for mapping to sound processing modules changes dynamically during playing. This relies on the acoustic phonmena of “when a higher hole is open, all the lower holes doesn’t contribute to anything, i.e., they are extra and can be used for extra controls! See the image below for the availability of holes for mapping, and the next one for how I am calculating the availability continuously from the capacitive sensor input.\n\n\n  \n  \n    Availability of holes for mapping under different scenarios of hole usage\n  \n\n\n\n  \n  \n    The calculation of hole availability, where where H1 to H5 are the closure status of each hole, and S1 to S5 are the states of hole availability, as given in the figure above}\n  \n\n\nI implemented a many-to-many mapping of the sensor data to audio processing and synthesizing modules. The lower holes are occupied for controlling a delay effect, and a granular synthesis module which is based on a steel hang audio recording. The higher holes were occupied for controlling a complex frequency modulator combined with a noise generator. At the same time, majority of the holes are used for determining a carrier frequency that is used in many placed in my sound synthesis and processing modules.\n\nThe Hyper-Ney is also available when the Ney is not being played in a conventional way. It’s still possible to play it on your hand without even touching your lips. However, if you still want to blow while playing your flute, that’s fine, you can make music by blowing on the contact microphone!\n\nDid it really work?\nI conducted a user study to evaluate the insturment from the audience’s perspective. Adopting Bellotti and colleagues’ method, I asked five questions to gather scores for different aspects of how the audience percieve and experience the instrument. These aspects are:\n\n  Cause comprehension - “How well could you identify the specific parts of the performer’s body and the instrument that were utilized to interact with the system?”\n  Effect comprehension - “To what extend the system and the performance provided enough audiovisual input to understand the relationship between the performer’s intentions and the resulting output?”\n  Mapping comprehension - “To what extend do you think you understood the mapping of the gestures to sound produced by the instrument?”\n  Intention comprehension - “To what extend could you understand the user’s intention while his performance?”\n  Error comprehension - “To what extend could you perceive the errors (technical problems, failed or unmapped gestures, unwanted audio) during the performance?”\n\n\nAfter a performance with The Hyper-Ney, I asked these questions to 3 people who were moderately familiar with the instrument’s design (scored 5.333 in 10) and here are the results of this survey.\n\n\n  \n    \n      Question\n      Mean Score\n    \n  \n  \n    \n      Familiarity with the Hyper-Ney\n      5.333\n    \n    \n      General Impression of the Performance\n      8\n    \n    \n      Cause Comprehension\n      6.333\n    \n    \n      Effect Comprehension\n      7.333\n    \n    \n      Mapping Comprehension\n      5.667\n    \n    \n      Intention Comprehension\n      7.667\n    \n    \n      Error Comprehension\n      2\n    \n  \n\n\n\n\nThe quantitative evaluation of the Hyper-Ney from the audience perspective revealed generally positive feedback. While participants found it moderately challenging to identify specific body parts and instrument interactions (6.33 out of 10), the system and performance provided sufficient audiovisual input for understanding the relationship between the performer’s intentions and output (7.33 out of 10). The mapping of gestures to sound was perceived slightly less clearly, with a score of 5.67 out of 10. However, the audience demonstrated a relatively high level of understanding regarding the performer’s intentions during the performance (7.67 out of 10). Notably, the lack of perception of errors (2 out of 10), as reflected in the low score, although potentially suggesting a smooth and technically sound execution, may also suggest a potential disconnect or low engagement with the technical aspects of the performance, as there were small errors during my performance. I can reflect more after analyzing the recording of the performance.\n\nAdditionally, written feedback results indicate overall positive feedback on the instrument, praising its captivating sound and the seamless integration of synthesis and ney audio. The mapping of each hole was appreciated for efficient bandwidth utilization. However, some participants found it challenging to grasp the correlation between gestures and specific parameters. Suggestions for improvement included establishing a clearer visual connection between gestures and sound parameters to enhance audience understanding. While the Hyper-Ney was commended for its aesthetics and craftsmanship, some attendees wished for a more visible demonstration of the affects of motion during play. Despite difficulty in recognizing specific mappings, participants enjoyed the performance and highlighted the instrument’s organic and natural sound, along with effective use of effects like delay.\n\nIn wrapping up…\nAll in all, the Hyper-Ney takes your traditional ney flute and enriches its expressive potential. By using the untapped bandwidth of the instrument, I’ve opened up a bunch of creative possibilities while keeping the soul of the ney intact. It was so fun to work on this creative instrument, and I learned a lot!\n\nPerforming with the Hyper-Ney\n\n\n\n\n\nYou can read the paper HERE\n\nSee more interactive music works by SMC students HERE.\n\nReferences\n\n  \n    V. Bellotti, M. Back, W. K. Edwards, R. E. Grinter, A. Henderson, and C. Lopes. Making sense of sensing systems: Five questions for designers and researchers. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’02, page 415–422, New York, NY, USA, 2002. Association for Computing Machinery.\n  \n  \n    P. R. Cook. Principles for designing computer music controllers. In Proceedings of the International Conference on New Interfaces for Musical Expression, pages 3–6, Seattle, WA, 2001.\n  \n  \n    J. Impett. A meta trumpet(er). In International Conference on Mathematics and Computing, 1994\n  \n\n\n&lt;/br&gt;\n&lt;/br&gt;\n1 Based on content from the Wikipedia article Turkish ney, accessed on 05 September 2024.\n",
        "url": "/interactive-music/2023/12/01/ahmetem-the-hyper-ney.html"
      },
    
      {
        "title": "Voice/Bend",
        "author": "\n",
        "excerpt": "Microphone Gestural controller\n",
        "content": "Introduction\n\n\n    \n     \n        \n        Flex Sensors\n    \n\n    \n    \n        \n        IR Sensor\n    \n\n\nVoice/Bend is an interactive gestural controller for improvised vocal performance. The idea is to interact with the sound like a physical object.\n\nIt is wearable glove a singer wears on their right hand. It’s fitted with bend sensors that detect the flexing of the fingers and an Infrared Distance (IR) sensor on the palm to measure how far the hand is from other objects. The left-hand holds a microphone, capturing the performer’s voice.\n\nThis approach differs from traditional electronic instruments, which typically rely on knobs or sliders for sound control. With Voice/Bend, the concept is more about physically interacting with sound, as if you could shape and touch it with your hands.\nThis concept emerged from understanding how singers often use hand movements as part of their expression. The development of interactive systems in music performances has been increasing. These systems, particularly sensor-based ones, have made it simpler for musicians and vocalists to interact with their music.\n\nGestures play an important role in performances. They can add a different dimension beyond just their visual aspect. This was a key consideration in creating Voice/Bend. The glove’s sensors and the dynamic microphone allow for a different way to control sound based on the singer’s movements.\n\nTechnical Implementation\n\n\n    \n    \n    \n    Voice/Bend Diagram\n\n\nThe glove has flex sensors attached to the index and ring fingers, which detect the amount of  fingers deflection. These sensors change their electrical resistance based on the bending, which is captured to understand the finger’s position. I used double-sided tape to mount these sensors onto the glove, ensuring they stay in place during performances.\n\nAdditionally, there’s an Infrared Distance (IR) sensor on the palm of the glove. This sensor is crucial for detecting how far the hand is from various objects. It generates an electrical potential that changes with distance, making it a reliable proximity sensor.\n\nRegarding sound, voice manipulation can be tricky, especially to avoid feedback issues in live settings. I chose the Shure SM58 microphone, known for its resilience against feedback and excellent sound rejection capabilities.\n\nI utilized Pure Data (Pd), an open-source graphical programming language, and Bela, a unique hardware-software environment. This combination allowed me to prototype the sound and sensor mappings rapidly.\n\nThe mapping process in Voice/Bend is multi-layered, translating physical gestures into sound through various stages. This process is integral in converting the data from the glove’s sensors into meaningful musical parameters.\n\nThe microphone captures the voice and tracks the pitch, generating two distinct sound waves. These waves are then modified based on the glove’s sensor data, offering an intuitive way for vocalists to add new dimensions to their performance.\n\nUser-Study\n\nIn an evaluation of the Voice/Bend a performer with a background in singing participated. This user-centric evaluation aimed to assess the controller’s usability, musicality, expressiveness, and overall performance utility, integrating subjective and objective elements.\n\nThe performer found the Voice/Bend controller relatively straightforward in usability. While certain aspects were intuitive and easy to use, others presented a moderate learning curve. The controller excelled in its expressiveness, significantly enhancing the dynamic and emotional range of the performance. However, the intuitiveness of control received mixed responses, with some functions integrating smoothly into the performer’s style while others required more adaptation.\n\nThe user reported high levels of enjoyment and engagement, appreciating the controller’s ability to add new dimensions of sound and expression. Objective measurements indicated a reasonable learning time, with precise and accurate control over sound modulation tasks. The performer proficiently managed various musical aspects, including polyphony and feature modulation, essential elements of the controller’s design.\nFeedback also included suggestions for ergonomic improvements and increased sensor sensitivity, indicating areas for refinement. The flex sensors for controlling FM synthesis parameters were rated positively, with the index finger sensor rated as ‘Good’ and the ring finger sensor as ‘Very Good’. The IR sensor, used for controlling other sound parameters, was rated as ‘Fair’, suggesting a need for enhancement in its design for better precision and responsiveness.\n\nOverall, the Voice/Bend showed the potential to enhance interactive music performances. The performer’s feedback highlighted the importance of continuous refinement to realize the controller’s capabilities in live settings fully.\n\nFuture Considerations\n\nTo make the system more expressive, a key upgrade will be the addition of three extra flex sensors on each glove. These sensors will allow for a more comprehensive tracking of finger movements, enabling precise modulation of sound parameters like pitch, timbre, and volume. By accurately capturing the bending of each finger, musicians can express more complex gestures, thus enriching their performance with greater depth and nuance.\n\nAnother significant enhancement will be integrating an Inertial Measurement Unit (IMU) into each glove. Positioned centrally on the back of the gloves, the IMU, with its 9 degrees of freedom, will track the orientation and movement of the hands. This technology is particularly effective in influencing the sound’s spatial characteristics, such as panning and spatial effects. With this addition, performers can manipulate the spatial positioning of the sound in real time, creating a more dynamic and immersive auditory experience.\n\nTo ensure the optimal functioning of these sensors, I will design custom 3D-printed mounts. These mounts will secure the sensors on the gloves while allowing full finger mobility. Additionally, I plan to use soft, stretchable fabric for the gloves, similar to materials used for osteoarthritis patients. This choice will ensure comfort and adaptability to various hand sizes, crucial for prolonged use without compromising the sensors’ sensitivity or responsiveness. These future enhancements aim to make Voice/Bend a more powerful and versatile tool for interactive music performance.\n\n",
        "url": "/interactive-music/2023/12/04/ninojak-voicebend.html"
      },
    
      {
        "title": "Using Features of Groove in Music Recommendation Systems",
        "author": "\n",
        "excerpt": "A study on analyzing groove in musical items and the effects of groove on musical recommendation.\n",
        "content": "Abstract\n\nMusic streaming services rely on music recommendation systems (MRSs) to keep users engaged and shape their musical taste. These systems rely on a combination of user and item modeling, and are adept at serving relevant recommendations to users through the analysis of collected data. Streaming services must now focus on combating user feelings of stagnation and listening fatigue associated with not receiving exciting and unique recommendations. This thesis proposes  that incorporating elements of groove into a music recommendation system’s features can produce higher quality and more surprising recommendations by being genre agnostic while still recommending tracks based on one of the most important characteristics of music. To accomplish this, a beat tracking and onset detection system was used to analyze two varieties of percussive source separated audio to quantify features of groove. These features were then used to sort items into clusters, which were tested in evaluation sessions to determine if groove could influence quality or expectedness of recommendations. While the clusters had little effect on quality of recommendations, participants were consistently reporting items as unexpected and high quality, showing that recommending items based on features of groove could be useful in producing more serendipitous recommendations.\n\nDesign and Implementation\n\nExtracting Groove Features\n\nThe audio files used in this project were taken from the Free Music Archive (FMA). Spleeter and Librosa’s HPSS function were used to isolate the percussive elements in each musical item. These two methods were chosen after testing the onset and beat detection system using test audio files (view code and results on GitHub). Librosa was also used for its beat tracking and onset detection functions, which were run on each audio file in the fma_small dataset from the FMA. Using the beats and onsets, the features of groove known as pulse, subdivisions, and syncopation were calculated to be used in the simulated MRS.\n\nAccording to Câmara and Danielsen, the pulse of the groove is defined as the steady beat that keeps the groove going, acting as the foundation to add other groove characteristics on top of, the subdivisions of a beat are defined as the notes played at faster metrical levels than the beat, which are generally considered necessary to establish a groove and give a sense of drive to the groove, and syncopation enhances a groove by temporarily displacing the normal accent of the meter, and is considered to be the most important element in defining a style of groove.\n\nEach beat for all musical items in this project is defined in a 12-unit timespan, which allows groupings based on three, four, and six units. The table belowshows the different rhythms that were analyzed for each item and which unit the onset needs to fall on in order to be on-beat for that specific rhythm, which is represented by an x.\n\n\n  \n  Rhythms in a 12-unit time span.\n\n\nThe fourth note rhythm can be considered the pulse of the beat, while all other unit divisions besides those marked “off beat” are the subdivisions. After all the onset beat bin totals were found, the number of times an onset appears in an on-beat bin for each rhythm is divided by the total number of onsets found for all beat bins to normalize their values. These values numerically represent the likelihood of an onset in a rhythm being played across all onsets in the song segment. These normalized values are then used as features in the ML system.\n\nSyncopation is measured in two ways. First, the “off beat” bins are normalized in the same way as the pulse and subdivision rhythms. Second, the weighted note-to-beat distance (WNBD) is calculated. To find the WNBD of a note, the T(x) value, which equals the minimum distance between the note and the previous or next pulse as a fraction, must first be defined. If the beat occurs on a pulse, the WNBD is 0. If not, the WNBD value is then defined in relation to the end of the beat, which is considered to be the start of the next beat. If the beat ends before the next pulse, on the next pulse, or any time after the next two pulses, WNBD = 1 / T(x). If the beat ends between after the next pulse, but before or on the pulse after the next pulse, WNBD = 2 / T(x).\n\nClustering Musical Items\n\nThe k-means clustering algorithm is a popular option for solving clustering problems due to its relative ease of implementation and its computational efficiency. A k-means clustering algorithm was created in order to cluster items together based upon similar groove features using both Librosa and Spleeter PSS detected beats and onsets. The number of clusters were determined through manual analysis using the elbow and silhouette methods, as well as analyzing the resulting sizes of the clusters to make sure they were relatively even. To determine the cluster validity, the six most central tracks of each cluster were examined in a cluster tendency analysis in order to determine if the percussive elements have groove characteristics in common\n\nEvaluation Session\n\nThe evaluation session consists of a participant listening to multiple 30 second segments of songs (download on GitHub). Each of these items are one of the six closest items to the centroids of either a Librosa PSS cluster or a Spleeter PSS cluster, since these can be considered the most representative items of each cluster. The participants then rank these musical items on two different seven point Likert scales. The first scale determines the participant’s subjective enjoyment of the track, with 1 corresponding to “extreme dislike” and 7 corresponding to “extreme like.” The second scale determines how likely the participant would be to listen to the track, or in other words how expected the item was, with one corresponding to “extremely unlikely” and seven corresponding to “extremely likely.”\n\nResults and Discussion\n\nQuality and Expectedness Results\n\nThe standard deviation and average values of the quality of all items in each Librosa and Spleeter PSS cluster can be viewed below.\n\n\n  \n\n\n\n  \n\n\nThe standard deviation and average values of the expectedness of all items in each Librosa and Spleeter PSS cluster can be viewed below.\n\n\n  \n\n\n\n  \n\n\nAssuming a normal distribution for quality and expectedness ratings, the average quality and expectedness values across all tracks for both PSS methods should be around 4. Across all participant ratings for all items of the evaluation session, an average quality rating of 4.25 in the Librosa segment suggests a fairly normal distribution, while an average quality rating of 3.52 suggests that other factors were at play that pushed the average rating further from 4. This could be because there were not enough ratings for a normal distribution, due to only having 5 Spleeter PSS clusters with 540 total ratings compared to 6 Librosa PSS clusters with 648 total ratings. Another explanation for this discrepancy is that the overall quality of the items served in the Spleeter system may have been less desirable than the items served by the Librosa system, which will be discussed further while analyzing true positives and true negatives later in this section.\n\nIn terms of the average expectedness, both Librosa and Spleeter PSS clusters have lower than average values, with the Librosa method’s average value being 3.37 and the Spleeter method’s average value being 2.91. This is mostly likely on account of the content of the FMA, which includes more experimental and left field musical items than streaming services. While streaming services contain plenty of experimental music themselves, it also contains nearly the entire history of popular music, which could result in a more normal distribution of expectedness. With lower than average expectedness values, it is also expected that there will be more serendipity and true negatives found compared to anti-serendipity and true positives, respectively.\n\nStandard deviation of quality values per cluster is analyzed to determine if solely using features of groove in an MRSs can serve relevant recommendations to users. Assuming a normal distribution for quality and expectedness ratings, around 68% of the ratings are within one standard deviation of the mean and 95% of the ratings are with two standard deviations of the mean. Therefore, if the average standard deviation across all clusters is below 1.5 then it can be reasonably assumed that the recommendation system is better than random guessing.\n\nWith an average quality rating standard deviation per cluster of 1.42 for Librosa PSS clusters and 1.54 for Spleeter PSS clusters, it can be concluded that participants were more likely to receive relevant recommendations using the Librosa method, but both methods were fairly close to random guessing. Therefore, for the goal of suggesting high quality items to users, using only features of groove does not produce relevant results. However, the slightly lower standard deviation per cluster average that the Librosa method produces suggests that with further fine tuning and possibly more features, a more refined recommendation system could be developed. It is also worth noting that the average standard deviation of expectedness ratings per cluster for both Librosa and Spleeter methods are fairly similar and above 1.5, suggesting a truly random distribution of expectedness values per cluster.\n\nSerendipity and Anti-Serendipity Per Cluster\n\nThe charts showing serendipity and anti-serendipity per cluster for both the Librosa and Spleeter methods can be viewed below.\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\nParticipants reported higher than expected accounts of serendipity, with both Spleeter and Librosa PSS items having higher than 10% serendipity accounts. This could be attributed to the diverse catalog of the FMA, or a general tendency for users to encounter positive surprises when exploring a wide variety of music. However, participants tended not to report instances of anti-serendipity. This could be a result of the combination of lower than average expectedness values for both Librosa and Spleeter PSS clusters and the aforementioned accounts of the low quality of some items in the FMA. The perceived low quality may have resulted in lower rated items receiving equally low expectedness values due to the intensely negative reaction they elicited in some participants. However, since items near the centroid of Spleeter PSS clusters produced more than double the instances of anti-serendipity compared to Librosa PSS clusters, it can be stated that Librosa PSS clusters do a better job at combating stagnation and steering users away from low quality items in genres they are familiar with. Overall, the high values of serendipitous items versus the much lower values of anti-serendipitous items shows that if music streaming services expanded their horizons by introducing new styles of music to listeners that do not have a history listening to that style of music, they would be much more likely to find positive surprises rather than negative surprises, and that achieving serendipity through the lens of groove could be a worthwhile approach to address this issue.\n\nTrue Positive and True Negative Per Cluster\n\nThe charts showing serendipity and anti-serendipity per cluster for both the Librosa and Spleeter methods can be viewed below.\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\n\n  \n\n\nTracks from the Librosa PSS clusters contained a fairly even amount of true positive ratings (27.01% of all ratings) and true negative ratings (27.31% of all ratings), further suggesting a fairly even set of musical items in terms of quality and expectedness for each participant. Comparatively, items from the Spleeter segment of the evaluation session contained a significant number of true negatives (42.59% of all ratings), compared to only 15% of all ratings being marked as true positives. This is backed up anecdotally by some users pointing out the the end of the test, when the items from the Spleeter clusters were played, contained more strange and experimental music.\n\nEffects of Serendipity and Anti-Serendipity On Average Cluster Quality and Expectedness\n\nTo begin the discussion of the effects of serendipity and anti-serendipity on average cluster quality and expectedness, it is worth noting that the number of clusters containing anti-serendipity were miniscule and therefore did not produce statistically significant results. However, there were many instances of serendipity across both Spleeter and Librosa PSS clusters, which gives a better insight into how serendipity affects quality and expectedness values. In general, Librosa PSS clusters had a much better spread of clusters that contained serendipitous items than Spleeter PSS clusters. However, both Librosa and Spleeter PSS clusters that contained accounts of serendipity only had higher than average quality values a little over half of the time. While this suggests at least a little correlation between accounts of serendipity and an increase in quality, this could also be explained by the fact that an account of serendipity guarantees that at least one item in the cluster was rated above a 4 for quality. This could be all that a cluster needs to boost its average above what was typical for a user, so it can be stated that correlation between serendipity and quality was not significant for either PSS method. Similarly, average expectedness in Spleeter PSS clusters with accounts of serendipity was below average a little over half of the time, suggesting low correlation. However, a significant number of Librosa PSS clusters with accounts of serendipity had below average expectedness values, with 66.13% of the clusters having below average expectedness values. Therefore, the most significant finding in this analysis is that for the Librosa PSS clusters, a cluster that contains an instance of serendipity has some correlation with a lower expectedness value across all items in the cluster.\n\nConclusion\n\nUsing Librosa HPSS was found to be superior to using Spleeter in nearly all scenarios, including finding beats and onsets, analyzing groove, and clustering items with similar grooves together. Even though recommending musical items solely based on groove will not produce higher quality recommendations for the average streaming service user using either PSS method, the findings in this study could be used to fuel future research on this topic. Combining features of groove into other state-of-the-art MRSs could help fight feelings of stagnation by helping to recommend music outside a streaming service user’s typical consumption. Future studies should focus on building a more robust beat tracking system, testing different styles of HPSS, combining groove features with other MRS features, using different music archives, and expanding the scope of evaluation sessions with more participants, musical items, and a better system for evaluation sessions.\n\nSources &amp; Resources\n\n  All code and results on GitHub: https://github.com/jpclemente97/SMCThesis\n  Thumbnail: https://www.vecteezy.com/vector-art/210521-party-people-dancing\n  G. S. Câmara and A. Danielsen, “Groove,” in The Oxford Handbook of Critical Concepts in Music Theory, A. Rehding and S. Rings, Eds., Oxford University Press, 2020, pp. 270–294. doi: 10.1093/oxfordhb/9780190454746.013.17.\n  L. A. C A and A. C A, “Research on DNN Methods in Music Source Separation Tools with emphasis to Spleeter,” Int. Res. J. Adv. Sci. Hub, vol. 3, no. Special Issue 6S, pp. 24–28, Jun. 2021, doi: 10.47392/irjash.2021.160.\n  M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, “FMA: A Dataset For Music Analysis.” arXiv, Sep. 05, 2017. Accessed: Sep. 23, 2023. [Online]. Available: http://arxiv.org/abs/1612.01840\n  F. Gomez, A. Melvin, D. Rappaport, and G. T. Toussaint, “Mathematical Measures of Syncopation”.\n\n",
        "url": "/masters-thesis/2023/12/04/josephcl-groove-thesis.html"
      },
    
      {
        "title": "Xyborg 2.0: A Data Glove-based Synthesizer",
        "author": "\n",
        "excerpt": "Learn about my adventures in designing and playing a wearable instrument.\n",
        "content": "Hand-based motion-focused music systems like an earlier version of Xyborg quickly became my field of interest during my studies at IMV. There is just something ideologically captivating about freeing myself from standing in front of a keyboard looking like I am google searching something. Prototyping instruments involving some sort of data glove has a long history dating back to the 1980s. One of these early designs piqued my interest in particular: Michel Waiswisz’ first iteration of “The Hands” clearly influenced Xyborg’s design. I wanted to see how far I could take this first iteration concept while incorporating my own ideas, dependencies, and, well, limited knowledge. Really, my thought in the beginning was: “Can I play a synthesizer strapped to my hands while walking around?\n\nFrom Paper to Prototype\n\nMy system consists of two wooden frames, two accelerometer sensors, sixteen DIY capacitive sensors (eight for each hand) and four buttons + LED’s. All of this is connected via solid core wire with the brain of the system: my pure data sound patches run on a hardware system called Bela - a tiny powerful computer specifically designed for musical purposes. The Bela sits inside a plastic casing and is powered by a power bank. Everything is strapped around the waist. You are fully power-independent and mobile while wearing the instrument.\n\n\n  \n  \n    How to wear Xyborg + Model shots\n  \n\n\nThe mental model of Xyborg is indeed a keyboard with twelve keys, representing twelve steps in a chromatic scale. In addition to that, you have your movement control three effects: a lowpass filter, distortion, and modulation. Changing the octave range as well as effect on/off is provided by pressing additional buttons. Instead of using buttons as pitch keys, I opted to try a DIY design and build sensors. Capacitive touch sensing was the hottest candidate for that, and luckily, Bela supports that with the trill craft. The sensors are fairly simple. I cut 2x2 cm squares from a PCB board and wrapped it in aluminium tape. Then, I fixed a stranded wire between tape and PCB, on the conductive side of the PCB square. The thumb on both hands controls the buttons for effect on/off as well as the octave range touch sensors. But how do you manipulate effect parameters with motion?\n\n\n   \n    The hardware layout of Xyborg  \n\n\nGesture-to-Sound (Manipulation)\n\nI installed a three-axis digital accelerometer on top of each frame. This sensor gives me acceleration data for x, y and z axes, and, to some degree, the orientation of these axes in a coordinate system. Now it’s only a matter of mapping this sensor data to effect parameters inside my software instrument. You can see the mapping and signal flow for each hand in the figure down below. To offer a degree of transparency for performer and audience, I tried to keep the mapping as simple and clear as possible. This means the instrument has an initial position - a pose with your arms completely relaxed. From there on, roughly speaking, the parameter space changes in such a manner that increasing the height of your hands ‘opens up’ the effects. The mapping for distortion is based on a regression machine learning model made possible with this awesome framework for pure data. I think the idea of a cloud is quite a fitting analogy to describe how the movement - sound connection works here. In a cloud you have regions with different densities - much like as in a musical parameter space based on a regression model. For the filter, the cutoff frequency is mapped to the pitch (y-axis) of the accelerometer. Simply moving your hand up increases the filter cutoff, essentially opening up the filter. The vibrato is mapped to the sum of all acceleration data of the left hand - the faster you perform a vibrato motion, similar to the same motion performed on a string instrument, the faster the vibrato will be.\n\n\n   \n    The signal flow - mapping sensor data to parameters \n\n\nJudgement Day\n\nI am actually super satisfied with the sound of the instrument as well as effect manipulation capabilities. Controlling the filter cutoff and vibrato speed as well as moving through the distortion cloud all feels natural and intuitive. I would not go any further and include more features and effects, but rather focus on practice. Where the instrument is still clearly lacking is in the field of ergonomics and general design. First of all, it is hard to strap the system onto your arms all by yourself. Secondly, while playing around with it for roughly 5 hours total I could clearly see the downsides of my design. One of the biggest downsides is the exclusion of the thumb for playing a pitch key. My reasoning was that I would provide myself with greater flexibility in manipulating the sound if the thumb solely focused on control functionality. In the end, this does not make up for the missing fifth finger, especially when you are used to playing any type of key-based instrument. Subjectively, cognitive load actually increased for me since I had to re-map finger positions to pitches mentally from what I am used to on a commercial key layout. For the next prototype, I would simply erase the upper section of Xyborg and instead tilt it 90 degrees and mount it to the side of the base plate. I would then include a touch capacitor for the thumb while making sure that the control buttons and octave buttons are still in reach. Funnily enough, I also rarely used the octave range keys and rather stayed in the same octave. It is just too complex to find a desired pitch range fast enough for two hands. However, this gave me an interesting set of constraints. Instead of focusing on arpeggi or melodies over a wide range of notes, I intuitively focused on playing continuous notes and dissonances. This play style fits rather neat with the slow movements of effect manipulation. So I already developed a style!\n\nDespite the apparent room for improvement I can clearly see potential for a long-term engagement with Xyborg. It is fun to use, feels very intimate while playing, and the mapping is complex enough to allow for skill development over time, in my opinion. \nIf you want, you can check me out playing for a couple of minutes down below!\n\n\n",
        "url": "/interactive-music/2023/12/04/kristeic-xyborg.html"
      },
    
      {
        "title": "Documenting Networked Music Performances: Tips, Tricks and Best Practices",
        "author": "\n",
        "excerpt": "Effectively documenting networked music performances can lead to better experiences for physical and digital audiences, and your academic explorations.\n",
        "content": "\n\nAt its simplest, a networked music performance (NMP) is a musical performance done in real time over a computer network. It enables people to be in two different locations and perform and practice music together synchronously as if they were in the same room. NMPs can connect musicians who would not usually be able to play together, help researchers gain insight into technology and performance interactions, and make concerts more accessible to audiences.\n\nA crucial part of any NMP is documentation. Firstly, for audience members. It is important to ensure every audience member in any location has adequate “documentation” of the parts of the performance they are not physically in the room to see. Secondly, there are considerations to consider if you will document an NMP for academic purposes so that your evidence supports answering your research questions.\n\nIf you are an SMC student or a person completing an NMP in an academic setting, you will likely have to consider these factors when deciding the best ways to document your NMP. This blog post will provide tips, tricks, and best practices for documenting NMP for physical audiences, digital audiences, and academic purposes.\n\nAt the Aalborg University, SMC students use LoLa, a low latency, high-quality audio and video system, to host multiple NMPs each term. Within the current constraints of technology in 2023, at UiO, we can host NMPs using LoLa from two locations, with one camera feed sent from each site. This blog post is written with these technological limitations in mind so future SMC students can use it as they plan and host NMPs.\n\nGeneral Tips\n\n1. Make a technical diagram (or a few!)\n\nMaking at least one technical diagram is vital to planning and hosting a successful NMP. You can do this on a free and easy-to-use website, draw.io.\n\nNo matter what software, hardware, locations, or performers you are using, a technical diagram can help you gain clarity from the start about what equipment you need, how many people need to be at each location, where the best places to set cameras may be, etc. It can also help you foresee any documentation challenges that may come up. For example, will you need an extra laptop and camera feed to record the performance? A technical diagram lets you prioritize your documentation needs from the beginning.\n\nBelow is an example of a technical diagram SMC’s 2023 cohort made for their first autumn 2023 NMP.\n\n\n\nDiagram made by Karenina Juarez\n\n2. Ask for consent\n\nBefore beginning documentation, especially if the concert is live-streamed or data from it is used for academic purposes, ask all people participating if they are okay with being photographed, recorded, and interviewed. Doing this before the performance can help you plan your documentation of the concert and any data you will collect during it. For example, if you know some performers don’t want to be interviewed or recorded, you can plan ahead and shift your methods.\n\n3. Take Photos\n\nThrough set-up, rehearsal, and during the performance, assign a technician to take photos. Often, in research-based contexts, it is helpful to have pictures of where any cameras are placed, where people are standing, and where other equipment is in relation to performers. These photos can then be used in reports to aid in explaining specific elements of the event.\n\n\n\nExample photo from the 2023 cohort’s second autumn NMP.\n\nMultiple audiences\n\n1. Consider camera placement in each location\n\nThis point explicitly concerns LoLa, where we currently can only send one camera feed to another location. Ensure both audiences can see all performers on the projector screen and in the room. It is helpful to test this during the setup of the concert. Make sure there is enough time left to adjust if needed.\n\n2. Consider extra cameras or software you may need\n\nSuppose you are testing a specific research variable. In that case, you may need a LoLa camera in one location to show a particular angle of one location. However, the angle you use for your research might not be the best for the audience at the other location.\n\nConsider if you need an extra laptop or USB camera (and of what quality) to send a more appropriate view for the audience to a projector screen through Zoom. Then, performers can see the lowest latency and specifically angled view on a gaming monitor separately. Drawing a technical diagram and including documentation software and hardware can help prepare for this situation.\n\nOnline audiences\n\nOne benefit to NMPs is that they can easily be accessible to digital audience members. You can easily live stream NMPs with a bit of planning. Open Broadcasters Software (OBS) is a free application that allows you to arrange multiple camera feeds to live stream on Twitch or YouTube. Again, this should be considered beforehand as it could require an additional computer, software, and cameras at both locations. Having a technician to set up, check, and run the stream will also be important.\n\nIf you want to learn more about OBS, check out their Quick Start Guide. Additionally, OBS can be used to stream to social media platforms, opening up options to share NMP on social media platforms such as TikTok, Instagram, Facebook and X (formerly known as Twitter). If you are interested in using OBS to stream to social media check out Dreamcast’s guide on using OBS to stream across platforms. Facebook also has created their own guide for using OBS to stream to Facebook Live.\n\nAcademic Purposes\n\nThere are a few key things to consider when documenting a NMP for academic purposes. This year’s SMC students have found it helpful to rely on some of the concepts present in phenomenological research. Phenomenological research is a qualitative research approach that seeks to explain things by exploring people’s lived experiences of phenomena. For example, it can explore connections between musicians during performance and rehearsal. Here are a few things you should consider when documenting an NMP for academic purposes:\n\n1. Video and Audio Recording\n\nTaking an extra camera to record a wider angle of the entire performance in all locations can be helpful. Using a field recorder to record all the audio could also be beneficial. Later, you can look back at these recordings by yourself, with performers, or audience members and identify moments that stood out or that you would like to gain more insight into.\n\n2. Observational Notes\n\nDuring the planning, set up, performance, and after an NMP, it is beneficial to make observational notes about your experience. You could carry a small notebook and a pen, record short voice notes on your phone, or take notes on a notes app for a phone or tablet. As starting points, you could focus on how you feel, conversations you notice happening, and decisions you see being made. Try not to jump to justifications for what is happening, but focus on recording what is happening and the feelings surrounding it.\n\n3. Interviews\n\nInterviews can be crucial for gaining more in-depth knowledge about the experiences of performers, technicians, and audience members. Consider who you would like to interview for your specific research variables and remember to get their contact information. When interviewing, it may be helpful to take a semi-structured approach where you come into the interview with a list of topics and questions you have. Then, during the interview, you can ask clarifying questions or discuss any issues. No matter what approach you take, it will likely be important for you to ask open-ended questions.\n\nCheck out Amberscript’s guide as a starting place to learn more about open-ended questions and the interview process.\n\nDocumenting NMPs requires planning, good communication. Prioritizing good documentation at all stages of a NMP, particularly in the planning stage, can lead to a better audience experience, more academic possibilities, and reflection that will help create better NMPs in the future. Happy documenting, and good luck with your next networked music performance!\n",
        "url": "/networked-music/2023/12/11/julianmb-documenting-nmp-best-practices.html"
      },
    
      {
        "title": "The Shapeshifter",
        "author": "\n",
        "excerpt": "Co-constructing the body with optical, marker-based motion capture in live dance performance\n",
        "content": "\n  \n  Oskar Schlemmer's depiction of the laws of cubic space. From (Schlemmer, 1987).\n\n\n\n  The full text of this thesis is available  here. All source code developed can be accessed at this GitHub repository.\n\n\nWhen we use motion capture or tracking (MoCap) systems to capture human body motion for artistic purposes, it can be easy to fall into ways of thinking that conceptually merge the physical body and the representation constructed from the captured or tracked data. A recent, high-profile example of this can be found in the rhetoric surrounding ABBA’s comeback Voyage tour, which features avatars of the members of ABBA as they appeared in the 1970s created through the use of large scale motion capture, with the show’s producer, Ludvig Andersson, employing strong terms to elide the bodies of ABBA and their ABBA-tars, stating that “when you see this show it is not a version of, or a copy of, or four people pretending to be ABBA, it is actually them” (ABBA Voyage, 2021).\n\n\n  \n  ABBA-tars in performance. From (‘The bigger picture: ABBA Voyage’, 2022).\n\n\nHowever, many actors and technological systems contributed to the production of the ABBA-tars. A team of engineers and animators were involved in the process of constructing the body from the motion data. The technologies involved implied ways of working suggesting the forms that the body can take and what it can do. A number of younger stand-ins even provided the motion from which the virtual body was constructed (Plaete et al., 2022). Although choreographer Wayne McGregor noted that it was a “technical and emotional challenge” to get the members of ABBA “back into their bodies” (ABBA Voyage, 2021), the bodies visible in the show are, in effect, the result of a co-constructive process including all actors involved. In this context, eliding the physical body with representation can lead to the obscuring of assumptions about the body that all these actors bring.\n\nThis idea is the central to the research aim of this thesis:\n\n\n  to critically explore the co-construction of a virtual representation of the body through the use of  optical, marker-based motion capture  in live performance, specifically focusing on dance.\n\n\nThis aim is formalised in the following research questions:\n\n\n  RQ1. How does the use of a motion capture system co-construct a virtual representation of the body in live performance, and which assumptions about the body does it make?\n\n\n\n  RQ2. How can a multi-modal interactive system be iteratively designed from a perspective which foregrounds motion capture as co-constructing a representation of the body?\n\n\n\n  RQ3. How does a performer experience their body in relation to the technological components in performance with a system for interactive dance?\n\n\nAccordingly, this thesis offers three main contributions:\n\n\n  1. A theoretical framework which models the use of motion capture technologies to create a virtual representation of a physical body as a co-constructive process.\n\n\n\n  2. The Shapeshifter, a performance and multi-modal interactive system.\n\n\n\n  3. A qualitative analysis of the embodied experience of the project's collaborator of performing the work and their relationship to the technological components of the system from an embodied perspective.\n\n\n\nAlthough addressing each of the research questions required a diverse set of theoretical, qualitative, and quantitative methods, this thesis is encapsulated in a wider research-creation project. This is a methodological approach focused the combination of research methods and creative practices in a causal manner which leads to academic and artefactual results (Stévance and Lacasse, p. 123). This approach is well-suited for collaboration, and accordingly the work done for this thesis was carried out in collaboration with a performer who specialises in physical theatre and dance.\n\nThe Co-Construction Model\n\nThe main theoretical contribution of this thesis is a model of MoCap in live dance performance as a co-constructive process.\n\n\n  \n  The co-construction model of MoCap in live dance performance.\n\n\nThis model was developed to encourage a way of thinking about performance involving the use of motion capture that centres upon critical reflection on the assumptions and values that are embedded in the representation of the body created through the technology’s use. The model depicts this occurring across five layers. These should not be understood as distinct and separated, but rather as nested within one another. They depict the increasing concretisation of the form of the virtual body, until this is realised in performance in relation to the physical body of the performer. Crucially, the flow of the embedding of assumptions and values is not depicted as unidirectional, starting in the outer-most layer and flowing in towards the performance, but rather shows the propagation of assumptions throughout the entirety of the model. Through systematically assessing the locations of actors and technological systems, the assumptions and values that they bring, and how these manifest and interact in the representations that are co-constructed.\n\nThe co-construction model was developed upon a methodological foundation based upon the concepts of normalcy, a key concept relating to theorising the body in disability studies (Davis, 1995; Garland-Thomson, 1997), mediate auscultation, in which sensing devices conceptually stand in for that which is being sense (Sterne, 2001), and the propagation of aesthetic and ethical values through the appropriation of sensing devices for artistic practice (Naccarato and MacCallum, 2017).\n\nA full description of the model, how it can be applied as a framework for system design or analysis, and its methodological basis can be found in chapter three of the thesis manuscript.\n\nThe Shapeshifter\n\n\n\n\n\nA demonstration of The Shapeshifter. The video contains a binaural audio mix, so should be viewed with headphones.\n\n\n\nIn view of the co-construction model, together with the research-creation collaborator, we developed The Shapeshifter, an interactive system and peformance to explore the ways in which optical, marker-based motion capture systems construct the representation of the body. With this, we aimed to likewise investigate the performer’s relationship to the representation, as well as the technological components of the system.\n\nThe Shapeshifter is an improvisatory dance work for a single performer. Prior to the performance, the performer positions up to 30 position markers wherever they please, either on the body, attached to other objects, or placed within the environment. During the performance, they are also free to reposition these whenever and wherever they wish. A performance consists of nine phases, during each of which the performer improvises a motion pattern and accompanying vocalisations. To trigger the end of a phase, each of the position markers must be located within a corresponding physical space (a bounding box) in the physical performance area. Each phase presents a different visualisation style both for the virtual representation of the position marker and any connections drawn between markers as well as distinct limitations on how the visualisation of each marker can move. At the end of the nine phases, the cycle begins again. During the second run-through of the phases, the performer’s vocalisations for each phase from the previous run-through are looped within the corresponding phase in the current run-through. Starting in the third run-through of the motion phases, the representations of the markers and connections and their motion limitations begin to shift, interpolating between combinations of the representations of all nine phases. The interpolation is based upon several factors, relating to the similarity of the performer’s motion and vocalisations to the motion patterns and vocalisations performed in the previous run-throughs. Likewise, the looped vocalisations begin to twist and distort away from the original recordings. When the performer vocalises during the current run-through, both the audible and visual representations are pulled back into their original state from the first run-through. As the number of repetitions increases, it becomes increasingly difficult for the performer to purposefully control the representations, building to a climax in the seventh and final run-through of the nine motion patterns. A performance takes place with the performer facing a video wall which mirrors the physical capture volume with a virtual capture volume. The audience is also positioned within the performance space, with the performer moving around the audience. To support the idea of the audience being within the performance space, the looped vocalisations are played back over a spatial audio system in two manners. The first is an underlying sound bed that slowly envelops the performance area over the course of the performance. The second positions each vocalisation at the position of the performer at the time that the vocalisation was recorded. The playback shifts between the two techniques based upon whether the performer is currently vocalising. The first technique is used to playback the looped vocalisations with the parameters of the current interpolation applied. The second, to reproduce the vocalisations in their original state.\n\nWe employed several quantitative and qualitative evaluations of the system and the collaborator’s performance. A full description of these evalutions, as well as a full technical description of The Shapeshifter system, can be found in chapters 4 to 8 of the thesis manuscript.\n\nWorks Cited\n\nABBA Voyage. (2021, October 13). ABBA Voyage: How ABBA used motion capture to create their avatars. Facebook. https://www.facebook.com/ABBAVoyage/videos/abba-voyage-how-abba-used-motion-capture-to-create-their-avatars/399264848445591/\n\nDavis, L. J. (1995). Enforcing Normalcy: Disability, Deafness, and the Body. Verso.\nGarland-Thomson, R. (1997). Extraordinary Bodies: Figuring Physical Disability in American Culture and Literature. Columbia University Press.\n\nNaccarato, T. J., &amp; MacCallum, J. (2017). Critical Appropriations of Biosensors in Artistic Practice. Proceedings of the 4th International Conference on Movement Computing, 1–7. https://doi.org/10.1145/3077981.3078053\n\nPlaete, J., Bradley, D., Warner, P., &amp; Zwartouw, A. (2022). ABBA voyage: High volume facial likeness and performance pipeline. ACM SIGGRAPH 2022 Talks. https://doi.org/10.1145/3532836.3536260\n\nSchlemmer, O., Moholy-Nagy, L., &amp; Molnár, F. (1987). The theater of the Bauhaus (W. Gropius &amp; A. S. Wensinger, Eds.). Wesleyan university press.\n\nSterne, J. (2001). Mediate Auscultation, the Stethoscope, and the “Autopsy of the Living”: Medicine’s Acoustic Culture. Journal of Medical Humanities, 22(2), 115–136. https://doi.org/10.1023/A:1009067628620\n\nStévance, S., &amp; Lacasse, S. (2018). Research-creation in music: Towards a collaborative interdiscipline. Routledge.\nThe bigger picture: ABBA Voyage. (2022). Engineering &amp; Technology, 17(6), 14–15. https://doi.org/10.1049/et.2022.0622\n",
        "url": "/masters-thesis/2023/12/12/hughav-the-shapeshifter.html"
      },
    
      {
        "title": "We Are Sitting In Rooms",
        "author": "\n",
        "excerpt": "Recreating the most famous piece by composer Alvin Lucier as a network music performance\n",
        "content": "Concept\n\nAlvin Lucier was an artist and composer and a long time professor at Wesleyan University. He was famous not only for his experimental music but how he approached music altogether, through a scientific lens focusing on the acoustics and physics of sound. His most famous work is “I Am Sitting in a Room”, in which the composer records himself speaking for about 1 minute and 20 seconds. In that time he describes exactly what he does for the rest of the recording:\n\n“I am sitting in a room different from the one you are in now. I am recording the sound of my speaking voice and I am going to play it back into the room again and again until the resonant frequencies of the room reinforce themselves so that any semblance of my speech, with perhaps the exception of rhythm, is destroyed.”\n\nI have always found this piece of music particularly interesting in that it starts out so simple but gradually becomes a work of art that helps us to better appreciate the field of acoustics. We begin to understand that something usually imperceptible to the human ear becomes more and more important as we dig deeper into a space and can completely change a piece of sound.\n\nPerformance\n\nFor our group project in our class, Networked Music Performances, I wanted to see if this piece could be translated into a telematic performance. Instead of having a solo performance I wanted to see what it would be like to have the acoustics of two rooms to see what that recording would look like . We used two rooms: the SMC Portal Room and the SMC VIdeo Room. We knew that as opposed to Alvin Lucier’s long piece of art we comparatively had a limited time to create our room frequencies.\n\n\n\nBecause of this we decided to do a short snippet, a little melody less than 10 seconds long. We knew that this would greatly decrease the performance time as we were able to create more loops in the piece with a shorter snippet of sound. Once we figured out what we were going to sing we decided of the software we would use to create an networked performance of Lucier’s piece we finally decided that two programs we had been using in class were our best bets in making this piece a reality, Sonobus and Pure Data. We used the Sonobus software to send the sound from both computers connected to the interfaces of the singers over to one main computer that would then delay and send the other signal back to each of the rooms.\n\n\n\nThe main problem we faced was trying to control the sound so that there wouldn’t be too much feedback but also enough sound so that the loop would not deteriorate as time went by. Once we found a sweet spot in which this was possible we recorded our piece. Though slightly different in sound and nature than the original piece, this new piece illuminated a bridge between the inspiring work of Alvin Lucier and the future of telematic performances.\n\n\n\n",
        "url": "/networked-music/2023/12/20/karenij-sitting-in-rooms.html"
      },
    
      {
        "title": "Formatting WebPD Projects: An Introduction to WebPD, HTML and CSS Styling",
        "author": "\n",
        "excerpt": "Styling your WebPD application can lead to greater user experience and accessibility.\n",
        "content": "\n\nIntroduction to WebPD\nDeveloped by Miller Puckette in the 1990s, Pure Data is an open source visual audio programming language that can be used to create interactive computer music and other visual and audio media. While Pure Data is open-source and can be integrated with OSC apps, which can make patches more accessible to use to a larger general audience, the patches (the name for Pure Data projects) can be hard to use and distribute beyond the audio community. However, there is now a way to host Pure Data patches entirely online without users needing to ever open or install the Pure Data software: WebPD.\n\nWebPD is a compiler that takes Pure Data patches and converts audio graph and processing objects from a patch and converts them into JavaScript or AssemblyScript 3. This can then be integrated into any web application, without needing to access the compiler or Pure Data again.\n\nThe accompanying video can be found at the bottom of this blog post. This blog post will not serve as a how-to guide on using WebPD but rather a short guide on how to use HTML and CSS to customize the styling of elements you are likely to use in a WebPD application.\n\nIf you’d like to learn about WebPD you can access the WebPD Github. When using WebPD there are considerations you will want to take when building your Pure Data patch, such as only using objects compatible with WedPD. A full list of compatible objects is available on the WebPD Roadmap.\n\nHTML and CSS\nHTML stands for HyperText Markup Language. It is used to define the content and structure of web content. Using HTML you can create headings, paragraphs, range sliders, toggles and a wide variety of other objects.\n\nCSS stands for Cascading Style Sheets and it is used to style elements of web content. It can be written into the HTML portion of an application, but objects can be grouped together in an HTML sheet and then one CSS style sheet can be used to control the styling of all elements in the same group.\n\nIf you don’t already have a good grasp on how CSS and HTML work together you can access the resources below before continuing with this blog tutorial:\n\n\n  What is HTML, CSS and JavaScript? YouTube Video\n  Khan Academy HTML/CSS Unit\n  HTML and CSS Tutorial for Beginners Youtube Playlist\n\n\nBasic Styling for Your WebPD Application\n\n\n\nHeadings\nHTML\nYou can create headings on your page by tagging them in the body section of the HTML document of your application. Here is what this would look like in HTML:\n\n&lt;h1&gt;Heading One&lt;/h1&gt;\n&lt;h2&gt;Heading Two&lt;/h2&gt;\n&lt;h3&gt;Heading Three&lt;/h3&gt;\n&lt;h4&gt;Heading Four&lt;/h4&gt;\n&lt;h5&gt;Heading Five&lt;/h5&gt;\n&lt;h6&gt;Heading Six&lt;/h6&gt;\n\nAnd this is what that same code would look like on your page:\n\nHeading One\nHeading Two\nHeading Three\nHeading Four\nHeading Five\nHeading Six\n\n\nHeadings follow a hierarchy and are a key factor in web accessibility. There should be only one heading one on a webpage, the title. The other main headings in the content would all be classified as heading twos. For example, all the main headings in this blog post (Introduction to WebPD, HTML and CSS, etc.) fall under the category of heading two. Other heading and subheadings would follow suit, as heading three, four, etc. HTML offers up to six levels of headings.\n\nCSS\nA browser will automatically apply a styling to your headings, but you are able to control this using several parameters. First, you must use the class attribute (div class) to classify your headers.\n\n&lt;div class=\"header\"&gt;\n\t&lt;h1&gt;Title of Your Web Application&lt;/h1&gt;\n\t&lt;p&gt; You can write about your WebPD application here.&lt;/p&gt;\n\t&lt;h2&gt;Element of Patch&lt;/h2&gt;\n\t&lt;p&gt; Any writing needed to describe the element/&gt;\n&lt;/div&gt;\n\n\nThen, in the style portion of your main HTML document, or your style sheet referenced to in your HTML document, you can adjust the size, color, style, and padding of the headings.\tThe CSS for this could look like this:\n\n.header {\n    width: 50%;\n    height: 100px;\n    text-align: right;\n    }\n    h1 {\n        padding-top: 10px;\n        font-size: 30px;\n        color: black;\n        }\n    h2 {\n        padding-top: 10px;\n        font-size: 20px;\n        font-style: italic;\n        color: rgb(10, 10, 133);\n        }\n\nThe headers with the styling above would look like this on your webpage. This blog doesn’t support color changes, but notice the size and style differences between heading one and heading two:\n\n\n\n\n\tTitle of Your Web Application\n\n\n\tElement of Patch\n\n\n\n\n\nBackground Color\n\nThe background color of the entire webpage is another element you are able to style and can add a nice touch to your WebPD applications. This way you can customize the color of your application to the website or larger application it will be displayed to users on.\n\nCSS\nTo do this you can reference the body of the HTML document in your CSS style sheet. Any styling you make to the body section will impact the entire application. For instance, you can set a font-family for the entire application, and control the padding, or alignment for every element created in your HTML. This could look something like this in your CSS:\n\nbody {\n    background-color:rgb(210, 224, 224);\n    font-family: 'Lucida Sans', 'Lucida Sans Regular', 'Lucida Grande', 'Lucida Sans Unicode', Geneva, Verdana, sans-serif ;\n    padding-left: 15px;\n}\n\n\nIn this example, a background color has been set, a font family selected, and the padding has been adjusted to bring all the elements away from the very edge of the page. \n\n\n\n\nSliders\n\nHTML\nTo add a slider in HTML you will use insert to make a range element. An example of this is shown below. Note the “div id” attribute assigning the slider to a class. You will also see parameters available to set the minimum and maximum for the element, as well as give it an ID, which is used when making the element interactive using JavaScript.\n\n &lt;div id=\"slider\"&gt;\n&lt;input type=\"range\" value=\"0\" min=\"0\" max=\"500\" id=\"freqSlider\" /&gt;\n&lt;/div&gt;\n\nThis is what this would look like on your page:\n\n\n\n\n\nCSS\nBelow are examples of parameters you can customize with range sliders using CSS. Note the # symbol at the beginning of the block, which refers to the ID set in the HTML. Some of the most helpful parameters listed below are the border-radius, which allows you to create rounded corners, and the box-shadow, which allows you to add a drop-shadow making your slider stand out from the page. The width, height and background parameters all control the size and color of the background:\n\n#slider {\n    position:flex;top: 50%;\n    left: 50%;\n    width: 130px;\n    height: 10px;\n    padding: 30px;\n    background: #78abc4;\n    border-radius: 30px;\n    display: flex;\n    align-items: center;\n    box-shadow: 0px 5px 10px #7E6D5766;\n}\n\nAnd here is what the slider with all the customized styling above would look like on your page:\n\n\n\n\n\n\n\n\n\n\nConclusion and Resources\nUtilizing HTML and CSS to build more accessible and visually appealing WebPD applications can take your work to the next level. With the basics from this blog post you can begin to style your interactive audio applications to match the styling of your portfolio website, a client’s website, and more.\n\nIf you’re interested in seeing a video version of the post with additional information about how to style button, the accompanying video to this post can be found below:\n\n\n\n\nAdditionally, all of the code from today’s post can be found and download from my GitHub so that you can follow along. You’ll need to have access to a code editor and something to serve the application so you can see changes in real time. I like Visual Studio Code and the Live Server Extension.\n\nIf you are interested in learning more about HTML and CSS check out W3Schools.com. They offers tutorials in HTML and CSS, example blocks of code, and an interactive platform so that you are able to see how the changes you make to code in real time.\n\nHappy styling!\n",
        "url": "/networked-music/2024/03/15/julianmb-webPD.html"
      },
    
      {
        "title": "Introduction to Open Sound Control (OSC)",
        "author": "\n",
        "excerpt": "This post contains a brief overview of OSC and a tutorial on how to make a connection and send data between devices.\n",
        "content": "Introduction\nOSC - Open Sound Control. Like MIDI, but better. Below you will find some key knowledge to get started working with OSC and a build along in Max to make the start of a gyroscope_powered wireless musical controller.\n\n\n\nSo what is OSC?\nOSC is a protocol used for sending data between devices over a network. It was developed by Matthew Wright and Adrian Freed at CNMAT (Center for New Music and Audio Technologies) at Berkeley, California, in 1997. They describe OSC as “efficient, transport-independent, message-based protocol developed for communication among computers, sound synthesizers, and other multimedia devices.” (1)\n\nLike it’s older brother - MIDI, OSC was designed for use in music performance contexts but has become popular among a wide range of disciplines. Just a few examples of softwares with OSC capability include Ableton Live, Reaper, Max, Pure Data, QLab and Unreal Engine.\n\nOne clear advantage of OSC over MIDI is that it sends data over network so we can make wireless controllers and that’s what we’re going to do today but first we need to look a little deeper at the types of data messages OSC can handle.\n\nThe data\n\n“The arguments to OSC messages can be strings, 32-bit floating point or integer numbers, arbitrary ‘blobs’ of binary data, or any of a dozen optional data types.” (2)\n\nThis is much more variety of data than is sent via MIDI. A 32-bit float for example is a 7 digit number with a decimal place, allowing us to create much smoother transitions when we turn a dial. In comparison, MIDI sends integers (whole numbers) on a range between 0 and 127. For many uses, the degree of control that MIDI gives us works fine, but in some cases, we may need a higher resolution of control. The YouTube channel ‘ZEAL CO - Creative Research Operation’ demonstrates this very will in their video titled ‘Introduction to Open Sound Control (OSC)’ by changing colour hue using dials representing the accuracy of MIDI and OSC messages (3).\n\n\n\nAnother advantage of OSC is being able to label our incoming messages however we like. In MIDI, CC (control change) messages are given a number as an ID but this number doesn’t mean a lot to us, especially when we start to create more complex mapping of our controls. OSC allows us to instead give a parameter a text name so we can more easily identify it later on. “… a message to set the frequency of the fourteenth oscillator in the third additive synthesis voice might be named ‘/voices/3/osc/ 14/freq’.” (2.)\n\nOSC messages are made up of two parts, the Address Pattern and the Arguments. The address pattern can be thought of as the ID of the parameter and the argument is the value or message it sends. This knowledge will be important later on when we start to route messages for mapping to different parameters. Here we can see an example of the address pattern argument.\n\n\n\nBuilding a wireless controller\n\nThe following guide is based on an example of a motion-controlled musical instrument. An iPhone app called GyrOSC sends out motion data from the gyroscope via OSC messages which are received by Max - an OSC capable visual programming language. Although you may not have access to these exact tools, most of the terminology and processes will be very similar across platforms. Check the documentation of your software for more information about how to implement OSC.\n\nStep 1 - Making a connection\n\nOpen up the app on the device you want to send from. It requires two bits of information - a target IP (can be called host IP) and a port number.\n\nThe target IP is simply the IP address of device you want to send the message to. On my MacBook, I can go to settings, network, click on the network you are connected to and click details to see your IP address.\n\n\n\nIf the IP address is like a street address (42 Main Road, Townsville etc) then port number is essentially the apartment number within the building you are trying to send the message to.\n\nSome applications have specific port numbers which are usually predefined, but we can often specify our own port number. Computers are often already using some port numbers, so it’s best to aim above about 5000.\n\nIf you are using a public WiFi network, such as ‘eduroam’, these sometimes have restrictions on lower port numbers but it can work to set the port number to a value above 31000.\n\nStep 2 - Receiving messages\n\nNow we can open Max and create the [ udpreceive ] object. This is the object that will receive the message but it need to know which port number to listen too. To this by banging the message ‘port’ followed by your port number into the [ udpreceive ]. Connect a message box or [ print ] below the [ udpreceive ] to check message are coming in. It can be helpful to connect a button which gets banged every time a message is received.\n\n\n\nStep 3 - Handling the data\n\nNow you have data coming in but everything is traveling down the same channel. We first need to separate messages with different address patterns so messages go to the correct parameter and secondly we need to separate the address patterns from the arguments themselves. The [route] object luckily does both of these tasks.\n\n\n\nIn my case, both gyroscope and accelerometer have 3 arguments (one for X, Y and Z) as a list which can be separated using the [ unpack ] object.\n\nNow you can begin to process these arguments and map them to whatever parameters you like.\n\nStep 4 - Improving your device\n\nDepending on network conditions, there can be significant jitter in the data stream received in Max. Below are results from testing GyrOSC at different sampling rates on my home network compared to a clock pulse. To mitigate the jitter, you can explore using a buffer to store samples and request them at a regular time interval or limit the speed of data coming through the chain. You may also want to consider smoothing the data using a [line] or similar object if argument values jerk around.\n\n\n\n\n\nCheck out this accompanying video for a build-along in Max!\n\nSources\n\n1 - Wright, M., &amp; Freed, A. (1997, September). Open soundcontrol: A new protocol for communicating with sound synthesizers. In ICMC.\n\n2 - Wright, M. (2005). Open Sound Control: an enabling technology for musical networking. Organised Sound, 10(3), 193-200.\n\n3 - ZEAL CO - Creative Research Operation - Introduction to Open Sound Control (OSC)\n",
        "url": "/networked-music/2024/03/17/thomaseo-intro_to_OSC.html"
      },
    
      {
        "title": "Digital Signal Processing Basics",
        "author": "\n",
        "excerpt": "Virtually any song you can listen to on the radio has examples of Digital Signal Processing.\n",
        "content": "\n\nWhat is Digital Signal Processing?\n\nVirtually any song you can listen to on the radio has examples of Digital Signal Processing.\n\nBut let me not get ahead of myself, let me start with a simple definition of what Digital Signal Processing (or DSP for short) is:\n\nDigital Signal Processing (DSP) -the manipulation, analysis, and transformation of signals, which are typically represented as sequences of numbers, using digital computing techniques. It involves applying mathematical operations to these numerical representations to extract information, enhance features, or modify the signal in some way to achieve desired outcomes.\n\nSince this definition is on the wordier side, let me break it down a little bit for you. Let’s start with the word signal, when we talk about audio signals, we define it as a detectable physical quantity or impulse (such as a voltage, current, or magnetic field strength) by which messages or information can be transmitted.\n\nIn this case the information we want to manipulate in DSP are the binary representations of the analog audio signal. This is because the modern computers you use every day to perform actions such as DSP need discrete values, unlike the infinite values you find in analog signals.\n\nThis is where an analog to digital converter comes in.\n\nADC and DAC\n\n\n\nADC, a bilingual interpreter who can speak both languages, takes in the analog signal from something such as a microphone and converts it to a digital signal, the language of the computer. It does this by extracting tiny sliced samples thousands of times per second, the rate at which is done is called the sampling rate. This data can then be together and spit out the other side to be translated in the reverse back into analog with the help of ADC’s opposite, the Digital Audio Converter (DAC). In between these two processes, we have a lot of creative freedom on what we can do to manipulate these samples.\n\n\n\nThis in between processing is DSP.\n\nAlthough we talk about DSP primarily in between the ADC and DAC there is still some DSP happening within the ADC and DAC tHemselves. Within the ADC, there sometimes is an antialias filter and within the DAC there is sometimes a reconstruction filter.\n\n\n\nThe other definition that is commonly used for DSP is the process of analyzing and modifying a signal to optimize or improve its efficiency or performance.While you can use DSP to achieve the results, this isn’t the only thing it can be used for, Sometimes you just want things to sound cool ,Sometimes you want things to sounds weird, It’s all up to you and your imagination!\n\nHere are just Some of the things you can do that are considered DSP:\n\nCompression:\n\n\n\nControls the dynamic range of a sound. This means that it decreases the variance between the highest and lowest amplitudes of an audio signal by either reducing the loudest parts aka downward compression), or increasing the quietest parts aka (upward compression). This helps to make sure all the desired notes in an audio recording can be heard clearly.\n\nFiltering:\n\n\n\nA filter lowers or completely cuts unwanted frequencies from an audio signal. This can be done to a large amount of frequencies such as a High Pass Filter, which cuts lower frequencies and preserves the higher ones, or the Opposite called a low pass filter which cuts higher frequencies and preserves lower ones\n\nWe also have band pass filters which only allows frequencies \nwithin a certain range to pass through and band stop filters in which everything is passed through except the band of frequencies around the desired cutoff.\n\nModulation:\n\n\n\nModulation is a process of varying an aspect of an original signal by another signal. There are two main kinds of modulation called Amplitude Modulation and Frequency Modulation.\n\nAmplitude modulation is when the amplitude of the carrier wave is modified in order to send the data or information.\n\nFrequency Modulation is when the frequency of the carrier wave is modified in order to send the data or information.\n\nConclusion\n\nI hope this blog post helped you learn a little something about DSP.\n\nNow go forth and make weird sounds!\n\nSources\n\n1 - Analog Devices -  A Beginner’s Guide to Digital Signal Processing (DSP)\n\n2 - Zola, Andrew -  TechTarget - Digital Signal Processing (DSP)\n\n3 - Geeks For Geeks (2024)- Difference between Amplitude Modulation and Frequency Modulation\n",
        "url": "/networked-music/2024/03/18/karenij-DSPBasics.html"
      },
    
      {
        "title": "Using convolutional neural networks to classify music genres",
        "author": "\n",
        "excerpt": "When classifying genres in music, CNNs are a popular choice because of their ability to capture intricate patterns in data.\n",
        "content": "When classifying genres in music, CNNs are a popular choice because of their ability to capture intricate patterns in data.\n\nSo, how can we implement a CNN of our own? Well first of all, the key to any successful machine learning project is the data that we base our work on. For music genre classification, one popular choice is the GTZAN Dataset. The GTZAN dataset is a collection of 1000, 30 second song snippets, divided into 10 different genres. Perfect for our task.\n\nThe next step is to prepare each audio file to be used with our CNN. We don’t want to pass the CNN entire audio files. As you’ll remember, CNNs are effective on images. Therefore, we want to faithfully represent our audio data as image data. 2 ways to do this is by extracting the MFCCs-, or by extracting the Mel spectrograms of the audio files. For this project we will extract both MFCCs and Mel spectrograms, and compare the results when using either one.\n\n\n    \n    Example Mel spectrogram on the left, MFCC on the right\n\n\n\nExample Mel spectrogram on the left, MFCC on the right\nTo visualize the difference in MFCCs and in Mel spectrograms from genre to genre, I have added every MFCC and Mel spectrogram in each genre together and extracted the averages. For the MFCCs, every genre has it’s characteristics and nuances and some genres are more distinct than others. Metal is easily distinguishable from the rest with it’s blue rows, while classical also stands out with it’s almost undefined individual rows. The Mel spectrograms also has their nuances. Again, classical stands out with less activity in the lower frequencies than the rest of the genres.\n\n\n    \n    Average MFCCs for every genre on the left, Mel spectrograms on the right\n\n\n\nAverage MFCCs for every genre on the left, Mel spectrograms on the right\nAs we can see, there lies some differences in the MFCCs and Mel spectrograms from genre to genre, that our CNN will hopefully be able to pick up on.\n\nLet’s move on to see how the program itself works. At the core of our program is the model. A single model can be trained on the dataset in less than a minute. One model is the result of many parameters and choices that can be tweaked and tuned to improve performances. However, doing this manually can be tedious. Therefore, we implement something called grid search to optimize parameters. Grid search works by defining a grid of parameters, simply put a list of parameters, each containing a list of values to choose from. Our goal is to find the best combination of values for every parameter. We can achieve this by working through the parameter grid in a loop, and keeping track of our results.\n\nAlready, we are training a lot more models. In fact, because we have 2 different types of features (MFCCs and Mel spectograms), 2 different types of CNN architectures and 2 different types of batch sizes, we end up training 8 models to find the ideal one. Already a substantial increase in training time, but it gets longer.\n\nWhenever we train a model, we can get lucky and we can get unlucky. If we ran 8 models one day, we would probably not get the same results when running the 8 models the next day. Therefore we introduce something called stratified k fold. Stratified k fold allows us to re-run a model, with different portions of the audio files each time. In the case of our program, we run the models 5 times, and retrieve the average accuracy of every model. This serves as a much more reliable way to assess the performance of models. The trade-off is that the training now takes 5 times as long, in total 40 times as long as our original single model.\n\nNow that we have the most important pieces of our program, let’s run it and explore the results. The first thing we are met with after training the models is this table:\n\n\n    \n    Average MFCCs for every genre on the left, Mel spectrograms on the right\n\n\n\nIt contains all 8 combination of parameters, and their respective average accuracy. The first thing we can see is that in this program, using the MFCCs performs consistently better than using the Mel spectrograms. The second thing we can see is that the deeper CNN actually performs worse than the basic CNN. However, the accuracy is only a small part of the results, and doesn’t tell the whole story by itself.\n\n\n    \n    Loss function for best performing model on the left, worst performing model on the right\n\n\n\nLoss function for best performing model on the left, worst performing model on the right\nAbove is the loss function from the most and least accurate models. The loss function tells us something about how good the model’s predictions are. When the loss is low, the model should be able to consistently predict the right genre. One important difference in the loss functions is that in the least accurate model, the gap between the red training lines and the green validation lines grow bigger over time. This is a sign that the model is only improving on the exact data it’s training on. When presented with other data, the model will perform worse.\n\n\n    \n    Loss function for best performing model on the left, worst performing model on the right\n\n\n\nConfusion matrix for the models with the best combination of parameters\nHere is the confusion matrix for the best model. The confusion matrix simply shows what genres audio files are predicted as. There is a clear diagonal pattern, which is a good sign, but there are also many misclassified genres, leading to a less than optimal result. If we look at the individual genres, we can see that classical performs very well with a 92 % accuracy, followed by metal with a 73 % accuracy. These 2 genres also happen to be the most visually distinct genres if we look back at the average MFCCs and Mel spectrograms. On the other side, rock is the worst performing genre by far. It is one of 2 genres to be misclassified as every other genre at least once, and is consistently misclassified as country and blues.\n",
        "url": "/machine-learning/2024/04/28/eastorks-classify.html"
      },
    
      {
        "title": "The Chiptransformer",
        "author": "\n",
        "excerpt": "The Chiptransformer an my attempt at building a machine learning model using the transformer architecture to generate music based on a dataset of Nintendo NES music.\n",
        "content": "Early to mid 1980s game consoles and home computers often included a sound chip that could produce a few basic waveforms, like pulses and sawtooths. The style of music that emerged from this period is often referred to as chip music. The Chiptransformer is an attempt at building a machine learning model using the transformer architecture to generate music based on a dataset of Nintendo NES music.\n\nThe transformer is a machine learning architecture commonly used in language processing. While not particularly intuitive to understand, transformers have a number of interesting characteristics and possibilities that I was curious to explore. In particular I wanted to try some ideas concerning the data representation and the ability of the transformer to make sense of musical data. I’ll first give a short overview of how transformers work, and then talk about what I wanted to accomplish.\n\n\n    \n    Transformer architecture\n\n\nTransformer attention\n\nTransformers do not process sequences step-by-step like Recurrent Neural Networks (RNNs) do. Instead, they read and process whole sequences all at once. The way transformers model relationships between items in the sequence is called attention. Attention roughly works by creating a table of similarity scores between each item pair in the sequence. The important part is that the transformer in this way can look for patterns at many different levels of abstraction in the sequences and encode those patterns using attention.\n\nPositional encoding\n\nFurthermore, transformers don’t actually know the order in which items are supposed to occur. To give the transformer this information, we create positional encodings, which are often sine and cosine values for different frequencies, and then add these to the vector values of each item. This means that the item vectors will have unique values based on their position in the sequence.\n\n\n    \n    Position encoding of notes\n\n\nEncoders and decoders\n\nTransformers first got their name because they were used for translation, by transforming a sequence into another via an encoder-decoder structure. However, for sequence generation we need only a single transformer block, called a decoder. This takes a sequence as input, and then produces an output sequence which is identical to the input sequence, only shifted one position. The important part here is that each item vector needs to keep all the contextual information from the preceding sequence of items that the model thinks is important. Therefore these vectors need to be fairly big, since there is potentially a lot of information that they need to keep track of. To make predictions then, we first need to look at the last item of the output, then place that item back at the end of our input sequence, and then run the prediction machine again.\n\nThe NES music database\n\nSo why did I choose game music from the NES? Firstly, I thought it would make some things a bit simpler. I wanted to generate polyphonic music, but I also wanted to keep the complexity low. There are many datasets available for classical music, but I was afraid they would be too complex. The NES on the other hand has only three voices for melodies and one noise channel for drums. Furthermore, old game music tends to be very structured, with simple rhythms and repeating melodies, which should make it easier to distill some general patterns from the data. Second, having grown up in the 1980s and 90s I also have a soft spot for this kind of music. It often reminds me of having fun with old computers in my childhood.\n\nNote pitch representation\n\nThe first idea I wanted to try was using regression rather than classification. Transformers normally use something called tokens as inputs, which are simply categorical values. For the transformer to make sense of this data, tokens pass through an embedding layer, which converts each token into a vector with a lot of dimensions. The reason is that these vectors need to reflect all the contextual relationships between the tokens. With categorical input and output, prediction is also a classification task. However, I think it would be a good idea to use the numerical information we have if we encode the pitch, onset time and duration as numerical values. Therefore, I decided to make my transformer use continuous input and output rather than tokens, which meant prediction would become a regression task rather than a classification task.\n\nI think this opens up a lot of possibilities. When trying to create note representations that could make sense to my transformer, I was thinking about how musical pitch is circular in nature: When we move from a C up on a piano, we end up at another C, one octave higher. To create a representation for this, I decided to add two features for each note: I considered each note in the chromatic scale a point distributed around a unit circle, and added the sine and cosine values to the note features. Another idea was to add similar circle-of-fifths values to the notes, to increase the harmonic relationships between notes, but I didn’t get around to implementing that.\n\n\n    \n    Circular pitch\n\n\nTimestep based position encoding\n\nI also wanted to give the model timing information that was more rhythmically relevant. Positional encodings usually indicate each item’s position in the input sequence. In a chord all notes are played simultaneously, but they must be fed to the transformer in a sequence. I first added an “onset delta” to show the time difference between two subsequent notes and counters to indicate where in a bar a note would fall. Then I decided to try changing the positional encodings to represent one timestep of music (which corresponds to a 32nd note), rather than just a sequence index. So if several notes land on the same timestep, they will receive the same positional encoding, and the difference in encodings show the difference in time between notes. My intention was that this would give the model more meaningful timing information that would eventually create more rhythmically coherent output.\n\n\n    \n    Position encoding timestep\n\n\n\n    \n    Position counters\n\n\nExamples\n\nTo conclude, I had a lot of fun working with the Chiptransformer. I learned a lot and I felt I could use my creativity when implementing things the way I wanted. Unfortunately the outputs I generated were far from as meaningful as I hoped, which was a bit frustrating. Still, I am motivated to continue working on this project to make it work better.\n\n\n    \n    Example audio output piano roll 1\n\n\n\n    \n    Example audio output piano roll 2\n\n\n\n    \n    Your browser does not support the audio element.\n\n",
        "url": "/machine-learning/2024/04/28/sondrerk-chiptransformer.html"
      },
    
      {
        "title": "Connecting Eigenrhythms and Human movement",
        "author": "\n",
        "excerpt": "Connecting Machine Learning and Human understanding of rhythm.\n",
        "content": "This blog post explores a machine learning pipeline designed to classify music genres based on MIDI drum patterns and how to relate it to human movement.\n\nTwo different CNN models is proposed that aim to discern rhythm data from the accelerometers in smartphones, bridging the gap between music analytics and human movement. Much like how we instinctively nod along to the groove of a song, this project seeks to delve deeper into this intuitive interaction.\n\nThe objective here is not just to identify well-known rhythmic patterns but also to provide insights into how these rhythms are physically executed. Exploring the “feel” of a genre’s groove to see if machine learning can enhance our understanding of, and interaction with, musical rhythms.\n\nA lot of the work done is based on the idea of “eigenrhythms”, which by John Arroyo et. al, is defined as an high dimensional rhythm pattern, which has been reduced to a lower-dimensional representation [2].\n\nBy using movement data, in this case provided by a phone’s accelerometers, we can then try to create a one dimensional rhythm and see what genre the model would classify it as. In the video we can see how the accelerometer data is generated, and in figure 1 we can see what genre one of the model classified my rhythm with.\n\n\n  \n    \n  \n    Video 1: Showing what gets analyzed\n\n\n\n\n    \n    Figure 1: Results from PCA-CNN Model trained on 10 Genres analysing the movement from Video 1\n\n\n\nDataset\nThe dataset used is made by Magenta [1] and contains over 1,150 midi files, performed by 10 different drummers playing up to 17 different genres. The dataset is however skewed where there is definitely most rock, jazz and latin songs, which does effect the result somewhat.\n\nData processing\nThis pipeline uses signal processing and machine learning techniques, including Principal Component Analysis (PCA) and convolutional neural networks (CNNs).\n\nThe first step of preproccesing the data is sequencing the midi data to not overload the CNN with too much data at once. As the goal is for the model to recognize temporal patterns, I have segmented each midi file into 2 bars with 32 timesteps. We can see one such representation in the picture here, showing 22 drum instruments, over 32 timesteps.\n\n\n    \n    Figure 2: Piano roll example of 22 drum instruments, 32 timesteps\n\n\nPCA allows for these methods allow the system to effectively reduce dimensionality and highlight critical features within the data. It works by identifying the axes (principal components) along which the variance of the data is maximized.\n\nIn figure 3 we can see the same drum roll in figure 2after it has been reduced to a one dimentional eigenrhythm using the first (best) principle component.\n\n\n    \n    Figure 3: CNN input after figure 2 has gone through PCA layer\n\n\n\nAnother method introduced is Linear Discriminant Analysis (LDA). It is employed in machine learning and statistics to improve class separability by finding a lower-dimensional space that enhances the distinction between class means while minimizing in-class variance. This layer could further help discriminate between genres.\n\nCNN\nFirst a baseline CNN model was made. It trained on several thousand segments with 22 instrument dimensions over 32 timesteps as in figure 2, to recognize the statistical patterns in the temporal dimension of the midi signal.\n\n\n    \n    Figure 4: Normalized confusion matrix for CNN model trained on 10 genres\n\n\n\nTwo eigenrtyhm models were proposed to expand on the baseline CNN model. One model used a PCA layer on each timestep, finding statistical variations for each timesteps, reducing every step down to 1 dimension as seen in figure 3. The other used a global PCA layer to reduce down to 5 dimension, and then a LDA layer to further reduce down to 1 dimension.\n\n\n    \n    Figure 5: Normalized confusion matrix for PCA CNN model trained on 10 genres. One can see it has lower scores and skewes more towards rock\n\n\n\n\n    \n    Figure 6: Normalized confusion matrix for PCA LDA CNN model trained on 10 genres. One can its similar accuracy to the PCA CNN model\n\n\n\nThe eigenrhythm models definitely decreased the accuracy compared to the baseline CNN. Which is not strange as there are now 21 less dimensions the CNN can train on. The change is however only around 5–10% depending on how many genres the model is trained on.\n\nIf only the classification was in focus, the pure CNN model might still have the best results, but to explore the eigenrhythms and relate it to the movement data, it is imperative to reduce the dimensions down to 1 to be able to compare the data.\n\n\n    \n    Figure 7: 3D PCA Analysis\n\n\n\nUsing repeated k-fold analysis the different models, I got these results:\n\n\n    \n    Figure 8: Repeated k-fold accuracy results with 10 folds repeated 5 times\n\n\n\nConnecting to movement data\nFor accelerator data, I have used an app called phyphox (but any similar app would work), which simply records accelerator data from my phone, and exports it as a CSV file (Type:Comma, decimal point). When loading the data, it puts it into an empty 22x32 matrix, and then uses the same PCA filter as the model is trained on to reduce it down to 1 dimension again, and then feed back into the PCA-CNN or PCA-LDA-CNN model to make an prediction. It does however matter what type of data the model is trained on.\n\nAfter having trained the model on the chosen genres, I then wanted to see how this relate to how we humans would “feel” the rhythm of a genre. Since the model is trained on a large viarity of samples, it could be able to identify some of the same patterns. If the model is only trained on jazz &amp; rock, the model would simply output if they think your rhythm is more alike rock, or more alike jazz. The model utilizes a softmax filter at the end which simply picks the genre based on the highest probability. So if it was very unsure, but thought it was 51% rock and 49% jazz, it would say that it think your rhythm is rock with 51% confidence.\n\nHaving more genres increases the possibilities, but having consistent larger than 1/n in predicted outcome, where n is number of genres, would imply the prediction is based on more than just pure probability.\n\nConclusion\nThe work here is a continuation on the idea of “eigenrhythm”, and to see if there is a connection to how we “feel” rhythm. There are a lot of variables that could be tweaked to improve all the aspects of the pipeline, and even more that could be added. Using a mocap suit would increase the number of movement dimensions available, increasing the number of dimensions the model can be trained on.\n\nIt can have practical applications of drum training tools. By tailoring the training datasets to specific needs — whether different musical genres or distinct styles within a band’s discography. This approach not only allows for genre-specific training but also enables exploration of diverse rhythmic “grooves,” potentially changing the way we understand and teach musical rhythm.\n\nSources\n\n[1] https://magenta.tensorflow.org/datasets/groove\n\n[2] https://academiccommons.columbia.edu/doi/10.7916/D8PG2230Links\n\n[3] https://www.mdpi.com/2227-7390/10/23/4604\n",
        "url": "/machine-learning/2024/04/28/olavus-eigenryths.html"
      },
    
      {
        "title": "AcidLab: deep acid bass-line generation",
        "author": "\n",
        "excerpt": "Using machine learning to generate acid bass-line melodies with user-created datasets.\n",
        "content": "Introduction\nAcidLab is a machine learning system for symbolic music generation. It composes melodic midi loops for use in dance music genres such as bass-lines in acid house. One of the key points about this system is its use of small, user-composed datasets. Traditionally in machine learning, ‘more is more’ when it comes to dataset size, but AcidLab explores the viability of using small data sets in this context. This blog post will describe the machine learning techniques used, data flow of the system and discuss the results of a comparative study looking at the ability of AcidLab to learn harmonic structures from different sized datasets.\n\n\nWhy use a small dataset??\nSo why should we use small, user-composed datasets in the first place? Well, AcidLab is a tool for musicians to embellish their own ideas rather than simply producing new ideas based on hundreds of files downloaded from the internet. This gives the artist more ownership over the outputs of the system. Also, if the datasets are user-composed, then they have to be small; if the objective is to generate variations of a musical motif, this goal is achieved without the need of a machine learning system.\n\nThis does come with some disadvantages. Even a dataset of limited size does take time to curate and prepare. Not to mention the fact that if we give less data we give to the model, the outputs will be less varied. AcidLab attempts to mitigate some of these disadvantages by using a probability-based model and dataset augmentation techniques.\n\n\nThe Variational Autoencoder\n\nAcidLab uses a Variational Autoencoder (VAE) to generate symbolic music patterns. VAEs are a type of Artificial Neural Network (ANN) and consist of two main parts - an encoder and a decoder.\n\nThe encoder performs dimensionality reduction. It takes high dimensional features and reduces them down to a smaller number of dimensions. The small layer at the end of the encoder is called the encoded vector, or ‘bottleneck’, and the data inside this layer, the lower-dimensional representation of the input data, is referred to as the latent space. Below is an diagram of a traditional Autoencoder.\n\n\n\n\nThe decoder is a mirror image of the encoder. The low-dimensional representation is converted back to it’s original dimension. This type of model is often used to repair corrupted images or audio.\n\nWhat makes the VAE different from a basic autoencoder is the introduction of probability to the latent space. Instead of being deterministically mapped in the latent space, values are now mapped to a distribution, typically a Gaussian distribution. Once this model is trained, the latent space distribution can sampled. Values which represent this distribution can be feed to the decoder, generating completely new outputs which ‘look like’ the training data.\n\n\n\n\n\n\nBuilding a dataset\n\nTo test initial designs of AcidLab, I created a data set of 50 examples. I used an Arturia KeyStepPro sequencer to generate and edit patterns and Ableton Live to record the midi data. All patterns begin on midi note 36 and last for 1 bar of 4/4. Before exporting each clip to a midi file, the notes were quantised on a 16th note grid and the tempo was set to 120 bpm. The whole process took approximately 90 minutes. Below you can see one example pattern inside the piano roll editor in Ableton Live.\n\n\n\n\nYou can listen to some examples from the dataset here:\n\n\n    \n    Your browser does not support the audio element.\n\n\n\n\nData processing\n\nOnce a data set was created, it required processing to be interpreted by the model. The MIDI files were imported to a python script and a piano roll extracted using the pretty_midi library. Velocity values are replaced with 1s and the upper register of notes are removed, leaving a 3 octave range from midi note 36 to 73.\n\n7, 9 and 10 step patterns are created by replacing notes with zeros. These pattern lengths are commonly used in acid house music for creating polyrhythms. This dataset augmentation technique is known as cropping.\n\n\n\n\n\nDesign and Implementation\n\nUsing Tensorflow and Keras, I implemented a VAE architecture in python. Although I experimented with different numbers and sizes of hidden layers, every input layer contained 592 neurons, one for each ‘pixel’ of the example data. Here is a diagram of one tested architecture:\n\n\nThe model was trained on the dataset in the same way as a traditional autoencoder, where the goal is to reconstruct the input at the output. Here are some of the results from early testing.\n\n\nThen, randomly generated values within the boundaries of the latent space distribution are fed to the output, generating images that look similar to the inputs. These images are then processed to create monophonic patterns. The image below shows a before and after of this processing.\n\n\n\nNow the piano roll images are converted back into MIDI files to be approved by the musician.\n\n\n\nComparative Study\n\nA study was conducted to see how well AcidLab learns harmonic data from datasets of differing sizes. For the study, datasets were created using a combination of the original 50 examples dataset and outputs generated by the model during the design and testing process. Sizes of dataset tested were 10, 20, 50 and 100 examples and there were 2 different harmonic structures, minor and Phrygian mode - totalling 8 datasets.\n\nThe results of this study found that AcidLab does learn the harmony from its given dataset. Often 10 example datasets gave the best results. This can be expected because there is less data to choose from. Below shows a Pitch Class Transition Matrix (PCTM) for the minor 10 and 25 example datasets. PCTMs describe the relationships between different notes and are a way of displaying the harmonic relationships in a song.\n\n\n\n\nSummary\n\nIn summary, I believe there is cause for further exploration in this area. With a better model, AcidLab could become a like a jam partner that gets better the more you play with them. Possibilities for live performance, if a transition to real-time processing is made, beckon. In the meantime, here’s some outputs from the modal harmony dataset with some drum backing, enjoy!\n\n\n    \n    Your browser does not support the audio element.\n\n\n\n\n\n\n\n\nSources\n\nAutoencoder diagrams from this article on Towards Data Science\n\nCover image by imustbedead\n",
        "url": "/machine-learning/2024/04/28/thomaseo-AcidLab.html"
      },
    
      {
        "title": "Playlist Data and Recommendations Using Artificial Neural Networks",
        "author": "\n",
        "excerpt": "a machine-learning algorithm that pairs independent artists, with curated playlists that best fit based on musical attributes\n",
        "content": "Concept\n\nThe usage of playlists on music streaming platforms, such as Spotify, has become an effective outlet for artists to be showcased to build their fanbases.  But with the over-saturation of artists trying to build their fanbases,\n many do not know which curators would suit them best promotion.With the trend of music listening gravitating towards these platforms, several music-focused businesses, such as radio stations and music discovery sites, have begun \n keeping their audience base and relevancy by curating regularly updated playlists of songs, featured in their respective shows, on Spotify.  I will be discussing a machine-learning algorithm that pairs songs by independent artists,\n  trying to gain a reputable fanbase ,with Non-Spotify curated playlists that best fit their musical profiles based on Spotify’s organization of musical attributes.\n\nData Mining\n\nI initially utilized a platform called Chartmetric which provides insights into the top playlists on Spotify.  Upon further investigation, I discovered many of the most popular playlists were used by artists who wanted higher streaming\n numbers would turn to these playlists and pay for their songs to be featured. Many of these playlists seemed to be streaming bots, and not real people. I decided to explore more consistently established and reputable sources when trying\n  to showcase music to a broader audience. This led me to explore playlists created and curated by popular music stations, music magazines, and similar sources, well known to avid music listeners for the discovery of new releases.\n\nExtraction of Features\n\nThe criteria for the playlist I used in my initial dataset consisted of the following: a focus on independent artists, a following of over 20,000, around 100 songs, regularly updated (additions in the past month), and mostly new releases all from the past month. Once the five playlists were chosen, I then processed the playlist in the Spotify Playlist Analyzer, an online tool that extracts data from Spotify playlists using the Spotify Web API and can convert the data into a CSV file. I used the Spotify Playlist Analyzer because I wanted to work with easily accessible data,  particularly for playlists that are updated regularly, perhaps even daily.\nThe analyzer extracts the musical attributes calculated by Spotify for various factors in the audio with scores of 1-100. These factors include Happiness, Danceability, Energy, Acoustic-ness, Instrumental-ness, Liveliness, and Speechiness. The analyzer also extracts other musical data such as Tempo, Key, and Length, Genre, and Parent Genre. I extracted unwanted columns and gave each song a label based on their respective playlist.\n\nThe labels for the playlist are as follows:\n0 - Indie Pop / Rock Playlist | BIRP! April 2024\n1 - Songpickr 2024 - Best New Americana, Alt Country, Folk, Blues, Retro Soul, Rock Music\n2 - Pigeons &amp; Planes\n3 - UKF Drum And Bass\n4 -Future House 2024 \n\nTokenizing Parent Genres Labels\n\nI decided on the MinMax Scaler Method over the Standard Scaler as the normalization  technique. The MinMax Scaler is more suitable for algorithms, like ANNs, that require features to lie on a similar scale and performs the best on classifying problems (Raju 2020).  I also performed Principal Component Analysis (PCA), the most popular form of dimensionality reduction which retains a lot of the variance in the data more compatible with the neural network (Sorzano). The scaler and PCA processing was done before concatenating this data with the tokenized data of the Parent Genres. Tokenized data, while a numeric value, does not hold value in the way the musical attribute data does and therefore could cause problems. This is also the reason why I decided not to go with the pipeline in this model because these two kinds of arrays must be kept separate before the dimensionality reduction process. \n\nTokenizing Parent Genres Labels\n\n As I wanted to use the associated parent genres as a feature as well I had to tokenize each type of Parent genre and assign them a number so that they could have a numeric value that would be compatible with the numpy array. After this process, I concatenated the two arrays with different data types so that the data was finally ready for processing. I then extracted the label data and split the dataset for training and validation\n\n\nThe Model\n\nOnce the playlist data was extracted and labeled in this way I was able to start with the formation of the machine learning algorithm. For the classification process, I decided to employ an Artificial Neural Network (ANN) as my model of choice.  I went with the High-Level API integrated into TensorFlow, known as Keras API, because of its user-friendly interface that provides easy-to-use tools to make the process of creating a neural network efficient. \n\nSoftmax Layer and Probability\n\nThe reason for this decision was the ability of ANNs to analyze the data after the softmax layer in TensorFlow.  The softmax layer normalizes the scores and shows us the probabilities for each outcome.  This provides not only a single label, for the best fitting playlist, but also the probabilities the machine learning model gave to the other labels, which could be useful to artist wanting to submit to multiple playlists. ##\n\nReLU\nI experimented with the different kinds of activation layers to see if I got different results. Ultimately I found that an input layer with a rectified linear units (ReLU) activation plus two hidden layers with ReLU activations yielded the best results.  ReLU function is the most used and best activation function for deep learning tasks (Krishnamuthy 2024). When I ran the model,  the default Adam optimizer yielded the best results for my model. \n\nResults\n\n\n\nFor analysis of the machine model, I used multiple tools to understand what was happening as I continued my process. I examined the test accuracy score as well as the F1 score to tune the parameters as well as the model accuracy chart to examine how many epochs the model went through before termination. I also used the confusion matrix to examine which classes were mostly being confused with another. \nThe average test loss for this model over 5 tests was 1.098 telling us that there is room for improvement of the accuracy of the model.\n\n\n\nThe average test Accuracy score was 0.732 as well as the average F1 Score was 0.731. Since this dataset is pretty evenly distributed, I didn’t expect a huge difference between the accuracy score and the F1 score. The model accuracy charts show the validation curve diverging from the training curve, suggesting some overfitting when it comes to the training data.\n\n\n\n The majority of the misclassified data for the first two similarly focused playlists were from each other.  For the “Birp!” Playlist, 19 of the  40 misclassified songs had a True Label of SongPicker. Songpicker had a total of 29 misclassified songs and 25 of these songs were that of the “Birp!” playlist. The Pigeons and Planes playlist also had a majority of 15 out of 25 mislabeled songs that have a true label of SongPickr. Unsurprisingly, the same trend was found with the two majority Electronic Playlists. The UKF playlist had a majority of 9 out of 13 songs mislabeled as Future House, and the misclassified files of the Future House playlists were exclusively labeled as UKF. This checks out as songs in the UKF Playlist were almost exclusively within 1 number away from 174 or 84 bpm. In looking at this chart, I also note that the playlist “Birp!” has an almost equally varied group of misclassified files when it comes to those that do not belong in the SongPickr Playlist suggesting that the “Birp!” playlist is more varied in the metrics of the features we use in this model.#\n\nConclusion\nThis model provides us with a framework that does not do the job for the curator but rather makes the process easier on both ends of the curating process. This, however, is still a limited view of the song data and must be coupled with the work it takes a curator to complete their work. We must also keep in mind other unexplored factors such as song order in the playlist itself which could affect exposure. Overall, this project has offered insight into creating machine learning models that work with people and not in place of people and how that can provide those, who aim to share their music with potential fans, some help along the way.\n\n\nBibliography\n\nAguiar, Luis. “Platforms, Promotion, and Product Discovery: Evidence from Spotify Playlists.” Working Paper. Working Paper Series. National Bureau of Economic Research, June 2018. http://www.nber.org/papers/w24713.\n\nAjay, Muktha Sai. “Introduction to Artificial Neural Networks.” Introduction to Artificial Neural Networks (blog), May 29, 2020. https://towardsdatascience.com/introduction-to-artificial-neural-networks-ac338f4154e5.\n\nElite Data Science “Overfitting in Machine Learning: What It Is and How to Prevent It,” July 6, 2022. https://elitedatascience.com/overfitting-in-machine-learning.\n\nH. Good, “The new gatekeepers: searching for bias in Spotify’s curated playlists”. Macquarie University, 17-Oct-2022 [Online]. Available: https://figshare.mq.edu.au/articles/thesis/The_new_gatekeepers_searching_for_bias_in_Spotify_s_curated_playlists/21343314/1. [Accessed: 29-Apr-2024]\n\nKrishnamurthy, Bharath. “An Introduction to the ReLU Activation Function,” February 26, 2024. https://builtin.com/machine-learning/relu-activation-function.\n\nKumawat, Dinesh. “7 Types of Activation Functions in Neural Network,” August 22, 2019. https://www.analyticssteps.com/blogs/7-types-activation-functions-neural-network.\n\nRaju, V N Ganapathi, K Prasanna Lakshmi, Vinod Mahesh Jain, Archana Kalidindi, and V Padma. “Study the Influence of Normalization/Transformation Process on the Accuracy of Supervised Classification.” In 2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT), 729–35, 2020. https://doi.org/10.1109/ICSSIT48917.2020.9214160.\n\nSanderson, Grant. “What Is Backpropagation Really Doing?” 3Blue1Brown (blog), November 2, 2017. https://www.3blue1brown.com/lessons/backpropagation#title.\n\nSiles, Ignacio, Andrés Segura-Castillo, Mónica Sancho, and Ricardo Solís-Quesada. “Genres as Social Affect: Cultivating Moods and Emotions through Playlists on Spotify.” Social Media + Society 5, no. 2 (2019): 2056305119847514. https://doi.org/10.1177/2056305119847514.\n\nSorzano, C. O. S., J. Vargas, and A. Pascual Montano. “A Survey of Dimensionality Reduction Techniques,” 2014.\n",
        "url": "/machine-learning/2024/04/30/karenij-ANNPlaylistClassifier.html"
      },
    
      {
        "title": "KineMapper",
        "author": "\n",
        "excerpt": "A Max for Live device that maps motion data from a smartphone to controls in Ableton Live.\n",
        "content": "Introduction\nKineMapper is a framework for motion control in music. It is a free and open-source Max for Live device which receives motion data from a smartphone via OSC (Open Sound Control) messages. Using the ‘Nodes’ object in Max, this data is translated to 8 mappable dials which are able to control parameters within Ableton Live. The key advantages of KineMapper are as follows;\n\n\n  \n    Flexibility - KineMapper encourages the user to develop motion controlled instruments through freely experimenting with different mappings in Ableton Live. Outputs from KineMapper could be mapped to sound parameters, arpeggiators, audio effects, triggering clips and so on.\n  \n  \n    Easy to use - The system is designed to make connecting devices simple and once connected, the transparent mapping of motion to sound makes for a fast learning process.\n  \n  \n    Ubiquitous technology - KineMapper uses technology that you probably already have so there is no need to invest in expensive specialist equipment to start experimenting with motion and music. A full list of requirements can be seen below.\n  \n\n\n\n\n\n\nList of required technology\n\n  A smartphone.\n  An app for transmitting motion data via OSC (I recommend GyrOSC).\n  A computer running Ableton Live Suite (for Max for Live devices).\n\n\n\nHow does it work?\n\nKineMapper works with data from the gyroscope inside your phone. This means rotational movements are detected rather than linear or translational movements. The difference between rotational and translational movements can be seen in the image below. (Image from ‘What are the “6 Degrees of Freedom”? available here.)\n\n\n\n\nKineMapper is designed to work with the iOS app called GyrOSC, although adaptations can be made to function with any compatible smartphone application.\n\n\nWhat is OSC anyway?\n\nOSC (Open Sound Control) is a protocol for sending messages between computers and musical devices. OSC messages are similar to MIDI messages in their function - they can be used to achieve the same result - although there are some key differences between the two.\n\n\n  OSC messages are sent via a network, this can be through ethernet cable or WIFI connection.\n  MIDI sends integers between 0 and 127 whereas OSC can send many different types of message, including strings of text, integers and 32-bit floating point values.\n  MIDI messages are labelled in a fixed way but labelling of OSC messages is much more flexible.\n\n\nFor a more in depth overview of OSC, check out this blog post.\n\n\nHow to connect devices\n\nIn order to send and receive OSC messages, both devices must be connected to a network. To get the best results when using KineMapper, use an external router which is not connected to the internet or activate the ‘hotspot’ feature on the smartphone (this does not use up your mobile data allowance). Connect your computer to the same network as the smartphone. Find out the IP address of the computer and enter this into the smartphone app - this will let the app know where to send messages to.\n\nThe app will also ask for a port number. The default port number for KineMapper is 6000 but this can be changed by opening the Max patch. The port number is contained within a message box in the top left of the patch. Ports can sometimes be used by other tasks your computer is running, so changing the port number can be a good place to start when troubleshooting. Click the message box to start a connection. If a successful connection is made, the light should flash green.\n\nSummary:\n\n\n  Open the Max for Live device on an audio or MIDI track in Ableton.\n  Connect both smartphone and computer to the same network.\n  Open GyrOSC and enter the IP address of the computer.\n  Set the port number to 6000.\n  Click the message box ‘6000’ in the top left corner of the Max for Live device.\n  A green light indicates a good connection.\n\n\nNow for the fun stuff … almost.\n\nCalibrating KineMapper\n\nBefore KineMapper can be used effectively, a calibration process must be carried out. This process tells the device the size of space you would like to use the controller in by measuring the maximum values expected for each axis of rotation on the gyroscope.\n\nCalibration is done by pointing the controller towards the centre of this imaginary space and pressing the ‘calibrate’ button. Next move the arm left and right to the edges of the imagined space. Then, return to the centre and move up and down to the edges. Finally, return to the centre and rotate the arm left and right remaining in the boundaries of what is comfortable. Click the calibrate button (‘now labelled as ‘stop’) once more the end the procedure.\n\n\n\n\nNodes\n\nNodes is a Max object which uses a pointer to navigate on a 2d surface. On this surface, ‘nodes’ can be placed at specific position. Nodes have a circle around them, the size of which can also be altered. The object outputs a value between 0 and 1 based on the proximity of the pointer to each node. (The pointer must be inside the boundary of the circle to register a positive output.) The pitch and yaw (left-right and up-down) rotation of the smartphone is mapped to the X and Y coordinates of the pointer. Basically, the pointer follows where you point. KineMapper can have up to 7 nodes by default (this can be edited inside the Max patch).\n\n\n\n\nExtra features\n\nKineMapper also boasts a motion record feature where data can be recorded over a period of 1, 2 or 4 bars (synced with Ableton’s clock). After recording, the data is played back in a loop and new data coming in is stopped. This allows the user to experiment with a loop of movement, record it then move on to interacting with another device.\n\nIncoming data can also be gated and each axis inverted independently.\n\n\n\n\nMappable dials\n\nThe final output of KineMapper is 8 mappable dials which you can use to control anything in Ableton which is able to be mapped. 1 - 7 represent each node value and the 8th is linked to the roll axis of rotation. (Note: roll also alters the size of the pointer for visual feedback but this does not have any effect on the outputted values from Nodes. Click the ‘map’ button followed by the parameter of choice in Ableton to create a mapping. Click ‘unmap’ to reset the mapping.\n\n\n\n\nPro tips\n\n\n  Press cmd/ctrl+m in Ableton to see all mappable controls (shaded in blue).\n  Don’t make your space to big while calibrating, otherwise you might not be able to reach the edges. This is especially relevant for the roll axis as twisting the wrist can be a little uncomfortable.\n  If you can’t connect, double check the IP address or try changing the port number.\n  Open GyrOSC facing the direction you want to use the device.\n\n\nHappy motion controlling!\n",
        "url": "/motion-capture/2024/05/05/thomaseo-KineMapper.html"
      },
    
      {
        "title": "CLAP Models and How To Make Them",
        "author": "\n",
        "excerpt": "Is there anything CLAP models can’t do?\n",
        "content": "As the meeting point between machine learning, language, and audio, Contrastive Language-Audio Pre-Trained models (CLAP) has potential for a plethora of language-aligned audio applications, including:\n\n\n  Zero-Shot Classification\n    \n      Instruments, Voice Emotion, Music Genres, Counting Speakers and much more!\n    \n  \n  Class Separation\n  Music Retrieval\n  Automatic Captioning\n  Audio Content Filtering\n  Sound Event Detection\n\n\nAdditionally, contrastive representations have been used for conditioning Text-To-Audio systems. So, if CLAPs are so great, how do they work?\n\nContrastive Representation Learning\nThe bad news is that such an architecture is quite complex to build from the ground up. The good news is that we can use pre-trained models within our contrastively pre-trained model, which can then be trained pre-integration in larger audio systems. Neat! All we need is a language model (BERT, RoBERTa, or T5 are common choices) and an audio model (HTSAT or PANN for instance). Projecting their logits into the same latent space using an appropriate loss function will give us the desired result.\n\n\n  \n\n\nA contrastive loss function will be able to pull text logits and audio logits that belong together closer (positive pairs), but repell those that do not (negative pairs). What does such a loss function look like? I’m glad you asked! You can find code to get you started on my UCS-CLAP Github.\n\nCLAP Comparison\nI recently compared two CLAP models through the embeddings they produce, and through my experiments I can safely say that I am 76% confident that inspecting the proximity of CLAP embeddings through a 2D PCA plot gives a solid estimate of classification accuracy!\n\n\n  \n\n\nDisclaimer: these findings arenot backed by the statistical power necessaryto say for certain :(\n\nDataset &amp; Code\nShould you want to recreate my results, you may get audio embeddings here. Notebooks for the project and a reproduction guide may be found on Github.\n\nAcknowledgements\nMy thesis was made possible with help from Stian Aagedal and Peder Jørgensen of HANCE.AI. I would like to thank them for their continued support and for granting me access to the entire Soundly audio collection for my work.\n",
        "url": "/masters-thesis/2024/05/08/olivegr-clap_model_evaluation.html"
      },
    
      {
        "title": "Cyclic Patterns and Spatial Orientations in Artificial Impulsive ASMR Sounds",
        "author": "\n",
        "excerpt": "An exploratory study on the effects of cyclic patterns and spatial orientations in synthesized impulsive ASMR sounds.\n",
        "content": "Abstract\nThis thesis investigates the perception of Autonomous Sensory Meridian Re-\nsponse (ASMR) stimuli, focusing on the impact of different cyclic patterns and spatial orientations on inducing ASMR experiences. By employing statistical analysis, significant correlations were revealed between ASMR perception and other factors. The results demonstrate that both the type of cyclic pattern and spatial orientation significantly influence the intensity and nature of ASMR experiences. Furthermore, the study explores the synthesis of ASMR-inducing sounds while preserving key audio characteristics from acoustically recorded ASMR content. Through the analysis of survey data and regression modeling, distinct patterns emerge regarding the relationship between personality traits and ASMR perception. The findings contribute to a deeper understanding of ASMR as a sensory phenomenon and provide insights into the potential applications of artificially generated ASMR stimuli. Additionally, the research sheds light on the role of spatiality in ASMR experiences and the synthesis of ASMR-inducing sounds for future studies and practical applications.\n\nPrimary Contributions\nThe thesis study of ASMR holds potential therapeutic benefits, particularly regarding mental health, where ASMR could be harnessed as affordable, and easily available, tool for the alleviation of anxiety and insomnia. Thus, by advancing the research on ASMR, this thesis contributes to the academic field and potential practical applications that could help individuals find solace in the simple act of listening to audio-based stimuli. New ways of creating audio-based ASMR is explored through the research’s implementation of synthesized ASMR stimuli. This can contribute to the field of ASMR by potentially making it more available to an audience who find the social aspects, introduced through the act of sounds being recorded by another human being, disturbing in the listening situation. The generative aspect of sound production can potentially introduce the use of sensors and data for personalized ASMR content generation.\n\nBackground\nASMR has garnered increasing attention due to its diverse effects on individuals, induced by auditory, visual, and tactile stimuli. This phenomenon is exemplified by the proliferation of ASMR content across various online platforms, where creators manipulate everyday objects to produce soothing sounds aimed at relaxation. At the core of ASMR lies a reaction generally referred to as tingles, an electrostatic-like feeling often emerging from the scalp and shooting down through the body (Barratt and Davis, 2015). Tingles play a central role in what usually facilitates the relaxed state of mind among ASMR recipients who may use ASMR to manage anxiety, stress and sleep, as well as for general relaxation.\n\nHowever, despite its growing popularity, ASMR remains enigmatic and polarizing. While some experience relaxation and comfort, others find the same sounds unsettling or irritating. This variability in responses has led to a social divide regarding the acceptance of ASMR. Consequently, there exists a stigma surrounding ASMR experiences in broader social contexts.\n\nThe study explores the intersection of ASMR with personality traits, such as neuroticism and openness, shedding light on potential connections between ASMR experiences and psychological characteristics (Eid, 2022)(Fredborg, 2017). Additionally, it investigates the physiological similarities between ASMR-induced tingles and musical frisson, offering insights into the therapeutic potential of ASMR in promoting comfort and well-being (Lochte, 2018).\n\nBuilding upon the findings of Fang et.al. stating that artificially produced audio holds the potential of inducing ASMR sensations, the thesis research examines alternative ways of generating ASMR audio as well as investigating the impact of cyclic patterns and spatial orientations (Fang, 2023).\n\nOverall, this research contributes to advancing knowledge in the fields of music psychology and technology, offering new avenues for understanding how sensory experiences impact mood and cognition. It also holds potential for practical applications in mental health, where ASMR could serve as a tool for anxiety and insomnia relief.\n\nMethods\nThe methods employed in the thesis research constitute a combination of exploratory and quantitative, with some qualitative elements regarding testing the user survey and the development of sound examples. The combination creates a comprehensive image of how ASMR sounds are perceived. The exploratory part of the thesis emphasizes the inclusion of synthesized ASMR stimuli subject to perceptual survey study. The survey resembles the quantitative part of the research. The qualitative part was focused on the generation of the ASMR stimuli, both through the work on extracting audio features from source material recordings and through applying these in the sonification process while working with the sound engine.\n\nThe selected cyclic patterns resemble different degrees of predictability in the way sound triggering events were organized rhythmically.\n\nSelection of Cyclic Patterns and Spatial Orientations\n\n\n  \n    \n      Cyclic Pattern\n      Spatial Orientation\n    \n  \n  \n    \n      MIR\n      Stereophonic\n    \n    \n      RAN\n      Monophonic\n    \n    \n      FM\n      -\n    \n  \n\n\nMIR was based on a source material recording of someone tapping long artificial nails on an acrylic board recorded using a stereophonic microphone setup.\n\nRAN and FM were algorithmically generated using Low Frequency Oscillators (LFO) with modulator speeds of 0.62 Hz (generating a new section of rhythms every 1.6 seconds). The RAN cyclic pattern generated a random rhythmical section, whereas the FM cyclic pattern generated a similar rhythmical pattern for every iteration.\n\nThe spatial orientations were resembled as monophonic or stereophonic, while including specific motions in the stereophonic binaural field would have introduced implications when comparing the results regarding both cyclic patterns and spatial orientations. This remains a task for future work on the topic. The stereophonic spatial orientation was derived from stereophonic audio feature analysis on the same source material recording that laid the foundation for the MIR cyclic pattern. Utilizing the same spatial orientation data for all cyclic pattern was deemed beneficial for executing meaningful comparisons on the results.\n\nGenerated Sound Examples\n\n\n  \n    \n    Error loading audio content\n  \n  MIR Monophonic\n\n\n\n  \n    \n    Error loading audio content\n  \n  MIR Stereophonic\n\n\n\n  \n    \n    Error loading audio content\n  \n  RAN Monophonic\n\n\n\n  \n    \n    Error loading audio content\n  \n  RAN Stereophonic\n\n\n\n  \n    \n    Error loading audio content\n  \n  FM Monophonic\n\n\n\n  \n    \n    Error loading audio content\n  \n  FM Stereophonic\n\n\nThe online survey, published through gorilla.sc, asked for perceptual ratings of the generated ASMR stimuli, as well as including a shortform Big Five Index (BFI) questionnaire for personality traits mapping. The perceptual questions asked for physical and psychological reaction (Q1), focus during listening (Q2), whether the rhythmical content was perceived as natural or artificial (Q3), and the general experience being relaxing or stressful (Q4). Additionally, demographic questions and questions regarding prior knowledge and experience with ASMR were asked. 67 responses were gathered.\n\nDiscussion\n\nRQ1: In which ways do different cyclic patterns affect the perception of artificially generated ASMR? How do artificially generated cyclic patterns (generated using algorithms) compare to naturally occurring cyclic patterns (recorded from human interaction with objects) in ASMR stimulus? In which ways does this relate to personality traits?\n\nRQ2: How does monophonic and stereophonic binaural spatial orientation impact ASMR stimulus? In which ways does this relate to personality traits?\n\nRQ3: How can sound be generated and synthesized while maintaining key audio characteristics in acoustically recorded ASMR sounds to induce ASMR stimuli?\n\n\n   \n   \n   Heatmap of response by cyclic pattern and spatial orientation for\n   Q1 reaction strength. The MIR and RAN cyclic patterns are pointed out as the most effective cyclic patterns, and the stereophonic spatial orientation is pointed out as the most effective spatial orientation, regarding reaction strength to all ASMR stimuli in the thesis research.\n   \n\n\nBased on the results, a suggestion in this thesis is that the immersive properties in sound may be an important factor in inducing ASMR reactions. Immersiveness relates to the stereophonic spatial orientation’s impact on ASMR perception, eliciting the ability to navigate in sound and perceive different sound events at different levels of proximity.\n\nIncreased immersiveness in sound could occupy more of the listener’s attention resources, explaining why some can find this relaxing and comforting while others find the same sound stressful and aversive.\n\nConcerning cyclic patterns, immersiveness could relate to rhythmic unpredictability, which was identified as yielding stronger reactions. A cyclic pattern with this character might require more reactive listening, taking up more of the listener’s attention resources. This relates to the sensation of occupancy of attention resources being highly subjective.\n\nImmersiveness could also contribute to explaining how people with high levels of neuroticism often are recipients of ASMR. As ASMR has gained a reputation for being an aid in managing anxiety and stress, it can by itself recruit people who struggle with these things as they might be likely to seek tools to help with their struggles. This could prime people characterized by high levels of neuroticism, such as in anchoring, biasing their introduction to the phenomenon and eliciting more ASMR recipient individuals among the high neuroticism population.\n\nConclusion\nThe investigation into the impact of different cyclic patterns, spatial orientations, and the synthesis of ASMR sounds sheds light on several aspects influencing the perception of ASMR stimuli.\n\nCyclic Patterns\nThe findings suggest a significant correlation between the predictability of cyclic patterns and the strength of ASMR reactions. Specifically, the more unpredictable cyclic patterns, such as MIR and RAN, were found to induce stronger ASMR responses compared to more predictable patterns like FM. This aligns with previous research suggesting that unpredictability plays a role in triggering ASMR and related sensory experiences. Additionally, the correlation between cyclic patterns and personality traits, such as extraversion and openness, offers insights into individual differences in ASMR responsiveness.\n\nSpatial Orientation\nThe study highlights the importance of spatial orientation, particularly stereophonic audio, in enhancing the immersive quality of ASMR content. Stereophonic spatial orientation was found to positively correlate with the strength of ASMR reactions, indicating its role in creating a more immersive auditory experience. This finding resonates with existing literature on the significance of spatial audio cues in eliciting emotional and sensory responses in listeners. Furthermore, the correlation between spatial orientation and personality traits underscores the interplay between auditory stimuli and individual predispositions.\n\nSynthesis of ASMR Sounds\nThe results suggest that synthesized ASMR sounds, generated to replicate key audio characteristics present in acoustically recorded ASMR content, can effectively induce ASMR responses. This opens up possibilities for creative exploration in ASMR content creation, offering avenues for the development of novel stimuli and experiences. However, further research is needed to explore how different characteristics in synthesis affect ASMR perception and to compare synthesized artificial ASMR with conventionally recorded content.\n\nThis study’s findings contribute to our understanding of the perceptual mechanisms underlying ASMR experiences and offer insights into the role of cyclic patterns, spatial orientation, and sound synthesis in shaping ASMR stimuli. By elucidating these factors, future research can advance our knowledge of ASMR and inform the development of tailored stimuli for therapeutic and recreational purposes.\n\nReferences\n\n  Barratt, E. and Davis, N. (2015). Autonomous Sensory Meridian Response (ASMR): A flow-like mental state. PeerJ, 3:e851.\n  Eid, C. M., Hamilton, C., and Greer, J. M. H. (2022). Untangling the tingle: Investigating the association between the Autonomous Sensory Meridian Response (ASMR), neuroticism, and trait &amp; state anxiety. PLOS ONE, 17(2):e0262668. Publisher: Public Library of Science.\n  Fang, Z., Han, B., Cao, C. C., and Schotten, H. D. (2023). Artificial ASMR: A Cyber-Psychological Approach. arXiv:2210.14321 [cs, eess].\n  Fredborg, B., Clark, J., and Smith, S. (2017). An Examination of Personality Traits Associated with Autonomous Sensory Meridian Response (ASMR). Frontiers in Psychology, 8:247.\n  Lochte, B. C., Guillory, S. A., Richard, C. A. H., and Kelley, W. M. (2018). An fMRI investigation of the neural correlates underlying the autonomous sensory meridian response (ASMR). BioImpacts : BI, 8(4):295–304.\n\n",
        "url": "/masters-thesis/2024/05/11/henrikhs-asmr.html"
      },
    
      {
        "title": "Deep Steps: A Generative AI Step Sequencer",
        "author": "\n",
        "excerpt": "A stand alone MIDI step sequencer application with a user-trainable generative neural network\n",
        "content": "Last year during the Music and Machine Learning course, I began explore incorporating the autoencoder into a musical step sequencer.  I wrote about this initial work here and here.  My jumping off point was the similarity between a step sequencer and using  many-hot encoding for representing music.  The work there used Pure Data for the sequencer and GUI interactions communicating with a Python script running the ML models.  My other intention was to provide a format, interface and dataset pipeline which could potentially be accessible for music producers.  As such, I chose audio loops as the format for training data due to their ubiquity in electronic music production workflows.\n\nDeep Steps\n\nFor my SMC Masters thesis I chose to develop this work into a more unified implementation.  This meant bringing all the aspects together into a single interface that would be potentially usable for electronic music producers.  In doing so, the intention was to have model training and model generation being part of a real-time musical interaction.  Rather than provide a fixed pre-trained model, users would instead use their own training data to customise the model for their own intentions and styles.  I decided to call this combination of a deep learning model and a step sequencer, Deep Steps.\n\n\n  \n\n\nNew Interactions\n\nThe traditional machine learning pipeline is to train and optimise a model and then deploy it for use.  This is appropriate for many situations where use of such a model is appropriate but it is debatable for applications involving creativity.  A comparison can be made to the evolution of drum machines from having fixed sounds and rhythms in their early days to now having full user control over programming and timbre.\n\nIn developing Deep Steps as a GUI-based interactive system, the intention was to expose some machine learning parameters to user control.  Though numerous systems exist which use machine learning and deep learning models interactively, these often focus on the means of control for triggering and steering a model’s generative output.  In creating an accessible framework for training an ML model on the user’s own data (audio loops), the intention is to explore the potential for this interaction as part of creativity.  In a similar way to how Generative Music is a practice of system building as a form of creativity, could the selection and use of training data and parameters for model training also become part of the creative process?\n\n\n  \n     The design principals for Deep Steps. \n\n\nImplementation\nDeep Steps was developed as a stand alone application using the openFrameworks C++ creative toolkit.  This allowed for the creation of the GUI, embedding of Pure Data and the compiling of the application itself.  The machine learning model was an autoencoder as before.  Rather than use a large library like Tensorflow, I instead adapted code from the ML-From-Scratch library which features examples of low-level implementations of ML algorithms using Numpy.  For audio analysis of onsets in the training data I used the aubio library’s C++ framework.\n\nThe model generates material by having values fed through its decoder stage either through the GUI generate button or through the A, B, C, D sliders.\n\n\n  \n     Using Deep Steps to generate rhythms \n\n\nThe source code for Deep Steps is available here\n\nEvaluation\nDeep Steps was evaluated two times.  After a first round of development, it was evaluated in a short workshop-based user study where 11 participants used the application under controlled conditions and then responded to questionnaires.  The intention of this was to test the core principals, functionality and usability of the system.  Thankfully, these were all mostly validated.\n\nDeep Steps was then distributed to three music producers to use as part of their music making for a week.  The participants were then interviewed to capture their user experiences.\n\nOverall, the findings from the evaluations found Deep Steps to be usable and indeed provided an accessible interface for interaction with machine learning parameters.  Training data as an engaging interaction and a part of the creative process was evident in the feedback.  The participants’ feedback also highlighted several areas where the software could be improved to make it more appealing for future adoption and long-term engagement.  A key theme here was the extra required to engage with the ML processes, especially when existing tools such as randomisers can potentially do a comparable job with less effort.  This is a recurring point of friction for these types of implementations which to some extent can be viewed as simply re-inventing an existing algorithmic process.\n\nIn creating a GUI to make the ML processes accessible, what I had also done was to hide them away behind the interface.  One recommendation was to offer a visualisation of these processes.  For instance, the processing of the onset data could be visualised through waveform displays.  The feeding forward of the data through the decoder to generate material could also be visualised.  This could be used to illustrate the causal relationship of the sliders, which many users reported as feeling like an unsatisfying and arbitrary feeling interaction.\n\nThe findings overall are generally positive with Deep Steps representing a functional proof-of-concept for its intentions and design principals.\n\nDemonstration\n\nFinally, here is Deep Steps in action.  Here, I am using Deep Steps for rhythmic triggers in my Eurorack modular synth.  I am not using the pitch controls in the application, instead looping random voltages from elsewhere in the modular system.  I am controlling the models generative output via a MIDI controller with knobs mapped to the four ‘‘latent dimension’’ sliders to pass values into the decoder.\n\n\n",
        "url": "/masters-thesis/2024/05/14/alexanjw-DeepSteps.html"
      },
    
      {
        "title": "Strung Along: an extended violin for real-time accompaniment generation and timbral control",
        "author": "\n",
        "excerpt": "An extended violin for real-time chordal accompaniment generation and timbral control.\n",
        "content": "Introduction\n\nFor my thesis project, I designed and developed Strung Along, a violin based interactive music system which can accompany the violinist (me) in real time by generating chords, while also allowing for timbral, volume and chord voicing control through modulating the force applied through the bow.\n\nThe system is borne from my interest in two types of violin music; self-accompanying solo violin repertoire like this and this, and string chamber music like this. The system aims to offer something of the incredible feeling of being musically in synchrony with other musicians, while also offering the intimate control of timbre and volume that solo violin repertoire affords.\n\nRead on to find out how I went about it.\n\nA Tale of Two Sub-Systems\n\nStrung Along consists of two sub-systems, which are integrated into the final system. They are:\n\n  A chord generation sub-system, which generates chords as MIDI notes to accompany a melody played in real time.\n  A bow tracking sub-system, which allows for the way the violinist uses the bow to be tracked in real time, and then used to affect the timbre of the sound engine, and the chord generation.\n\n\nChord Generation Sub-System\n\nThe premise of the chord generation sub-system is quite simple; one plays a note of a melody, and the system generates a suitable chord to go with it (and, hopefully, works with the ones it generated previously too). By repeating this process of multiple melody notes, we can generate a sequence of chords to accompany the melody. Even better, we get to do it in real time - it’s like a string orchestra in your pocket!\n\nBackground\n\nChord progressions are a fundamental aspect of many musical genres, and researchers have been developing systems to generate them using machine learning for decades. Such systems can be applied an a wide range of contexts, from harmonising existing melodies, suggesting alternative chords in pre-composed progressions, to use in interactive systems for live performance.\n\nMany early chord generation systems were designed to harmonise Bach Chorales. These chorales (the very mention of which will raise the blood pressure of anyone who remembers learning figured bass in music theory classes) are a useful judge of chord generation systems, as they offer a very concrete set of rules that determine whether the harmony is correct. These systems were generally offline and static - the user would feed in a melody, and receive back a suitable harmony in some representation\n\nMany more recent systems generate chords for a variety of use cases. ChordRipple aims to help novice composers break outside of rigid harmonic boxes by suggesting alternative chords during composition, while a system proposed by Garoufis et al. for use in live performance allows users to direct an evolving chord progression by selecting a chord from multiple options.\n\nA common thread among these systems is the way in which they represent chords. Typically, this is done using a ‘chord vocabulary’ - effectively a pre-determined list of the chords known by the system, and which it is able to generate. These vocabularies can be very small, consisting of only 12 major and 12 minor chords in the case of Lim et al., or rather large, consisting of over 100 chords in the case of ChordRipple. In machine learning lingo, we call such systems ‘classification’ systems as their output is one or several options from a fixed set of ‘classes’, each of which represents a chord.\n\nDesign &amp; Implementation\n\nThe chord generation sub-system in Strung Along aims to buck this trend by instead representing chords using regression machine learning techniques. Rather than selecting an appropriate chord from a pre-defined set, a chord in this sub-system is represented using a chroma histogram, which is effectively a list of 12 floating point numbers, with each one corresponding to a degree of the chromatic scale (i.e., tonic, minor second, major second, minor third, etc) above a certain root note. Each value in the histogram is determined by the amount of that pitch class in a given set of notes over which the histogram is calculated. The image below shows a Cmaj7 chord, and how it is then represented as a chroma histogram.\n\n\n  \n  A Cmaj7 chord represented as sheet music, and as both a standard (length in beats), and normalised (ratios i.e., sums to 1) chroma histograms. Assume this example is in C major.\n\n\nI propose two key advantages to such an approach.\n\nFirstly, this approach is not limited by a chord vocabulary, as all the information needed to reconstruct the chord is contained in some form in the chroma histogram. This makes the system more flexible, as it does not need to be retrained to add new chords, while also allowing the number of different chords it can generate to be theoretically endless.\n\nSecondly, the chroma histogram can also encode some basic information about how the chord was voiced, which is conspiciously absent from many of the other approaches mentioned above. The histogram can tell us not only which pitch classes are present in the music it represents, but also something about the quantity or presence of each in the music. In the example above, the C note is used twice in the chord as opposed to every other note which are only used once, so that note aquires double the value of the others in the chroma histogram. Assuming that notes that are used more often are more important to the chord, we can infer some aspects of chord voicing from the histogram, while using only a simple list of 12 numbers.\n\nThe machine learning model at the core of this sub-system is trained to generate these chroma histograms. For each note of a melody, it generates a histograms for the chord to accompany that melody note, and then voices it into a set of MIDI notes which are sounded using a software synthesiser. Each newly generated chroma histogram is influenced by the eight previous melody notes and generated histograms. This allows the sub-system to not only generate a chord that works harmonically with a given melody note, but also one that fits with the chords that preceded it.\n\nBow Tracking Sub-System\n\nFor a violinist or other bowed string player, the way in which they use the bow is most often described through gestures and techniques, like staccato, legato, or spiccato. However, it is also possible to describe bowing technique in scientific terms through a set of positions, rotations, and forces that describe how the bow interacts with the strings of the instrument. These are visualised in the image below.\n\n\n  \n  The eight bowing parameters are: bow position, bow velocity, bow acceleration, bow force, bow bridge distance, bow tilt, bow inclination, and bow skew. Each is labelled on the diagram.\n\n\nThe bow tracking sub-system adapts an existing approach proposed by Pardue et al. to tracking bow position and bow force. This approach uses four distance sensors which are mounted to the underside of the bow stick at different positions along the bow length, facing towards the bow hair. As force is applied downwards through the bow onto the string, the bow deflects around the contact point, reducing the distance between the bow hair and bow stick at each sensor location. The amount of deflection at each point is determined by the combination of bow position and bow force, such that each combination theoretically results in a unique combination of sensor distances.\n\nWe can leverage this fact to train a small machine learning model which, given the distances readings from the four sensors, predicts the bow position and bow force. The bow tracking sub-system of Strung Along does just this in real time, meaning it can be used as a mapping parameter in the combined system.\n\nCreating Strung Along\n\nComing soon…\n\nEvaluation\n\nComing soon…\n\nConclusion\n\nComing soon…\n\nSee It In Action\n\nTake a look at a brief performance using Strung Along here:\n\n\n\nSee Also\n\n\n  The codebase and implementation instructions for Strung Along are available on GitHub here.\n  I also built CordChord, a digital string instrument inspired by the bow tracking approach used in this thesis. You can read about it here.\n\n",
        "url": "/masters-thesis/2024/05/14/jackeh-Strung-Along.html"
      },
    
      {
        "title": "Review of Sounding Brush: a graphic score IMS? ",
        "author": "\n",
        "excerpt": "A critical review of a drawing-based interactive music system\n",
        "content": "Introduction\nGraphic notation/scores have been used in experimental music composition since the 1950s, pioneered by notable composers such as John Cage, Karlheinz Stockhausen, Krzysztof Penderecki and Iannis Xenakis. The graphic score exemplifies a shift away from the rigidity of traditional western classical music towards a more interpretive and experimental style. Here, the doodles and written instructions of a composer become the music when performed by a musician, but what if the score could become the instrument too?\n\n\n\n  \n  An example of a graphic score from John Cage’s book: Notations (1)\n\n\nSounding Brush\nFor this, we must look to a more recent area of music research called Interactive Music Systems (IMS) and specifically, a work called Sounding Brush. The Sounding Brush is, in the words of it’s creators, a “tablet based musical instrument for drawing and mark making” (2). The instrument takes the form of an iOS application designed to be used with an iPad which harnesses the gestures of drawing to generate a variety of synthesised sounds and textures. Sounding Brush was developed by Sourya Sen, Koray Tahiroglu and Julia Lohmann working at Aalto University in Finland and was presented at the New Interfaces for Musical Expression (NIME) conference in 2020.\n\n\n\n  \n  A screenshot of Sounding Brush (3)\n\n\nBut is it an ‘instrument’?\nHow can drawing on a tablet be considered a musical instrument? Well, according to Atau Tanaka an instrument can be described as an ‘Open-ended system’ made up of different parts; an input device, mapping algorithms, a sound synthesis engine, compositional structure and an output system (4). Sounding Brush definitely ticks these boxes. This diagram of a gestural controller for sound synthesis can help us to examine the Sounding Brush in more detail.\n\n\n\n  \n  A diagram of a gesturally controlled IMS (5)\n\n\nIn the top left, we have the incoming gestures. In our case, gestures of drawing/painting being captured by touchscreen of the iPad. The user has the additional control of choosing a particular ‘brush’ which represents a different synthesis technique alongside selections for colour and character. In addition to the capture of drawing/painting gestures, the Sounding Brush also utilises the accelerometer sensors within the iPad to manipulate sound whilst in a specific mode.\n\n\n\n  \n  UI menu of Sounding Brush (3)\n\n\nIn this case, the Primary feedback can be considered the visual representation of these gestures as the image on the screen (although some of these also undergo some additional processing to make them more representative of the type of brush used).\n\n\nMappings\nThe Sounding Brush most commonly utilises a series of explicit mapping strategies in a ‘one-to-one’ relationship (6). In the image below, this is demonstrated by any arrow which begins at one performance action and arrives at one synthesis input without any splitting off or being influenced by another arrow. In our case, the Y coordinate of the pen on the tablet could be mapped to the frequency of a sinusoidal wave, for example.\n\nGenerative mappings are employed in the more ‘textural’ brushes - Particles and Crackling. The gesture of painting with these brushes controls some elements of the sound generation but others are controlled by generating random values which also evolve over time. This can also be considered a ‘one-to-many’ mapping strategy as one action affects multiple synthesis parameters.\n\n\n\n  \n  A diagram of mapping strategies (6)\n\n\nThat’s enough diagrams. Let’s talk music.\nThe Sounding Brush makes different sounds depending on the selected mode. These modes are based on various synthesis/sound generation techniques such as: additive, subtractive, granular and proceedural (2). Sounds are both performed as the pen touches the touchscreen, and layered to create an overall soundscape-type composition. Sounds are mostly ‘frozen’ at the last point the pen touched the tablet (for each line) which, like painting on a canvas, encourages the user to draw a soundscape frozen in one particular moment in time.\n\n\nA different interface?\nOne of the advantages of Sounding Brush is it’s user interface (UI) design which provides an alternative to the vast majority of available mobile music applications. More often than not, these are based on a pre-existing non-mobile instruments or music interfaces such as Digital Audio Workstations (DAWs), sequencers, pianos and analog synthesisers. The authors see this as a  democratisation of mobile music applications by removing labels like frequency or resonance and in fact, any remanence of traditional music interfaces and replacing it with an activity that almost everyone can relate to - drawing. Anyone from young children to adults, from zero musical training to professional musicians, doodlers to painters can generate audio-visual art using this device.\n\n\nSummary\nDespite being (relatively) conceptually straightforward, this work occupies an interesting intersection between audio and visual creative practice. The metaphor of a painter with a blank canvas, choosing colours and brush types, encourages the user to approach making music in a novel way and builds on the tradition of graphic scores in contemporary composition.\n\nThe developers mention their ideas for improvements in (2) including the ability to save and load compositions and import samples to the granular synth engine. I would like to see further development on the motion control aspect. Currently this generates new sounds but would it be possible to use this as a way of manipulating the entire composition? This would give a greater separation to the functionality of the two different control methods where at the moment they essentially have similar results to using the pen in the granular synthesiser mode. Could global parameters such as dynamics, filtering, reverb/delay and other FX be controlled in this manner?\n\nAccessibility and low entry level aside, I would love to see a scaled up version of this, perhaps as an installation, or live performance piece, with a much larger touchscreen and greater controls over the colour and texture of the image.\n\n\nReferences\n\n  ’Notations’ by John Cage. Available here.\n\n  S. Sen, K. Tahirog, and J. Lohmann, “Sounding Brush: A Tablet based Musical Instrument for Drawing and Mark Making”, Proceedings of the International Conference on New Interfaces for Musical Expression, 2020 DOI: 10.5281/zenodo.4813398\n\n  Presentation video for Sounding Brush. Available here.\n\n  A. Tanaka, “Sensor-Based Musical Instruments and Interactive Music”, Oxford University Press, 2011. doi: 10.1093/oxfordhb/9780199792030.013.0012.\n\n  M. M. Wanderley and P. Depalle, “Gestural control of sound synthesis,” in Proceedings of the IEEE, vol. 92, no. 4, pp. 632-644, April 2004, doi: 10.1109/JPROC.2004.825882.\n\n  A. Hunt, M. M. Wanderley, and R. Kirk, “Towards a Model for Instrumental Mapping in Expert Musical Interaction”, In ICMC, 2000.\n\n",
        "url": "/interactive-music/2024/09/12/thomaseo-Sounding_Brush_review.html"
      },
    
      {
        "title": "Cosmic Clash!",
        "author": "\n",
        "excerpt": "Cosmic Clash - exploring gamification of physiotherapy exercises at Rosklde Festival using the Biopoint sensor\n",
        "content": "Introduction\n\nCan a game be used to help festival attendees do their physiotherapy exercises even during the festival?\n\n\n  Concept : Develop a game that integrates physiotherapy exercises into its mechanics, making it fun and engaging for participants.\n  Implementation : Use wearable sensors to track movements and provide feedback, ensuring exercises are performed correctly.\n  Benefits : Encourages adherence to physiotherapy routines, promotes physical well-being, and adds an element of fun to the festival experience.\n\n\nThe Biopoint Sensor\n\nSiFi Labs are currently developing their line of sensors called Biopoint. These are small, wearable devices containing an array of sensors which collect data from the body. These include an Inertial Measurement Unit (IMU), Electrocardiogram (ECG), Electromyography (EMG), Electrodermal Activity (EDA/GSR), Photoplethysmography (PPG) and Skin Temperature. Data is transmitted via Bluetooth to a computer running either the standalone application or an executable program which can be run from the terminal or in python.\n&lt;br&gt;\n\nCosmic Clash!\n\nCosmic Clash! is the name of the game. It’s an arcade-style point-and-shoot game set in space. The objective is to shoot or avoid obstacles which damage the player whist trying to pick up lives and power ups which improve player abilities. Two of the sensors in the Biopoint - IMU and EMG - are put into action here to control movement and shooting mechanics.\n&lt;br&gt;\n\n\n  \n  Screenshot of the game\n\n\n\n\nData Processing\n\n\n  \n    IMU\n     \nAn IMU is a sensor used to measure motion and orientation. The BioPoint IMU, in this instance, integrates a 3-axis accelerometer and a 3-axis gyroscope, which allow it to measure linear acceleration and angular velocity. The IMU data packets from the BioPoint include a quaternion — a four-component representation of the device's orientation in 3D space. The sampling rate of the IMU is 100Hz. Check the diagram for a reminder of Euler (rotational) angles.\n  \n  \n    \n    Image credit: machinedesign.com\n  \n\n\n\nThe following equations are used to compute roll, pitch, and yaw from quaternion components (qw), (qx), (qy), and (qz):\n\n\n  \n\n\n\n\n  \n    EMG\n    \n    EMG sensors detect the electrical activity produced by muscles during contraction, providing information about muscle activation levels, timing, and patterns. The EMG sensor in the Biopoint has a sample rate of 2 kHz. Similar to an audio waveform image, EMG data moves between positive and negative values - the further away from zero in either direction, the higher the signal level. To utilise this signal in our game, found the absolute value of a rolling average (across 8 samples in each data packet) and applied a threshold. When the signal goes over the threshold, a shoot command is sent.\n  \n  \n    \n    Image credit: delsys.com\n  \n\n\n\nSystem Architecture\n\nThe game was developed using the node.js runtime. This environment allowed us to develop a browser-based game with the capability for two players to collaborate simultaneously. Currently, data is acquired and processed in python and passed to a node server using web sockets. In the project folder, java script files handle game mechanics and html files render objects in the browser.\n&lt;br&gt;\n\n\n  \n  System Architecture diagram\n\n\n\nCheck out some videos from the development process\n\n\nInital tests using the gyroscope of a smart phone to control a player\n\n\n\nImplementing two player functionality using node.js\n\n\n\nOne player using the EMG to shoot\n\n\n\nTwo players moving and shooting\n\n\n\n\nUse Cases\n\nSo we made a fun game using the Biopoint device as a control interface, but what does this have to do with physiotherapy exercises? How can Cosmic Clash be applied to real world situations? Well, games have been used to great effect in the medical field. Take Re-mission or MindMotion® GO for example. What our system can offer is a take home experience for patients to use repeatedly and this fits rehabilitation treatments in the field of physiotherapy.\n\nAccording to this study from the University of Auckland, as much as 65% of patients do not fully complete their physiotherapy exercise treatment plans. It is difficult to pin down the exact reasons for this as the situation of each patient is different, but a common problems seems to be motivation. A lack of motivation can occur for many reasons. Perhaps the patient thinks the exercises are boring or not challenging enough, or they experience pain whilst performing certain movements.\n\nThis is where we believe our system could help! By gamifying exercise routines, patients could perform the movements more precisely with the correct timing and be distracted from pain or discomfort. Features such as a ‘streak’ reward system which incentivises consistent use over time, could enhance motivation, adherence, and long-term engagement with their therapy. Finally, raw data from sessions could be saved and used to track progress of patients, although there would be data protection rules to consider for this feature.\n&lt;br&gt;\n\nSummary / Future Work\n\nCosmic Clash is a game prototype which utilises signals from IMU and EMG sensors to control the player. The game was developed in order to show how the Biopoint device could be used to gamify rehabilitation exercises in physiotherapy treatments. This could help to improve the quality of exercise and solve problems such as motivating patients to stick with their work out programs. In the future, we would like to implement other data types influencing the game such as ECG and skin temperature. Collaborating with experts in various medical fields would help us to design game mechanics which suit specific exercises. Finally, the Biopoint device contains a vibration motor which would be perfect for providing tactile feedback, responding to events within the game.\n",
        "url": "/applied-project/2024/11/24/thomaseo-cosmic_clash.html"
      },
    
      {
        "title": "The SlapBox: A DMI designed to last",
        "author": "\n",
        "excerpt": "A critical review of a durable digital musical instrument\n",
        "content": "Introduction\nWhile many Digital Musical Instruments (DMIs) today have been designed to showcase new ideas in research and academic spaces and offer us different perspectives on how to interact with sound, most of these DMIs lack the longevity of traditional instruments. This difference hinders many benefits of robust design in that it does not encourage long-lasting commitment to establishing the use of techniques and repertoire. The disconnect here, unfortunately, causes many of these instruments to be put on the shelf while the designers pursue new ideas or dismantle vital parts to be used in other projects. With conferences like NIME placing a lot of thought in recent years to sustainability in practices of DMI creation, The Slapbox offers us an example of what robust design built with prolonged performance in mind can produce.\n\n\n\n  \n  Slapbox Design(1)\n\n\nThe Concept\n\nThe Slapbox is actually the second iteration of a DMI for percussionists, with the original iteration being named The Tapbox (1). The team behind the new iteration specifically chose to work on the Slapbox design based on The Tapbox to highlight how solving old problems on older designs can improve the product while also introducing future design. \n\n\n\n  \n  The Tapbox(3)\n\n\nThe Original Design\nThe design of this instrument takes its inspiration from commercial digital percussion controllers, which usually have some form of velocity-sensitive pads. When looking at the available examples of drum controllers, they noticed a lack of modulation gestures, which influenced the new elements they wanted in their design. The original Tapbox design was based on the Cajon instrument, and its multiple surfaces showcased how dynamic an instrument with multiple playable surfaces could be in performance. The main problems they saw in this original were the lack of sensitivity of some sensors as well as a lack of differing interaction types within the outside of the box.\n\n\nImprovements with New Iteration\n\nThe new iteration still retains many of the older parts, including front panel speakers, auxiliary controls, and most of the internal electronics.  The sensor pads on the Slap Box iteration require a unique solution capable of capturing extremely quick body movements. To find a solution that is commercially available and captured Velocity, Continuous Pressure, and Position, the team behind Slapbox decided to use a Force Sensitive Resistor with Velostat.\nThe Velostat was paired with a configuration of conductive tape that allowed the pressure source to be estimated and mapped to synthesis parameters of sampled percussion sounds. \n\n\n\n  \n  GUI of SlapBox\n\n\nThe new design also comes with a GUI that can be used as a visual representation of real-time strikes alongside visual feedback from the LEDs on the device itself\n\n\nTesting\n\nPerformers were very quick to achieve interesting interactions almost immediately after picking it up for the first time. While initial trials exposed some false triggering this was quickly fixed with a new back panel. Those who used it appreciated its many unique features such as the gui, and modulation features. \n\n\nWhat makes it stand out?\n\nThe Slapbox being designed around the Cajon makes for an instrument that many musicians, percussionists or not, have some idea of how to approach. Even so, the Slapbox is intrinsically its own thing, allowing the user to play this in a way unlike the average percussion set. It will enable the user to play in the way they want and develop the technique that suits their practice or setup. The sensitivity of the position estimation allows for the real-time synthesis to bring individuality to each strike. This can encourage users to develop a virtuosity with the Slapbox that might have yet to come about with synthesis less adept. \n\nAlong with all the features, the Slapbox is very obviously made to last. Unlike other DMIs, the Slapbox’s design feels permanent. Although the ideas surrounding the synthesis and sound samples still loom, the structure appears to be made to withstand much experimentation. The GUI allows for another dimension of knowledge of the instrument that can encourage those weary of trying a sort of instrument to be drawn to seeing exactly what their actions translate to in the synthesized world. \n\n\n\n  \n\n\n\n\nWhy it matters?\n\nDMIs should be made to weather a lengthy musical career. The disconnect between the commercial market and academic spaces in terms of durability makes musicians weary of trying experimental products in research spaces. Nine times out of ten, if a musician thinks of adding new interactions into their daily practice, they will go with a product that can withstand a fall or two and still work. To observe research instruments in performance spaces, we need to make something that musicians are not afraid to use. Moreover, we need instruments that are able to establish confidence in the relationship between academics and performance. In order to see prolonged results and to make something worth more than just being a novelty, the user must be able to establish a habit with the object. Only then can DMIs be worthwhile to performers. \n\n\nWhere does it go from here?\n\nEven though, according to Perry R. Cooks’s first Principle for Computer Music COntrollers list, “Programmability is a curse,” many evaluators of the Slapbox showed interest in the ability to load samples onto the instrument (3). This improvement would be an interesting implementation that could prolong the life cycle of this system after the preset sounds have been exhausted by prolonged users.  I’m excited to see where the next iteration is heading and hope more DMI designers prioritize durability in the way we see in the Slapbox.\n\n\nReferences\n\n  Boettcher, B., Sullivan, J., &amp; Wanderley, M. M. (2021). Slapbox: Redesign of a Digital Musical Instrument Towards Reliable Long-Term Practice. NIME 2022. https://doi.org/10.21428/92fbeb44.78fd89cc\n\n  Dobrian, C., &amp; Koppelman, D. (2006). The E in NIME: Musical Expression with New Computer Interfaces. Proceedings of the International Conference on New Interfaces for Musical Expression, 277–282. https://doi.org/10.5281/zenodo.1176893\n\n  Cook, Perry R.. “Re-Designing Principles for Computer Music Controllers: a Case Study of SqueezeVox Maggie.” New Interfaces for Musical Expression (2009).\n\n\n\n",
        "url": "/interactive-music/2024/11/24/karenij-Slapbox_review.html"
      },
    
      {
        "title": "RFART: A Tectonic Interactive Music System ",
        "author": "\n",
        "excerpt": "Creative festival goers need an interactive and low-stakes way to create and collaborate because it can provide a relaxing reprieve from the festival while still being engaging\n",
        "content": "Creative festival goers need an interactive and low-stakes way to create and collaborate because it can provide a relaxing reprieve from the festival while still being engaging.\n\nSome kind of interactive art piece. I'm thinking similar to those big\nphotos that everyone who comes in at an event like a wedding can sign,\nbut instead of signatures they can leave some kind of audio/visual\ninput. Would contain some kind of adaptable controller to interface\nwith, with the potential to be remapped for different individual use\nsetups on the fly. Could make extensive use of physical models for audio\ninput controls and ML for mappings?\n\n\n  CreativeEngagement\n  InteractiveArt\n  Collaboration\n  UserInput\n  AudioVisual\n  PhysicalModeling\n  MachineLearning\n  FestivalArt\n  EIM\n  SPIS\n\n",
        "url": "/interactive-music/2025/02/04/RFART.html"
      },
    
      {
        "title": "BrailleGuide",
        "author": "\n",
        "excerpt": "Blind/visually impaired festival goers need an easy way to get information about things near them because it can enable a more autonomous experience for them.\n",
        "content": "I do not know if Roskilde already has something like this, but project would develop some kind of device similar to audio guides used in museums. In such a high noise environment, could be a physical device with a braille grid that displays words based on what the user is nearby using some kind of sensor or geo-marker. Could also have some sort of audio/earphone component as well. Perhaps similar to this exhibit at the Bauhaus museum: https://www.studiokamp.com/bauhaus-sound.\n\nCould also include some kind of vibrational feedback similar to this project https://www.notimpossible.com/projects/music-not-impossible for deaf/hearing impaired.\n\n\n  Accessibility\n  Blindness\n  AudioGuide\n  MEL\n  Braille\n  GeoLocation\n  VibrationFeedback\n\n",
        "url": "/applied-project/2025/02/10/BrailleGuide.html"
      },
    
      {
        "title": "DDSP-FM",
        "author": "\n",
        "excerpt": "Explore latent space for learning the parameters of a Differentiable FM Synthesizer\n",
        "content": "Master Thesis, May 2021\nAuthor: Juan Alonso\nSupervisor: Cumhur Erkut\nSound and Music Computing - Aalborg University, Copenhagen\n\nVisit the audio examples page to listen to the results.\n\nDDSP-FM is a fork of the official DDSP library. This version includes the following new features:\n\n\n  a differentiable 4-op FM synthesizer\n  a differentiable AM synthesizer\n  a new operator, mult\n  a new TFRecord, suitable for future develpments, such as preset matching\n  a flag for stopping the training if losses are NaN\n\n\n💡 Timbre matching demo\n\n This notebook will create a random preset for the FM synth, generate 48 pitches and train a NN that will create a patch as similar as possible as the original one, using only the spectrograms of the original preset.\n",
        "url": "/masters-thesis/2025/02/13/kristeic-screamscape.html"
      },
    
      {
        "title": "Zen Soundscape Installation/Sculpture",
        "author": "\n",
        "excerpt": "A serene installation that creates a harmonious soundscape transporting listeners to a state of zen.\n",
        "content": "##\n\nTags:\n\n  installation\n  sculpture\n  sound art\n  tranquility\n  nature sounds\n  meditation\n  mindfulness\n  ADHD friendly\n  interactive art\n  sensory experience\n\n\nA sculpture, inspired by George Koutsouris and Takis creates a harmonious soundscape that transports listeners to a state of zen. As visitors interact with the sculpture, gentle sounds reminiscent of nature—such as rustling leaves, flowing water, strings vibrating and soft chimes—emerge, fostering a meditative atmosphere. The installation not only serves as a visual and auditory delight but also as a sanctuary for mindfulness, inviting individuals to pause, breathe, and reconnect with their inner peace. It is designed to reduce overstimulation to population with ADHD and other disorders.\n\nHaptic Vibration API for Weather Translation\n\nThis innovative project aims to bridge the gap between weather conditions and the visually impaired community through the use of haptic vibrations and sound. By developing an API that translates real-time weather data into tactile and auditory signals, users can experience weather changes in a unique and accessible way. For instance, the sensation of wind might be conveyed through gentle vibrations accompanied by the sound of chimes, while rain could be represented by a rhythmic tapping and the sound of droplets. This system not only enhances accessibility but also enriches the sensory experience, allowing users to perceive and interact with their environment in a meaningful manner.\n\nInspired by the work of Eirini Liapikou in this project https://www.forsamlingshusene.dk/om-vaerket.\n",
        "url": "/applied-project/2025/02/13/Eirini-Liapikou.html"
      },
    
  
  
  
  {
    "title": "About",
    "author": "\n",
    "excerpt": "\n",
    "content": "This is the student-led blog of the the Aalborg University Copenhagen (AAU-CPH) master’s programme in Sound and Music Computing (SMC) offered by the Department of Architecture, Design, and Media Technology\n\nMore information:\n\n\n  Programme structure\n  Learning outcomes\n  Why choose this programme\n  Admission\n  Study abroad\n  Career opportunities\n  Events\n\n\nThe SMC master was established in 2014 and until 2024 it was offered as a distinct MSc programme at AAU. In 2025 it will graduate its last cohort, and afterwards it will merge as a specialization into the Medialogy programme.\n\nOfficial media accounts of the SMC programme:\n\n\n  YouTube\n  Twitter\n  GitHub\n\n\nOther websites and blogs led by students of the SMC programme:\n\n\n  Can You Hear Me? - wiki and guide for an ever expanding list of audio-video communication softwares.\n\n\n\n",
    "url": "/about/"
  },
  
  {
    "title": "All Topics",
    "author": "\n",
    "excerpt": "\n",
    "content": "\n\n  \n    \n    \n      RFART: A Tectonic Interactive Music System \n      interactive-music\n      \n        Feb 4, 2025 • Benjamin Melvin Stein\n      **Creative festival goers** need **an interactive and low-stakes way to create and collaborate** because **it can provide a relaxing reprieve from the festival while still being engaging**\n  \n\n\n  \n    \n    \n      The SlapBox: A DMI designed to last\n      interactive-music\n      \n        Nov 24, 2024 • Karenina Juarez\n      A critical review of a durable digital musical instrument\n  \n\n\n  \n    \n    \n      Review of Sounding Brush: a graphic score IMS? \n      interactive-music\n      \n        Sep 12, 2024 • Tom Oldfield\n      A critical review of a drawing-based interactive music system\n  \n\n\n  \n    \n    \n      KineMapper\n      motion-capture\n      \n        May 5, 2024 • Tom Oldfield\n      A Max for Live device that maps motion data from a smartphone to controls in Ableton Live.\n  \n\n\n  \n    \n    \n      Playlist Data and Recommendations Using Artificial Neural Networks\n      machine-learning\n      \n        Apr 30, 2024 • Karenina Juarez\n      a machine-learning algorithm that pairs independent artists, with curated playlists that best fit based on musical attributes\n  \n\n\n  \n    \n    \n      AcidLab: deep acid bass-line generation\n      machine-learning\n      \n        Apr 28, 2024 • Tom Oldfield\n      Using machine learning to generate acid bass-line melodies with user-created datasets.\n  \n\n\n  \n    \n    \n      Connecting Eigenrhythms and Human movement\n      machine-learning\n      \n        Apr 28, 2024 • Olav Utne Skjeldal\n      Connecting Machine Learning and Human understanding of rhythm.\n  \n\n\n  \n    \n    \n      The Chiptransformer\n      machine-learning\n      \n        Apr 28, 2024 • Sondre Røvik Kippenes\n      The Chiptransformer an my attempt at building a machine learning model using the transformer architecture to generate music based on a dataset of Nintendo NES music.\n  \n\n\n  \n    \n    \n      Using convolutional neural networks to classify music genres\n      machine-learning\n      \n        Apr 28, 2024 • Erlend André Lie Størkson\n      When classifying genres in music, CNNs are a popular choice because of their ability to capture intricate patterns in data.\n  \n\n\n  \n    \n    \n      Digital Signal Processing Basics\n      networked-music\n      \n        Mar 18, 2024 • Karenina Juarez\n      Virtually any song you can listen to on the radio has examples of Digital Signal Processing.\n  \n\n\n  \n    \n    \n      Introduction to Open Sound Control (OSC)\n      networked-music\n      \n        Mar 17, 2024 • Tom Oldfield\n      This post contains a brief overview of OSC and a tutorial on how to make a connection and send data between devices.\n  \n\n\n  \n    \n    \n      Formatting WebPD Projects: An Introduction to WebPD, HTML and CSS Styling\n      networked-music\n      \n        Mar 15, 2024 • Juliana Bigelow\n      Styling your WebPD application can lead to greater user experience and accessibility.\n  \n\n\n  \n    \n    \n      We Are Sitting In Rooms\n      networked-music\n      \n        Dec 20, 2023 • Karenina Juarez\n      Recreating the most famous piece by composer Alvin Lucier as a network music performance\n  \n\n\n  \n    \n    \n      Documenting Networked Music Performances: Tips, Tricks and Best Practices\n      networked-music\n      \n        Dec 11, 2023 • Juliana Bigelow\n      Effectively documenting networked music performances can lead to better experiences for physical and digital audiences, and your academic explorations.\n  \n\n\n  \n    \n    \n      Xyborg 2.0: A Data Glove-based Synthesizer\n      interactive-music\n      \n        Dec 4, 2023 • Kristian Eicke\n      Learn about my adventures in designing and playing a wearable instrument.\n  \n\n\n  \n    \n    \n      Voice/Bend\n      interactive-music\n      \n        Dec 4, 2023 • Nino Jakeli\n      Microphone Gestural controller\n  \n\n\n  \n    \n    \n      The Hyper-Ney\n      interactive-music\n      \n        Dec 1, 2023 • Emin Memis\n      Electrizing an ancient flute using capacitive and motion sensors\n  \n\n\n  \n    \n    \n      An Interactive Evening on Karl Johans Gate\n      interactive-music\n      \n        Dec 1, 2023 • Maham Riaz\n      What if everyday objects decide to kick it up a notch and embrace a life of their own?\n  \n\n\n  \n    \n    \n      CordChord - controlling a digital string instrument with distance sensing and machine learning\n      interactive-music\n      \n        Dec 1, 2023 • Jack Hardwick\n      How can we use sensors to control a digital string instrument? Here's one idea.\n  \n\n\n  \n    \n    \n      The Paperback Singer\n      interactive-music\n      \n        Dec 1, 2023 • Fabian Stordalen\n      An interactive granular audio book\n  \n\n\n  \n    \n    \n      Touch/Tap/Blow - Exploring Intimate Control for Musical Expression\n      interactive-music\n      \n        Dec 1, 2023 • Alexander Wastnidge\n      Touch/Tap/Blow is, as its name suggests, an interactive music system which aims to combine three forms of intimate control over a digital musical instrument.  Notes and chords can be played via the touch interface while bass accompaniment can be driven by the player’s foot tapping. Below are the details of it’s main elements.\n\n  \n\n\n  \n    \n    \n      Sync your synths and jam over a network using Sonobus\n      networked-music\n      \n        Nov 30, 2023 • Tom Oldfield\n      A quick start guide to jamming over a network. Designed for instruments which can synchronize using an analog clock pulse.\n  \n\n\n  \n    \n    \n      The Saxelerophone: Demonstrating gestural virtuosity\n      interactive-music\n      \n        Nov 30, 2023 • Joachim Poutaraud\n      A hyper-instrument tracking data from a 3-axis accelerometer and a contact microphone to create new interactive sounds for the saxophone.\n  \n\n\n  \n    \n    \n      A Critical Look at Cléo Palacio-Quintin’s Hyper-Flute\n      interactive-music\n      \n        Nov 11, 2023 • Emin Memis\n      A Boehm flute enhanced with sensors\n  \n\n\n  \n    \n    \n      The Daïs: Critical Review of a Haptically Enabled NIME\n      interactive-music\n      \n        Sep 25, 2023 • Kristian Eicke\n      Is this a violin?\n  \n\n\n  \n    \n    \n      Shadows As Sounds\n      interactive-music\n      \n        Sep 23, 2023 • Nino Jakeli\n      4-step sequencer using seeds and corns\n  \n\n\n  \n    \n    \n      The Augmented Violin: Examining Musical Expression in a Bow-Controlled Hyper-Instrument\n      interactive-music\n      \n        Sep 22, 2023 • Jack Hardwick\n      A brief look at the affordance for musical expression in a violin-based interactive music system.\n  \n\n\n  \n    \n    \n      The Tickle Tactile Controller - Review\n      interactive-music\n      \n        Sep 22, 2023 • Maham Riaz\n      Like many digital instruments I have come across, the instrument design takes its initial inspiration from the piano, a fixed-key instrument.\n  \n\n\n  \n    \n    \n      Review of On Board Call: A Gestural Wildlife Imitation Machine\n      interactive-music\n      \n        Sep 22, 2023 • Masoud Niknafs\n      Critical Review of On Board Call: A Gestural Wildlife Imitation Machine\n  \n\n\n  \n    \n    \n      How to break out of the comping loop?\n      interactive-music\n      \n        Sep 22, 2023 • Joachim Poutaraud\n      A critical review of the Reflexive Looper.\n  \n\n\n  \n    \n    \n      Exploring Breath-based DMIs: A Review of the KeyWI\n      interactive-music\n      \n        Sep 21, 2023 • Alexander Wastnidge\n      The relationship between musician and instrument can be an extremely personal and intimate one\n  \n\n\n  \n    \n    \n      Controling Guitar Signals Using a Pick?\n      interactive-music\n      \n        Sep 19, 2023 • Fabian Stordalen\n      A deeper dive into the Magpick\n  \n\n\n  \n    \n    \n      Music Between Salen and the World\n      networked-music\n      \n        May 18, 2023 • Jack Hardwick, Alexander Wastnidge, Masoud Niknafs, Emin Memis, Nino Jakeli, Henrik Sveen, Kristian Eicke, Fabian Stordalen, Aysima Karcaaltincaba\n      We played in a global NMP concert. Check out our experiences.\n  \n\n\n  \n    \n    \n      A Body Instrument: Exploring the Intersection of Voice and Motion\n      motion-capture\n      \n        May 9, 2023 • Emin Memis\n      Manipulate your voice with your body\n  \n\n\n  \n    \n    \n      Generative Music with IMU data\n      motion-capture\n      \n        May 9, 2023 • Alexander Wastnidge\n      Eight routes to meta-control\n  \n\n\n  \n    \n    \n      Simple yet unique way of playing music\n      motion-capture\n      \n        May 9, 2023 • Nino Jakeli\n      Gestures can be more intuitive to play around with\n  \n\n\n  \n    \n    \n      Xyborg: Wearable Control Interface and Motion Capture System for Manipulating Sound\n      motion-capture\n      \n        May 8, 2023 • Kristian Eicke\n      Witness my transition from human to machine - with piezo discs\n  \n\n\n  \n    \n    \n      Motion Controlled Sampler in Ableton\n      motion-capture\n      \n        May 8, 2023 • Fabian Stordalen\n      A fun and creative way of sampling\n  \n\n\n  \n    \n    \n      Scream Machine: Voice Conversion with an Artifical Neural Network\n      machine-learning\n      \n        Apr 26, 2023 • Kristian Eicke\n      Using a VAE to transform one voice to another.\n  \n\n\n  \n    \n    \n      Generating music with an evolutionary algorithm\n      machine-learning\n      \n        Apr 26, 2023 • Noor Othmani\n      Looking at a theoretical and general implementation of an evolutionary algorithm to generate music.\n  \n\n\n  \n    \n    \n      Persian classical instruments recognition and classification\n      machine-learning\n      \n        Apr 26, 2023 • Masoud Niknafs\n      This blog post will go over various feature extraction techniques used to identify Persian classical music instruments.\n  \n\n\n  \n    \n    \n      Isn&#39;t Bach deep enough?\n      machine-learning\n      \n        Apr 26, 2023 • Masoud Niknafs\n      Deep Bach is an artificial intelligence that composes like Bach.\n  \n\n\n  \n    \n    \n      Recognizing Key Signatures with Machine Learning\n      machine-learning\n      \n        Apr 26, 2023 • Jack Hardwick\n      The first rule of machine learning? Understand your data! A look into how music theory came to my rescue for classifying key signatures.\n  \n\n\n  \n    \n    \n      Breakbeat Science\n      machine-learning\n      \n        Apr 26, 2023 • Fabian Stordalen\n      AI-Generated amen breakbeats\n  \n\n\n  \n    \n    \n      Spotlight: AutoEncoders and Variational AutoEncoders\n      machine-learning\n      \n        Apr 26, 2023 • Fabian Stordalen, Alexander Wastnidge, Kristian Eicke\n      A simple generative algorithm\n  \n\n\n  \n    \n    \n      Music AI, a brief history\n      machine-learning\n      \n        Apr 26, 2023 • Noor Othmani\n      Chronicling the field of AI-generated music's start, where it went from there, and what you can expect from musical AI right now.\n  \n\n\n  \n    \n    \n      A caveman&#39;s way of making art\n      machine-learning\n      \n        Apr 26, 2023 • Emin Memis\n      ...and art in the age of complexity.\n  \n\n\n  \n    \n    \n      Clustering audio features\n      machine-learning\n      \n        Apr 25, 2023 • Nino Jakeli\n      Music information retrieval(MIR)\n  \n\n\n  \n    \n    \n      What&#39;s wrong with singing voice synthesis\n      machine-learning\n      \n        Apr 25, 2023 • Nino Jakeli\n      Dead can sing\n  \n\n\n  \n    \n    \n      The whistle of the autoencoder\n      machine-learning\n      \n        Apr 25, 2023 • Emin Memis\n      How I used autoencoders to create whistling.\n  \n\n\n  \n    \n    \n      Chroma Representations of MIDI for Chord Generation\n      machine-learning\n      \n        Apr 25, 2023 • Jack Hardwick\n      Understanding two ways of representing and generating chords in machine learning.\n  \n\n\n  \n    \n    \n      Generating Video Game SFX with AI\n      machine-learning\n      \n        Apr 25, 2023 • Oliver Getz\n      A first look at text-to-audio sound effect generation for video games.\n  \n\n\n  \n    \n    \n      Pytorch GPU Setup Guide\n      machine-learning\n      \n        Apr 25, 2023 • Oliver Getz\n      Having trouble getting Pytorch to recognize your GPU? Try this!\n  \n\n\n  \n    \n    \n      A Rhythmic Sequencer Driven by a Stacked Autoencoder\n      machine-learning\n      \n        Apr 25, 2023 • Alexander Wastnidge\n      Sometimes you need to leave room for the musician\n  \n\n\n  \n    \n    \n      Comparing MIDI Representations: The Battle Between Efficiency and Complexity\n      machine-learning\n      \n        Apr 25, 2023 • Trym Bø\n      A comparing of different MIDI representations for generative machine learning\n  \n\n\n  \n    \n    \n      Programming with OpenAI\n      machine-learning\n      \n        Apr 24, 2023 • Aysima Karcaaltincaba\n      How OpenAI solutions help us to program?\n  \n\n\n  \n    \n    \n      Challenges with Midi\n      machine-learning\n      \n        Apr 24, 2023 • Aysima Karcaaltincaba\n      Is it easy to create chord progression from midi files?\n  \n\n\n  \n    \n    \n      Basics of Computer Networks in NMPs\n      networked-music\n      \n        Apr 23, 2023 • Emin Memis\n      Crash course on computer network used in Network Music Performances.\n  \n\n\n  \n    \n    \n      Audio Codecs and the AI Revolution\n      networked-music\n      \n        Apr 23, 2023 • Jack Hardwick\n      I dove headfirst into the world of machine learning-enhanced audio codecs. Here's what I found out.\n  \n\n\n  \n    \n    \n      NMP kit Tutorial\n      networked-music\n      \n        Apr 23, 2023 • Henrik Sveen\n      A practical tutorial on the NMP kits.\n  \n\n\n  \n    \n    \n      Spatial Audio with Max-Msp and Sonobus\n      networked-music\n      \n        Apr 23, 2023 • Masoud Niknafs\n      This post's video tutorial aims to introduce readers to the many uses for which the Sonobus can be implemented as a VST in Max-Msp.\n  \n\n\n  \n    \n    \n      SonoBus setup - Standalone App and Reaper Plugin\n      networked-music\n      \n        Apr 23, 2023 • Kristian Eicke\n      Check out my tutorial on how to use SonoBus for your networked performance.\n  \n\n\n  \n    \n    \n      Integrating JackTrip and Sonobus in a DAW\n      networked-music\n      \n        Apr 23, 2023 • Fabian Stordalen\n      How you can integrate both JackTrip and Sonobus into your DAW\n  \n\n\n  \n    \n    \n      Pair Programming Over Network\n      networked-music\n      \n        Apr 21, 2023 • Aysima Karcaaltincaba\n      Pair Programming from different locations, is it possible?\n  \n\n\n  \n    \n    \n      Audio Engineering for NMPs: Part 2\n      networked-music\n      \n        Apr 20, 2023 • Alexander Wastnidge\n      A deeper dive into mixer work for NMPs\n  \n\n\n  \n    \n    \n      MIDI music generation, the hassle of representing MIDI\n      machine-learning\n      \n        Apr 19, 2023 • Trym Bø\n      A brief guide of the troubles with MIDI representation for generative AI\n  \n\n\n  \n    \n    \n      Playing Jazz Over Network\n      networked-music\n      \n        Mar 12, 2023 • Aysima Karcaaltincaba, Fabian Stordalen, Henrik Sveen, Kristian Eicke, Nino Jakeli\n      A live performance by Edvard Munch High School students with collaborative networked music technology.\n  \n\n\n  \n    \n    \n      Jazz Over the Network at 184,000km/h\n      networked-music\n      \n        Mar 12, 2023 • Jack Hardwick, Alexander Wastnidge, Masoud Niknafs, Emin Memis\n      We worked with local high school students to put together a jazz concert over the LOLA network. Here's a retrospective from Team RITMO.\n  \n\n\n  \n    \n    \n      Testing Two Approaches to Performing with Latency\n      networked-music\n      \n        Feb 16, 2023 • Aysima Karcaaltincaba, Emin Memis, Jack Hardwick, Kristian Eicke\n      We tested two approaches to dealing with latency in network music. Read all about it!\n  \n\n\n  \n    \n    \n      Designing DFA and LAA Network Music Performances\n      networked-music\n      \n        Feb 16, 2023 • Alexander Wastnidge, Fabian Stordalen, Henrik Sveen, Masoud Niknafs, Nino Jakeli\n      Music Performances in High Latency\n  \n\n\n  \n    \n    \n      Markov Chain Core in PD\n      sound-programming\n      \n        Dec 10, 2022 • Masoud Niknafs, Nino Jakeli\n      Markov Chain Core in PD\n  \n\n\n  \n    \n    \n      Make The Stocks Sound\n      sound-programming\n      \n        Dec 10, 2022 • Masoud Niknafs, Nino Jakeli\n      Make The Stocks Sound\n  \n\n\n  \n    \n    \n      SMC Blog Sonified!\n      sound-programming\n      \n        Dec 9, 2022 • Emin Memis\n      Making sound out of this blog.\n  \n\n\n  \n    \n    \n      Sonifying the Northern Lights: Two Approaches in Python &amp; Pure Data\n      sound-programming\n      \n        Dec 9, 2022 • Jack Hardwick, Kristian Eicke, Emin Memis\n      Making 'music' from the Aurora Borealis.\n  \n\n\n  \n    \n    \n      Towards a Claptrap-Speaking Kastle Maus\n      interactive-music\n      \n        Dec 9, 2022 • Kristian Wentzel\n      Once upon a time, there was a maus living in a kastle..\n  \n\n\n  \n    \n    \n      Expressive Voice: an IMS for singing\n      interactive-music\n      \n        Dec 9, 2022 • Sofía González\n      Take a peak at my IMS and the reasoning behind its design.\n  \n\n\n  \n    \n    \n      Interactive Music Systems, Communication and Emotion\n      interactive-music\n      \n        Dec 9, 2022 • Sofía González\n      Talking about IMSs, expressing and even inducing emotions.\n  \n\n\n  \n    \n    \n      The sound of rain\n      sound-programming\n      \n        Dec 9, 2022 • Alexander Wastnidge, Aysima Karcaaltincaba, Fabian Stordalen\n      How we sonified the rainfall data.\n  \n\n\n  \n    \n    \n      Pringles, I love You (not)\n      interactive-music\n      \n        Dec 8, 2022 • Jakob Høydal\n      No, that is not true. I do not like Pringles. But I like the tube it comes with! That’s why I invited a friend over to eat the chips, so I could use the tube for my 4054 Interactive Music Systems project.\n  \n\n\n  \n    \n    \n      Three Takeaways as a musicologist\n      interactive-music\n      \n        Dec 8, 2022 • Ole Tveit Hana\n      Recounting the experience of making an instrument from scratch for the first time.\n  \n\n\n  \n    \n    \n      SR-01\n      interactive-music\n      \n        Dec 8, 2022 • Henrik Sveen\n      The climate aware synthesizer. Based on using few components while reacting to changes in light and temperature around it, causing it to sound different today than in a changed climate.\n  \n\n\n  \n    \n    \n      How to make your screen time a natural experience\n      interactive-music\n      \n        Dec 8, 2022 • Ole Tveit Hana\n      Learn how playing with mud could equate to playing with computer.\n  \n\n\n  \n    \n    \n      The Feedback Mop Cello: Making Music with Feedback - Part 2\n      interactive-music\n      \n        Dec 8, 2022 • Hugh Alexander von Arnim\n      Using a mop to play the feedback\n  \n\n\n  \n    \n    \n      Cellular Automata - Implementation in Pure Data\n      sound-programming\n      \n        Dec 8, 2022 • Kristian Eicke\n      Check out the concept of Cellular Automata and my implementation in Pure Data.\n  \n\n\n  \n    \n    \n      Concussion Percussion: A Discussion\n      interactive-music\n      \n        Dec 8, 2022 • Joseph Clemente\n      Whether it’s riding a bike or building an handpan-esque interactive music system, always remember to wear a helmet\n  \n\n\n  \n    \n    \n      Shimmerion - A String Synthesizer played with Light\n      interactive-music\n      \n        Dec 8, 2022 • Iosif Aragiannis\n      Use your phone's flashlight to make music!\n  \n\n\n  \n    \n    \n      Clean code\n      sound-programming\n      \n        Dec 8, 2022 • Aysima Karcaaltincaba\n      Any fool can write code that a computer can understand. Good programmers write code that humans can understand.\n  \n\n\n  \n    \n    \n      A Christmas tale of cookie boxes and soldering\n      interactive-music\n      \n        Dec 7, 2022 • Arvid Falch\n      How I got the cookie box drum I never knew I wanted for Christmas\n  \n\n\n  \n    \n    \n      Creating Complex Filters in Pure Data with Biquad~\n      sound-programming\n      \n        Dec 7, 2022 • Jack Hardwick\n      One approach to building/rebuilding complex filters in Pure Data, with a little help from Python.\n  \n\n\n  \n    \n    \n      Out-Of-The-Box Sound Sources for your IMS\n      interactive-music\n      \n        Dec 7, 2022 • Kristian Wentzel\n      Exploring alternatives for generating sounds with your interactive music system.\n  \n\n\n  \n    \n    \n      The Feedbackquencer: Making Music with Feedback - Part 1\n      interactive-music\n      \n        Dec 6, 2022 • Hugh Alexander von Arnim\n      Using feedback in a sequencer\n  \n\n\n  \n    \n    \n      Wii controller as the Gestural Controller\n      interactive-music\n      \n        Dec 6, 2022 • Thyra Liang Aakvåg\n      Read this post to find information on a different use of a Wiimote than playing games on your Wii console.\n  \n\n\n  \n    \n    \n      Music By Laser: The Laser Harp\n      interactive-music\n      \n        Dec 6, 2022 • Thyra Liang Aakvåg\n      If you want to know how to play music with lasers, and maybe learn something about the laser harp, then you should give this a read.\n  \n\n\n  \n    \n    \n      JackTrip Vs Sonobus - Review and Comparison\n      networked-music\n      \n        Dec 6, 2022 • Nino Jakeli\n      Low-latency online music performance platforms\n  \n\n\n  \n    \n    \n      The SMC Audio Vocabulary\n      networked-music\n      \n        Dec 6, 2022 • Kristian Eicke\n      Click this post if you need some explanation on jargon, mainly related to the Portal and the NMP kits.\n  \n\n\n  \n    \n    \n      Making A Telematic Concert Happen - A Quick Technical Look\n      networked-music\n      \n        Dec 4, 2022 • Emin Memis, Nino Jakeli\n      Background of A Telematic Experience\n  \n\n\n  \n    \n    \n      Live Streaming A Telematic Concert\n      networked-music\n      \n        Dec 4, 2022 • Emin Memis\n      Whys, How-tos and Life-Saving Tips on Telematic Concert Streaming.\n  \n\n\n  \n    \n    \n      Yggdrasil: An Environmentalist Interactive Music Installation\n      interactive-music\n      \n        Dec 3, 2022 • Oliver Getz\n      Plant trees and nurture your forest to generate sound!\n  \n\n\n  \n    \n    \n      Why Your Exhibit Tech Failed (and how to fix it)\n      interactive-music\n      \n        Dec 3, 2022 • Oliver Getz\n      Why are visitors not using your installation?  You might just disagree with yourself, the visitors, and your department or client about their needs and wants. The reason for this disagreement is always the same.\n  \n\n\n  \n    \n    \n      New Interface for Sound Evolution (NISE)\n      interactive-music\n      \n        Dec 1, 2022 • Björn Þór Jónsson\n      Robiohead is an attempt in a box to explore how an Interactive Music System (IMS) can offer ways to explore sound spaces by evolving sound producing genes in a tactile manner.\n  \n\n\n  \n    \n    \n      In search of sounds\n      other\n      \n        Dec 1, 2022 • Björn Þór Jónsson\n      Are you looking for sounds to inspire your next project? At synth.is you can discover new sounds by evolving their genes, either interactively or automatically.\n  \n\n\n  \n    \n    \n      An instrument, composition and performance. Meet the Transformer #1\n      interactive-music\n      \n        Nov 30, 2022 • Arvid Falch\n      A review of Natasha Barretts Transformer #1\n  \n\n\n  \n    \n    \n      Blackhole, Open Source MacOS Virtual Audio Device Solution for Telematic Performance\n      networked-music\n      \n        Nov 29, 2022 • Masoud Niknafs\n      MacOS guide to Blackhole\n  \n\n\n  \n    \n    \n      The Optic-Fibre Ensemble\n      networked-music\n      \n        Nov 28, 2022 • Fabian Stordalen, Masoud Niknafs\n      Folk inspired soundscapes + No input mixing=True\n  \n\n\n  \n    \n    \n      Bringing the (Optic) Fibre Ensemble to Life - Behind the Scenes of a Telematic Music Performance\n      networked-music\n      \n        Nov 28, 2022 • Kristian Eicke, Jack Hardwick\n      What does it take to put on a telematic music performance? Cable spaghetti of course!\n  \n\n\n  \n    \n    \n      Telematic Concert between Salen and Portal - Performers’ Reflections\n      networked-music\n      \n        Nov 28, 2022 • Alexander Wastnidge, Aysima Karcaaltincaba, Iosif Aragiannis\n      Reflections on semester's first Telematic Concert\n  \n\n\n  \n    \n    \n      Music Mode for Online Meetings\n      networked-music\n      \n        Nov 28, 2022 • Aysima Karcaaltincaba\n      It is possible to use popular meeting products for music!\n  \n\n\n  \n    \n    \n      Performing &amp; Recording with JackTrip\n      networked-music\n      \n        Nov 27, 2022 • Jack Hardwick\n      Performing and recording network music at home? Thanks to JackTrip, now you can do it too.\n  \n\n\n  \n    \n    \n      Zeusaphone - The singing Tesla coil\n      interactive-music\n      \n        Nov 26, 2022 • Iosif Aragiannis\n      Have you ever seen choreographed lightning?\n  \n\n\n  \n    \n    \n      Audio Engineering for Network Music Performances\n      networked-music\n      \n        Nov 24, 2022 • Alexander Wastnidge\n      How much more difficult could it POSSIBLY be?\n  \n\n\n  \n    \n    \n      Rehearsing Music Over the Network\n      networked-music\n      \n        Nov 21, 2022 • Fabian Stordalen\n      My experiences with rehearsing music telematically and some tips.\n  \n\n\n  \n    \n    \n      Dråpen, worlds largest interactive music system? \n      interactive-music\n      \n        Nov 13, 2022 • Jakob Høydal\n      This may be the largest interactive music system you have heard about\n  \n\n\n  \n    \n    \n      A Contact Microphone And A Dream: To Loop\n      interactive-music\n      \n        Nov 11, 2022 • Joseph Clemente\n      Join a humble contact microphone on its quest for freedom!\n  \n\n\n  \n    \n    \n      FAM Synthesizer\n      interactive-music\n      \n        Nov 10, 2022 • Henrik Sveen\n      A simple digital synthesizer with the potential of sounding big, complex and kind of analog. Screenless menudiving included.\n  \n\n\n  \n    \n    \n      Euclidean Rhythms in Pure Data\n      sound-programming\n      \n        Oct 21, 2022 • Alexander Wastnidge\n      What are Euclidean Rhythms and how can you program them?\n  \n\n\n  \n    \n    \n      The impact and importance of network-based musical collaboration (in the post-covid world)\n      networked-music\n      \n        Oct 3, 2022 • Iosif Aragiannis\n      The Covid-19 pandemic offered a unique opportunity (and necessity) to focus on the creative usage (and further development) of the technological tools used for network-based musical collaboration.\n  \n\n\n  \n    \n    \n      How to do No-Input Mixing in Pure Data\n      sound-programming\n      \n        Oct 1, 2022 • Fabian Stordalen\n      No-Input Mixing in Pure Data\n  \n\n\n  \n    \n    \n      Norway&#39;s First 5G Networked Music Performance\n      networked-music\n      \n        Jul 5, 2022 • Joachim Poutaraud, Kristian Wentzel, Leigh Murray, Lindsay Charles\n      We played Norway's first 5G networked music performance in collaboration with Telenor Research.\n  \n\n\n  \n    \n    \n      One Last Hoorah: A Telematic Concert in the Science Library\n      networked-music\n      \n        May 27, 2022 • Jakob Høydal, Joachim Poutaraud, Joseph Clemente, Kristian Wentzel\n      In which we go over the most ambitious telematic concert of our SMC careers.\n  \n\n\n  \n    \n    \n      Spring Telematic Concert 2022: Portal Perspectives\n      networked-music\n      \n        May 26, 2022 • Oliver Getz, Sofía González, Arvid Falch, Hugh Alexander von Arnim\n      Guitar and MoCap in the Portal + behind the scenes documentary!\n  \n\n\n  \n    \n    \n      Reverb Classification of wet audio signals\n      machine-learning\n      \n        May 20, 2022 • Jakob Høydal\n      Differenciating reverberation times of wet audio signals using machine learning. \n  \n\n\n  \n    \n    \n      Piano Accompaniment Generation Using Deep Neural Networks\n      machine-learning\n      \n        May 20, 2022 • Kristian Wentzel\n      How I made use of Fourier Transforms in deep learning to generate expressive MIDI piano accompaniments.\n  \n\n\n  \n    \n    \n      Recognizing and Predicting Individual Drumming Groove Styles Using Artificial Neural Networks\n      machine-learning\n      \n        May 20, 2022 • Joseph Clemente\n      Can we teach an algorithm to groove exactly like a specific drummer?\n  \n\n\n  \n    \n    \n      Developing Techniques for Air Drumming Using Video Capture and Accelerometers\n      motion-capture\n      \n        May 20, 2022 • Joseph Clemente\n      Creating MIDI scores using only data from air drumming\n  \n\n\n  \n    \n    \n      Reconfigurations: Reconfiguring the Captured Body in Dance\n      motion-capture\n      \n        May 20, 2022 • Hugh Alexander von Arnim\n      Building cooperative visual, kinaesthetic, and sonic bodies\n  \n\n\n  \n    \n    \n      Estimating the repertoire size in birds\n      machine-learning\n      \n        May 20, 2022 • Joachim Poutaraud\n      Estimating the repertoire size in birds using unsupervised clustering techniques\n  \n\n\n  \n    \n    \n      Myo My – That keyboard sure tastes good with some ZOIA on top\n      motion-capture\n      \n        May 20, 2022 • Kristian Wentzel\n      Extending the keyboard through gestures and modular synthesis.\n  \n\n\n  \n    \n    \n      Emulating analog guitar pedals with Recurrent Neural Networks\n      machine-learning\n      \n        May 19, 2022 • Arvid Falch\n      Using LSTM recurrent neural networks to model two analog guitar pedals.\n  \n\n\n  \n    \n    \n      Playing music standing vs. seated; whats the difference?\n      motion-capture\n      \n        May 19, 2022 • Jakob Høydal\n      A study of saxophonist in seated vs standing position\n  \n\n\n  \n    \n    \n      What is a gesture?\n      motion-capture\n      \n        May 19, 2022 • Sofía González\n      There are many takes on gesticulation and its meanings, however, I wanted to take the time to delimit what a gesture is, possible categories and gesturing patters.\n  \n\n\n  \n    \n    \n      Generating Samples Through Dancing\n      machine-learning\n      \n        May 3, 2022 • Hugh Alexander von Arnim\n      Using a VAE to build a generative sampler instrument\n  \n\n\n  \n    \n    \n      Setting up a controlled environment\n      networked-music\n      \n        Apr 24, 2022 • Joseph Clemente, Kristian Wentzel\n      Taking advantage of light-weight control messages to do Networked Music Performances\n  \n\n\n  \n    \n    \n      Ambisonics: Under the Hood\n      networked-music\n      \n        Apr 24, 2022 • Arvid Falch, Hugh Alexander von Arnim\n      What happens when we encode/decode Ambisonics\n  \n\n\n  \n    \n    \n      How to Set Up Hybrid Learning Environments\n      networked-music\n      \n        Apr 24, 2022 • Sofía González, Oliver Getz\n      Learn how to set up a hybrid learning environment, ranging from simple to complex, serving as a starting point to help your classroom catch up to the digital age.\n  \n\n\n  \n    \n    \n      Managing Network Performance\n      networked-music\n      \n        Apr 22, 2022 • Jakob Høydal, Joachim Poutaraud\n      Managing Network Performance using Python\n  \n\n\n  \n    \n    \n      5G Networked Music Performances - Will It Work?\n      networked-music\n      \n        Apr 11, 2022 • Aleksander Tidemann, Stefano Fasciani\n      In collaboration with Telenor Research, we explored the prospects of doing networked music performances over 5G. Here are the preliminary results.\n  \n\n\n  \n    \n    \n      Generating Music Based on Webcam Input\n      machine-learning\n      \n        Mar 23, 2022 • Arvid Falch, Hugh Alexander von Arnim, Jakob Høydal, Joachim Poutaraud\n      Do you miss PS2 EyeToy? Then you have to check this out!\n  \n\n\n  \n    \n    \n      Using Live OSC Data From Smartphones To Make Music With Sonic Pi\n      machine-learning\n      \n        Mar 23, 2022 • Joseph Clemente, Kristian Wentzel\n      Easy as pi!\n  \n\n\n  \n    \n    \n      Mastering Latency\n      networked-music\n      \n        Feb 21, 2022 • Hugh Alexander von Arnim, Kristian Wentzel, Sofía González, Joachim Poutaraud\n      Testing two techniques to work with latency when playing music telematically\n  \n\n\n  \n    \n    \n      Latency: To Accept or to Delay Feedback?\n      networked-music\n      \n        Feb 21, 2022 • Arvid Falch, Joseph Clemente, Jakob Høydal, Oliver Getz\n      We tested two of the solutions—the Delayed Feedback Approach (DFA) and the Latency Accepting Approach (LAA)—so you don’t have to!\n  \n\n\n  \n    \n    \n      &#39;Chasing Stars&#39;: An interactive spatial audio application\n      spatial-audio\n      \n        Dec 10, 2021 • Pedro Lucas\n      Let's explore an interactive 3D-audio application under an ethereal sound landscape.\n  \n\n\n  \n    \n    \n      The Granjular Christmas Concert (Portal View)\n      networked-music\n      \n        Dec 10, 2021 • Joseph Clemente, Kristian Wentzel, Sofía González, Arvid Falch\n      A report on our telematic performance in the Portal.\n  \n\n\n  \n    \n    \n      The Telematic Experience: Space in Sound in Space\n      networked-music\n      \n        Dec 10, 2021 • Hugh Alexander von Arnim, Jakob Høydal, Joachim Poutaraud, Oliver Getz\n      A report on our telematic experience in Salen\n  \n\n\n  \n    \n    \n      Setting Levels in Virtual Communication\n      networked-music\n      \n        Nov 26, 2021 • Jarle Steinhovden\n      Inspired by how virtual communication has been adopted and integrated in daily life around the globe, this post looks at how simple control messages might help prevent acoustic feedback.\n  \n\n\n  \n    \n    \n      Chorale Rearranger: Chopping Up Bach\n      sound-programming\n      \n        Nov 26, 2021 • Joseph Clemente, Hugh Alexander von Arnim, Jakob Høydal, Oliver Getz\n      Can we use Midi data to rearrange a recording of a Bach chorale?\n  \n\n\n  \n    \n    \n      Mixing a multitrack project in Python\n      sound-programming\n      \n        Nov 24, 2021 • Joachim Poutaraud, Kristian Wentzel, Sofía González, Arvid Falch\n      Attempting to mix a multitrack song with homemade FX in our very own mini PythonDAW\n  \n\n\n  \n    \n    \n      Audio-video sync\n      networked-music\n      \n        Nov 15, 2021 • Anders Lidal\n      Due to the fact that sound travels a lot slower through air than the light, our brain is used to seeing before hearing.\n  \n\n\n  \n    \n    \n      Video latency: definition, key concepts, and examples\n      networked-music\n      \n        Nov 15, 2021 • Alena Clim\n      This blogpost is made after the video lecture on the same topic and it includes a definition of video latency and other related key concepts, as well as concrete examples from the SMC portals.\n  \n\n\n  \n    \n    \n      Latency as an opportunity to embrace\n      networked-music\n      \n        Nov 15, 2021 • Wenbo Yi\n      How can we turn unavoidable latency into an integral part of telematic performance?\n  \n\n\n  \n    \n    \n       Audio Latency in the Telematic Setting\n      networked-music\n      \n        Nov 15, 2021 • Abhishek Choubey\n      Latency and its fundamentals in the telematic performance context. \n  \n\n\n  \n    \n    \n      Room acoustics: what are room modes and how do they influence the physical space?\n      networked-music\n      \n        Nov 15, 2021 • Lindsay Charles\n      This blog post explains what room modes are, how they affect the physical space and what can be done about it. It was made together with a video lecture.\n  \n\n\n  \n    \n    \n      Audio-Video Synchronization for Streaming\n      networked-music\n      \n        Nov 15, 2021 • Pedro Lucas\n      This approach considers a streaming solution from multiple sources and different locations (Salen, Video Room, Portal)\n  \n\n\n  \n    \n    \n      Touchpoint that can potentially improve the audio latency in a communication system\n      networked-music\n      \n        Nov 14, 2021 • Joni Mok\n      Quick tips for future SMC-ers or external partners who will be using the SMC Portal for the first time: this article gives you practical information to start with improving audio latency.\n  \n\n\n  \n    \n    \n      A short post about feedback\n      networked-music\n      \n        Nov 14, 2021 • Stephen Gardener\n      Feeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeedback\n  \n\n\n  \n    \n    \n      Telematic Conducting: Modelling real-world orchestral tendencies via video latency\n      networked-music\n      \n        Nov 14, 2021 • Willie Mandeville\n      A conductor in one portal. An orchestra in another. What could go wrong?\n  \n\n\n  \n    \n    \n      EarSketch (Or How to Get More Python in Your Life)\n      networked-music\n      \n        Nov 5, 2021 • Joachim Poutaraud, Hugh Alexander von Arnim\n      A review of the asynchronous music production software 'EarSketch'\n  \n\n\n  \n    \n    \n      SkyTracks: What’s the Use?\n      networked-music\n      \n        Nov 3, 2021 • Arvid Falch, Joseph Clemente\n      A review and critique of the online DAW SkyTracks for asynchronous collaboration\n  \n\n\n  \n    \n    \n      The Triadmin\n      interactive-music\n      \n        Nov 3, 2021 • Halvor Sogn Haug\n      An instrument without any tangible interface.\n  \n\n\n  \n    \n    \n      The algorithmic note stack juggler\n      interactive-music\n      \n        Nov 3, 2021 • Stephen Gardener\n      Interactive composition with the Algorithmic Note Stack Juggler.\n  \n\n\n  \n    \n    \n      Satellite Sessions - Connecting DAWs\n      networked-music\n      \n        Nov 3, 2021 • Jakob Høydal, Kristian Wentzel\n      A review of Satellite Sessions, a plugin that connects digital audio workstations and creators.\n  \n\n\n  \n    \n    \n      Sequencephere-Linesequencer\n      interactive-music\n      \n        Nov 3, 2021 • Abhishek Choubey\n      Exploration and design of a drum Sequencer and synth using Bela as an interactive music system with Csound\n  \n\n\n  \n    \n    \n      D&#39;n&#39;B\n      interactive-music\n      \n        Nov 1, 2021 • Lindsay Charles\n      Exploration and design of the 'Drum and Bass' interactive music system with Csound\n  \n\n\n  \n    \n    \n      Ethrio\n      interactive-music\n      \n        Nov 1, 2021 • Pedro Lucas\n      Ethereal sounds from the three dimensions of music: Melody, Harmony, and Rhythm.\n  \n\n\n  \n    \n    \n      Soundation Review: What to Expect\n      networked-music\n      \n        Nov 1, 2021 • Sofía González, Oliver Getz\n      A concise review on the collaborative online DAW: Soundation.\n  \n\n\n  \n    \n    \n      Audio Blending in Python\n      sound-programming\n      \n        Oct 27, 2021 • Hugh Alexander von Arnim, Joachim Poutaraud, Sofía González, Oliver Getz\n      Blending audio tracks based on transients in Python.\n  \n\n\n  \n    \n    \n      A Brief History of Improvisation Through Network Systems\n      networked-music\n      \n        Sep 26, 2021 • Kristian Wentzel\n      A glimpse into the evolution of online improvisation and shared sonic environments.\n  \n\n\n  \n    \n    \n      Embodiment and Awareness in Telematic Music and Virtual Reality\n      networked-music\n      \n        Sep 23, 2021 • Jakob Høydal\n      A discisson about what Embodiment and Awareness means, and how its beeing used in Telematic Music and VR.\n  \n\n\n  \n    \n    \n      Telematic Reality Check: An Evaluation of Design Principles for Telematic Music Applications in VR Environments\n      networked-music\n      \n        Sep 22, 2021 • Oliver Getz\n      7 steps for better virtual reality music applications!\n  \n\n\n  \n    \n    \n      Applying Actor-Network Methodology to Telematic Network Topologies\n      networked-music\n      \n        Sep 22, 2021 • Hugh Alexander von Arnim\n      An inquiry into the application of an actor-network theory methodology to Gil Weinberg's telematic musical network topologies.\n  \n\n\n  \n    \n    \n      Telematic performance and communication: tools to fight loneliness towards a future of connection.\n      networked-music\n      \n        Sep 22, 2021 • Sofía González\n      With telematic interaction on the rise during a global pandemic, we should explore telematic performances to help prevent loneliness and feelings of isolation through art.\n  \n\n\n  \n    \n    \n      Approaches Toward Algorithmic Interdependence in Musical Performance\n      networked-music\n      \n        Sep 22, 2021 • Joseph Clemente\n      Is it possible to program interdependent algorithms to perform with each other?\n  \n\n\n  \n    \n    \n      Fight latency with latency\n      networked-music\n      \n        Sep 22, 2021 • Arvid Falch\n      Alternative design approaches for telematic music systems.\n  \n\n\n  \n    \n    \n      Can Machine learning classify audio effects, a dry to wet sound ?\n      machine-learning\n      \n        Sep 20, 2021 • Abhishek Choubey\n      Distortion or No Distortion - Machine learning magic\n  \n\n\n  \n    \n    \n      Ensemble algorithms and music classification\n      machine-learning\n      \n        Sep 20, 2021 • Alena Clim\n      Playing around with some supervised machine learning - genre classification is hard!\n  \n\n\n  \n    \n    \n      Internet time delay: a new musical language for a new time basis\n      networked-music\n      \n        Sep 20, 2021 • Joachim Poutaraud\n      Logistical considerations for large-scale telematic performances involving geographically displaced contributors still remain strongly present. Therefore, if networked performers are still to the vagaries of speed and bandwidth of multiple networks and if latency  problems remain a significant issue for audio-visual streaming of live Network Music Performance (NMP), one can rather reflect on trying  to find a new musical language for a new time basis.\n  \n\n\n  \n    \n    \n      Classifying Classical Piano Music Based On Composer’s Native Language Using Machine Learning\n      machine-learning\n      \n        Sep 19, 2021 • Wenbo Yi\n      How does the language we speak help computers to classify classical music?\n  \n\n\n  \n    \n    \n      Estimation of Direction of Arrival (DOA) for First Order Ambisonic (FOA) Audio Files\n      machine-learning\n      \n        Sep 18, 2021 • Pedro Lucas\n      Where is that sound coming from? Let's explore how a machine could answer this question.\n  \n\n\n  \n    \n    \n      Ensemble algorithms and music classification\n      machine-learning\n      \n        Sep 17, 2021 • Stephen Gardener\n      'Why is shoegaze so hard to classify?' and other pertinent questions for the technologically inclined indie-kid.\n  \n\n\n  \n    \n    \n      Concert preparations: exploring the Portal\n      networked-music\n      \n        May 5, 2021 • Alena Clim, Leigh Murray, Abhishek Choubey\n      What we learned about the Portal and telematic performances in general while preparing our musical pieces for the end of semester concert. Details about our instrumentation and effects.\n  \n\n\n  \n    \n    \n      Preparing NTNU portal for the spring 2021 Concert.\n      networked-music\n      \n        May 5, 2021 • Abhishek Choubey\n      How the NTNU Portal was setup for the spring concert.\n  \n\n\n  \n    \n    \n      End of semester reflections for the Portal experience\n      networked-music\n      \n        May 4, 2021 • Wenbo Yi, Lindsay Charles, Stephen Gardener\n      The first physical portal experience during the pandemic\n  \n\n\n  \n    \n    \n      The collision of Jazz and Classical\n      networked-music\n      \n        May 4, 2021 • Wenbo Yi, Lindsay Charles, Stephen Gardener\n      The rehearsal experience for our telematic performance.\n  \n\n\n  \n    \n    \n      The SMC Portal through the eyes of a noob\n      networked-music\n      \n        May 4, 2021 • Alena Clim\n      The experiences I had in the SMC Portal during the spring 2021 semester and my evolution from a complete beginner to an almost-average user of a technical and awesome room like the SMC Portal.\n  \n\n\n  \n    \n    \n      End of Semester Concert - Spring 2021: Audio Setup\n      networked-music\n      \n        May 4, 2021 • Anders Lidal, Pedro Lucas\n      As part of the audio team for the end-of-semester telematic concert, Pedro and Anders spent several hours in the portal, exploring different ways to organize audio routing. They also found time to experiment with effect loops. Check out the nice musical collaboration between two different musical cultures.\n  \n\n\n  \n    \n    \n      Cross the Streams\n      networked-music\n      \n        May 3, 2021 • Leigh Murray\n      When performing in two locations we need to cross the streams\n  \n\n\n  \n    \n    \n      Dispatch from the Portal: Dueling EQs\n      networked-music\n      \n        May 3, 2021 • Henrik Sveen, Anders Lidal, Pedro Lucas, Willie Mandeville\n      How do I sound? Good? What does good mean? How do I sound? Sigh...\n  \n\n\n  \n    \n    \n      Spring Concert 2021: Team B&#39;s Reflections\n      networked-music\n      \n        May 1, 2021 • Henrik Sveen, Anders Lidal, Pedro Lucas, Willie Mandeville\n      We'll do it live. Team B gets its groove Bach.\n  \n\n\n  \n    \n    \n      Walking in Seasons\n      motion-capture\n      \n        Apr 21, 2021 • Abhishek Choubey, Lindsay Charles, Joel Vide Hynsjö\n      Sonification of motion\n  \n\n\n  \n    \n    \n      Exploring the influence of expressive body movement on audio parameters of piano performances\n      motion-capture\n      \n        Apr 18, 2021 • Wenbo Yi\n      How expressive body movement influence music?\n  \n\n\n  \n    \n    \n      Motion (and emotion) in recording\n      motion-capture\n      \n        Apr 18, 2021 • Anders Lidal\n      The first time I went to a recording studio in the early nineties, my eagerness to (music)-world domination—as well as my fascination for the possibility to put my beautiful playing to a magnetic tape—totally over-shadowed that the result sounded crappy, at least for a while.\n  \n\n\n  \n    \n    \n      Shim-Sham Motion Capture\n      motion-capture\n      \n        Apr 18, 2021 • Alena Clim\n      We've learned about motion capture in a research environment. But what about motion capture in the entertainment field? In this project I attempted to make an animation in Blender based on motion captured in the lab using the Optitrack system. Beside this, I also analysed three takes of a Shim Sham dance. For more details and some sneak peaks read this blog post.\n  \n\n\n  \n    \n    \n      Kodaly EarTrainer-App\n      motion-capture\n      \n        Apr 18, 2021 • Thomas Anda\n      App for training your ears based on old Hungarian methodolgy\n  \n\n\n  \n    \n    \n      &#39;Air&#39; Instruments Based on Real-Time Motion Tracking\n      motion-capture\n      \n        Apr 17, 2021 • Pedro Lucas\n      Let's make music with movements in the air.\n  \n\n\n  \n    \n    \n      An air guitar experiment with OpenCV\n      motion-capture\n      \n        Apr 17, 2021 • Stephen Gardener\n      Get to know OpenCV by building an air guitar player. Shaggy perm not required.\n  \n\n\n  \n    \n    \n      Posture Guard\n      motion-capture\n      \n        Apr 16, 2021 • Henrik Sveen\n      Back pains, neck pains, shoulder pains - what do they all have in common? They are caused by bad posture while working on a laptop. So I made a program that makes the laptop help out maintaining a good posture while working.\n  \n\n\n  \n    \n    \n      Growing Monoliths Discovered On Mars\n      sonification\n      \n        Mar 26, 2021 • Henrik Sveen, Stephen Gardener, Pedro Lucas\n      Mars is a hot topic these days, and weather seems to always be a hot topic too. So how about making a project with both? We ended up gamifying the weather on Mars by discovering the musical potential it may have.\n  \n\n\n  \n    \n    \n      Valkyrie: Aurora Sonified\n      sonification\n      \n        Mar 26, 2021 • Abhishek Choubey, Lindsay Charles, Joel Vide Hynsjö\n      The sound of Aurora sonified in a synth\n  \n\n\n  \n    \n    \n      Pixel by pixel\n      sonification\n      \n        Mar 25, 2021 • Anders Lidal, Mari Lesteberg, Sebastian Olsen\n      The pixel sequencer is an interactive web app for sonification of images. Get online, upload your favorite picture, sit back and listen to it.\n  \n\n\n  \n    \n    \n       Tele-A-Jammin: the recipe\n      networked-music\n      \n        Mar 5, 2021 • Abhishek Choubey, Alena Clim, Leigh Murray\n      Our portal experience mapped on a delicious Jam recipe\n  \n\n\n  \n    \n    \n      First Weeks Of Portaling With Team B\n      networked-music\n      \n        Mar 3, 2021 • Henrik Sveen, Anders Lidal, Pedro Lucas, Willie Mandeville\n      Starting to figure out the Portal and what we did/found out in the first weeks using it. How to play and some feedback fixing.\n  \n\n\n  \n    \n    \n      Liebesgruß or we can put that &#39;Liebe&#39; aside\n      networked-music\n      \n        Feb 28, 2021 • Joni Mok, Wenbo Yi, Lindsay Charles, Stephen Gardener\n      It's simply a gruß from Team C with our first telematic setup.\n  \n\n\n  \n    \n    \n      Twinkle Twinkle (sad) Little Mars\n      networked-music\n      \n        Feb 28, 2021 • Henrik Sveen, Anders Lidal, Pedro Lucas, Willie Mandeville\n      Finally the B-boys found some hours one evening to spend in the portal, Willie up north, and Pedro, Henrik and Anders down south. This was the first day on their Mission to Mars. Enter our B-oyager, and join us.\n  \n\n\n  \n    \n    \n      Tapeloop DT-20\n      sound-programming\n      \n        Feb 19, 2021 • Henrik Sveen, Anders Lidal, Pedro Lucas, Stephen Gardener\n      For the SMC4048 we wanted to make some looper with layers and FX. But what did we get? The DT-20, a 4-track inspired digital tape looper with FX channels and a 'WASD'-mixer. Enjoy! - Stephen, Pedro, Anders &amp; Henrik\n  \n\n\n  \n    \n    \n      Get unstressed with Stress-less\n      sound-programming\n      \n        Feb 19, 2021 • Joni Mok, Dongho Kwak\n      Acoustically-triggered heart rate entrainment (AHRE)\n  \n\n\n  \n    \n    \n      Trinity: Triple Threat\n      sound-programming\n      \n        Feb 18, 2021 • Abhishek Choubey, Tom Ignatius, Lindsay Charles\n      Trinity: What happens when you combine Grain shimmer + Chorus + Stereo Width? Click to find.\n  \n\n\n  \n    \n    \n      Funky Balls\n      sound-programming\n      \n        Feb 17, 2021 • Alena Clim, Leigh Murray\n      Want a more organic and dynamic way of mixing and applying effects? Experiment with funky balls!\n  \n\n\n  \n    \n    \n      Clubhouse\n      other\n      \n        Feb 2, 2021 • Henrik Sveen\n      Just a new phone or could it actually contribute to our online lives? This is not part of any course or anything, but I wanted to put down my thoughts on this new app. After all, it's all about audio and streaming which is very SMC, so I hope it's OK.\n  \n\n\n  \n    \n    \n      Strange Fragmented Music\n      sound-programming\n      \n        Dec 8, 2020 • Lindsay Charles\n      A strange song from Sequencing fragments of stranger things and Astronomia \n  \n\n\n  \n    \n    \n      A new season of chopped music\n      sound-programming\n      \n        Dec 5, 2020 • Abhishek Choubey\n      The arbitrary mixing of two seasons (two songs representing seasons)\n  \n\n\n  \n    \n    \n      Chopping Chopin and Kapustin preludes\n      sound-programming\n      \n        Dec 5, 2020 • Alena Clim\n      Chopping a prelude by Chopin and one by Kapustin and then merging the slices based on their loudness (RMS) and tonality (Spectral Flatness).\n  \n\n\n  \n    \n    \n      Chaotic Battle of Music Slices\n      sound-programming\n      \n        Dec 5, 2020 • Wenbo Yi\n      Slicing up the music and reorganize them with four kinds of Librosa features.\n  \n\n\n  \n    \n    \n      Merry slicemas\n      sound-programming\n      \n        Dec 5, 2020 • Henrik Sveen\n      Let it slice, let it slice, let it sequence.\n  \n\n\n  \n    \n    \n      Graphic score no. 7\n      sound-programming\n      \n        Dec 4, 2020 • Anders Lidal\n      So … it’s 00:01 in Oslo, December 1st, and the final SciComp assignment is here … And … it’s a though one … 2345 words … AI must be involved here …\n  \n\n\n  \n    \n    \n      Chop it Up: Merging Rearranged Audio\n      sound-programming\n      \n        Dec 4, 2020 • Willie Mandeville\n      Taking beautiful music and making it less so.\n  \n\n\n  \n    \n    \n      My Bloody Zerox\n      sound-programming\n      \n        Dec 4, 2020 • Stephen Gardener\n      So the plan was to take two audio files, chop them up in some random way, mix up the pieces and stitch them back together again in a totally different order. Doesn’t have to be musical, they said. Well, just to make sure, I decided to chose two pieces of music that weren’t particularly musical to begin with - Only Shallow by shoegaze band My Bloody Valentine, and Zerox, by post punk combo Adam and the Ants.\n  \n\n\n  \n    \n    \n      Segmentation and Sequencing from and to Multichannel Audio Files\n      sound-programming\n      \n        Dec 4, 2020 • Pedro Lucas\n      The strategy explained here considers the slicing of stereo audio files and the production of a new stereo output file based on a multichannel solution. The spectral centroid is used to reorder and intercalate segments in an ascending or descending order according to some rules.\n  \n\n\n  \n    \n    \n      Dark Tetris\n      sound-programming\n      \n        Dec 2, 2020 • Joni Mok\n      Scientific Computing assignment for a non-programmer, please don't expect anything special when you see the title about DARK Tetris, even I really want to create that...\n  \n\n\n  \n    \n    \n      Slicing and Dicing Audio Samples\n      sound-programming\n      \n        Dec 2, 2020 • Leigh Murray\n      Slicing up two songs and re-joining them with RMS and Spectral Centroid values.\n  \n\n\n  \n    \n    \n      How To Save The World In Three Chords\n      spatial-audio\n      \n        Nov 17, 2020 • Aleksander Tidemann, Paul Koenig\n      Don't run! We come in peace!\n  \n\n\n  \n    \n    \n      Music for dreams\n      spatial-audio\n      \n        Nov 16, 2020 • Ulrik Halmøy, Tom Ignatius, Simon Sandvik, Thibault Jaccard\n      We tried to make a spatial audio composition based around a dream\n  \n\n\n  \n    \n    \n      Peace in Chaos: A Spatial Audio Composition\n      spatial-audio\n      \n        Nov 16, 2020 • Jackson Goode, Mari Lesteberg, Thomas Anda\n      Our spatial audio composition highlighted the ways audio can represent both peaceful and chaotic environments and transitions in between.\n  \n\n\n  \n    \n    \n      Zoom here &amp; Zoom there: Ambisonics\n      networked-music\n      \n        Nov 14, 2020 • Alena Clim, Abhishek Choubey, Leigh Murray\n       A more natural way of communicating online, wherein it feels like all the members are in the same room talking.\n  \n\n\n  \n    \n    \n      Ambisonic as a ‘mental’ management tool in Zoom?\n      networked-music\n      \n        Nov 14, 2020 • Joni Mok, Stephen Gardener, Wenbo Yi, Lindsay Charles\n      An immersive audio as a tool to keep our state of mind peaceful.\n  \n\n\n  \n    \n    \n      Zoom + Ambisonics\n      networked-music\n      \n        Nov 10, 2020 • Pedro Lucas, Willie Mandeville, Henrik Sveen, Anders Lidal\n      Talking together in an online room is an interesting topic, as the mono summed communication in regular Zoom can be tiring when meeting for several hours a day. Could ambisonics in digital communication be the solution we're all waiting for?\n  \n\n\n  \n    \n    \n      SYNTHa\n      sound-programming\n      \n        Nov 6, 2020 • Anders Lidal\n      While this is not a clone of the classic EMS Synthi A, it might have a trick or two up it's sliders, this one to: A softsynth with hard features.\n  \n\n\n  \n    \n    \n      Everything&#39;s Out of Tune (And Nothing Is)\n      sound-programming\n      \n        Nov 6, 2020 • Willie Mandeville\n      Who says that Csound and early music can't mix? Building a VSTi for swappable historical temperaments.\n  \n\n\n  \n    \n    \n      Butter: a multi-effect plugin\n      sound-programming\n      \n        Nov 6, 2020 • Abhishek Choubey\n      An easy to use and fun plugin to make your sound smooth as butter.\n  \n\n\n  \n    \n    \n      A fake Steinway made by Csound\n      sound-programming\n      \n        Nov 6, 2020 • Wenbo Yi\n      How to use Csound to create a digital Steinway?\n  \n\n\n  \n    \n    \n      Spaty Synthy\n      sound-programming\n      \n        Nov 6, 2020 • Lindsay Charles\n      Attempt at modelling a Delay Repeating Spatial Synthesizer using Csound\n  \n\n\n  \n    \n    \n      Phade - a phaser delay FX\n      sound-programming\n      \n        Nov 5, 2020 • Stephen Gardener\n      A swirly, shimmery shoegazey multi-fx.\n  \n\n\n  \n    \n    \n      Beethoven under Moonlight\n      sound-programming\n      \n        Nov 5, 2020 • Alena Clim\n      Creating a CSound project that uses Frequency Modulation synthesis (carrier and modulating oscillator) and plays the first few bars of the Moonlight Sonata by Beethoven based on a score.\n  \n\n\n  \n    \n    \n      Ondes Martenot&#39;s brother - Orange Cabbage\n      sound-programming\n      \n        Nov 5, 2020 • Joni Mok\n      The evolution of Ondes Martenot.\n  \n\n\n  \n    \n    \n      &#39;El Camino de los Lamentos&#39;: A performance using custom Csound VST Plugins\n      sound-programming\n      \n        Nov 3, 2020 • Pedro Lucas\n      This performance is using two VST plugins produced in Cabbage through Csound. A synthesizer based on elemental waveforms, frequency modulation, and stereo delay, and an audio effect for pitch scaling and reverb.\n  \n\n\n  \n    \n    \n      Kovid Keyboard\n      sound-programming\n      \n        Nov 3, 2020 • Leigh Murray\n      A web based CSound Synth letting you play together online!\n  \n\n\n  \n    \n    \n      Shaaape\n      sound-programming\n      \n        Nov 2, 2020 • Henrik Sveen\n      Distorting signals with ghost signals.\n  \n\n\n  \n    \n    \n      Breathe the light, scream arpeggios!\n      interactive-music\n      \n        Oct 20, 2020 • Rayam Luna\n      Multisensorial music interface aiming to provide a synesthetic experience. Touch, light, breathe, scream - make sound!\n  \n\n\n  \n    \n    \n      The Ring Synth\n      interactive-music\n      \n        Oct 20, 2020 • Thibault Jaccard\n      Exploring speed as sound shaping parameter\n  \n\n\n  \n    \n    \n      HyperGuitar\n      interactive-music\n      \n        Oct 19, 2020 • Thomas Anda\n      An exploration of limitations and how to create meaningful action-sound couplings.\n  \n\n\n  \n    \n    \n      The singing shelf bracket\n      interactive-music\n      \n        Oct 18, 2020 • Mari Lesteberg\n      Pure Data (PD) has a lot of possibilities, but when getting the opportunity of putting together all of those digital features into the real word: with real wires, real buttons, real sensors - I must admit - I got a little over-excited!\n  \n\n\n  \n    \n    \n      Plunged Into Chaos\n      interactive-music\n      \n        Oct 18, 2020 • Paul Koenig\n      Wherein the lowly Oompa-Doompa assumes its ultimate form.\n  \n\n\n  \n    \n    \n      MIDI and Effects: the Musicpathy \n      interactive-music\n      \n        Oct 18, 2020 • Abhishek Choubey, Lindsay Charles\n      The magic of controlling instruments from 1000km apart\n  \n\n\n  \n    \n    \n      Voice augmentation with sensors\n      interactive-music\n      \n        Oct 18, 2020 • Ulrik Halmøy\n      Trying to achieve a choir-like effect by augmenting microphone input with sensory features\n  \n\n\n  \n    \n    \n      SamTar\n      interactive-music\n      \n        Oct 17, 2020 • Aleksander Tidemann\n      An interactive music system exploring sample-based music and improvisation through an augmented electric guitar \n  \n\n\n  \n    \n    \n      The Psychedelic Journey of an Unexpected Spaceship\n      interactive-music\n      \n        Oct 16, 2020 • Pedro Lucas\n      An electronic music performance in an audio-visual digital environment. Let's go through the galaxy in a crazy spaceship and have an experience full of color and funny turbulence.\n  \n\n\n  \n    \n    \n      A Live Mixer made from mobile devices\n      interactive-music\n      \n        Oct 16, 2020 • Wenbo Yi\n      How does it look to control audio effects in real-time using gestures?\n  \n\n\n  \n    \n    \n      cOSmoChaos\n      interactive-music\n      \n        Oct 16, 2020 • Anders Lidal\n      Cosmos and chaos are opposites—known/unknown, habitated/unhabitated—and man has through all times been seeking to create cosmos out of chaos. But what has this to do with GyrOSC controlling my hardware … well, everything.\n  \n\n\n  \n    \n    \n      Chance Operations, Rudimentary Pure Data (PD), and a Bunch of Spinning in Circles\n      interactive-music\n      \n        Oct 16, 2020 • Willie Mandeville\n      Sometimes you want to compose and get your workout. Experience a chance composition that may leave the performer sweating.\n  \n\n\n  \n    \n    \n      Improvised electronica with TouchOSC\n      interactive-music\n      \n        Oct 16, 2020 • Stephen Gardener\n      In this project, I wanted to explore the options available when performing electronic music live with no pre-recorded / pre-sequenced material.\n  \n\n\n  \n    \n    \n      Real-time audio processing with oscHook and Reaper\n      interactive-music\n      \n        Oct 16, 2020 • Alena Clim\n      Fun and not too complicated interactive audio processing.Using oscHook to transmit sensordata from an Android phone to OSC Router and then to Reaper to control the values of certain effects' parameters.\n  \n\n\n  \n    \n    \n      The Dolphin Drum\n      interactive-music\n      \n        Oct 16, 2020 • Simon Sandvik\n      My granular synthesis percussive instrument from the Interactive Music Systems course.\n  \n\n\n  \n    \n    \n      Musings with Bela\n      interactive-music\n      \n        Oct 16, 2020 • Jackson Goode\n      A tale of accelerometers, knobs, an EEG and the attempt to tame sound with my mind. Follow along!\n  \n\n\n  \n    \n    \n      The voice of a loved one\n      interactive-music\n      \n        Oct 16, 2020 • Joni Mok\n      Can AI really know what our facial expressions mean?\n  \n\n\n  \n    \n    \n      Team C&#39;s reflections on Scientific Computing\n      sound-programming\n      \n        Oct 15, 2020 • Joni Mok, Stephen Gardener, Wenbo Yi, Lindsay Charles\n      Despite our diverse professional backgrounds, three of us are pretty much beginner pythonista.\nAll of our code seems pretty readable. Joni’s code is very clean, possible because she’s a designer, with good commenting. Wenbo favoured a single cell for his code. Upon the review on everyone’s submissions, we agreed that our team activity in the future has to increase.\nMeister Stephen will have to be more patient with us!\n\n  \n\n\n  \n    \n    \n      Reflections on Scientific Computing (The B Team Bares All)\n      sound-programming\n      \n        Oct 15, 2020 • Pedro Lucas, Willie Mandeville, Henrik Sveen, Anders Lidal\n      Team B reflects on a week of coding ups and downs.\n  \n\n\n  \n    \n    \n      Making virtual guitar playing feel more natural\n      interactive-music\n      \n        Oct 15, 2020 • Leigh Murray\n      Can using sensors, buttons and joysticks to play a virtual Guitar resemble the experience of playing a real guitar and result in a more natural performance than using a keyboard for input?\n  \n\n\n  \n    \n    \n      Étude de téléphone portable et popsocket\n      interactive-music\n      \n        Oct 14, 2020 • Henrik Sveen\n      Click to see a cute dog making strange music. Unbelievable. I think that sums it up.\n  \n\n\n  \n    \n    \n      Scientific Computing Midterm\n      sound-programming\n      \n        Oct 14, 2020 • Alena Clim, Leigh Murray, Abhishek Choubey\n      Team A's reflection on the midterm scientific computing assignment. Shortly, this had a purpose of creation of a general python program that would read audio files from a specified folder based on a csv file and would output another csv file with an added values for each individual audio file: the average Root Mean Square, Zero Crossing Rate, Spectral Centroid. Moreover, the program displays and saves to a file several scatterplots.\n  \n\n\n  \n    \n    \n      Musicking with JackTrip, JamKazam, and SoundJack - Presentations\n      networked-music\n      \n        Oct 8, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      The class of 2021 presented on JackTrip, SoundJack and JamKazam and their networked musicking potential. Presentations are included in this blog post as pdfs.\n  \n\n\n  \n    \n    \n      The Joys of Jitsi\n      networked-music\n      \n        Sep 28, 2020 • Joni Mok, Wenbo Yi, Lindsay Charles, Stephen Gardener\n      Jitsi is a venerable open source chat and video-conferencing app thats been around since 2003. It's multi-platform, and runs pretty much everywhere. We were given the task of testing the desktop apps on MacOS and Windows, and the mobile apps on Android and iOS.\n  \n\n\n  \n    \n    \n      Zoom - High Fidelity and Stereo\n      networked-music\n      \n        Sep 28, 2020 • Pedro Lucas, Willie Mandeville, Henrik Sveen, Anders Lidal\n      For the members of Team B, Zoom meetings are an everyday occurrence. Like most people during the covid era, we spend much of our professional time communicating from the comfort of our living rooms. These days, using Zoom feels akin to wearing clothes; we’re almost always doing it (sometimes even at the same time).\n  \n\n\n  \n    \n    \n      Pedál to the Metal\n      networked-music\n      \n        Sep 26, 2020 • Alena Clim, Leigh Murray, Abhishek Choubey\n      Another week, another audio-streaming platform to test. Here is Team A's impressions of Pedál. According to their website, Pedál is a cross platform service to stream audio, speak, share screen, and record high quality audio together and the simplest way to make music together online.\n  \n\n\n  \n    \n    \n      A Brief Workshop on Motion Tracking\n      motion-capture\n      \n        Sep 25, 2020 • Jackson Goode\n      Since our spring semester of motion tracking was a purely digital experience, a few of us got to together to quickly test out the OptiTrack system within the Portal.\n  \n\n\n  \n    \n    \n      ML something creative\n      machine-learning\n      \n        Sep 21, 2020 • Gaute Wardenær\n      Intuition tells me that a larger network should be better. More is more, as Yngwie says, but that is definitely not the case.\n  \n\n\n  \n    \n    \n      Multilayer Perceptron Classification\n      machine-learning\n      \n        Sep 21, 2020 • Simon Rønsholm Sandvik\n      Multi-layer Perceptron classification. Big words for a big task. During this two-week course in machine learning all my brain cells were allocated in solving this task. Initially I wanted something simple to do for my project since wrapping my head around ML was a daunting enough task. I soon realized there really is no such thing as simple in machine learning.\n  \n\n\n  \n    \n    \n      Scale over Chord using ANN\n      machine-learning\n      \n        Sep 21, 2020 • Thibault Jaccard\n      Try to learn scale over chord choices of great jazz improvisers\n  \n\n\n  \n    \n    \n      Learning to sequence drums\n      machine-learning\n      \n        Sep 21, 2020 • Ulrik Halmøy\n      Can reinforcement learning be a useful tool to teach a neural network to sequence drums?\n  \n\n\n  \n    \n    \n      Beneficial Unintelligence\n      machine-learning\n      \n        Sep 21, 2020 • Mari Lesteberg\n      In the future, when the robots take over the world, we all will be listening to 24/7 live streamed death metal until infinity\n  \n\n\n  \n    \n    \n      Postcard from The Valley of Despair\n      machine-learning\n      \n        Sep 20, 2020 • Paul Koenig\n      Well, that was fun.\n  \n\n\n  \n    \n    \n      Support Vector Machine Attempt\n      machine-learning\n      \n        Sep 20, 2020 • Tom Ignatius\n      Not fun\n  \n\n\n  \n    \n    \n      Classifying Urban Sounds in a Multi-label Database\n      machine-learning\n      \n        Sep 20, 2020 • Jackson Goode\n      How well does a convolution neural network perform at detecting multiple classes within a single sample? This experiment explores augmenting the UrbanSound8K database to test a well performing CNN architecture in a multi-label, multi-class scenario.\n  \n\n\n  \n    \n    \n      [ Music Mood Classifrustration ]\n      machine-learning\n      \n        Sep 20, 2020 • Rayam Luna\n      This is an attempt to create a Music Mood Classifier with feature extractions from Librosa.\n  \n\n\n  \n    \n    \n      Classification of guitar playing techniques\n      machine-learning\n      \n        Sep 20, 2020 • Thomas Anda\n      An attempt at making a model which can classify 6 different playing techniques on the guitar\n  \n\n\n  \n    \n    \n      Exploring Music Preference Recognition Using Spotify&#39;s Web API\n      machine-learning\n      \n        Sep 18, 2020 • Aleksander Tidemann\n      A proposed ML model that predicts the degree to which I will enjoy specific Spotify tracks based on previous preference ratings.\n  \n\n\n  \n    \n    \n      Jamulus test session.\n      networked-music\n      \n        Sep 7, 2020 • Stephen Gardener, Wenbo Yi, Joni Mok, Lindsay Charles\n      Can a physical metronome keep us in time? Experiences from the jamulus test session in the Physical-Virtual Communication and Music course.\n  \n\n\n  \n    \n    \n      Jamulus: Can y-- hear -e now?\n      networked-music\n      \n        Sep 6, 2020 • Alena Clim, Leigh Murray, Abhishek Choubey\n      During the second session of the Physical-Virtual Communication and Music course from 2020, we had our first experience with telematic music performance. It was not the greatest jam we ever had, but we learned from it. \n  \n\n\n  \n    \n    \n      Jamulus, or jamu-less?\n      networked-music\n      \n        Sep 3, 2020 • Pedro Lucas, Willie Mandeville, Henrik Sveen, Anders Lidal\n      Playing music together is not at all only about hearing, but also about the visual. Today the SMC students of 2020 experienced this in a “low latency jam session” in Jamulus.\n  \n\n\n  \n    \n    \n      Audio and Networking in the Portal - Presentations\n      networked-music\n      \n        Aug 31, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      The class of 2021 recently presented broadly on networking and audio within the context of the Portal. Presentations are included in this blog post as pdfs.\n  \n\n\n  \n    \n    \n      a t-SNE adventure\n      motion-capture\n      \n        May 20, 2020 • Ulrik Halmøy\n      A system for interactive exploration of sound clusters with phone sensors\n  \n\n\n  \n    \n    \n      Breathing through Max\n      motion-capture\n      \n        May 18, 2020 • Jackson Goode\n      For the COVID-19 version of motion-capture, I developed a system to track your rate of breath and sonify it through Max. It emphasized the tenants of biofeedback and hopes to serve as a responsive system for stress relief.\n  \n\n\n  \n    \n    \n      I scream, you scream, we all scream for livestream\n      networked-music\n      \n        May 18, 2020 • Rayam Luna\n      Some cameras won't allow you to film for more than 30 minutes, don't use those.\n  \n\n\n  \n    \n    \n      Sound Painting\n      motion-capture\n      \n        May 14, 2020 • Aleksander Tidemann\n      An application that tracks, visualizes and sonifies the motion of colors.\n  \n\n\n  \n    \n    \n      NINJAM, TPF and Audiomovers\n      networked-music\n      \n        Apr 17, 2020 • Simon Sandvik, Tom Ignatius, Jarle Steinhovden\n      During these last few weeks of “quarantine” during the COVID-19 outbreak, we have tested out several TCP/UDP audio transmission software’s from home to check for latency and user-friendliness. Our group consisting of Simon, Iggy, and Jarle, were tasked with looking into NINJAM and TPF.\n  \n\n\n  \n    \n    \n      On communication in distributed environments\n      networked-music\n      \n        Apr 15, 2020 • Karolina Jawad\n      Photo by Dr. Andrea Glang-Tossing. At ISE Europe, a trade fair in Amsterdam ( 11.02.-14.02. 2020) I was presenting together with SALTO the SMC course at the AVIXA Higher Education Conference. Researchers and students were invited to highlight emerging innovative methods that enhance learning and teaching experiences through AV technologies. The ISE Europe is the world's biggest pro AV event for integrated systems, with 80000 visitors and 1400 vendors, spread over a dozens of halls. For the conference I was specifically asked to contrast the overall technically curated program with social aspects from a student perspective. A retrospective from current conditions.\n  \n\n\n  \n    \n    \n      Soniweb: Epilogue\n      sonification\n      \n        Apr 13, 2020 • Jackson Goode\n      An update to the Soniweb project featuring dynamic sonification!\n  \n\n\n  \n    \n    \n      Zoom and other streaming services for jamming\n      networked-music\n      \n        Apr 6, 2020 • Gaute Wardenær\n      One thing you can do is to enable the option of 'preserve original audio' in Zoom.\n  \n\n\n  \n    \n    \n      Exploring SoundJack\n      networked-music\n      \n        Apr 5, 2020 • Aleksander Tidemann, Rayam Luna, Thibault Jaccard\n      SoundJack is a p2p browser-based low-latency telematic communications system.\n  \n\n\n  \n    \n    \n      Smokey and the Bandwidth\n      networked-music\n      \n        Apr 4, 2020 • Paul Koenig\n      Hijacking Old Tech for New Uses\n  \n\n\n  \n    \n    \n      Testing out Jacktrip\n      networked-music\n      \n        Mar 31, 2020 • Jackson Goode, Thomas Anda, Ulrik Halmøy\n      As we have begun settling into the COVID tech cocoon of isolation, we test out a technology that might be able to fulfull our dreams of real-time audio communication.\n  \n\n\n  \n    \n    \n      SMC vs. Corona\n      other\n      \n        Mar 13, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      In light of the recent microbial world war, we have taken matters into our own hands by sharing audio programming expertise through small introductory courses on Zoom.\n  \n\n\n  \n    \n    \n      Soniweb: An Experiment in Web Traffic Sonification and Ambisonics\n      sonification\n      \n        Mar 9, 2020 • Jackson Goode, Thibault Jaccard, Thomas Anda\n      For the course SMC4046 Sonification and Sound Design, our group was tasked to create a system that collects and sonifies data in real time. For this project, we went even further(!) and spatialized the network data that passes through a computer.\n  \n\n\n  \n    \n    \n      Weather Music\n      sonification\n      \n        Mar 9, 2020 • Aleksander Tidemann, Jarle Steinhovden, Gaute Timian Dahl Wardenær, Tom Ignatius Wee\n      Experiencing weather is a multi-sensory and complex experience. The objective of our sonification project was to sonify the weather through the use of online video streams.\n  \n\n\n  \n    \n    \n      Sonifying The Coronavirus Pandemic\n      sonification\n      \n        Mar 9, 2020 • Rayam Soeiro, Paul Koenig, Simon Sandvik, Donho Kwak\n      Finding a voice for difficult data\n  \n\n\n  \n    \n    \n      The Immersive Portal\n      networked-music\n      \n        Feb 24, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      The SMC portal is rigged for ambisonics to fit a virtual classroom in a classroom\n  \n\n\n  \n    \n    \n      [ pd Loop Station ]\n      sound-programming\n      \n        Feb 13, 2020 • Rayam Luna\n      This is an attempt to create a Loop Station with features that I wish I had in such a pedal / software.\n  \n\n\n  \n    \n    \n      Making Noises With |noise~|\n      sound-programming\n      \n        Feb 11, 2020 • Paul Koenig\n      Wherein I attempt to either program paradise or just make bacon-frying noises, which could be the same thing, actually\n  \n\n\n  \n    \n    \n      Sonification of plants through Pure Data\n      sound-programming\n      \n        Feb 11, 2020 • Gaute Wardenær\n      I am not sure if I am going crazy or if I am actually interacting with plants, but here me out here\n  \n\n\n  \n    \n    \n      Strumming through space and OSC\n      sound-programming\n      \n        Feb 10, 2020 • Jackson Goode\n      A gesture-driven guitar built in Puredata and utilizing OSC\n  \n\n\n  \n    \n    \n      Practice-Toolbox-for-students-of-music\n      sound-programming\n      \n        Feb 10, 2020 • Thomas Anda\n      In our audio programming course we were tasked to make a PD-patch without any restrictions on what it should be. I wanted to make something useful I could incorporate in my daily practice routine, and also distribute to some of my guitar students.\n  \n\n\n  \n    \n    \n      Multi voice mobile sampler\n      sound-programming\n      \n        Feb 10, 2020 • Ulrik Halmøy\n      A mobile tool to dabble with small audio recordings wherever you encounter them\n  \n\n\n  \n    \n    \n      The Delay Harmonizer\n      sound-programming\n      \n        Feb 9, 2020 • Thibault Jaccard\n      This chord generator uses a variable length delay fed by a microphone input as sound source.\n  \n\n\n  \n    \n    \n      The MIDI Remixer\n      sound-programming\n      \n        Feb 8, 2020 • Aleksander Tidemann\n      This sequencer based poly FM-synthesizer invites its users to remix and play with some of Johann Sebastian Bach's most famous preludes and fuges.\n  \n\n\n  \n    \n    \n      Camera Optimization in the Portal\n      networked-music\n      \n        Feb 3, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      On the quest for optimizing the visual aspect of the Portal\n  \n\n\n  \n    \n    \n      Testing Latency in the Portal\n      networked-music\n      \n        Jan 27, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      We 'officially' test some the latency in the Oslo and Trondheim Portal\n  \n\n\n  \n    \n    \n      Staggering towards the light\n      interactive-music\n      \n        Dec 30, 2019 • Thomas Anda, Aleksander Tidemann, Paul Koenig, Tom Ignatius\n      During a hackathon in our introduction course to physical computing, we developed a prototype of a DMI. In our blog post from this project we explained how the system was built and gave a short summary of our performance. In this blog post however, we will look at the instrument from an HCI-perspective. Where the main focus will be a summary of the problem space, the research question, the methodology used and our main findings and contributions. \n  \n\n\n  \n    \n    \n      Prototyping musical instruments\n      interactive-music\n      \n        Dec 27, 2019 • Rayam Luna, Gaute Wardenær, Thibault Jaccard, Ulrik Halmøy\n      Prototyping musical instruments in the name of recycling - exploring Orchestrash from an HCI point of view\n  \n\n\n  \n    \n    \n      Portal ideas\n      networked-music\n      \n        Dec 2, 2019 • Rayam Luna, Gaute Wardenær, Thibault Jaccard, Ulrik Halmøy\n      Instead of starting up the M32 every day, recalling the correct preset, adjusting the faders, turning on the screens, turning on the speakers, opening LOLA, connecting to the other side, pulling your hair out because nothing will work... Imagine just pressing a button and it all just works. \n  \n\n\n  \n    \n    \n      The B Team Wraps up the Portal\n      networked-music\n      \n        Dec 1, 2019 • Jackson Goode, Jarle Steinhovden, Simon Sandvik\n      We made it out of the Portal, now what?\n  \n\n\n  \n    \n    \n      Reflections on the Christmas concert\n      networked-music\n      \n        Nov 30, 2019 • Ulrik Halmøy, Thibault, Gaute, Rayam\n      Trondheim reflects on the Christmas concert 2019\n  \n\n\n  \n    \n    \n      The B Team Dives in Deep (Learning!)\n      machine-learning\n      \n        Nov 25, 2019 • Jackson Goode, Jarle Steinhovden, Simon Sandvik\n      Well, here we are, at the end of a semester where one of the most challenging courses remain - only Rebecca Fiebrink can save us now.\n  \n\n\n  \n    \n    \n      Group C Learns to Think about how Machines Learn to Think\n      machine-learning\n      \n        Nov 25, 2019 • Paul Koenig, Aleksander Tidemann, Tom Ignatius, Thomas Anda\n      Wherein we describe the denouement of SMC4000, Module #9: Machine Learning.\n  \n\n\n  \n    \n    \n      Music and machine learning - Group A\n      machine-learning\n      \n        Nov 25, 2019 • Rayam Luna, Gaute Wardenær, Thibault Jaccard, Ulrik Halmøy\n      Using machine learning for performance and for classifying sounds\n  \n\n\n  \n    \n    \n      3D Radio Theater - Lilleby\n      spatial-audio\n      \n        Nov 5, 2019 • Karolina Jawad, Jørgen Varpe, Sepehr Haghighi, Eigil Aandahl\n      A 3D radio theater, produced by SMC students, Trondheim\n  \n\n\n  \n    \n    \n      Spatial Trip - a Spatial Audio Composition\n      spatial-audio\n      \n        Nov 3, 2019 • Ashane Silva, Sam Roman, Elias Andersen, Guy Sion\n      In this project, we recorded and synthesized sounds with spatial aspects which is meant to be heard over headphones or loudspeakers (8-channel setup in Oslo/Trondheim). Coming to simulate both the indoor and outdoor real and or fictional scenarios.\n  \n\n\n  \n    \n    \n      Orchestrash hackathon performance\n      interactive-music\n      \n        Oct 24, 2019 • Rayam Luna, Gaute Wardenær, Thibault Jaccard, Ulrik Halmøy\n      The title of our project is \"Orchestrash\" inspired by the theme of the competition and our approach to solving it, by making individual instruments controlled by recycled materials and \"recycling\" sound by sampling\n  \n\n\n  \n    \n    \n      The B Team: Mini-Hackathon\n      interactive-music\n      \n        Oct 24, 2019 • Jackson Goode, Magda Futyma, Simon Sandvik, Jarle Steinhovden\n      For the SMC 4000 mini-hackathon in the physical computing module we tried to send sound at the speed of light.\n  \n\n\n  \n    \n    \n      Physical Computing: Heckathon: Group C\n      interactive-music\n      \n        Oct 22, 2019 • Paul Koenig, Thomas Anda, Tom Ignatius, Aleksander Tidemann\n      Taking our cue from the main theme of the Day 4 Hackathon of “Recycling”, Team C chose the 2017 U.S. withdrawal from the Paris Agreement on climate change mitigation as a central theme in our work.\n  \n\n\n  \n    \n    \n      Microphone Testing Results\n      networked-music\n      \n        Oct 21, 2019 • Jackson Goode\n      We've spent a few days (in addition to the many miscellaneous hours during class) reconfiguring the Portal and testing out new hardware and how it might improve the quality of our sound.\n  \n\n\n  \n    \n    \n      Wizard_of_Vox\n      interactive-music\n      \n        Oct 18, 2019 • Notto Thelle\n      Wizard Of Vox - Wizard Of Vox is a gesture-based speech synthesis system that can be can be “made to speak”\n  \n\n\n  \n    \n    \n      The Fønvind Device\n      interactive-music\n      \n        Oct 18, 2019 • Eigil Aandahl\n      For my interactive music systems project, I wanted to make use of the Bela's analog inputs and outputs to make a synthesizer capable of producing not only sound, but also analog control signals that can be used with an analog modular synthesizer. This post goes briefly through some of the features and the design of my system, and at the end there is a video demonstration of the system in use.\n  \n\n\n  \n    \n    \n      Instant Music, Subtlety later\n      interactive-music\n      \n        Oct 17, 2019 • Karolina Jawad\n      When drafting ideas in unknown territory one can become overwhelmed with the sheer endless options to create an IMS (interactive music system). Here a real-time processing board for voice with gesture control.\n  \n\n\n  \n    \n    \n      The B Team: To Heck and Back\n      interactive-music\n      \n        Oct 15, 2019 • Jackson Goode, Magda Futyma, Simon Sandvik, and Jarle Steinhovden\n      Today we began our experiments with some lofi hardware, simple contact mics, speakers, batteries, and some crocodile cables to connect it all. We left in pieces.\n  \n\n\n  \n    \n    \n      Physical Computing Day One: Victorian Synthesizer Madness! Group C Reports In From Heck\n      interactive-music\n      \n        Oct 15, 2019 • Paul Koenig, Aleksander Tidemann, Thomas Anda, Tom Ignatius\n      The first day of Physical Computing started and ended with a bit of confusion in the Portal, but that is par for the course. Once we set up the various cameras and microphones, and dealt with feedback, echo, etc, the fun began! \n  \n\n\n  \n    \n    \n      Physical computing Day 1 - Group B\n      interactive-music\n      \n        Oct 15, 2019 • Rayam Luna, Gaute Wardenær, Thibault Jaccard, Ulrik Halmøy\n      First day of physical computing\n  \n\n\n  \n    \n    \n      AudioBend\n      interactive-music\n      \n        Oct 14, 2019 • Ashane Silva\n      My project idea for the Interactive Music Systems was to build a glove that can manipulate sound.  It was actually inspired by seeing the “mi.mu Gloves”.  The paper on the “Data Glove” gave me ideas on the design aspect of the glove although the way it works is a bit more different than what I use in my glove. “Data glove” uses multiple flex sensors on the fingers and force sensitive sensors to contact the finger tips and an accelerometer to get data from the wrist control. In my glove I used flex sensor on index finger, 3 – axis accelerometer on my hand and a Distance Ultrasonic sensor on my palm. Attaching those stuff to the glove was a bit tricky but “ducktape” saved my life.\n  \n\n\n  \n    \n    \n      Alien_Hamster_Ball\n      interactive-music\n      \n        Oct 14, 2019 • Samuel Roman\n      The Alien Hamster ball - an instrument expressed through a 3D space\n  \n\n\n  \n    \n    \n      LoopsOnFoam\n      interactive-music\n      \n        Oct 13, 2019 • Jørgen Varpe\n      During a 2-week intensive workshop in the course Interactive Music Systems I worked on the development of an instrument prototype, which I named LoopsOnFoam.\n  \n\n\n  \n    \n    \n      Microture\n      interactive-music\n      \n        Oct 13, 2019 • Sepehr Haghighi\n      Microture is an interactive music system, based on manipulation of the input sound (microphone sound) with small gestures..\n  \n\n\n  \n    \n    \n      Body Drums - A wearable drumset\n      interactive-music\n      \n        Oct 12, 2019 • Elias Andersen\n      Body Drums\nFor the course SMC4045 - Interactive Music Systems, I built a wearable drumset.\nThe wearable drumset consists of a piezo-element placed on one hand, a force-sensing resistor on the other and a accelerometer on the fot. These sensors are then used to trigger one file each. In my case I used a kick drum sound for the foot, snare drum sound for the piezo element and a hi hat sound for the FSR. Then when these sensors are triggered, the sound that are mapped to the sensor will be played. For example if I stump my foot, the kick drum sound will be played.\n\n  \n\n\n  \n    \n    \n      The HySax - Augmented saxophone meant for musical performance \n      interactive-music\n      \n        Oct 12, 2019 • Guy Sion\n      an augmented saxophone meant for musical performance, enabling background layer and delay to be controlled via gestures.\n  \n\n\n  \n    \n    \n      Portal Flowchart\n      networked-music\n      \n        Oct 10, 2019 • Aleksander Tidemann, Jackson Goode, Paul Koenig, Tom Ignatius\n      The SMC portal has been subject to many configurations over the last couple of months. In this post, we explore how flowcharts may help us see a brighter tomorrow (we need as much light as we can get here).\n  \n\n\n  \n    \n    \n      Recovio: Entrepeneurship - Group C\n      other\n      \n        Oct 8, 2019 • Jackson Goode, Aleksander Tidemann, Thomas Anda, Tom Ignatius\n      Group C's project for the SMC 4015 Entrepeneurship course. Recovio is an audio digitization and storage company that serves companies at a scale and pricepoint that best fits their needs.\n  \n\n\n  \n    \n    \n      Entrepreneurship for SMC - Group B\n      other\n      \n        Oct 7, 2019 • Paul Koenig, Gaute Wardenær, Magda Futyma, Ulrik Halmøy\n      Summary of SMC4015 project from group B\n  \n\n\n  \n    \n    \n      The importance of sound quality\n      networked-music\n      \n        Sep 27, 2019 • Ulrik Halmøy, Thibault Jaccard\n      Reflections after several lectures with less sub-optimal sound quality\n  \n\n\n  \n    \n    \n      UltraGrid\n      networked-music\n      \n        Sep 22, 2019 • Aleksander Tidemann, Thomas Anda, Paul Koenig, Tom Ignatius\n      Exploring an alternative audio and video network transmission software in the SMC portal.\n  \n\n\n  \n    \n    \n      Machine Learning, it&#39;s all about the data\n      machine-learning\n      \n        Sep 19, 2019 • Eigil Aandahl\n      For my machine learning project, I wanted to see if I could teach my laptop to distinguish between different types of music using a large amount of data. Using metadata from a large dataset for music analysis, I tested different machine learning classifiers with supervised learning to distinguish between tracks labeled belonging to 'Rock' and 'Electronic'. The project was developed using Python and libraries for data analysis and machine learning.\n  \n\n\n  \n    \n    \n      Clustering high dimensional data\n      machine-learning\n      \n        Sep 17, 2019 • Karolina Jawad\n      In the project for Music and Machine Learning I was using raw audio data to see how well the K-Mean clustering technique would work for structuring and classifying an unlabelled data-set of voice recordings. \n  \n\n\n  \n    \n    \n      MIDI drum beat generation\n      machine-learning\n      \n        Sep 16, 2019 • Elias Andersen\n      Most music production today depend strongly on technology, from the beginning of a songs creation, till the the last final tunings during mix and master. Still their is usually many human aspect involved, like singing, humans playing instruments, humans using a music making software etc..\n  \n\n\n  \n    \n    \n      Could DSP it?\n      networked-music\n      \n        Sep 16, 2019 • Jackson Goode\n      Is polarity the solution?\n  \n\n\n  \n    \n    \n      IR Reverberation Classifier using Machine Learning\n      machine-learning\n      \n        Sep 16, 2019 • Sam Roman\n      Using Machine Learning to classify different reverb spaces (using impulse response files)\n  \n\n\n  \n    \n    \n      Multi-Layer Perceptron Classifier of Dry/Wet Saxophone Sound\n      machine-learning\n      \n        Sep 16, 2019 • Guy Sion\n      The application I have decided to work on is of a machine learning model that can ultimately differentiate between a saxophone sound with effect (wet) and without effect (dry).\n  \n\n\n  \n    \n    \n      Triggering effects, based on playing style\n      machine-learning\n      \n        Sep 14, 2019 • Sepehr Haghighi\n      Technology in collaboration with art could create creative solutions and achievements. In here we use machine learning in order to ease the work of player while playing an instrument.\n  \n\n\n  \n    \n    \n      Classification of string instruments\n      machine-learning\n      \n        Sep 14, 2019 • Jørgen Varpe\n      During a 2 week intensive workshop in the course Music and Machine Learning I had to develop a machine learning system for the field of music technlogy.\n  \n\n\n  \n    \n    \n      An introduction to automix\n      networked-music\n      \n        Sep 10, 2019 • Gaute Wardenær\n      At first glance, automix might look like your regular old expander or gate, but what makes automix special is that it does not only work on a channel to channel basis, but links all the channels in an automix group together and opens up the channel that has the strongest signal, while ducking the others. \n  \n\n\n  \n    \n    \n      Reflections on diversity\n      other\n      \n        Aug 27, 2019 • Magda Futyma, Thibault Jaccard, Tom Ignatius, Simon Sandvik, Ulrik Halmøy\n      First year students in Trondheim reflects on diversity in SMC\n  \n\n\n  \n    \n    \n      It&#39;s not just phase, mom!\n      sound-programming\n      \n        Aug 24, 2019 • Gaute Wardenær\n      Why phase coherency in loudspeakers matters.\n  \n\n\n  \n    \n    \n      Sonification of Near Earth Objects\n      sonification\n      \n        May 20, 2019 • Jørgen Varpe, Guy Sion, Eigil Aandahl\n      As a part of a two-week workshop in the Sonification and Sound design course, we worked on the development of a self-chosen sonification project. For three days we explored how to design and build an auditory model of Near-Earth Objects (NEO) with data collected from NASA.\n  \n\n\n  \n    \n    \n      Experiencing Ambisonics Binaurally\n      networked-music\n      \n        May 19, 2019 • Guy Sion\n      During the SMC2022 Physical-virtual communication course we have had two sessions where we explored multichannel and ambisonic decoding. The first session, on February 27th, was mainly about recording a four channel A-format stream with an Ambisonic 360 sound microphone. We converted the A-format mic signal into a full-sphere surround sound format which was then decoded to 8 loudspeakers layout we placed evenly across the portal space. We have used the AIIRADecoder from the free and open source IEM plug-in suite.\n  \n\n\n  \n    \n    \n      Blue sky visions on sonic futures\n      sonification\n      \n        May 17, 2019 • Karolina Jawad\n      Pedro Duarte Pestana was the final guest speaker who joined us virtually with his presentation 'Career Management in Music Technology / Knowledge Engineering in Music Technology and Sonic Arts' from Porto. Pedro was the first student in Portugal who held a PhD in computer music. He works as a researcher and consulter for emerging audio technologies and intelligent systems.\n  \n\n\n  \n    \n    \n      Scenarios in the Trondheim Portal during the spring semester-2019\n      networked-music\n      \n        May 12, 2019 • Shreejay Shrestha\n      We have had numerous scenarios set up in the portal in the period of January-May 2019. Each of the scenarios are unique and therefore serve specific functions. This blog presents four of such scenarios with a bit of discussions on the advantages and challenges with the set ups.\n  \n\n\n  \n    \n    \n      Augmented Reality\n      networked-music\n      \n        May 10, 2019 • Mari Lesteberg, Sam Roman, Karolina Jawad, Jørgen Varpe, Sepehr Saghigi\n      In the final Portal workshop of this semester we were looking at Ambisonics as a potential way to create an augmented auditory space from the perspective of sound.\n  \n\n\n  \n    \n    \n      A generic overview to the &#39;Sound in Space&#39; exhibition at KIT\n      sonification\n      \n        Apr 30, 2019 • Karolina Jawad\n      Marking the final event before Easter and from our Sonification and Sound design course I was tasked to visit the group exhibition 'Sound in Space' that took place on the 11th of April at Gallery KIT, Trondheim. It was also the closing event for the sound art course in which music-technology and fine art academy students (NTNU) could participate.\n  \n\n\n  \n    \n    \n      How music related motion tracking can sound\n      motion-capture\n      \n        Apr 24, 2019 • Karolina Jawad, Jørgen Varpe, Eirik Dahl\n      During the course 'Music related Motion Tracking' there were several approaches among the students to realize their ideas. The Opti-Track system, new to all of us consists of infrared-cameras, markers and a software with calibration tools. We were exploring the functions from scratch during the first week when hosting the 'Nordic-stand-still-championship' on both campus.\n  \n\n\n  \n    \n    \n      Brexisonification\n      sonification\n      \n        Apr 19, 2019 • Sam Roman, Sepehr Haghighi, Eirik Dahl\n      The goal in the project is to sonify Brexit, in a way that the audience could interpret new insight from the data through audio.\n  \n\n\n  \n    \n    \n      The Sound of Traffic - Sonic Vehicle Pathelormeter\n      sonification\n      \n        Apr 19, 2019 • Ashane Silva, Karolina Jawad, Shreejay Shrestha\n      Is it possible to transmit complex data-sets within an instance of a sound, so the content gets revealed? As communication and dissemination of information in our modern digital world has been highly dominated by visual aspects it led to the fact that the modality of sound got neglected. In order to test the hypothesis, the project presents a model for sonification of temporal-spatial traffic data, based on principle of Parametric Mapping Sonification (PMSon) technique.\n  \n\n\n  \n    \n    \n      MuX -playground for real-time sound exploration\n      sonification\n      \n        Apr 13, 2019 • Ashane Silva\n      It was so fascinating to have Edo Fouilloux in the SMC sonification seminar series. Edo is a visual innovator and a multidisciplinary craftsman of graphics, sound, computing, and interaction. He co-founded Decochon in 2015 to work with Mixed Reality (XR) technologies. MuX is a pioneering software in the field of interactive music in virtual reality systems. Edo demonstrated the concepts and philosophies inside Mux, where it is possible to build and play new instruments in the virtual space.\n  \n\n\n  \n    \n    \n      Ole Neiling Lecture\n      sonification\n      \n        Apr 12, 2019 • Guy Sion\n      I had the pleasure of introducing Ole Neiling, a extradisciplinary 'life-style' artist. This was part of a series of lectures held over the portal in the sonification and sound design module.\n  \n\n\n  \n    \n    \n      Presentation by Pamela Z\n      sonification\n      \n        Apr 7, 2019 • Guy Sion\n      As part of our Sonification and Sound Design course (SMC4046), we were fortunate enough to host scholars and artists which are well established within the sonification and sound design field. Pamela Z is a composer, performer and a media artist who is known for her work of voice with electronic processing. Pamela arrived in Norway for several workshops and performances, and we were lucky enough to have her for a short presentation on April 4th. After a brief introduction by Tone Åse who has been a long-time fan of Pamela’s work, Pamela started the session with a 10 minutes performance of a live improvised mashup of several existing pieces she often performs. While performing, Pamela is being circled by several self-made sensory devices that are connected to her laptop. On her hands, she wears sensors that send signals to her hardware setup. She sings and makes sounds with her voice, hands, and body and manipulates all that with hand gestures.\n  \n\n\n  \n    \n    \n       An Overview of Sonification by Thomas Hermann\n      sonification\n      \n        Apr 5, 2019 • Shreejay Shrestha\n      It was my privilege and honour to facilitate a guest lecture and introduce one of the 'Gurus' in the field of sonification, Dr. Thomas Hermann. He shared his enormous knowledge on sonification with hands on exercises for two days (March 28, and 29, 2019) through the SMC portal in Trondheim. I am quite excited to share my notes and will try to cover the summary of his talks in this blog.\n  \n\n\n  \n    \n    \n      Using Speech As Musical Material\n      sonification\n      \n        Apr 3, 2019 • Jørgen Varpe\n      As a part of a three-week workshop in the course Sonification and Sound design at SMC, we were lucky to have Daniel Formo as a guest speaker.\n  \n\n\n  \n    \n    \n      Advanced collaborative spaces, tele-immersive systems and the Nidarøs Sculpture\n      sonification\n      \n        Apr 3, 2019 • Karolina Jawad\n      Leif Arne Rønningen introduced us to 'Advanced Collaboration Spaces, requirements and possible realisations' and to the 'Nidarø Sulpture', a dynamic vision and audio sculpture. In both parts Leif's main research areas on tele-immersive collaboration systems and low latency networks are at the forfront.\n  \n\n\n  \n    \n    \n      MoCap Recap - Two weeks recap of a Motion Tracking workshop\n      motion-capture\n      \n        Apr 1, 2019 • Guy Sion, Sam Roman, Elias Anderson\n      During weeks 10-11 we attended the Music related motion tracking course (SMC4043) as part of the SMC program. The week started with the installation of the OptiTrack system in Oslo, placement of cameras, connecting wires to hubs and software installation and setup. we got familiar with the Motive:Body software and was able to run calibrations, set and label markers, record motion data, export it in a correct way and experiment with sonifying the results with both recorded and streamed motion capture data.\n  \n\n\n  \n    \n    \n      Composition and mapping in sound installations “Flyndre” and “VLBI Music”\n      sonification\n      \n        Mar 27, 2019 • Eirik Dahl\n      Øyvind Brandtsegg talks about the creation and life cycle of two art installations in this inspiring talk. Follow the link to read about the first lecture in a series about sonification.\n  \n\n\n  \n    \n    \n      Repairing scissors and preparing the portal for talks\n      networked-music\n      \n        Mar 20, 2019 • Mari Lesteberg, Elias Andersen\n      \n  \n\n\n  \n    \n    \n      Ambisonics!\n      networked-music\n      \n        Feb 27, 2019 • Mari Lesteberg, Elias Andersen, Ashane Silva\n      On 27 February 2019, we had a workshop on Ambisonics in the portal course. Anders Tveit gave us a lecture on how to encode and decode sound inputs from Lola, using the AIIRADecoder in Reaper.\n  \n\n\n  \n    \n    \n      Touch the Alien\n      sound-programming\n      \n        Feb 22, 2019 • Eigil Aandahl, Jonas Bjordal, Mari Lesteberg, Sam Roman\n      The web audio synth 'Touch the Alien', a project by Eigil Aandahl, Sam Roman, Jonas Bjordal and Mari Lesteberg at the master's programme Music, Communication and Technology at Aalborg University and Norwegian University of Science and Technology. The application offers touchscreen functionality, Oscillators, FM Oscillator &amp;  Delay, phaser, Chorus &amp; Filter on Dry/wet slider,Canvas UI with follow visual FX\tand it's alien themed for your pleasure!\n  \n\n\n  \n    \n    \n      The Magic Piano\n      sound-programming\n      \n        Feb 22, 2019 • Jørgen Varpe, Ashane Silva, Guy Sion\n      During our second week learning about Audio programing and web Audio API we were divided into groups and had to come up with an idea for a final project. The main challenges were to find an idea that is doable within 4 days, to code collaboratively and to prepare for the presentation of our project. Guy had an Idea for building a piano keyboard that will help beginners play a simple melody and Ashane and Jørgen agreed to collaborate and join forces in creating 'The Magic Piano'.\n  \n\n\n  \n    \n    \n      Convolverizer\n      sound-programming\n      \n        Feb 19, 2019 • Eirik Dahl, Karolina Jawad, Shreejay Shrestha, Sepehr Haghighi\n      Convolverizer, Real-time processing of ambient sound, voice or live instruments, utilizing the convolution effect.\n  \n\n\n  \n    \n    \n      The Giant Steps Player\n      sound-programming\n      \n        Feb 11, 2019 • Guy Sion\n      As part of the SMC master program we are being introduced to a variety of technologies for creating music and sounds. We have just finished a week long workshop learning about Audio programing and web audio API. The benefits of this technology are helpful and relevant in areas like art, entertainment or education. We were introduced to several ways for creating and manipulating sound, follow tutorials and experiment on our own during the days. I must admit that I do not have intensive knowledge in programing in general and javaScript in particular. Many failures accrued while trying, from simple syntax errors to flawed design. But understanding the idea behind each process and striving towards the wanted result was an important progress.\n  \n\n\n  \n    \n    \n      The Spaghetti Code Music Player\n      sound-programming\n      \n        Feb 11, 2019 • Mari Lesteberg\n      The Spaghetti Code Music Player is a simple music player that is loaded with one of my own tracks. The player allows you to play and stop the tune, turn on and off a delay effect and control a filter with your computer mouse. The player also has a volume control.\n  \n\n\n  \n    \n    \n      Odyssey\n      sound-programming\n      \n        Feb 11, 2019 • Shreejay Shrestha\n      Odyssey is a simple prototype of a Web Audio API envisioned to immerse users into a misty jungle environment. Besides soundscape of a jungle, the application adds bits of flavour of few domestic animals and mix them all together with a piece of jazz music. The web audio application is developed using HTML5 and javascript.\n  \n\n\n  \n    \n    \n      Catch the wave – First week&#39;s dive into web audio programming\n      sound-programming\n      \n        Feb 11, 2019 • Karolina Jawad\n      It is possible to create simple, but effective applications on the web browser, even without prior knowledge. However, it took way longer to implement those ideas, but luckily there was always someone around to ask.\n  \n\n\n  \n    \n    \n      The Pointilator Sequence Synthesizer\n      sound-programming\n      \n        Feb 11, 2019 • Eigil Aandahl\n      The Pointilator sequence synth is an experimental instrument that can be played directly from a web browser! It is tested to work with Opera and Chrome, but does not work in Safari. It is based around entering a sequence of notes as points on a Canvas that registers each click and draws a circle where the note was put. It can then play back the notes from left to right with the height of the click translating to pitch. The result is a sequencing synthesizer that has a finely detailed scope in both time and pitch, although it is not easy to control based on traditional musical scales or rhythmic time.\n  \n\n\n  \n    \n    \n      SineWave Pad\n      sound-programming\n      \n        Feb 11, 2019 • Ashane Silva\n      It was a wonderful journey we had for a week getting hands-on experience with Web audio API and JavaScript. In the beginning, I was tensed about the way that I will handle coding with zero prior experience. But, at the end of the week, I was happy about what I have managed to achieve. I was lacking ideas to start a project for the week but after getting introduced to oscillators, I thought of making a synthesizer or a drum pad that works on the browser. So it was either to work with Oscillators or sound loops.\n  \n\n\n  \n    \n    \n      Reese da Alien!\n      sound-programming\n      \n        Feb 11, 2019 • Sam Roman\n      The project I have developed on over the first week of web audio based programing is called Reese da Alien - a web based synth of sorts with mouse functionality. The idea is that the program presents a relatively novel way of producing a reese, by the user moving around the mouse on the the page to find different sweet spots as they affect the pitch and amplitude of two oscillators with the movements. The persona of the application came after early in development I likened the sounds to an alien talking – I felt it a fitting title for the weird, abrasive sounds that the program creates.\n  \n\n\n  \n    \n    \n      Freak Show\n      sound-programming\n      \n        Feb 11, 2019 • Sepehr Haghighi\n      As my first experience working with Web Audio API, utilizing JS, HTML and CSS; it was quite a challenge, but a pleasant one that lead to the outcome that I wanted and also broadened my perspective, in regards of my future plans.\n  \n\n\n  \n    \n    \n      The Mono Synth\n      sound-programming\n      \n        Feb 10, 2019 • Eirik Dahl\n      This blog post outlines the production of the MonoSynth. The Mono Synth is drawn by Jørgen N. Varpe, who also wrote a lot of the code. The objective of this prototype was to improve my familiarity with coding, and at the same time be able to have a working chromatic instrument. Working with a cromatic instrument is interesting because it allows me to have a less abstract understanding of what happens in the code - behind the scenes if you will.\n  \n\n\n  \n    \n    \n      The Wavesynth\n      sound-programming\n      \n        Feb 10, 2019 • Jørgen Varpe\n      During the first workshop week in the course Audio Programming, I have been working on a project which I have called \"The Wavesynth\". I have called it this because I have chosen to use wavetables to shape the output of an oscillator. I have not made a wavetable synthesizer like for instance Ableton's Wavetable, where you can interpolate between waves. instead I use some wavetables created by Google Chrome Labs to make it sound like \"real\" instruments. The synth is played by using the computer keyboard, and the user can choose the output sound, and adjust three different effects to shape the it the way they want. The synthesizer is made using web technologies, including HTML, JavaScript, Web Audio API, and more.\n  \n\n\n  \n    \n    \n      Documentation and recommendations from the latest Portal Jam\n      networked-music\n      \n        Feb 10, 2019 • Karolina Jawad, Eirik Dahl, Espen Wik, Shreejay Shrestha, Jørgen Varpe\n      As the Portal is still in its infancy, pushing and exploring its technical possibilities is an ongoing process. We still encounter different issues while actually seeking a smooth and standardized setup of the signal routing and performance space. At the end it is about optimizing the telematic experience but not getting stucked in technicalities at the same time.\n  \n\n\n  \n    \n    \n      How to stream content from the portal\n      networked-music\n      \n        Jan 30, 2019 • Eigil Aandahl, Sepehr Haghighi\n      In this blogpost, we will try to explain in more detail how these streams have been set up using OBS, Open Broadcaster Software and Youtube Live while being connected between Trondheim and Oslo. This can be of use for anyone looking to set up a co-located stream of a speaker or performance.\n  \n\n\n  \n    \n    \n      The Sounds We Like\n      sonification\n      \n        Jan 23, 2019 • Guy Sion\n      For our first lesson in the Sonification and Sound Design course, we were asked to give a short presentation of a sonification or a sound design of choice. It was interesting to see the variety of examples among our classmates. Each of us brought a unique example and explained what is it about? why did they choose it? and how does it relate to our work at the SMC program?\n  \n\n\n  \n    \n    \n      Portal Jam 23 January 2019, Documentation from Oslo Team\n      networked-music\n      \n        Jan 23, 2019 • Mari Lesteberg, Ashane Silva, Elias Andersen\n      On the 23 of January, we were testing out to jam together through the Portal.\n  \n\n\n",
    "url": "/alltopics/"
  },
  
  {
    "title": "Semester Projects",
    "author": "\n",
    "excerpt": "\n",
    "content": "The blog posts of this section relate to the 15 ECTS semester projects in SMC7-9. The aim of these projects is to carry out a sound and music technological project in a real-world problem-based setting. The projects should have a societal, research or industrial impact.\n\nIn Spring 2025, the project themes relate to the brief given by Roskilde Festival (RF): in two main groups: INCLUSION or FIRST DAYS:\n\nINCLUSION: ACCESSIBILITY, EQUALITY, DIVERSITY\n\nRoskilde Festival has a strong focus on social sustainability and on being an inclusive and diverse festival where people can meet across differences. How can RF become an inviting, including, and open community, welcoming people whatever their functional capability?\n\n\n  How can we improve the accessibility of toilets and bathing facilities (design, location, etc.)?\n  How can the first encounter and arrival at the festival become a pleasant and safe experience?\n  How can we reach out to people with disabilities who may be reluctant to participate in festivals?\n  How can we design content and appearance of meeting points at the festival addressing diversity, equality, and accessibility?\n  Games and activities for HandiCamp that encourage people to meet and facilitate dialogue and interaction between participants in a respectful, funny, and thoughtful manner.\n  The accessible tent platform. At our camping facilities, we want to meet the different needs of participants with disabilities.\n\n\nFIRST DAYS: JOINING PEOPLE TOGETHER, MAKING A DIFFERENCE\n\nWe are interested in activities which can contribute to fun and new friendships among the participants at the campsite, and which further address our agendas on environmental, social, economic, and cultural sustainability development. We see arts and culture as driving forces in circular development.\n\n\n  Design and testing of experimental experiences, in which co-creation as well as arts and culture in a broad sense encourage to reflect on circularity and sustainability. Arts and culture can be, for example architecture, technology, and social intervention. Sustainability is seen in a broad sense, i.e. social, environmental, economic, and cultural aspects. Solutions can be specific technical designs, process-oriented courses, games, workshops, etc.\n  Relaxation and activities in a funny and respectful manner. We often find that after a few days at the campsite, the participants are interested in activities to gather around which only require the gear already present in the camp. Further, we are interested in activities which do not necessarily require alcohol as a central element\n\n\n\n  \n    \n    \n      Zen Soundscape Installation/Sculpture\n      applied-project\n      \n        Feb 13, 2025 • Eirini Liapikou\n      A serene installation that creates a harmonious soundscape transporting listeners to a state of zen. \n  \n\n\n  \n    \n    \n      BrailleGuide\n      applied-project\n      \n        Feb 10, 2025 • Benjamin Melvin Stein\n      Blind/visually impaired festival goers need an easy way to get information about things near them because it can enable a more autonomous experience for them.\n  \n\n\n  \n    \n    \n      Cosmic Clash!\n      applied-project\n      \n        Nov 24, 2024 • Tom Oldfield, Cumhur\n      Cosmic Clash - exploring gamification of physiotherapy exercises at Rosklde Festival using the Biopoint sensor\n  \n\n\n  \n    \n    \n      Intuitive Robotics\n      applied-project\n      \n        Nov 27, 2023 • Emin Memis, Fabian Stordalen, Masoud Niknafs, Theo Griffin Halvorsen\n      Controlling a robot arm with medical sensors\n  \n\n\n  \n    \n    \n      Developing for Muzziball\n      applied-project\n      \n        Nov 26, 2023 • Alexander Wastnidge, Jack Hardwick, Aysima Karcaaltincaba, Kristian Eicke, Nino Jakeli\n      Check out what we worked on as a team in this year's Applied Project.\n  \n\n\n  \n    \n    \n      Exploration of 5G networks for Networked Musical Performances\n      applied-project\n      \n        Nov 27, 2022 • Arvid Falch, Jakob Høydal, Joachim Poutaraud, Kristian Wentzel, Sofía González\n      A latency optimization methodology for NMP.\n  \n\n\n  \n    \n    \n      Popsenteret&#39;s Music Producer Experience\n      applied-project\n      \n        Nov 25, 2022 • Joseph Clemente, Hugh Alexander von Arnim, Oliver Getz, Henrik Sveen, Iosif Aragiannis\n      The top 4 components of a physical computing music production station - you'll never guess what #3 is!\n  \n\n\n  \n    \n    \n      Rockheim - about an interactive exhibition\n      applied-project\n      \n        Dec 20, 2021 • Alena Clim, Lindsay Charles, Pedro Lucas\n      Anyone who visited Rockheim knows about the Time Tunnel, where 6 screens show the history of Norwegian music from the 50s until today. If you have no idea what I am saying, add a visit to Rockheim on your bucket list! For this project, we had to investigate and offer improvement ideas.\n  \n\n\n  \n    \n    \n      HEARING NOMONO: Our Journey into Audio Branding and Feedback Sounds\n      applied-project\n      \n        May 11, 2021 • Abhishek Choubey, Lindsay Charles, Willie Mandeville, Wenbo Yi\n      Audio branding and audio feedback are everywhere. This semester, we tried our hand at designing some for a young, Trondheim-based audio technology company.\n  \n\n\n  \n    \n    \n      Designing a hybrid conference\n      applied-project\n      \n        May 11, 2021 • Joni Mok, Pedro Lucas, Rayam Luna, Stephen Gardener\n      How do you design conference that brings together both virtual and physical participants? This was the problem we explored for our Applied Project.\n  \n\n\n  \n    \n    \n      SMC Portal II - The Dungeon\n      applied-project\n      \n        May 10, 2021 • Henrik Sveen, Anders Lidal, Leigh Murray, Alena Clim\n      First time we entered the videolab, it was basically a storage room, full of outdated audio equipment and also hardware we would use. The ceiling lights didn’t work, and the cleaning personal hadn’t been there for quite a while.\n  \n\n\n  \n    \n    \n      Sound Design for IMTEL\n      applied-project\n      \n        Dec 10, 2020 • Simon Rønsholm Sandvik, Thibault Jaccard\n      For the applied project 2, we worked on enhancing the sound components of a VR language learning application in Unity\n  \n\n\n  \n    \n    \n      Expanding Collaboration in Pedál\n      applied-project\n      \n        Dec 10, 2020 • Ulrik Halmøy, Paul Koenig, Tom Ignatius, Jackson Goode\n      For our external partner, Pedál, we wanted to document and expand how multi-user interactions took place.  \n  \n\n\n  \n    \n    \n      Algorytme\n      applied-project\n      \n        Dec 10, 2020 • Thomas Anda, Aleksander Tidemann, Gaute Wardenær, Mari Lesteberg\n      A proposed application for generating personalized playlists based on emotional state.\n  \n\n\n  \n    \n    \n      NTNU Oceans\n      applied-project\n      \n        May 3, 2020 • Gaute Wardenær, Jarle Steinhovden, Thomas Anda, Ulrik Halmøy\n      An immersive installation on mercury pollution in the ocean. The aim of this project is to conceptualize and implement an installation in which visitors can interactively “see” and “hear” the status of oceans and seas worldwide.\n  \n\n\n  \n    \n    \n      Soundscapes for Dream Nest\n      applied-project\n      \n        May 3, 2020 • Jackson Goode, Paul Koenig, Tom Ignatius\n      For the spring applied project we created music for a hardware specific device to relax colicy babies in a collaboration with our external partner, Dream Nest. Our final product is a six track EP, engineered to put your baby to sleep (we hope!)\n  \n\n\n  \n    \n    \n      MotionComposer\n      applied-project\n      \n        May 2, 2020 • Aleksander Tidemann, Rayam Luna, Simon Sandvik, Thibault Jaccard\n      MotionComposer is a motion capture device that lets people make music with gestures. This is the presentation of our applied project, where we worked on building a new instrument for this device.\n  \n\n\n  \n    \n    \n      Trippi BySykkel Sounds\n      applied-project\n      \n        Dec 15, 2019 • Eigil Aandahl, Elias Andersen, Karolina Jawad, Sam Roman\n      Portray city bike user data into a sonified, interactive display by making use of public space and public data.’\n  \n\n\n  \n    \n    \n      Tree as Speakers\n      applied-project\n      \n        Dec 15, 2019 • Ashane Silva, Shreejay Shrestha, Jørgen Varpe\n      A project in collaboration with ÅF engineering. The goal of the project was to create a non-intrusive soundscape and/or noise-masking installations in an outdoor public space by using trees as speakers, installing audio exciters on trees.\n  \n\n\n  \n    \n    \n      Picture Carmen\n      applied-project\n      \n        Dec 11, 2019 • Guy Sion, Espen Wik, Sepehr Haghighi\n      Promotional video for a new chamber production of the Opera ‘Carmen’\n  \n\n\n  \n    \n    \n      Seismerssion: Retrospectives on an Audio-visual installations\n      applied-project\n      \n        Jul 14, 2019 • Karolina Jawad, Espen Wik, Sepehr Haghighi, Shreejay Shrestha, Jørgen Varpe\n      Seismerssion is the title we gave our Applied Project in the context of the SMC spring semester 2019. This audio-visual installation is dedicated to the widely unknown issue of sound pollution in the ocean. In collaboration with NTNU Oceans, an intership was established to develop and implement a public installation concept for 2 different venues.\n  \n\n\n",
    "url": "/applied-projects/"
  },
  
  {
    "title": "Guides",
    "author": "\n",
    "excerpt": "\n",
    "content": "Being an SMC student can be an overwhelming experience, especially at the beginning. You are required to use several new software tools and online environments from the beginning.\n\nIn light of this, we have prepared a collection of guides featuring step-by-step instructions, video tutorials, general tips and other valuable information on a range of topics designed to make the SMC experience as smooth as possible. We therefore recommend students go through each guide and do eventual preparatory work before starting the programme, or to access these guides later on when necessary.\n\n\n  SMC Student Guides Homepage\n  How to prepare for the SMC program\n  The software you need\n  How to use the SMC Blog\n  How to register for SMC courses\n  How to use Canvas\n  SMC code of conduct\n\n",
    "url": "/Guides/"
  },
  
  {
    "title": "New Interfaces for Musical Expression",
    "author": "\n",
    "excerpt": "\n",
    "content": "The blog posts of this section relate to the course New Interfaces for Musical Expression (2014-2025) and Mobile and Wearable Computing (2023 onwards). The aim of these courses is to develop knowledge of and practical experience with the design and implementation of systems intended for real-time sonic or musical interaction. This could be in the form of a performance-oriented musical instrument, or in various types of interactive sonic systems that explore collaborative physical, or virtual music-making.\n\n\n  \n    \n    \n      RFART: A Tectonic Interactive Music System \n      interactive-music\n      \n        Feb 4, 2025 • Benjamin Melvin Stein\n      **Creative festival goers** need **an interactive and low-stakes way to create and collaborate** because **it can provide a relaxing reprieve from the festival while still being engaging**\n  \n\n\n  \n    \n    \n      The SlapBox: A DMI designed to last\n      interactive-music\n      \n        Nov 24, 2024 • Karenina Juarez\n      A critical review of a durable digital musical instrument\n  \n\n\n  \n    \n    \n      Review of Sounding Brush: a graphic score IMS? \n      interactive-music\n      \n        Sep 12, 2024 • Tom Oldfield\n      A critical review of a drawing-based interactive music system\n  \n\n\n  \n    \n    \n      Xyborg 2.0: A Data Glove-based Synthesizer\n      interactive-music\n      \n        Dec 4, 2023 • Kristian Eicke\n      Learn about my adventures in designing and playing a wearable instrument.\n  \n\n\n  \n    \n    \n      Voice/Bend\n      interactive-music\n      \n        Dec 4, 2023 • Nino Jakeli\n      Microphone Gestural controller\n  \n\n\n  \n    \n    \n      The Hyper-Ney\n      interactive-music\n      \n        Dec 1, 2023 • Emin Memis\n      Electrizing an ancient flute using capacitive and motion sensors\n  \n\n\n  \n    \n    \n      An Interactive Evening on Karl Johans Gate\n      interactive-music\n      \n        Dec 1, 2023 • Maham Riaz\n      What if everyday objects decide to kick it up a notch and embrace a life of their own?\n  \n\n\n  \n    \n    \n      CordChord - controlling a digital string instrument with distance sensing and machine learning\n      interactive-music\n      \n        Dec 1, 2023 • Jack Hardwick\n      How can we use sensors to control a digital string instrument? Here's one idea.\n  \n\n\n  \n    \n    \n      The Paperback Singer\n      interactive-music\n      \n        Dec 1, 2023 • Fabian Stordalen\n      An interactive granular audio book\n  \n\n\n  \n    \n    \n      Touch/Tap/Blow - Exploring Intimate Control for Musical Expression\n      interactive-music\n      \n        Dec 1, 2023 • Alexander Wastnidge\n      Touch/Tap/Blow is, as its name suggests, an interactive music system which aims to combine three forms of intimate control over a digital musical instrument.  Notes and chords can be played via the touch interface while bass accompaniment can be driven by the player’s foot tapping. Below are the details of it’s main elements.\n\n  \n\n\n  \n    \n    \n      The Saxelerophone: Demonstrating gestural virtuosity\n      interactive-music\n      \n        Nov 30, 2023 • Joachim Poutaraud\n      A hyper-instrument tracking data from a 3-axis accelerometer and a contact microphone to create new interactive sounds for the saxophone.\n  \n\n\n  \n    \n    \n      A Critical Look at Cléo Palacio-Quintin’s Hyper-Flute\n      interactive-music\n      \n        Nov 11, 2023 • Emin Memis\n      A Boehm flute enhanced with sensors\n  \n\n\n  \n    \n    \n      The Daïs: Critical Review of a Haptically Enabled NIME\n      interactive-music\n      \n        Sep 25, 2023 • Kristian Eicke\n      Is this a violin?\n  \n\n\n  \n    \n    \n      Shadows As Sounds\n      interactive-music\n      \n        Sep 23, 2023 • Nino Jakeli\n      4-step sequencer using seeds and corns\n  \n\n\n  \n    \n    \n      The Augmented Violin: Examining Musical Expression in a Bow-Controlled Hyper-Instrument\n      interactive-music\n      \n        Sep 22, 2023 • Jack Hardwick\n      A brief look at the affordance for musical expression in a violin-based interactive music system.\n  \n\n\n  \n    \n    \n      The Tickle Tactile Controller - Review\n      interactive-music\n      \n        Sep 22, 2023 • Maham Riaz\n      Like many digital instruments I have come across, the instrument design takes its initial inspiration from the piano, a fixed-key instrument.\n  \n\n\n  \n    \n    \n      Review of On Board Call: A Gestural Wildlife Imitation Machine\n      interactive-music\n      \n        Sep 22, 2023 • Masoud Niknafs\n      Critical Review of On Board Call: A Gestural Wildlife Imitation Machine\n  \n\n\n  \n    \n    \n      How to break out of the comping loop?\n      interactive-music\n      \n        Sep 22, 2023 • Joachim Poutaraud\n      A critical review of the Reflexive Looper.\n  \n\n\n  \n    \n    \n      Exploring Breath-based DMIs: A Review of the KeyWI\n      interactive-music\n      \n        Sep 21, 2023 • Alexander Wastnidge\n      The relationship between musician and instrument can be an extremely personal and intimate one\n  \n\n\n  \n    \n    \n      Controling Guitar Signals Using a Pick?\n      interactive-music\n      \n        Sep 19, 2023 • Fabian Stordalen\n      A deeper dive into the Magpick\n  \n\n\n  \n    \n    \n      Towards a Claptrap-Speaking Kastle Maus\n      interactive-music\n      \n        Dec 9, 2022 • Kristian Wentzel\n      Once upon a time, there was a maus living in a kastle..\n  \n\n\n  \n    \n    \n      Expressive Voice: an IMS for singing\n      interactive-music\n      \n        Dec 9, 2022 • Sofía González\n      Take a peak at my IMS and the reasoning behind its design.\n  \n\n\n  \n    \n    \n      Interactive Music Systems, Communication and Emotion\n      interactive-music\n      \n        Dec 9, 2022 • Sofía González\n      Talking about IMSs, expressing and even inducing emotions.\n  \n\n\n  \n    \n    \n      Pringles, I love You (not)\n      interactive-music\n      \n        Dec 8, 2022 • Jakob Høydal\n      No, that is not true. I do not like Pringles. But I like the tube it comes with! That’s why I invited a friend over to eat the chips, so I could use the tube for my 4054 Interactive Music Systems project.\n  \n\n\n  \n    \n    \n      Three Takeaways as a musicologist\n      interactive-music\n      \n        Dec 8, 2022 • Ole Tveit Hana\n      Recounting the experience of making an instrument from scratch for the first time.\n  \n\n\n  \n    \n    \n      SR-01\n      interactive-music\n      \n        Dec 8, 2022 • Henrik Sveen\n      The climate aware synthesizer. Based on using few components while reacting to changes in light and temperature around it, causing it to sound different today than in a changed climate.\n  \n\n\n  \n    \n    \n      How to make your screen time a natural experience\n      interactive-music\n      \n        Dec 8, 2022 • Ole Tveit Hana\n      Learn how playing with mud could equate to playing with computer.\n  \n\n\n  \n    \n    \n      The Feedback Mop Cello: Making Music with Feedback - Part 2\n      interactive-music\n      \n        Dec 8, 2022 • Hugh Alexander von Arnim\n      Using a mop to play the feedback\n  \n\n\n  \n    \n    \n      Concussion Percussion: A Discussion\n      interactive-music\n      \n        Dec 8, 2022 • Joseph Clemente\n      Whether it’s riding a bike or building an handpan-esque interactive music system, always remember to wear a helmet\n  \n\n\n  \n    \n    \n      Shimmerion - A String Synthesizer played with Light\n      interactive-music\n      \n        Dec 8, 2022 • Iosif Aragiannis\n      Use your phone's flashlight to make music!\n  \n\n\n  \n    \n    \n      A Christmas tale of cookie boxes and soldering\n      interactive-music\n      \n        Dec 7, 2022 • Arvid Falch\n      How I got the cookie box drum I never knew I wanted for Christmas\n  \n\n\n  \n    \n    \n      Out-Of-The-Box Sound Sources for your IMS\n      interactive-music\n      \n        Dec 7, 2022 • Kristian Wentzel\n      Exploring alternatives for generating sounds with your interactive music system.\n  \n\n\n  \n    \n    \n      The Feedbackquencer: Making Music with Feedback - Part 1\n      interactive-music\n      \n        Dec 6, 2022 • Hugh Alexander von Arnim\n      Using feedback in a sequencer\n  \n\n\n  \n    \n    \n      Wii controller as the Gestural Controller\n      interactive-music\n      \n        Dec 6, 2022 • Thyra Liang Aakvåg\n      Read this post to find information on a different use of a Wiimote than playing games on your Wii console.\n  \n\n\n  \n    \n    \n      Music By Laser: The Laser Harp\n      interactive-music\n      \n        Dec 6, 2022 • Thyra Liang Aakvåg\n      If you want to know how to play music with lasers, and maybe learn something about the laser harp, then you should give this a read.\n  \n\n\n  \n    \n    \n      Yggdrasil: An Environmentalist Interactive Music Installation\n      interactive-music\n      \n        Dec 3, 2022 • Oliver Getz\n      Plant trees and nurture your forest to generate sound!\n  \n\n\n  \n    \n    \n      Why Your Exhibit Tech Failed (and how to fix it)\n      interactive-music\n      \n        Dec 3, 2022 • Oliver Getz\n      Why are visitors not using your installation?  You might just disagree with yourself, the visitors, and your department or client about their needs and wants. The reason for this disagreement is always the same.\n  \n\n\n  \n    \n    \n      New Interface for Sound Evolution (NISE)\n      interactive-music\n      \n        Dec 1, 2022 • Björn Þór Jónsson\n      Robiohead is an attempt in a box to explore how an Interactive Music System (IMS) can offer ways to explore sound spaces by evolving sound producing genes in a tactile manner.\n  \n\n\n  \n    \n    \n      An instrument, composition and performance. Meet the Transformer #1\n      interactive-music\n      \n        Nov 30, 2022 • Arvid Falch\n      A review of Natasha Barretts Transformer #1\n  \n\n\n  \n    \n    \n      Zeusaphone - The singing Tesla coil\n      interactive-music\n      \n        Nov 26, 2022 • Iosif Aragiannis\n      Have you ever seen choreographed lightning?\n  \n\n\n  \n    \n    \n      Dråpen, worlds largest interactive music system? \n      interactive-music\n      \n        Nov 13, 2022 • Jakob Høydal\n      This may be the largest interactive music system you have heard about\n  \n\n\n  \n    \n    \n      A Contact Microphone And A Dream: To Loop\n      interactive-music\n      \n        Nov 11, 2022 • Joseph Clemente\n      Join a humble contact microphone on its quest for freedom!\n  \n\n\n  \n    \n    \n      FAM Synthesizer\n      interactive-music\n      \n        Nov 10, 2022 • Henrik Sveen\n      A simple digital synthesizer with the potential of sounding big, complex and kind of analog. Screenless menudiving included.\n  \n\n\n  \n    \n    \n      The Triadmin\n      interactive-music\n      \n        Nov 3, 2021 • Halvor Sogn Haug\n      An instrument without any tangible interface.\n  \n\n\n  \n    \n    \n      The algorithmic note stack juggler\n      interactive-music\n      \n        Nov 3, 2021 • Stephen Gardener\n      Interactive composition with the Algorithmic Note Stack Juggler.\n  \n\n\n  \n    \n    \n      Sequencephere-Linesequencer\n      interactive-music\n      \n        Nov 3, 2021 • Abhishek Choubey\n      Exploration and design of a drum Sequencer and synth using Bela as an interactive music system with Csound\n  \n\n\n  \n    \n    \n      D&#39;n&#39;B\n      interactive-music\n      \n        Nov 1, 2021 • Lindsay Charles\n      Exploration and design of the 'Drum and Bass' interactive music system with Csound\n  \n\n\n  \n    \n    \n      Ethrio\n      interactive-music\n      \n        Nov 1, 2021 • Pedro Lucas\n      Ethereal sounds from the three dimensions of music: Melody, Harmony, and Rhythm.\n  \n\n\n  \n    \n    \n      Breathe the light, scream arpeggios!\n      interactive-music\n      \n        Oct 20, 2020 • Rayam Luna\n      Multisensorial music interface aiming to provide a synesthetic experience. Touch, light, breathe, scream - make sound!\n  \n\n\n  \n    \n    \n      The Ring Synth\n      interactive-music\n      \n        Oct 20, 2020 • Thibault Jaccard\n      Exploring speed as sound shaping parameter\n  \n\n\n  \n    \n    \n      HyperGuitar\n      interactive-music\n      \n        Oct 19, 2020 • Thomas Anda\n      An exploration of limitations and how to create meaningful action-sound couplings.\n  \n\n\n  \n    \n    \n      The singing shelf bracket\n      interactive-music\n      \n        Oct 18, 2020 • Mari Lesteberg\n      Pure Data (PD) has a lot of possibilities, but when getting the opportunity of putting together all of those digital features into the real word: with real wires, real buttons, real sensors - I must admit - I got a little over-excited!\n  \n\n\n  \n    \n    \n      Plunged Into Chaos\n      interactive-music\n      \n        Oct 18, 2020 • Paul Koenig\n      Wherein the lowly Oompa-Doompa assumes its ultimate form.\n  \n\n\n  \n    \n    \n      MIDI and Effects: the Musicpathy \n      interactive-music\n      \n        Oct 18, 2020 • Abhishek Choubey, Lindsay Charles\n      The magic of controlling instruments from 1000km apart\n  \n\n\n  \n    \n    \n      Voice augmentation with sensors\n      interactive-music\n      \n        Oct 18, 2020 • Ulrik Halmøy\n      Trying to achieve a choir-like effect by augmenting microphone input with sensory features\n  \n\n\n  \n    \n    \n      SamTar\n      interactive-music\n      \n        Oct 17, 2020 • Aleksander Tidemann\n      An interactive music system exploring sample-based music and improvisation through an augmented electric guitar \n  \n\n\n  \n    \n    \n      The Psychedelic Journey of an Unexpected Spaceship\n      interactive-music\n      \n        Oct 16, 2020 • Pedro Lucas\n      An electronic music performance in an audio-visual digital environment. Let's go through the galaxy in a crazy spaceship and have an experience full of color and funny turbulence.\n  \n\n\n  \n    \n    \n      A Live Mixer made from mobile devices\n      interactive-music\n      \n        Oct 16, 2020 • Wenbo Yi\n      How does it look to control audio effects in real-time using gestures?\n  \n\n\n  \n    \n    \n      cOSmoChaos\n      interactive-music\n      \n        Oct 16, 2020 • Anders Lidal\n      Cosmos and chaos are opposites—known/unknown, habitated/unhabitated—and man has through all times been seeking to create cosmos out of chaos. But what has this to do with GyrOSC controlling my hardware … well, everything.\n  \n\n\n  \n    \n    \n      Chance Operations, Rudimentary Pure Data (PD), and a Bunch of Spinning in Circles\n      interactive-music\n      \n        Oct 16, 2020 • Willie Mandeville\n      Sometimes you want to compose and get your workout. Experience a chance composition that may leave the performer sweating.\n  \n\n\n  \n    \n    \n      Improvised electronica with TouchOSC\n      interactive-music\n      \n        Oct 16, 2020 • Stephen Gardener\n      In this project, I wanted to explore the options available when performing electronic music live with no pre-recorded / pre-sequenced material.\n  \n\n\n  \n    \n    \n      Real-time audio processing with oscHook and Reaper\n      interactive-music\n      \n        Oct 16, 2020 • Alena Clim\n      Fun and not too complicated interactive audio processing.Using oscHook to transmit sensordata from an Android phone to OSC Router and then to Reaper to control the values of certain effects' parameters.\n  \n\n\n  \n    \n    \n      The Dolphin Drum\n      interactive-music\n      \n        Oct 16, 2020 • Simon Sandvik\n      My granular synthesis percussive instrument from the Interactive Music Systems course.\n  \n\n\n  \n    \n    \n      Musings with Bela\n      interactive-music\n      \n        Oct 16, 2020 • Jackson Goode\n      A tale of accelerometers, knobs, an EEG and the attempt to tame sound with my mind. Follow along!\n  \n\n\n  \n    \n    \n      The voice of a loved one\n      interactive-music\n      \n        Oct 16, 2020 • Joni Mok\n      Can AI really know what our facial expressions mean?\n  \n\n\n  \n    \n    \n      Making virtual guitar playing feel more natural\n      interactive-music\n      \n        Oct 15, 2020 • Leigh Murray\n      Can using sensors, buttons and joysticks to play a virtual Guitar resemble the experience of playing a real guitar and result in a more natural performance than using a keyboard for input?\n  \n\n\n  \n    \n    \n      Étude de téléphone portable et popsocket\n      interactive-music\n      \n        Oct 14, 2020 • Henrik Sveen\n      Click to see a cute dog making strange music. Unbelievable. I think that sums it up.\n  \n\n\n  \n    \n    \n      Staggering towards the light\n      interactive-music\n      \n        Dec 30, 2019 • Thomas Anda, Aleksander Tidemann, Paul Koenig, Tom Ignatius\n      During a hackathon in our introduction course to physical computing, we developed a prototype of a DMI. In our blog post from this project we explained how the system was built and gave a short summary of our performance. In this blog post however, we will look at the instrument from an HCI-perspective. Where the main focus will be a summary of the problem space, the research question, the methodology used and our main findings and contributions. \n  \n\n\n  \n    \n    \n      Prototyping musical instruments\n      interactive-music\n      \n        Dec 27, 2019 • Rayam Luna, Gaute Wardenær, Thibault Jaccard, Ulrik Halmøy\n      Prototyping musical instruments in the name of recycling - exploring Orchestrash from an HCI point of view\n  \n\n\n  \n    \n    \n      Orchestrash hackathon performance\n      interactive-music\n      \n        Oct 24, 2019 • Rayam Luna, Gaute Wardenær, Thibault Jaccard, Ulrik Halmøy\n      The title of our project is \"Orchestrash\" inspired by the theme of the competition and our approach to solving it, by making individual instruments controlled by recycled materials and \"recycling\" sound by sampling\n  \n\n\n  \n    \n    \n      The B Team: Mini-Hackathon\n      interactive-music\n      \n        Oct 24, 2019 • Jackson Goode, Magda Futyma, Simon Sandvik, Jarle Steinhovden\n      For the SMC 4000 mini-hackathon in the physical computing module we tried to send sound at the speed of light.\n  \n\n\n  \n    \n    \n      Physical Computing: Heckathon: Group C\n      interactive-music\n      \n        Oct 22, 2019 • Paul Koenig, Thomas Anda, Tom Ignatius, Aleksander Tidemann\n      Taking our cue from the main theme of the Day 4 Hackathon of “Recycling”, Team C chose the 2017 U.S. withdrawal from the Paris Agreement on climate change mitigation as a central theme in our work.\n  \n\n\n  \n    \n    \n      Wizard_of_Vox\n      interactive-music\n      \n        Oct 18, 2019 • Notto Thelle\n      Wizard Of Vox - Wizard Of Vox is a gesture-based speech synthesis system that can be can be “made to speak”\n  \n\n\n  \n    \n    \n      The Fønvind Device\n      interactive-music\n      \n        Oct 18, 2019 • Eigil Aandahl\n      For my interactive music systems project, I wanted to make use of the Bela's analog inputs and outputs to make a synthesizer capable of producing not only sound, but also analog control signals that can be used with an analog modular synthesizer. This post goes briefly through some of the features and the design of my system, and at the end there is a video demonstration of the system in use.\n  \n\n\n  \n    \n    \n      Instant Music, Subtlety later\n      interactive-music\n      \n        Oct 17, 2019 • Karolina Jawad\n      When drafting ideas in unknown territory one can become overwhelmed with the sheer endless options to create an IMS (interactive music system). Here a real-time processing board for voice with gesture control.\n  \n\n\n  \n    \n    \n      The B Team: To Heck and Back\n      interactive-music\n      \n        Oct 15, 2019 • Jackson Goode, Magda Futyma, Simon Sandvik, and Jarle Steinhovden\n      Today we began our experiments with some lofi hardware, simple contact mics, speakers, batteries, and some crocodile cables to connect it all. We left in pieces.\n  \n\n\n  \n    \n    \n      Physical Computing Day One: Victorian Synthesizer Madness! Group C Reports In From Heck\n      interactive-music\n      \n        Oct 15, 2019 • Paul Koenig, Aleksander Tidemann, Thomas Anda, Tom Ignatius\n      The first day of Physical Computing started and ended with a bit of confusion in the Portal, but that is par for the course. Once we set up the various cameras and microphones, and dealt with feedback, echo, etc, the fun began! \n  \n\n\n  \n    \n    \n      Physical computing Day 1 - Group B\n      interactive-music\n      \n        Oct 15, 2019 • Rayam Luna, Gaute Wardenær, Thibault Jaccard, Ulrik Halmøy\n      First day of physical computing\n  \n\n\n  \n    \n    \n      AudioBend\n      interactive-music\n      \n        Oct 14, 2019 • Ashane Silva\n      My project idea for the Interactive Music Systems was to build a glove that can manipulate sound.  It was actually inspired by seeing the “mi.mu Gloves”.  The paper on the “Data Glove” gave me ideas on the design aspect of the glove although the way it works is a bit more different than what I use in my glove. “Data glove” uses multiple flex sensors on the fingers and force sensitive sensors to contact the finger tips and an accelerometer to get data from the wrist control. In my glove I used flex sensor on index finger, 3 – axis accelerometer on my hand and a Distance Ultrasonic sensor on my palm. Attaching those stuff to the glove was a bit tricky but “ducktape” saved my life.\n  \n\n\n  \n    \n    \n      Alien_Hamster_Ball\n      interactive-music\n      \n        Oct 14, 2019 • Samuel Roman\n      The Alien Hamster ball - an instrument expressed through a 3D space\n  \n\n\n  \n    \n    \n      LoopsOnFoam\n      interactive-music\n      \n        Oct 13, 2019 • Jørgen Varpe\n      During a 2-week intensive workshop in the course Interactive Music Systems I worked on the development of an instrument prototype, which I named LoopsOnFoam.\n  \n\n\n  \n    \n    \n      Microture\n      interactive-music\n      \n        Oct 13, 2019 • Sepehr Haghighi\n      Microture is an interactive music system, based on manipulation of the input sound (microphone sound) with small gestures..\n  \n\n\n  \n    \n    \n      Body Drums - A wearable drumset\n      interactive-music\n      \n        Oct 12, 2019 • Elias Andersen\n      Body Drums\nFor the course SMC4045 - Interactive Music Systems, I built a wearable drumset.\nThe wearable drumset consists of a piezo-element placed on one hand, a force-sensing resistor on the other and a accelerometer on the fot. These sensors are then used to trigger one file each. In my case I used a kick drum sound for the foot, snare drum sound for the piezo element and a hi hat sound for the FSR. Then when these sensors are triggered, the sound that are mapped to the sensor will be played. For example if I stump my foot, the kick drum sound will be played.\n\n  \n\n\n  \n    \n    \n      The HySax - Augmented saxophone meant for musical performance \n      interactive-music\n      \n        Oct 12, 2019 • Guy Sion\n      an augmented saxophone meant for musical performance, enabling background layer and delay to be controlled via gestures.\n  \n\n\n",
    "url": "/interactive-music/"
  },
  
  {
    "title": "Machine Learning",
    "author": "\n",
    "excerpt": "\n",
    "content": "The blog posts of this section relate to the course SMC4052 Music and Machine Learning (2022 onwards) and to the discontinued course SMC4047 Music and Machine Learning (2019 to 2021). The aim of these courses is to develop knowledge of and practical experience with machine learning algorithms applied to music analysis, music information retrieval, interactive music systems, and algorithmic music.\n\n  \n    \n    \n      Playlist Data and Recommendations Using Artificial Neural Networks\n      machine-learning\n      \n        Apr 30, 2024 • Karenina Juarez\n      a machine-learning algorithm that pairs independent artists, with curated playlists that best fit based on musical attributes\n  \n\n\n  \n    \n    \n      AcidLab: deep acid bass-line generation\n      machine-learning\n      \n        Apr 28, 2024 • Tom Oldfield\n      Using machine learning to generate acid bass-line melodies with user-created datasets.\n  \n\n\n  \n    \n    \n      Connecting Eigenrhythms and Human movement\n      machine-learning\n      \n        Apr 28, 2024 • Olav Utne Skjeldal\n      Connecting Machine Learning and Human understanding of rhythm.\n  \n\n\n  \n    \n    \n      The Chiptransformer\n      machine-learning\n      \n        Apr 28, 2024 • Sondre Røvik Kippenes\n      The Chiptransformer an my attempt at building a machine learning model using the transformer architecture to generate music based on a dataset of Nintendo NES music.\n  \n\n\n  \n    \n    \n      Using convolutional neural networks to classify music genres\n      machine-learning\n      \n        Apr 28, 2024 • Erlend André Lie Størkson\n      When classifying genres in music, CNNs are a popular choice because of their ability to capture intricate patterns in data.\n  \n\n\n  \n    \n    \n      Scream Machine: Voice Conversion with an Artifical Neural Network\n      machine-learning\n      \n        Apr 26, 2023 • Kristian Eicke\n      Using a VAE to transform one voice to another.\n  \n\n\n  \n    \n    \n      Generating music with an evolutionary algorithm\n      machine-learning\n      \n        Apr 26, 2023 • Noor Othmani\n      Looking at a theoretical and general implementation of an evolutionary algorithm to generate music.\n  \n\n\n  \n    \n    \n      Persian classical instruments recognition and classification\n      machine-learning\n      \n        Apr 26, 2023 • Masoud Niknafs\n      This blog post will go over various feature extraction techniques used to identify Persian classical music instruments.\n  \n\n\n  \n    \n    \n      Isn&#39;t Bach deep enough?\n      machine-learning\n      \n        Apr 26, 2023 • Masoud Niknafs\n      Deep Bach is an artificial intelligence that composes like Bach.\n  \n\n\n  \n    \n    \n      Recognizing Key Signatures with Machine Learning\n      machine-learning\n      \n        Apr 26, 2023 • Jack Hardwick\n      The first rule of machine learning? Understand your data! A look into how music theory came to my rescue for classifying key signatures.\n  \n\n\n  \n    \n    \n      Breakbeat Science\n      machine-learning\n      \n        Apr 26, 2023 • Fabian Stordalen\n      AI-Generated amen breakbeats\n  \n\n\n  \n    \n    \n      Spotlight: AutoEncoders and Variational AutoEncoders\n      machine-learning\n      \n        Apr 26, 2023 • Fabian Stordalen, Alexander Wastnidge, Kristian Eicke\n      A simple generative algorithm\n  \n\n\n  \n    \n    \n      Music AI, a brief history\n      machine-learning\n      \n        Apr 26, 2023 • Noor Othmani\n      Chronicling the field of AI-generated music's start, where it went from there, and what you can expect from musical AI right now.\n  \n\n\n  \n    \n    \n      A caveman&#39;s way of making art\n      machine-learning\n      \n        Apr 26, 2023 • Emin Memis\n      ...and art in the age of complexity.\n  \n\n\n  \n    \n    \n      Clustering audio features\n      machine-learning\n      \n        Apr 25, 2023 • Nino Jakeli\n      Music information retrieval(MIR)\n  \n\n\n  \n    \n    \n      What&#39;s wrong with singing voice synthesis\n      machine-learning\n      \n        Apr 25, 2023 • Nino Jakeli\n      Dead can sing\n  \n\n\n  \n    \n    \n      The whistle of the autoencoder\n      machine-learning\n      \n        Apr 25, 2023 • Emin Memis\n      How I used autoencoders to create whistling.\n  \n\n\n  \n    \n    \n      Chroma Representations of MIDI for Chord Generation\n      machine-learning\n      \n        Apr 25, 2023 • Jack Hardwick\n      Understanding two ways of representing and generating chords in machine learning.\n  \n\n\n  \n    \n    \n      Generating Video Game SFX with AI\n      machine-learning\n      \n        Apr 25, 2023 • Oliver Getz\n      A first look at text-to-audio sound effect generation for video games.\n  \n\n\n  \n    \n    \n      Pytorch GPU Setup Guide\n      machine-learning\n      \n        Apr 25, 2023 • Oliver Getz\n      Having trouble getting Pytorch to recognize your GPU? Try this!\n  \n\n\n  \n    \n    \n      A Rhythmic Sequencer Driven by a Stacked Autoencoder\n      machine-learning\n      \n        Apr 25, 2023 • Alexander Wastnidge\n      Sometimes you need to leave room for the musician\n  \n\n\n  \n    \n    \n      Comparing MIDI Representations: The Battle Between Efficiency and Complexity\n      machine-learning\n      \n        Apr 25, 2023 • Trym Bø\n      A comparing of different MIDI representations for generative machine learning\n  \n\n\n  \n    \n    \n      Programming with OpenAI\n      machine-learning\n      \n        Apr 24, 2023 • Aysima Karcaaltincaba\n      How OpenAI solutions help us to program?\n  \n\n\n  \n    \n    \n      Challenges with Midi\n      machine-learning\n      \n        Apr 24, 2023 • Aysima Karcaaltincaba\n      Is it easy to create chord progression from midi files?\n  \n\n\n  \n    \n    \n      MIDI music generation, the hassle of representing MIDI\n      machine-learning\n      \n        Apr 19, 2023 • Trym Bø\n      A brief guide of the troubles with MIDI representation for generative AI\n  \n\n\n  \n    \n    \n      Reverb Classification of wet audio signals\n      machine-learning\n      \n        May 20, 2022 • Jakob Høydal\n      Differenciating reverberation times of wet audio signals using machine learning. \n  \n\n\n  \n    \n    \n      Piano Accompaniment Generation Using Deep Neural Networks\n      machine-learning\n      \n        May 20, 2022 • Kristian Wentzel\n      How I made use of Fourier Transforms in deep learning to generate expressive MIDI piano accompaniments.\n  \n\n\n  \n    \n    \n      Recognizing and Predicting Individual Drumming Groove Styles Using Artificial Neural Networks\n      machine-learning\n      \n        May 20, 2022 • Joseph Clemente\n      Can we teach an algorithm to groove exactly like a specific drummer?\n  \n\n\n  \n    \n    \n      Estimating the repertoire size in birds\n      machine-learning\n      \n        May 20, 2022 • Joachim Poutaraud\n      Estimating the repertoire size in birds using unsupervised clustering techniques\n  \n\n\n  \n    \n    \n      Emulating analog guitar pedals with Recurrent Neural Networks\n      machine-learning\n      \n        May 19, 2022 • Arvid Falch\n      Using LSTM recurrent neural networks to model two analog guitar pedals.\n  \n\n\n  \n    \n    \n      Generating Samples Through Dancing\n      machine-learning\n      \n        May 3, 2022 • Hugh Alexander von Arnim\n      Using a VAE to build a generative sampler instrument\n  \n\n\n  \n    \n    \n      Generating Music Based on Webcam Input\n      machine-learning\n      \n        Mar 23, 2022 • Arvid Falch, Hugh Alexander von Arnim, Jakob Høydal, Joachim Poutaraud\n      Do you miss PS2 EyeToy? Then you have to check this out!\n  \n\n\n  \n    \n    \n      Using Live OSC Data From Smartphones To Make Music With Sonic Pi\n      machine-learning\n      \n        Mar 23, 2022 • Joseph Clemente, Kristian Wentzel\n      Easy as pi!\n  \n\n\n  \n    \n    \n      Can Machine learning classify audio effects, a dry to wet sound ?\n      machine-learning\n      \n        Sep 20, 2021 • Abhishek Choubey\n      Distortion or No Distortion - Machine learning magic\n  \n\n\n  \n    \n    \n      Ensemble algorithms and music classification\n      machine-learning\n      \n        Sep 20, 2021 • Alena Clim\n      Playing around with some supervised machine learning - genre classification is hard!\n  \n\n\n  \n    \n    \n      Classifying Classical Piano Music Based On Composer’s Native Language Using Machine Learning\n      machine-learning\n      \n        Sep 19, 2021 • Wenbo Yi\n      How does the language we speak help computers to classify classical music?\n  \n\n\n  \n    \n    \n      Estimation of Direction of Arrival (DOA) for First Order Ambisonic (FOA) Audio Files\n      machine-learning\n      \n        Sep 18, 2021 • Pedro Lucas\n      Where is that sound coming from? Let's explore how a machine could answer this question.\n  \n\n\n  \n    \n    \n      Ensemble algorithms and music classification\n      machine-learning\n      \n        Sep 17, 2021 • Stephen Gardener\n      'Why is shoegaze so hard to classify?' and other pertinent questions for the technologically inclined indie-kid.\n  \n\n\n  \n    \n    \n      ML something creative\n      machine-learning\n      \n        Sep 21, 2020 • Gaute Wardenær\n      Intuition tells me that a larger network should be better. More is more, as Yngwie says, but that is definitely not the case.\n  \n\n\n  \n    \n    \n      Multilayer Perceptron Classification\n      machine-learning\n      \n        Sep 21, 2020 • Simon Rønsholm Sandvik\n      Multi-layer Perceptron classification. Big words for a big task. During this two-week course in machine learning all my brain cells were allocated in solving this task. Initially I wanted something simple to do for my project since wrapping my head around ML was a daunting enough task. I soon realized there really is no such thing as simple in machine learning.\n  \n\n\n  \n    \n    \n      Scale over Chord using ANN\n      machine-learning\n      \n        Sep 21, 2020 • Thibault Jaccard\n      Try to learn scale over chord choices of great jazz improvisers\n  \n\n\n  \n    \n    \n      Learning to sequence drums\n      machine-learning\n      \n        Sep 21, 2020 • Ulrik Halmøy\n      Can reinforcement learning be a useful tool to teach a neural network to sequence drums?\n  \n\n\n  \n    \n    \n      Beneficial Unintelligence\n      machine-learning\n      \n        Sep 21, 2020 • Mari Lesteberg\n      In the future, when the robots take over the world, we all will be listening to 24/7 live streamed death metal until infinity\n  \n\n\n  \n    \n    \n      Postcard from The Valley of Despair\n      machine-learning\n      \n        Sep 20, 2020 • Paul Koenig\n      Well, that was fun.\n  \n\n\n  \n    \n    \n      Support Vector Machine Attempt\n      machine-learning\n      \n        Sep 20, 2020 • Tom Ignatius\n      Not fun\n  \n\n\n  \n    \n    \n      Classifying Urban Sounds in a Multi-label Database\n      machine-learning\n      \n        Sep 20, 2020 • Jackson Goode\n      How well does a convolution neural network perform at detecting multiple classes within a single sample? This experiment explores augmenting the UrbanSound8K database to test a well performing CNN architecture in a multi-label, multi-class scenario.\n  \n\n\n  \n    \n    \n      [ Music Mood Classifrustration ]\n      machine-learning\n      \n        Sep 20, 2020 • Rayam Luna\n      This is an attempt to create a Music Mood Classifier with feature extractions from Librosa.\n  \n\n\n  \n    \n    \n      Classification of guitar playing techniques\n      machine-learning\n      \n        Sep 20, 2020 • Thomas Anda\n      An attempt at making a model which can classify 6 different playing techniques on the guitar\n  \n\n\n  \n    \n    \n      Exploring Music Preference Recognition Using Spotify&#39;s Web API\n      machine-learning\n      \n        Sep 18, 2020 • Aleksander Tidemann\n      A proposed ML model that predicts the degree to which I will enjoy specific Spotify tracks based on previous preference ratings.\n  \n\n\n  \n    \n    \n      The B Team Dives in Deep (Learning!)\n      machine-learning\n      \n        Nov 25, 2019 • Jackson Goode, Jarle Steinhovden, Simon Sandvik\n      Well, here we are, at the end of a semester where one of the most challenging courses remain - only Rebecca Fiebrink can save us now.\n  \n\n\n  \n    \n    \n      Group C Learns to Think about how Machines Learn to Think\n      machine-learning\n      \n        Nov 25, 2019 • Paul Koenig, Aleksander Tidemann, Tom Ignatius, Thomas Anda\n      Wherein we describe the denouement of SMC4000, Module #9: Machine Learning.\n  \n\n\n  \n    \n    \n      Music and machine learning - Group A\n      machine-learning\n      \n        Nov 25, 2019 • Rayam Luna, Gaute Wardenær, Thibault Jaccard, Ulrik Halmøy\n      Using machine learning for performance and for classifying sounds\n  \n\n\n  \n    \n    \n      Machine Learning, it&#39;s all about the data\n      machine-learning\n      \n        Sep 19, 2019 • Eigil Aandahl\n      For my machine learning project, I wanted to see if I could teach my laptop to distinguish between different types of music using a large amount of data. Using metadata from a large dataset for music analysis, I tested different machine learning classifiers with supervised learning to distinguish between tracks labeled belonging to 'Rock' and 'Electronic'. The project was developed using Python and libraries for data analysis and machine learning.\n  \n\n\n  \n    \n    \n      Clustering high dimensional data\n      machine-learning\n      \n        Sep 17, 2019 • Karolina Jawad\n      In the project for Music and Machine Learning I was using raw audio data to see how well the K-Mean clustering technique would work for structuring and classifying an unlabelled data-set of voice recordings. \n  \n\n\n  \n    \n    \n      MIDI drum beat generation\n      machine-learning\n      \n        Sep 16, 2019 • Elias Andersen\n      Most music production today depend strongly on technology, from the beginning of a songs creation, till the the last final tunings during mix and master. Still their is usually many human aspect involved, like singing, humans playing instruments, humans using a music making software etc..\n  \n\n\n  \n    \n    \n      IR Reverberation Classifier using Machine Learning\n      machine-learning\n      \n        Sep 16, 2019 • Sam Roman\n      Using Machine Learning to classify different reverb spaces (using impulse response files)\n  \n\n\n  \n    \n    \n      Multi-Layer Perceptron Classifier of Dry/Wet Saxophone Sound\n      machine-learning\n      \n        Sep 16, 2019 • Guy Sion\n      The application I have decided to work on is of a machine learning model that can ultimately differentiate between a saxophone sound with effect (wet) and without effect (dry).\n  \n\n\n  \n    \n    \n      Triggering effects, based on playing style\n      machine-learning\n      \n        Sep 14, 2019 • Sepehr Haghighi\n      Technology in collaboration with art could create creative solutions and achievements. In here we use machine learning in order to ease the work of player while playing an instrument.\n  \n\n\n  \n    \n    \n      Classification of string instruments\n      machine-learning\n      \n        Sep 14, 2019 • Jørgen Varpe\n      During a 2 week intensive workshop in the course Music and Machine Learning I had to develop a machine learning system for the field of music technlogy.\n  \n\n\n",
    "url": "/machine-learning/"
  },
  
  {
    "title": "Master's Theses",
    "author": "\n",
    "excerpt": "\n",
    "content": "The blog posts of this section relate to the master’s thesis course SMC4091 Master’s Thesis in Music, Communication and Technology (2023 onwards) and to the discontinued master’s thesis course SMC4090 Master’s Thesis in Music, Communication and Technology (2019 to 2022). The Master’s thesis is a research-based academic report based on individual research in the area of music, communication and technology. The topics of the theses represent a challenge within the topical areas of the SMC programme. Theses adheres to research ethical practices, and the project relates and contributes to the existing research literature. The project can include practical elements, but the thesis also include a literature review, theoretical discussion, methodological reflection and aesthetic evaluation.\n\n  \n    \n    \n      DDSP-FM\n      masters-thesis\n      \n        Feb 13, 2025 • Juan Alonso, Cumhur Erkut\n      Explore latent space for learning the parameters of a Differentiable FM Synthesizer\n  \n\n\n  \n    \n    \n      Strung Along: an extended violin for real-time accompaniment generation and timbral control\n      masters-thesis\n      \n        May 14, 2024 • Jack Hardwick\n      An extended violin for real-time chordal accompaniment generation and timbral control.\n  \n\n\n  \n    \n    \n      Deep Steps: A Generative AI Step Sequencer\n      masters-thesis\n      \n        May 14, 2024 • Alexander Wastnidge\n      A stand alone MIDI step sequencer application with a user-trainable generative neural network\n  \n\n\n  \n    \n    \n      Cyclic Patterns and Spatial Orientations in Artificial Impulsive ASMR Sounds\n      masters-thesis\n      \n        May 11, 2024 • Henrik Sveen\n      An exploratory study on the effects of cyclic patterns and spatial orientations in synthesized impulsive ASMR sounds.\n  \n\n\n  \n    \n    \n      CLAP Models and How To Make Them\n      masters-thesis\n      \n        May 8, 2024 • Oliver Getz\n      Is there anything CLAP models can't do?\n  \n\n\n  \n    \n    \n      The Shapeshifter\n      masters-thesis\n      \n        Dec 12, 2023 • Hugh Alexander von Arnim\n      Co-constructing the body with optical, marker-based motion capture in live dance performance\n  \n\n\n  \n    \n    \n      Using Features of Groove in Music Recommendation Systems\n      masters-thesis\n      \n        Dec 4, 2023 • Joseph Clemente\n      A study on analyzing groove in musical items and the effects of groove on musical recommendation.\n  \n\n\n  \n    \n    \n      Bandwidth to Band Together\n      masters-thesis\n      \n        Jun 15, 2023 • Jakob Høydal\n      A Study on Approaches for Remote Music Collaboration\n  \n\n\n  \n    \n    \n      Unsupervised Meta-Embedding for Bird Songs Clustering in Soundscape Recordings\n      masters-thesis\n      \n        Jun 12, 2023 • Joachim Poutaraud\n      A case study on nocturnal and crepuscular tropical bird songs.\n  \n\n\n  \n    \n    \n      RoboCapo: A Digitally Controlled Actuated Capo for Enhanced Guitar Playing \n      masters-thesis\n      \n        Jun 2, 2022 • Lindsay Charles\n      An augmentation device for the guitar, a robotic capo mechanism that explores the emergence of new complex and meaningful modes of interaction.\n  \n\n\n  \n    \n    \n      Unsupervised Classification of Sub-Genres of Electronic Music\n      masters-thesis\n      \n        Jun 2, 2022 • Abhishek Choubey\n      unsupervised machine learning classification and clustering of sub-genres of electronic music\n  \n\n\n  \n    \n    \n      Emotional Responses to Vibro-tactile Music\n      masters-thesis\n      \n        May 30, 2022 • Alena Clim\n      What happens when music is felt instead of heard? When music is just vibrations... can it still make people feel emotions?\n  \n\n\n  \n    \n    \n      When Hearts Beat as One – Cardiac Dynamics and Synchrony in String Quartet Performances\n      masters-thesis\n      \n        May 20, 2022 • Wenbo Yi\n      Investigating Cardiac Dynamics and Synchrony in String Quartet Performance\n  \n\n\n  \n    \n    \n      A Human-Machine Music Performance System based on Autonomous Agents\n      masters-thesis\n      \n        May 15, 2022 • Pedro Lucas\n      Let's make music with virtual fellows in mixed reality.\n  \n\n\n  \n    \n    \n      The Algorithmic Composition Explorer\n      masters-thesis\n      \n        May 15, 2022 • Stephen Gardener\n      For my masters thesis, I proposed a novel design for an interactive system that introduces people to algorithmic composition.\n  \n\n\n  \n    \n    \n      Visualizing Psyhophysiological Responses to Music Stimuli\n      masters-thesis\n      \n        May 15, 2022 • Rayam Pinto\n      Music can induce down-regulation of the nervous system, and in combination with technology, it has the potential to promote wellness.\n  \n\n\n  \n    \n    \n      Cross-modal correspondence: Different modes, common codes? Investigating musical engagement with an Ecological cognitive approach\n      masters-thesis\n      \n        May 12, 2022 • Joni Mok\n      Investigating musical engagement with an Ecological cognitive approach\n  \n\n\n  \n    \n    \n      Micro and Macro: Developing New Accessible Musicking Technologies\n      masters-thesis\n      \n        Dec 14, 2021 • Mari Lesteberg\n      Most of us will never be professional musicians, even though we have the musicality and the motor ability to achieve it...\n  \n\n\n  \n    \n    \n      ImSoTra\n      masters-thesis\n      \n        Jun 30, 2021 • Shreejay Shrestha\n      Footfall induced noise in buildings is traditionally assessed with Impact Sound Transmission (IST) measurements following diffuse field model of the receiving room which is not valid below Schr&ouml;der frequency neither it facilitates auralization. This master thesis aims to create a method to estimate low frequency (LF) sound pressure in the receiving room below Schr&ouml;der frequency based on modal sum theory in room acoustics followed by measurement of IST, impulse response of the receiving room and acceleration of the main floor at two fixed position in two vertically adjacent laboratories.\n  \n\n\n  \n    \n    \n      Designing Gesture-based Interactive Museum Exhibit\n      masters-thesis\n      \n        Jun 20, 2021 • Simon Sandvik\n      A short summary of my Masters Thesis on Gesture-based interaction for museum exhibits.\n  \n\n\n  \n    \n    \n      Theory Controller: A Silent IMS\n      masters-thesis\n      \n        Jun 3, 2021 • Thibault Jaccard\n      Interactive system to control music theory\n  \n\n\n  \n    \n    \n      Toward a Telepresence of Sound: Video Conferencing in Spatial Audio\n      masters-thesis\n      \n        May 18, 2021 • Jackson Goode\n      Teleconferencing in spatial audio with the help of Jitsi Meet and Web Audio\n  \n\n\n  \n    \n    \n      The Portable Portal: An Ecological Approach to Technology-Enhanced Learning in Bangladesh\n      masters-thesis\n      \n        May 15, 2021 • Paul Koenig\n      Working toward a cogent ecological framework for technological-aid development, or: We are our technologies, they are Us\n  \n\n\n  \n    \n    \n      Reinforcement learning for use in cross-adaptive audio processing\n      masters-thesis\n      \n        May 15, 2021 • Ulrik Halmøy\n      This thesis is a study of reinforcement learning as a possible method for finding mappings in cross-adaptive audio effects\n  \n\n\n  \n    \n    \n      Exploring Hardanger Fiddle Performance Patterns Through Interactive Computational Tools\n      masters-thesis\n      \n        May 14, 2021 • Aleksander Tidemann\n      This thesis presents the development and evaluation of two software applications that integrate contemporary research perspectives on the complex rhythmical structuring of Hardanger fiddle performances.\n  \n\n\n  \n    \n    \n      Interconnecting Modular Synthesizers Using the Web\n      masters-thesis\n      \n        Dec 15, 2020 • Eigil Aandahl\n       In my thesis project, I present an approach to interconnecting modular synthesizer systems using a prototype multi-channel audio network solution made with Max/MSP. The research explores emergent affordances of such a system in the context of telematics and network music performance. At the bottom of this post there is a video demonstrating the prototype in action.  \n  \n\n\n  \n    \n    \n      The design and evaluation of the Gyroshuffle\n      masters-thesis\n      \n        Jul 31, 2020 • Sam Roman\n      The aim of this study is to develop and evaluate the Gyroshuffle, a real time rhythmic instrument played with body movement. It is theorised that moving to the rhythm, whilst controlling the rhythm is possible with the Gyroshuffle, blurring the lines between dancing and producing music in real time.\n  \n\n\n  \n    \n    \n      Gatekeepers by design? Gender HCI for Audio and Music Hardware\n      masters-thesis\n      \n        Jul 4, 2020 • Karolina Jawad\n      This dissertation looks into investigating the design of hardware for audio and music which is commonly associated with the term ‘music technology’ under the aspect of Gender-HCI, studies on science and technology as well as design research.\n  \n\n\n  \n    \n    \n      Harmonic interaction for monophonic instruments through musical phrase to scale recognition\n      masters-thesis\n      \n        Jun 23, 2020 • Guy Sion\n      Introducing a novel approach for the augmentation of acoustic instrument by providing musicians playing monophonic instruments the ability produce and control the harmonic outcome of their performance. This approach is integrated in an interactive music system that tracks the notes played by the instrument, analyzes an improvised melodic phrase, and identifies the harmonic environment in which the phrase is played. This information is then used as the input of a sound generating module which generate harmonic textures in accordance with the identified scale. \n  \n\n\n  \n    \n    \n      Motivato\n      masters-thesis\n      \n        Jun 15, 2020 • Elias Andersen\n      Motivato: A standalone music selection system for seamless technology-mediated audience participation.\n  \n\n\n  \n    \n    \n      Multimedia Slideshow Maker\n      masters-thesis\n      \n        Jun 13, 2020 • Jørgen Varpe\n      During my master's thesis, I have designed and developed a tech platform where a mobile application creates slideshows from multimedia content uploaded in a web application titled “Multimedia Slideshow Maker” (MSM). The project is carried out for an external partner, Alight AS, for a project called Alight. Alight is a mobile tech platform aiding caregivers in sending personalised video sessions to patients with dementia. This thesis aims to determine to what degree MSM can be used independently by a caregiver, without instructions from others or prior experience in video editing.  \n  \n\n\n  \n    \n    \n      The Notion of Dialogue in the Interactive Dance\n      masters-thesis\n      \n        Jun 10, 2020 • Sepehr Haghighi\n      The constituent elements of interactive dance are human and computer, which in a human-computer interaction, create a feedback loop, and present the work of art. Considering that matter, each of the opponents in this interaction has their part and space and there is an aesthetic relationship ongoing, defining the quality and amount of each opponent's part and space. In this thesis, this ongoing matter is referred to as the notion of dialogue. To create this sense, the key element that will be discussed is surprise. In order to do that, following a certain design strategy, a practical system will be designed and executed to fortitude the logical argument that is presented in this research. In that performance, by the creative use of the body, space, time, popular art forms (i.e., Hip-Hop music and dance), and with the focus on the subject of sea-level rise, the research argument will be put in practice and further on evaluated. After the evaluation process, in conclusion - despite its limitation - it may be concluded that the use of surprise, will allow the computer to have an active role and possess a significant part in the interaction and convey a sense of dialogue in it. \n  \n\n\n  \n    \n    \n      Sonification of Standstill Recordings \n      masters-thesis\n      \n        May 13, 2020 • Ashane Silva\n      The goal of this thesis was to develop and experiment with a set of sonification tools to explore participant data from standstill competitions. Using data from the 2012 Norwegian Championship of Standstill, three sonification models were developed using the Max/MSP programming environment. The first section of the thesis introduces sonification as a method for data exploration and discusses different sonification strategies. Momentary Displacement of the position was derived from the position data and parameter mapping methods were used to map the data features with sound parameters. The displacement of position in the XY plane or the position changes along the Z-Axis can be mapped either to white-noise or to a sine tone. The data variables control the amplitude and a filter cut-off frequency of the white noise or the amplitude and frequency of the sine tone. Moreover, using sound spatialization together with sonification was explored by mapping position coordinates to spatial parameters of a sine tone. A “falling” effect of the standing posture was identified through the sonification. Also audible were the participants’ breathing patterns and postural adjustments. All in all, the implemented sonification methods can be effectively used to get an overview of the standstill dataset.\n  \n\n\n",
    "url": "/masters-thesis/"
  },
  
  {
    "title": "Motion Capture",
    "author": "\n",
    "excerpt": "\n",
    "content": "The blog posts of this section relate to the course SMC4053 Motion Capture (from 2022) and to the discontinued course SMC4043 Music-related Motion Tracking (2019 to 2021). The aim of these courses is to provide knowledge and skills in recording, visualising, and analysing human body motion. This includes learning about human anatomy and biomechanics and getting hands-on experience with setting up, calibrating, tracking, and recording with different types of motion capture systems.\n\n  \n    \n    \n      KineMapper\n      motion-capture\n      \n        May 5, 2024 • Tom Oldfield\n      A Max for Live device that maps motion data from a smartphone to controls in Ableton Live.\n  \n\n\n  \n    \n    \n      A Body Instrument: Exploring the Intersection of Voice and Motion\n      motion-capture\n      \n        May 9, 2023 • Emin Memis\n      Manipulate your voice with your body\n  \n\n\n  \n    \n    \n      Generative Music with IMU data\n      motion-capture\n      \n        May 9, 2023 • Alexander Wastnidge\n      Eight routes to meta-control\n  \n\n\n  \n    \n    \n      Simple yet unique way of playing music\n      motion-capture\n      \n        May 9, 2023 • Nino Jakeli\n      Gestures can be more intuitive to play around with\n  \n\n\n  \n    \n    \n      Xyborg: Wearable Control Interface and Motion Capture System for Manipulating Sound\n      motion-capture\n      \n        May 8, 2023 • Kristian Eicke\n      Witness my transition from human to machine - with piezo discs\n  \n\n\n  \n    \n    \n      Motion Controlled Sampler in Ableton\n      motion-capture\n      \n        May 8, 2023 • Fabian Stordalen\n      A fun and creative way of sampling\n  \n\n\n  \n    \n    \n      Developing Techniques for Air Drumming Using Video Capture and Accelerometers\n      motion-capture\n      \n        May 20, 2022 • Joseph Clemente\n      Creating MIDI scores using only data from air drumming\n  \n\n\n  \n    \n    \n      Reconfigurations: Reconfiguring the Captured Body in Dance\n      motion-capture\n      \n        May 20, 2022 • Hugh Alexander von Arnim\n      Building cooperative visual, kinaesthetic, and sonic bodies\n  \n\n\n  \n    \n    \n      Myo My – That keyboard sure tastes good with some ZOIA on top\n      motion-capture\n      \n        May 20, 2022 • Kristian Wentzel\n      Extending the keyboard through gestures and modular synthesis.\n  \n\n\n  \n    \n    \n      Playing music standing vs. seated; whats the difference?\n      motion-capture\n      \n        May 19, 2022 • Jakob Høydal\n      A study of saxophonist in seated vs standing position\n  \n\n\n  \n    \n    \n      What is a gesture?\n      motion-capture\n      \n        May 19, 2022 • Sofía González\n      There are many takes on gesticulation and its meanings, however, I wanted to take the time to delimit what a gesture is, possible categories and gesturing patters.\n  \n\n\n  \n    \n    \n      Walking in Seasons\n      motion-capture\n      \n        Apr 21, 2021 • Abhishek Choubey, Lindsay Charles, Joel Vide Hynsjö\n      Sonification of motion\n  \n\n\n  \n    \n    \n      Exploring the influence of expressive body movement on audio parameters of piano performances\n      motion-capture\n      \n        Apr 18, 2021 • Wenbo Yi\n      How expressive body movement influence music?\n  \n\n\n  \n    \n    \n      Motion (and emotion) in recording\n      motion-capture\n      \n        Apr 18, 2021 • Anders Lidal\n      The first time I went to a recording studio in the early nineties, my eagerness to (music)-world domination—as well as my fascination for the possibility to put my beautiful playing to a magnetic tape—totally over-shadowed that the result sounded crappy, at least for a while.\n  \n\n\n  \n    \n    \n      Shim-Sham Motion Capture\n      motion-capture\n      \n        Apr 18, 2021 • Alena Clim\n      We've learned about motion capture in a research environment. But what about motion capture in the entertainment field? In this project I attempted to make an animation in Blender based on motion captured in the lab using the Optitrack system. Beside this, I also analysed three takes of a Shim Sham dance. For more details and some sneak peaks read this blog post.\n  \n\n\n  \n    \n    \n      Kodaly EarTrainer-App\n      motion-capture\n      \n        Apr 18, 2021 • Thomas Anda\n      App for training your ears based on old Hungarian methodolgy\n  \n\n\n  \n    \n    \n      &#39;Air&#39; Instruments Based on Real-Time Motion Tracking\n      motion-capture\n      \n        Apr 17, 2021 • Pedro Lucas\n      Let's make music with movements in the air.\n  \n\n\n  \n    \n    \n      An air guitar experiment with OpenCV\n      motion-capture\n      \n        Apr 17, 2021 • Stephen Gardener\n      Get to know OpenCV by building an air guitar player. Shaggy perm not required.\n  \n\n\n  \n    \n    \n      Posture Guard\n      motion-capture\n      \n        Apr 16, 2021 • Henrik Sveen\n      Back pains, neck pains, shoulder pains - what do they all have in common? They are caused by bad posture while working on a laptop. So I made a program that makes the laptop help out maintaining a good posture while working.\n  \n\n\n  \n    \n    \n      A Brief Workshop on Motion Tracking\n      motion-capture\n      \n        Sep 25, 2020 • Jackson Goode\n      Since our spring semester of motion tracking was a purely digital experience, a few of us got to together to quickly test out the OptiTrack system within the Portal.\n  \n\n\n  \n    \n    \n      a t-SNE adventure\n      motion-capture\n      \n        May 20, 2020 • Ulrik Halmøy\n      A system for interactive exploration of sound clusters with phone sensors\n  \n\n\n  \n    \n    \n      Breathing through Max\n      motion-capture\n      \n        May 18, 2020 • Jackson Goode\n      For the COVID-19 version of motion-capture, I developed a system to track your rate of breath and sonify it through Max. It emphasized the tenants of biofeedback and hopes to serve as a responsive system for stress relief.\n  \n\n\n  \n    \n    \n      Sound Painting\n      motion-capture\n      \n        May 14, 2020 • Aleksander Tidemann\n      An application that tracks, visualizes and sonifies the motion of colors.\n  \n\n\n  \n    \n    \n      How music related motion tracking can sound\n      motion-capture\n      \n        Apr 24, 2019 • Karolina Jawad, Jørgen Varpe, Eirik Dahl\n      During the course 'Music related Motion Tracking' there were several approaches among the students to realize their ideas. The Opti-Track system, new to all of us consists of infrared-cameras, markers and a software with calibration tools. We were exploring the functions from scratch during the first week when hosting the 'Nordic-stand-still-championship' on both campus.\n  \n\n\n  \n    \n    \n      MoCap Recap - Two weeks recap of a Motion Tracking workshop\n      motion-capture\n      \n        Apr 1, 2019 • Guy Sion, Sam Roman, Elias Anderson\n      During weeks 10-11 we attended the Music related motion tracking course (SMC4043) as part of the SMC program. The week started with the installation of the OptiTrack system in Oslo, placement of cameras, connecting wires to hubs and software installation and setup. we got familiar with the Motive:Body software and was able to run calibrations, set and label markers, record motion data, export it in a correct way and experiment with sonifying the results with both recorded and streamed motion capture data.\n  \n\n\n",
    "url": "/motion-capture/"
  },
  
  {
    "title": "Networked Music",
    "author": "\n",
    "excerpt": "\n",
    "content": "The blog posts of this section relate to the courses SMC4024 Networked Music Performance Tehcnologies 1 and SMC4025 Networked Music Performance Tehcnologies 2 (from 2021), and to the discontinued courses SMC4021 Physical-Virtual Communication and Music 1, SMC4022 Physical-Virtual Communication and Music 2, and SMC4023 Physical-Virtual Communication and Music 3 (2018 to 2021).\n\nThese courses aim to provide theoretical and practical knowledge on advanced audio-visual systems for online synchronous musical collaboration and hybrid communication settings. The students also learn to operate, maintain and experiment with the physical-virtual Portal, a laboratory for network-based musical communication.\n\n  \n    \n    \n      Digital Signal Processing Basics\n      networked-music\n      \n        Mar 18, 2024 • Karenina Juarez\n      Virtually any song you can listen to on the radio has examples of Digital Signal Processing.\n  \n\n\n  \n    \n    \n      Introduction to Open Sound Control (OSC)\n      networked-music\n      \n        Mar 17, 2024 • Tom Oldfield\n      This post contains a brief overview of OSC and a tutorial on how to make a connection and send data between devices.\n  \n\n\n  \n    \n    \n      Formatting WebPD Projects: An Introduction to WebPD, HTML and CSS Styling\n      networked-music\n      \n        Mar 15, 2024 • Juliana Bigelow\n      Styling your WebPD application can lead to greater user experience and accessibility.\n  \n\n\n  \n    \n    \n      We Are Sitting In Rooms\n      networked-music\n      \n        Dec 20, 2023 • Karenina Juarez\n      Recreating the most famous piece by composer Alvin Lucier as a network music performance\n  \n\n\n  \n    \n    \n      Documenting Networked Music Performances: Tips, Tricks and Best Practices\n      networked-music\n      \n        Dec 11, 2023 • Juliana Bigelow\n      Effectively documenting networked music performances can lead to better experiences for physical and digital audiences, and your academic explorations.\n  \n\n\n  \n    \n    \n      Sync your synths and jam over a network using Sonobus\n      networked-music\n      \n        Nov 30, 2023 • Tom Oldfield\n      A quick start guide to jamming over a network. Designed for instruments which can synchronize using an analog clock pulse.\n  \n\n\n  \n    \n    \n      Music Between Salen and the World\n      networked-music\n      \n        May 18, 2023 • Jack Hardwick, Alexander Wastnidge, Masoud Niknafs, Emin Memis, Nino Jakeli, Henrik Sveen, Kristian Eicke, Fabian Stordalen, Aysima Karcaaltincaba\n      We played in a global NMP concert. Check out our experiences.\n  \n\n\n  \n    \n    \n      Basics of Computer Networks in NMPs\n      networked-music\n      \n        Apr 23, 2023 • Emin Memis\n      Crash course on computer network used in Network Music Performances.\n  \n\n\n  \n    \n    \n      Audio Codecs and the AI Revolution\n      networked-music\n      \n        Apr 23, 2023 • Jack Hardwick\n      I dove headfirst into the world of machine learning-enhanced audio codecs. Here's what I found out.\n  \n\n\n  \n    \n    \n      NMP kit Tutorial\n      networked-music\n      \n        Apr 23, 2023 • Henrik Sveen\n      A practical tutorial on the NMP kits.\n  \n\n\n  \n    \n    \n      Spatial Audio with Max-Msp and Sonobus\n      networked-music\n      \n        Apr 23, 2023 • Masoud Niknafs\n      This post's video tutorial aims to introduce readers to the many uses for which the Sonobus can be implemented as a VST in Max-Msp.\n  \n\n\n  \n    \n    \n      SonoBus setup - Standalone App and Reaper Plugin\n      networked-music\n      \n        Apr 23, 2023 • Kristian Eicke\n      Check out my tutorial on how to use SonoBus for your networked performance.\n  \n\n\n  \n    \n    \n      Integrating JackTrip and Sonobus in a DAW\n      networked-music\n      \n        Apr 23, 2023 • Fabian Stordalen\n      How you can integrate both JackTrip and Sonobus into your DAW\n  \n\n\n  \n    \n    \n      Pair Programming Over Network\n      networked-music\n      \n        Apr 21, 2023 • Aysima Karcaaltincaba\n      Pair Programming from different locations, is it possible?\n  \n\n\n  \n    \n    \n      Audio Engineering for NMPs: Part 2\n      networked-music\n      \n        Apr 20, 2023 • Alexander Wastnidge\n      A deeper dive into mixer work for NMPs\n  \n\n\n  \n    \n    \n      Playing Jazz Over Network\n      networked-music\n      \n        Mar 12, 2023 • Aysima Karcaaltincaba, Fabian Stordalen, Henrik Sveen, Kristian Eicke, Nino Jakeli\n      A live performance by Edvard Munch High School students with collaborative networked music technology.\n  \n\n\n  \n    \n    \n      Jazz Over the Network at 184,000km/h\n      networked-music\n      \n        Mar 12, 2023 • Jack Hardwick, Alexander Wastnidge, Masoud Niknafs, Emin Memis\n      We worked with local high school students to put together a jazz concert over the LOLA network. Here's a retrospective from Team RITMO.\n  \n\n\n  \n    \n    \n      Testing Two Approaches to Performing with Latency\n      networked-music\n      \n        Feb 16, 2023 • Aysima Karcaaltincaba, Emin Memis, Jack Hardwick, Kristian Eicke\n      We tested two approaches to dealing with latency in network music. Read all about it!\n  \n\n\n  \n    \n    \n      Designing DFA and LAA Network Music Performances\n      networked-music\n      \n        Feb 16, 2023 • Alexander Wastnidge, Fabian Stordalen, Henrik Sveen, Masoud Niknafs, Nino Jakeli\n      Music Performances in High Latency\n  \n\n\n  \n    \n    \n      JackTrip Vs Sonobus - Review and Comparison\n      networked-music\n      \n        Dec 6, 2022 • Nino Jakeli\n      Low-latency online music performance platforms\n  \n\n\n  \n    \n    \n      The SMC Audio Vocabulary\n      networked-music\n      \n        Dec 6, 2022 • Kristian Eicke\n      Click this post if you need some explanation on jargon, mainly related to the Portal and the NMP kits.\n  \n\n\n  \n    \n    \n      Making A Telematic Concert Happen - A Quick Technical Look\n      networked-music\n      \n        Dec 4, 2022 • Emin Memis, Nino Jakeli\n      Background of A Telematic Experience\n  \n\n\n  \n    \n    \n      Live Streaming A Telematic Concert\n      networked-music\n      \n        Dec 4, 2022 • Emin Memis\n      Whys, How-tos and Life-Saving Tips on Telematic Concert Streaming.\n  \n\n\n  \n    \n    \n      Blackhole, Open Source MacOS Virtual Audio Device Solution for Telematic Performance\n      networked-music\n      \n        Nov 29, 2022 • Masoud Niknafs\n      MacOS guide to Blackhole\n  \n\n\n  \n    \n    \n      The Optic-Fibre Ensemble\n      networked-music\n      \n        Nov 28, 2022 • Fabian Stordalen, Masoud Niknafs\n      Folk inspired soundscapes + No input mixing=True\n  \n\n\n  \n    \n    \n      Bringing the (Optic) Fibre Ensemble to Life - Behind the Scenes of a Telematic Music Performance\n      networked-music\n      \n        Nov 28, 2022 • Kristian Eicke, Jack Hardwick\n      What does it take to put on a telematic music performance? Cable spaghetti of course!\n  \n\n\n  \n    \n    \n      Telematic Concert between Salen and Portal - Performers’ Reflections\n      networked-music\n      \n        Nov 28, 2022 • Alexander Wastnidge, Aysima Karcaaltincaba, Iosif Aragiannis\n      Reflections on semester's first Telematic Concert\n  \n\n\n  \n    \n    \n      Music Mode for Online Meetings\n      networked-music\n      \n        Nov 28, 2022 • Aysima Karcaaltincaba\n      It is possible to use popular meeting products for music!\n  \n\n\n  \n    \n    \n      Performing &amp; Recording with JackTrip\n      networked-music\n      \n        Nov 27, 2022 • Jack Hardwick\n      Performing and recording network music at home? Thanks to JackTrip, now you can do it too.\n  \n\n\n  \n    \n    \n      Audio Engineering for Network Music Performances\n      networked-music\n      \n        Nov 24, 2022 • Alexander Wastnidge\n      How much more difficult could it POSSIBLY be?\n  \n\n\n  \n    \n    \n      Rehearsing Music Over the Network\n      networked-music\n      \n        Nov 21, 2022 • Fabian Stordalen\n      My experiences with rehearsing music telematically and some tips.\n  \n\n\n  \n    \n    \n      The impact and importance of network-based musical collaboration (in the post-covid world)\n      networked-music\n      \n        Oct 3, 2022 • Iosif Aragiannis\n      The Covid-19 pandemic offered a unique opportunity (and necessity) to focus on the creative usage (and further development) of the technological tools used for network-based musical collaboration.\n  \n\n\n  \n    \n    \n      Norway&#39;s First 5G Networked Music Performance\n      networked-music\n      \n        Jul 5, 2022 • Joachim Poutaraud, Kristian Wentzel, Leigh Murray, Lindsay Charles\n      We played Norway's first 5G networked music performance in collaboration with Telenor Research.\n  \n\n\n  \n    \n    \n      One Last Hoorah: A Telematic Concert in the Science Library\n      networked-music\n      \n        May 27, 2022 • Jakob Høydal, Joachim Poutaraud, Joseph Clemente, Kristian Wentzel\n      In which we go over the most ambitious telematic concert of our SMC careers.\n  \n\n\n  \n    \n    \n      Spring Telematic Concert 2022: Portal Perspectives\n      networked-music\n      \n        May 26, 2022 • Oliver Getz, Sofía González, Arvid Falch, Hugh Alexander von Arnim\n      Guitar and MoCap in the Portal + behind the scenes documentary!\n  \n\n\n  \n    \n    \n      Setting up a controlled environment\n      networked-music\n      \n        Apr 24, 2022 • Joseph Clemente, Kristian Wentzel\n      Taking advantage of light-weight control messages to do Networked Music Performances\n  \n\n\n  \n    \n    \n      Ambisonics: Under the Hood\n      networked-music\n      \n        Apr 24, 2022 • Arvid Falch, Hugh Alexander von Arnim\n      What happens when we encode/decode Ambisonics\n  \n\n\n  \n    \n    \n      How to Set Up Hybrid Learning Environments\n      networked-music\n      \n        Apr 24, 2022 • Sofía González, Oliver Getz\n      Learn how to set up a hybrid learning environment, ranging from simple to complex, serving as a starting point to help your classroom catch up to the digital age.\n  \n\n\n  \n    \n    \n      Managing Network Performance\n      networked-music\n      \n        Apr 22, 2022 • Jakob Høydal, Joachim Poutaraud\n      Managing Network Performance using Python\n  \n\n\n  \n    \n    \n      5G Networked Music Performances - Will It Work?\n      networked-music\n      \n        Apr 11, 2022 • Aleksander Tidemann, Stefano Fasciani\n      In collaboration with Telenor Research, we explored the prospects of doing networked music performances over 5G. Here are the preliminary results.\n  \n\n\n  \n    \n    \n      Mastering Latency\n      networked-music\n      \n        Feb 21, 2022 • Hugh Alexander von Arnim, Kristian Wentzel, Sofía González, Joachim Poutaraud\n      Testing two techniques to work with latency when playing music telematically\n  \n\n\n  \n    \n    \n      Latency: To Accept or to Delay Feedback?\n      networked-music\n      \n        Feb 21, 2022 • Arvid Falch, Joseph Clemente, Jakob Høydal, Oliver Getz\n      We tested two of the solutions—the Delayed Feedback Approach (DFA) and the Latency Accepting Approach (LAA)—so you don’t have to!\n  \n\n\n  \n    \n    \n      The Granjular Christmas Concert (Portal View)\n      networked-music\n      \n        Dec 10, 2021 • Joseph Clemente, Kristian Wentzel, Sofía González, Arvid Falch\n      A report on our telematic performance in the Portal.\n  \n\n\n  \n    \n    \n      The Telematic Experience: Space in Sound in Space\n      networked-music\n      \n        Dec 10, 2021 • Hugh Alexander von Arnim, Jakob Høydal, Joachim Poutaraud, Oliver Getz\n      A report on our telematic experience in Salen\n  \n\n\n  \n    \n    \n      Setting Levels in Virtual Communication\n      networked-music\n      \n        Nov 26, 2021 • Jarle Steinhovden\n      Inspired by how virtual communication has been adopted and integrated in daily life around the globe, this post looks at how simple control messages might help prevent acoustic feedback.\n  \n\n\n  \n    \n    \n      Audio-video sync\n      networked-music\n      \n        Nov 15, 2021 • Anders Lidal\n      Due to the fact that sound travels a lot slower through air than the light, our brain is used to seeing before hearing.\n  \n\n\n  \n    \n    \n      Video latency: definition, key concepts, and examples\n      networked-music\n      \n        Nov 15, 2021 • Alena Clim\n      This blogpost is made after the video lecture on the same topic and it includes a definition of video latency and other related key concepts, as well as concrete examples from the SMC portals.\n  \n\n\n  \n    \n    \n      Latency as an opportunity to embrace\n      networked-music\n      \n        Nov 15, 2021 • Wenbo Yi\n      How can we turn unavoidable latency into an integral part of telematic performance?\n  \n\n\n  \n    \n    \n       Audio Latency in the Telematic Setting\n      networked-music\n      \n        Nov 15, 2021 • Abhishek Choubey\n      Latency and its fundamentals in the telematic performance context. \n  \n\n\n  \n    \n    \n      Room acoustics: what are room modes and how do they influence the physical space?\n      networked-music\n      \n        Nov 15, 2021 • Lindsay Charles\n      This blog post explains what room modes are, how they affect the physical space and what can be done about it. It was made together with a video lecture.\n  \n\n\n  \n    \n    \n      Audio-Video Synchronization for Streaming\n      networked-music\n      \n        Nov 15, 2021 • Pedro Lucas\n      This approach considers a streaming solution from multiple sources and different locations (Salen, Video Room, Portal)\n  \n\n\n  \n    \n    \n      Touchpoint that can potentially improve the audio latency in a communication system\n      networked-music\n      \n        Nov 14, 2021 • Joni Mok\n      Quick tips for future SMC-ers or external partners who will be using the SMC Portal for the first time: this article gives you practical information to start with improving audio latency.\n  \n\n\n  \n    \n    \n      A short post about feedback\n      networked-music\n      \n        Nov 14, 2021 • Stephen Gardener\n      Feeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeedback\n  \n\n\n  \n    \n    \n      Telematic Conducting: Modelling real-world orchestral tendencies via video latency\n      networked-music\n      \n        Nov 14, 2021 • Willie Mandeville\n      A conductor in one portal. An orchestra in another. What could go wrong?\n  \n\n\n  \n    \n    \n      EarSketch (Or How to Get More Python in Your Life)\n      networked-music\n      \n        Nov 5, 2021 • Joachim Poutaraud, Hugh Alexander von Arnim\n      A review of the asynchronous music production software 'EarSketch'\n  \n\n\n  \n    \n    \n      SkyTracks: What’s the Use?\n      networked-music\n      \n        Nov 3, 2021 • Arvid Falch, Joseph Clemente\n      A review and critique of the online DAW SkyTracks for asynchronous collaboration\n  \n\n\n  \n    \n    \n      Satellite Sessions - Connecting DAWs\n      networked-music\n      \n        Nov 3, 2021 • Jakob Høydal, Kristian Wentzel\n      A review of Satellite Sessions, a plugin that connects digital audio workstations and creators.\n  \n\n\n  \n    \n    \n      Soundation Review: What to Expect\n      networked-music\n      \n        Nov 1, 2021 • Sofía González, Oliver Getz\n      A concise review on the collaborative online DAW: Soundation.\n  \n\n\n  \n    \n    \n      A Brief History of Improvisation Through Network Systems\n      networked-music\n      \n        Sep 26, 2021 • Kristian Wentzel\n      A glimpse into the evolution of online improvisation and shared sonic environments.\n  \n\n\n  \n    \n    \n      Embodiment and Awareness in Telematic Music and Virtual Reality\n      networked-music\n      \n        Sep 23, 2021 • Jakob Høydal\n      A discisson about what Embodiment and Awareness means, and how its beeing used in Telematic Music and VR.\n  \n\n\n  \n    \n    \n      Telematic Reality Check: An Evaluation of Design Principles for Telematic Music Applications in VR Environments\n      networked-music\n      \n        Sep 22, 2021 • Oliver Getz\n      7 steps for better virtual reality music applications!\n  \n\n\n  \n    \n    \n      Applying Actor-Network Methodology to Telematic Network Topologies\n      networked-music\n      \n        Sep 22, 2021 • Hugh Alexander von Arnim\n      An inquiry into the application of an actor-network theory methodology to Gil Weinberg's telematic musical network topologies.\n  \n\n\n  \n    \n    \n      Telematic performance and communication: tools to fight loneliness towards a future of connection.\n      networked-music\n      \n        Sep 22, 2021 • Sofía González\n      With telematic interaction on the rise during a global pandemic, we should explore telematic performances to help prevent loneliness and feelings of isolation through art.\n  \n\n\n  \n    \n    \n      Approaches Toward Algorithmic Interdependence in Musical Performance\n      networked-music\n      \n        Sep 22, 2021 • Joseph Clemente\n      Is it possible to program interdependent algorithms to perform with each other?\n  \n\n\n  \n    \n    \n      Fight latency with latency\n      networked-music\n      \n        Sep 22, 2021 • Arvid Falch\n      Alternative design approaches for telematic music systems.\n  \n\n\n  \n    \n    \n      Internet time delay: a new musical language for a new time basis\n      networked-music\n      \n        Sep 20, 2021 • Joachim Poutaraud\n      Logistical considerations for large-scale telematic performances involving geographically displaced contributors still remain strongly present. Therefore, if networked performers are still to the vagaries of speed and bandwidth of multiple networks and if latency  problems remain a significant issue for audio-visual streaming of live Network Music Performance (NMP), one can rather reflect on trying  to find a new musical language for a new time basis.\n  \n\n\n  \n    \n    \n      Concert preparations: exploring the Portal\n      networked-music\n      \n        May 5, 2021 • Alena Clim, Leigh Murray, Abhishek Choubey\n      What we learned about the Portal and telematic performances in general while preparing our musical pieces for the end of semester concert. Details about our instrumentation and effects.\n  \n\n\n  \n    \n    \n      Preparing NTNU portal for the spring 2021 Concert.\n      networked-music\n      \n        May 5, 2021 • Abhishek Choubey\n      How the NTNU Portal was setup for the spring concert.\n  \n\n\n  \n    \n    \n      End of semester reflections for the Portal experience\n      networked-music\n      \n        May 4, 2021 • Wenbo Yi, Lindsay Charles, Stephen Gardener\n      The first physical portal experience during the pandemic\n  \n\n\n  \n    \n    \n      The collision of Jazz and Classical\n      networked-music\n      \n        May 4, 2021 • Wenbo Yi, Lindsay Charles, Stephen Gardener\n      The rehearsal experience for our telematic performance.\n  \n\n\n  \n    \n    \n      The SMC Portal through the eyes of a noob\n      networked-music\n      \n        May 4, 2021 • Alena Clim\n      The experiences I had in the SMC Portal during the spring 2021 semester and my evolution from a complete beginner to an almost-average user of a technical and awesome room like the SMC Portal.\n  \n\n\n  \n    \n    \n      End of Semester Concert - Spring 2021: Audio Setup\n      networked-music\n      \n        May 4, 2021 • Anders Lidal, Pedro Lucas\n      As part of the audio team for the end-of-semester telematic concert, Pedro and Anders spent several hours in the portal, exploring different ways to organize audio routing. They also found time to experiment with effect loops. Check out the nice musical collaboration between two different musical cultures.\n  \n\n\n  \n    \n    \n      Cross the Streams\n      networked-music\n      \n        May 3, 2021 • Leigh Murray\n      When performing in two locations we need to cross the streams\n  \n\n\n  \n    \n    \n      Dispatch from the Portal: Dueling EQs\n      networked-music\n      \n        May 3, 2021 • Henrik Sveen, Anders Lidal, Pedro Lucas, Willie Mandeville\n      How do I sound? Good? What does good mean? How do I sound? Sigh...\n  \n\n\n  \n    \n    \n      Spring Concert 2021: Team B&#39;s Reflections\n      networked-music\n      \n        May 1, 2021 • Henrik Sveen, Anders Lidal, Pedro Lucas, Willie Mandeville\n      We'll do it live. Team B gets its groove Bach.\n  \n\n\n  \n    \n    \n       Tele-A-Jammin: the recipe\n      networked-music\n      \n        Mar 5, 2021 • Abhishek Choubey, Alena Clim, Leigh Murray\n      Our portal experience mapped on a delicious Jam recipe\n  \n\n\n  \n    \n    \n      First Weeks Of Portaling With Team B\n      networked-music\n      \n        Mar 3, 2021 • Henrik Sveen, Anders Lidal, Pedro Lucas, Willie Mandeville\n      Starting to figure out the Portal and what we did/found out in the first weeks using it. How to play and some feedback fixing.\n  \n\n\n  \n    \n    \n      Liebesgruß or we can put that &#39;Liebe&#39; aside\n      networked-music\n      \n        Feb 28, 2021 • Joni Mok, Wenbo Yi, Lindsay Charles, Stephen Gardener\n      It's simply a gruß from Team C with our first telematic setup.\n  \n\n\n  \n    \n    \n      Twinkle Twinkle (sad) Little Mars\n      networked-music\n      \n        Feb 28, 2021 • Henrik Sveen, Anders Lidal, Pedro Lucas, Willie Mandeville\n      Finally the B-boys found some hours one evening to spend in the portal, Willie up north, and Pedro, Henrik and Anders down south. This was the first day on their Mission to Mars. Enter our B-oyager, and join us.\n  \n\n\n  \n    \n    \n      Zoom here &amp; Zoom there: Ambisonics\n      networked-music\n      \n        Nov 14, 2020 • Alena Clim, Abhishek Choubey, Leigh Murray\n       A more natural way of communicating online, wherein it feels like all the members are in the same room talking.\n  \n\n\n  \n    \n    \n      Ambisonic as a ‘mental’ management tool in Zoom?\n      networked-music\n      \n        Nov 14, 2020 • Joni Mok, Stephen Gardener, Wenbo Yi, Lindsay Charles\n      An immersive audio as a tool to keep our state of mind peaceful.\n  \n\n\n  \n    \n    \n      Zoom + Ambisonics\n      networked-music\n      \n        Nov 10, 2020 • Pedro Lucas, Willie Mandeville, Henrik Sveen, Anders Lidal\n      Talking together in an online room is an interesting topic, as the mono summed communication in regular Zoom can be tiring when meeting for several hours a day. Could ambisonics in digital communication be the solution we're all waiting for?\n  \n\n\n  \n    \n    \n      Musicking with JackTrip, JamKazam, and SoundJack - Presentations\n      networked-music\n      \n        Oct 8, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      The class of 2021 presented on JackTrip, SoundJack and JamKazam and their networked musicking potential. Presentations are included in this blog post as pdfs.\n  \n\n\n  \n    \n    \n      The Joys of Jitsi\n      networked-music\n      \n        Sep 28, 2020 • Joni Mok, Wenbo Yi, Lindsay Charles, Stephen Gardener\n      Jitsi is a venerable open source chat and video-conferencing app thats been around since 2003. It's multi-platform, and runs pretty much everywhere. We were given the task of testing the desktop apps on MacOS and Windows, and the mobile apps on Android and iOS.\n  \n\n\n  \n    \n    \n      Zoom - High Fidelity and Stereo\n      networked-music\n      \n        Sep 28, 2020 • Pedro Lucas, Willie Mandeville, Henrik Sveen, Anders Lidal\n      For the members of Team B, Zoom meetings are an everyday occurrence. Like most people during the covid era, we spend much of our professional time communicating from the comfort of our living rooms. These days, using Zoom feels akin to wearing clothes; we’re almost always doing it (sometimes even at the same time).\n  \n\n\n  \n    \n    \n      Pedál to the Metal\n      networked-music\n      \n        Sep 26, 2020 • Alena Clim, Leigh Murray, Abhishek Choubey\n      Another week, another audio-streaming platform to test. Here is Team A's impressions of Pedál. According to their website, Pedál is a cross platform service to stream audio, speak, share screen, and record high quality audio together and the simplest way to make music together online.\n  \n\n\n  \n    \n    \n      Jamulus test session.\n      networked-music\n      \n        Sep 7, 2020 • Stephen Gardener, Wenbo Yi, Joni Mok, Lindsay Charles\n      Can a physical metronome keep us in time? Experiences from the jamulus test session in the Physical-Virtual Communication and Music course.\n  \n\n\n  \n    \n    \n      Jamulus: Can y-- hear -e now?\n      networked-music\n      \n        Sep 6, 2020 • Alena Clim, Leigh Murray, Abhishek Choubey\n      During the second session of the Physical-Virtual Communication and Music course from 2020, we had our first experience with telematic music performance. It was not the greatest jam we ever had, but we learned from it. \n  \n\n\n  \n    \n    \n      Jamulus, or jamu-less?\n      networked-music\n      \n        Sep 3, 2020 • Pedro Lucas, Willie Mandeville, Henrik Sveen, Anders Lidal\n      Playing music together is not at all only about hearing, but also about the visual. Today the SMC students of 2020 experienced this in a “low latency jam session” in Jamulus.\n  \n\n\n  \n    \n    \n      Audio and Networking in the Portal - Presentations\n      networked-music\n      \n        Aug 31, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      The class of 2021 recently presented broadly on networking and audio within the context of the Portal. Presentations are included in this blog post as pdfs.\n  \n\n\n  \n    \n    \n      I scream, you scream, we all scream for livestream\n      networked-music\n      \n        May 18, 2020 • Rayam Luna\n      Some cameras won't allow you to film for more than 30 minutes, don't use those.\n  \n\n\n  \n    \n    \n      NINJAM, TPF and Audiomovers\n      networked-music\n      \n        Apr 17, 2020 • Simon Sandvik, Tom Ignatius, Jarle Steinhovden\n      During these last few weeks of “quarantine” during the COVID-19 outbreak, we have tested out several TCP/UDP audio transmission software’s from home to check for latency and user-friendliness. Our group consisting of Simon, Iggy, and Jarle, were tasked with looking into NINJAM and TPF.\n  \n\n\n  \n    \n    \n      On communication in distributed environments\n      networked-music\n      \n        Apr 15, 2020 • Karolina Jawad\n      Photo by Dr. Andrea Glang-Tossing. At ISE Europe, a trade fair in Amsterdam ( 11.02.-14.02. 2020) I was presenting together with SALTO the SMC course at the AVIXA Higher Education Conference. Researchers and students were invited to highlight emerging innovative methods that enhance learning and teaching experiences through AV technologies. The ISE Europe is the world's biggest pro AV event for integrated systems, with 80000 visitors and 1400 vendors, spread over a dozens of halls. For the conference I was specifically asked to contrast the overall technically curated program with social aspects from a student perspective. A retrospective from current conditions.\n  \n\n\n  \n    \n    \n      Zoom and other streaming services for jamming\n      networked-music\n      \n        Apr 6, 2020 • Gaute Wardenær\n      One thing you can do is to enable the option of 'preserve original audio' in Zoom.\n  \n\n\n  \n    \n    \n      Exploring SoundJack\n      networked-music\n      \n        Apr 5, 2020 • Aleksander Tidemann, Rayam Luna, Thibault Jaccard\n      SoundJack is a p2p browser-based low-latency telematic communications system.\n  \n\n\n  \n    \n    \n      Smokey and the Bandwidth\n      networked-music\n      \n        Apr 4, 2020 • Paul Koenig\n      Hijacking Old Tech for New Uses\n  \n\n\n  \n    \n    \n      Testing out Jacktrip\n      networked-music\n      \n        Mar 31, 2020 • Jackson Goode, Thomas Anda, Ulrik Halmøy\n      As we have begun settling into the COVID tech cocoon of isolation, we test out a technology that might be able to fulfull our dreams of real-time audio communication.\n  \n\n\n  \n    \n    \n      The Immersive Portal\n      networked-music\n      \n        Feb 24, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      The SMC portal is rigged for ambisonics to fit a virtual classroom in a classroom\n  \n\n\n  \n    \n    \n      Camera Optimization in the Portal\n      networked-music\n      \n        Feb 3, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      On the quest for optimizing the visual aspect of the Portal\n  \n\n\n  \n    \n    \n      Testing Latency in the Portal\n      networked-music\n      \n        Jan 27, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      We 'officially' test some the latency in the Oslo and Trondheim Portal\n  \n\n\n  \n    \n    \n      Portal ideas\n      networked-music\n      \n        Dec 2, 2019 • Rayam Luna, Gaute Wardenær, Thibault Jaccard, Ulrik Halmøy\n      Instead of starting up the M32 every day, recalling the correct preset, adjusting the faders, turning on the screens, turning on the speakers, opening LOLA, connecting to the other side, pulling your hair out because nothing will work... Imagine just pressing a button and it all just works. \n  \n\n\n  \n    \n    \n      The B Team Wraps up the Portal\n      networked-music\n      \n        Dec 1, 2019 • Jackson Goode, Jarle Steinhovden, Simon Sandvik\n      We made it out of the Portal, now what?\n  \n\n\n  \n    \n    \n      Reflections on the Christmas concert\n      networked-music\n      \n        Nov 30, 2019 • Ulrik Halmøy, Thibault, Gaute, Rayam\n      Trondheim reflects on the Christmas concert 2019\n  \n\n\n  \n    \n    \n      Microphone Testing Results\n      networked-music\n      \n        Oct 21, 2019 • Jackson Goode\n      We've spent a few days (in addition to the many miscellaneous hours during class) reconfiguring the Portal and testing out new hardware and how it might improve the quality of our sound.\n  \n\n\n  \n    \n    \n      Portal Flowchart\n      networked-music\n      \n        Oct 10, 2019 • Aleksander Tidemann, Jackson Goode, Paul Koenig, Tom Ignatius\n      The SMC portal has been subject to many configurations over the last couple of months. In this post, we explore how flowcharts may help us see a brighter tomorrow (we need as much light as we can get here).\n  \n\n\n  \n    \n    \n      The importance of sound quality\n      networked-music\n      \n        Sep 27, 2019 • Ulrik Halmøy, Thibault Jaccard\n      Reflections after several lectures with less sub-optimal sound quality\n  \n\n\n  \n    \n    \n      UltraGrid\n      networked-music\n      \n        Sep 22, 2019 • Aleksander Tidemann, Thomas Anda, Paul Koenig, Tom Ignatius\n      Exploring an alternative audio and video network transmission software in the SMC portal.\n  \n\n\n  \n    \n    \n      Could DSP it?\n      networked-music\n      \n        Sep 16, 2019 • Jackson Goode\n      Is polarity the solution?\n  \n\n\n  \n    \n    \n      An introduction to automix\n      networked-music\n      \n        Sep 10, 2019 • Gaute Wardenær\n      At first glance, automix might look like your regular old expander or gate, but what makes automix special is that it does not only work on a channel to channel basis, but links all the channels in an automix group together and opens up the channel that has the strongest signal, while ducking the others. \n  \n\n\n  \n    \n    \n      Experiencing Ambisonics Binaurally\n      networked-music\n      \n        May 19, 2019 • Guy Sion\n      During the SMC2022 Physical-virtual communication course we have had two sessions where we explored multichannel and ambisonic decoding. The first session, on February 27th, was mainly about recording a four channel A-format stream with an Ambisonic 360 sound microphone. We converted the A-format mic signal into a full-sphere surround sound format which was then decoded to 8 loudspeakers layout we placed evenly across the portal space. We have used the AIIRADecoder from the free and open source IEM plug-in suite.\n  \n\n\n  \n    \n    \n      Scenarios in the Trondheim Portal during the spring semester-2019\n      networked-music\n      \n        May 12, 2019 • Shreejay Shrestha\n      We have had numerous scenarios set up in the portal in the period of January-May 2019. Each of the scenarios are unique and therefore serve specific functions. This blog presents four of such scenarios with a bit of discussions on the advantages and challenges with the set ups.\n  \n\n\n  \n    \n    \n      Augmented Reality\n      networked-music\n      \n        May 10, 2019 • Mari Lesteberg, Sam Roman, Karolina Jawad, Jørgen Varpe, Sepehr Saghigi\n      In the final Portal workshop of this semester we were looking at Ambisonics as a potential way to create an augmented auditory space from the perspective of sound.\n  \n\n\n  \n    \n    \n      Repairing scissors and preparing the portal for talks\n      networked-music\n      \n        Mar 20, 2019 • Mari Lesteberg, Elias Andersen\n      \n  \n\n\n  \n    \n    \n      Ambisonics!\n      networked-music\n      \n        Feb 27, 2019 • Mari Lesteberg, Elias Andersen, Ashane Silva\n      On 27 February 2019, we had a workshop on Ambisonics in the portal course. Anders Tveit gave us a lecture on how to encode and decode sound inputs from Lola, using the AIIRADecoder in Reaper.\n  \n\n\n  \n    \n    \n      Documentation and recommendations from the latest Portal Jam\n      networked-music\n      \n        Feb 10, 2019 • Karolina Jawad, Eirik Dahl, Espen Wik, Shreejay Shrestha, Jørgen Varpe\n      As the Portal is still in its infancy, pushing and exploring its technical possibilities is an ongoing process. We still encounter different issues while actually seeking a smooth and standardized setup of the signal routing and performance space. At the end it is about optimizing the telematic experience but not getting stucked in technicalities at the same time.\n  \n\n\n  \n    \n    \n      How to stream content from the portal\n      networked-music\n      \n        Jan 30, 2019 • Eigil Aandahl, Sepehr Haghighi\n      In this blogpost, we will try to explain in more detail how these streams have been set up using OBS, Open Broadcaster Software and Youtube Live while being connected between Trondheim and Oslo. This can be of use for anyone looking to set up a co-located stream of a speaker or performance.\n  \n\n\n  \n    \n    \n      Portal Jam 23 January 2019, Documentation from Oslo Team\n      networked-music\n      \n        Jan 23, 2019 • Mari Lesteberg, Ashane Silva, Elias Andersen\n      On the 23 of January, we were testing out to jam together through the Portal.\n  \n\n\n",
    "url": "/networked-music/"
  },
  
  {
    "title": "Other",
    "author": "\n",
    "excerpt": "\n",
    "content": "The blog posts of this section are not related to any specific course or relate to discontinued courses such as SMC4015 Entrepreneurship for SMC.\n\n  \n    \n    \n      In search of sounds\n      other\n      \n        Dec 1, 2022 • Björn Þór Jónsson\n      Are you looking for sounds to inspire your next project? At synth.is you can discover new sounds by evolving their genes, either interactively or automatically.\n  \n\n\n  \n    \n    \n      Clubhouse\n      other\n      \n        Feb 2, 2021 • Henrik Sveen\n      Just a new phone or could it actually contribute to our online lives? This is not part of any course or anything, but I wanted to put down my thoughts on this new app. After all, it's all about audio and streaming which is very SMC, so I hope it's OK.\n  \n\n\n  \n    \n    \n      SMC vs. Corona\n      other\n      \n        Mar 13, 2020 • Thomas Anda, Jackson Goode, Paul Koenig, Rayam Luna, Jarle Steinhovden, Aleksander Tidemann, Gaute Wardenær, Ulrik Halmøy, Tom Ignatius, Thibault Jaccard, Simon Sandvik\n      In light of the recent microbial world war, we have taken matters into our own hands by sharing audio programming expertise through small introductory courses on Zoom.\n  \n\n\n  \n    \n    \n      Recovio: Entrepeneurship - Group C\n      other\n      \n        Oct 8, 2019 • Jackson Goode, Aleksander Tidemann, Thomas Anda, Tom Ignatius\n      Group C's project for the SMC 4015 Entrepeneurship course. Recovio is an audio digitization and storage company that serves companies at a scale and pricepoint that best fits their needs.\n  \n\n\n  \n    \n    \n      Entrepreneurship for SMC - Group B\n      other\n      \n        Oct 7, 2019 • Paul Koenig, Gaute Wardenær, Magda Futyma, Ulrik Halmøy\n      Summary of SMC4015 project from group B\n  \n\n\n  \n    \n    \n      Reflections on diversity\n      other\n      \n        Aug 27, 2019 • Magda Futyma, Thibault Jaccard, Tom Ignatius, Simon Sandvik, Ulrik Halmøy\n      First year students in Trondheim reflects on diversity in SMC\n  \n\n\n",
    "url": "/other/"
  },
  
  {
    "title": "People",
    "author": "\n",
    "excerpt": "\n",
    "content": "On this page you can find information about current and past students. Directly below are class lists with links to each student’s blog pages, where you can see all posts authored by that student.\n\nCurrent and past students\n\n\n  2018-2020\n  \n    UiO, Oslo\n    Elias Andersen\n    Mari Lesteberg\n    Sam Roman\n    Ashane Silva\n    Guy Sion\n    Espen Wik\n  \n  \n    NTNU, Trondheim\n    Eigil Aandahl\n    Eirik Dahl\n    Sepehr Haghighi\n    Karolina Jawad\n    Shreejay Shrestha\n    Jørgen Varpe\n  \n\n  2019-2021\n  \n    UiO, Oslo\n    Thomas Anda\n    Jackson Goode\n    Paul Koenig\n    Rayam Luna\n    Jarle Steinhovden\n    Aleksander Tidemann\n    Gaute Wardenær\n  \n  \n    NTNU, Trondheim\n    Ulrik Halmøy\n    Tom Ignatius\n    Thibault Jaccard\n    Simon Sandvik\n  \n\n  2020-2022\n  \n    UiO, Oslo\n    Alena Clim\n    Stephen Gardener\n    Anders Lidal\n    Leigh Murray\n    Henrik Sveen\n    Pedro Lucas\n    Joni Mok\n    Wenbo Yi\n  \n  \n    NTNU, Trondheim\n    Willie Mandeville\n    Lindsay Charles\n    Abhishek Choubey\n  \n\n  2021-2023\n  \n    UiO, Oslo\n    Arvid Falch\n    Sofía González\n    Oliver Getz\n    Kristian Wentzel\n    Joseph Clemente\n    Joachim Poutaraud\n    Jakob Høydal\n    Hugh Alexander von Arnim\n  \n\n  2022-2024\n  \n    UiO, Oslo\n    Alexander Wastnidge\n    Aysima Karcaaltincaba\n    Emin Memis\n    Fabian Stordalen\n    Jack Hardwick\n    Kristian Eicke\n    Masoud Niknafs\n    Nino Jakeli\n  \n\n  2023-2025\n  \n    UiO, Oslo\n    Juliana Bigelow\n    Karenina Juarez\n    Tom Oldfield\n\n  \n\n\n\nIntroduction blog posts\n\nBelow are blog posts that present the various authors of this blog, which are the students the the Aalborg University (UiO) international master’s programme in Music, Communication &amp; Technology (SMC) offered by the Department of Musicology.\n\n\n\n  \n    \n    \n      Say Hello to Team U\n      people\n      \n        Aug 18, 2023 • Tom Oldfield, Juliana Bigelow, Karenina Juarez\n      Meet SMC’s three new first year students. Coming from the United Kingdom and the United States they have a wealth of diverse musical, professional and technological backgrounds.\n  \n\n\n  \n    \n    \n      Meet SMC Team C 2022\n      people\n      \n        Aug 26, 2022 • Alexander Wastnidge, Aysima Karcaaltincaba, Fabian Stordalen, Olve Skjeggedal\n      Hello! We are team C and new around here. Come and know us better!\n  \n\n\n  \n    \n    \n      Say Hi To Team B - 2022\n      people\n      \n        Aug 26, 2022 • Ahmet Emin Memis, Nino Jakeli, Endri Alickaj\n      Team B is made up of three enthusiastic individuals who share a common interest–a love for music and technology.\n  \n\n\n  \n    \n    \n      Meet Triple A - SMC 2022\n      people\n      \n        Aug 26, 2022 • Jack Hardwick, Christian Thobroe Anda, Kristian Eicke\n      Hi! This is SMC-Team Triple A - Read more on who we are and what we do besides hanging around trash bins.\n  \n\n\n  \n    \n    \n      Meet SMC Team A 2021\n      people\n      \n        Aug 27, 2021 • Sofía González, Joachim Poutaraud, Hugh Alexander von Arnim, Oliver Getz\n      Hello! We are the new A team in town! Click here to get to know us better in our introduction post where we talk about our backgrounds, our motivations and goals.\n  \n\n\n  \n    \n    \n      Meet SMC 2021 Team B\n      people\n      \n        Aug 27, 2021 • Arvid Falch, Joseph Clemente, Jakob Høydal, Kristian Wentzel\n      This is the brand new, fresh from the oven, Team B from the SMC 2021 program. Come and say hi!\n  \n\n\n  \n    \n    \n      Meet SMC Group C 2020\n      people\n      \n        Aug 21, 2020 • Joni Mok, Lindsay Charles, Wenbo Yi, Stephen Gardener\n      Joni, Lindsay, Wenbo &amp; Stephen are the inspiringly titled SMC Group C. Hailing from Hong Kong, India, China and Wales, they have backgrounds in everything from UX Design to audio engineering, music performance and printing multi-coloured giraffes on kids t-shirts.\n  \n\n\n  \n    \n    \n      B team, or not to Beam?\n      people\n      \n        Aug 21, 2020 • Pedro Lucas, Willie Mandeville, Henrik Sveen, Anders Lidal\n      In August 2020, a multi-headed beast of eclectic skills was born. From the South American Andes, north through the North American Rockies, and east to the beautiful fjords of the Norwegian coastline, the pieces came together and the beast emerged.\n  \n\n\n  \n    \n    \n      Meet SMC Group A 2020\n      people\n      \n        Aug 21, 2020 • Leigh Murray, Alena Clim, Abhishek Choubey\n      Alena wanted to call us KitKat but we all decided against it.\n  \n\n\n  \n    \n    \n      Meet SMC Group A 2019\n      people\n      \n        Aug 30, 2019 • Thibault Jaccard, Gaute Wardenær, Rayam Luna, Ulrik Halmøy\n      We are a very diverse group, with backgrounds ranging from graphic design to electrical engineering. This complementary has been felt since we first met, and it helps us considering many different points of view for a same problem\n  \n\n\n  \n    \n    \n      Meet SMC Group C 2019\n      people\n      \n        Aug 28, 2019 • Aleksander Tidemann, Antoine Hureau, Paul Koenig, Tom Ignatius and Thomas Anda\n      We are a diverse group of people, with members originating from France, Singapore, United States and Norway. Our background stems from different fields of the music industry. Music performance, music technology, musicology, sound design, spatial audio, acousmatic composition and music recording are amongst our expertise fields. We are all eager to get started with the hard work, and hopefully, change the world as technological humanists.\n  \n\n\n  \n    \n    \n      Meet SMC Group B 2019\n      people\n      \n        Aug 28, 2019 • Jackson Goode, Magda Futyma, Simon Sandvik, Jarle Steinhovden\n      Our group spans across Norway, Poland and the United States with a wealth of experience between us. We are funny too!\n  \n\n\n",
    "url": "/people/"
  },
  
  {
    "title": "Projects",
    "author": "\n",
    "excerpt": "\n",
    "content": "\n\n  \n    \n    \n      Zen Soundscape Installation/Sculpture\n      applied-project\n      \n        Feb 13, 2025 • Eirini Liapikou\n      A serene installation that creates a harmonious soundscape transporting listeners to a state of zen. \n  \n\n\n  \n    \n    \n      DDSP-FM\n      masters-thesis\n      \n        Feb 13, 2025 • Juan Alonso, Cumhur Erkut\n      Explore latent space for learning the parameters of a Differentiable FM Synthesizer\n  \n\n\n  \n    \n    \n      BrailleGuide\n      applied-project\n      \n        Feb 10, 2025 • Benjamin Melvin Stein\n      Blind/visually impaired festival goers need an easy way to get information about things near them because it can enable a more autonomous experience for them.\n  \n\n\n  \n    \n    \n      Cosmic Clash!\n      applied-project\n      \n        Nov 24, 2024 • Tom Oldfield, Cumhur\n      Cosmic Clash - exploring gamification of physiotherapy exercises at Rosklde Festival using the Biopoint sensor\n  \n\n\n  \n    \n    \n      Strung Along: an extended violin for real-time accompaniment generation and timbral control\n      masters-thesis\n      \n        May 14, 2024 • Jack Hardwick\n      An extended violin for real-time chordal accompaniment generation and timbral control.\n  \n\n\n  \n    \n    \n      Deep Steps: A Generative AI Step Sequencer\n      masters-thesis\n      \n        May 14, 2024 • Alexander Wastnidge\n      A stand alone MIDI step sequencer application with a user-trainable generative neural network\n  \n\n\n  \n    \n    \n      Cyclic Patterns and Spatial Orientations in Artificial Impulsive ASMR Sounds\n      masters-thesis\n      \n        May 11, 2024 • Henrik Sveen\n      An exploratory study on the effects of cyclic patterns and spatial orientations in synthesized impulsive ASMR sounds.\n  \n\n\n  \n    \n    \n      CLAP Models and How To Make Them\n      masters-thesis\n      \n        May 8, 2024 • Oliver Getz\n      Is there anything CLAP models can't do?\n  \n\n\n  \n    \n    \n      The Shapeshifter\n      masters-thesis\n      \n        Dec 12, 2023 • Hugh Alexander von Arnim\n      Co-constructing the body with optical, marker-based motion capture in live dance performance\n  \n\n\n  \n    \n    \n      Using Features of Groove in Music Recommendation Systems\n      masters-thesis\n      \n        Dec 4, 2023 • Joseph Clemente\n      A study on analyzing groove in musical items and the effects of groove on musical recommendation.\n  \n\n\n  \n    \n    \n      Intuitive Robotics\n      applied-project\n      \n        Nov 27, 2023 • Emin Memis, Fabian Stordalen, Masoud Niknafs, Theo Griffin Halvorsen\n      Controlling a robot arm with medical sensors\n  \n\n\n  \n    \n    \n      Developing for Muzziball\n      applied-project\n      \n        Nov 26, 2023 • Alexander Wastnidge, Jack Hardwick, Aysima Karcaaltincaba, Kristian Eicke, Nino Jakeli\n      Check out what we worked on as a team in this year's Applied Project.\n  \n\n\n  \n    \n    \n      Bandwidth to Band Together\n      masters-thesis\n      \n        Jun 15, 2023 • Jakob Høydal\n      A Study on Approaches for Remote Music Collaboration\n  \n\n\n  \n    \n    \n      Unsupervised Meta-Embedding for Bird Songs Clustering in Soundscape Recordings\n      masters-thesis\n      \n        Jun 12, 2023 • Joachim Poutaraud\n      A case study on nocturnal and crepuscular tropical bird songs.\n  \n\n\n  \n    \n    \n      Exploration of 5G networks for Networked Musical Performances\n      applied-project\n      \n        Nov 27, 2022 • Arvid Falch, Jakob Høydal, Joachim Poutaraud, Kristian Wentzel, Sofía González\n      A latency optimization methodology for NMP.\n  \n\n\n  \n    \n    \n      Popsenteret&#39;s Music Producer Experience\n      applied-project\n      \n        Nov 25, 2022 • Joseph Clemente, Hugh Alexander von Arnim, Oliver Getz, Henrik Sveen, Iosif Aragiannis\n      The top 4 components of a physical computing music production station - you'll never guess what #3 is!\n  \n\n\n  \n    \n    \n      RoboCapo: A Digitally Controlled Actuated Capo for Enhanced Guitar Playing \n      masters-thesis\n      \n        Jun 2, 2022 • Lindsay Charles\n      An augmentation device for the guitar, a robotic capo mechanism that explores the emergence of new complex and meaningful modes of interaction.\n  \n\n\n  \n    \n    \n      Unsupervised Classification of Sub-Genres of Electronic Music\n      masters-thesis\n      \n        Jun 2, 2022 • Abhishek Choubey\n      unsupervised machine learning classification and clustering of sub-genres of electronic music\n  \n\n\n  \n    \n    \n      Emotional Responses to Vibro-tactile Music\n      masters-thesis\n      \n        May 30, 2022 • Alena Clim\n      What happens when music is felt instead of heard? When music is just vibrations... can it still make people feel emotions?\n  \n\n\n  \n    \n    \n      When Hearts Beat as One – Cardiac Dynamics and Synchrony in String Quartet Performances\n      masters-thesis\n      \n        May 20, 2022 • Wenbo Yi\n      Investigating Cardiac Dynamics and Synchrony in String Quartet Performance\n  \n\n\n  \n    \n    \n      A Human-Machine Music Performance System based on Autonomous Agents\n      masters-thesis\n      \n        May 15, 2022 • Pedro Lucas\n      Let's make music with virtual fellows in mixed reality.\n  \n\n\n  \n    \n    \n      The Algorithmic Composition Explorer\n      masters-thesis\n      \n        May 15, 2022 • Stephen Gardener\n      For my masters thesis, I proposed a novel design for an interactive system that introduces people to algorithmic composition.\n  \n\n\n  \n    \n    \n      Visualizing Psyhophysiological Responses to Music Stimuli\n      masters-thesis\n      \n        May 15, 2022 • Rayam Pinto\n      Music can induce down-regulation of the nervous system, and in combination with technology, it has the potential to promote wellness.\n  \n\n\n  \n    \n    \n      Cross-modal correspondence: Different modes, common codes? Investigating musical engagement with an Ecological cognitive approach\n      masters-thesis\n      \n        May 12, 2022 • Joni Mok\n      Investigating musical engagement with an Ecological cognitive approach\n  \n\n\n  \n    \n    \n      Rockheim - about an interactive exhibition\n      applied-project\n      \n        Dec 20, 2021 • Alena Clim, Lindsay Charles, Pedro Lucas\n      Anyone who visited Rockheim knows about the Time Tunnel, where 6 screens show the history of Norwegian music from the 50s until today. If you have no idea what I am saying, add a visit to Rockheim on your bucket list! For this project, we had to investigate and offer improvement ideas.\n  \n\n\n  \n    \n    \n      Micro and Macro: Developing New Accessible Musicking Technologies\n      masters-thesis\n      \n        Dec 14, 2021 • Mari Lesteberg\n      Most of us will never be professional musicians, even though we have the musicality and the motor ability to achieve it...\n  \n\n\n  \n    \n    \n      ImSoTra\n      masters-thesis\n      \n        Jun 30, 2021 • Shreejay Shrestha\n      Footfall induced noise in buildings is traditionally assessed with Impact Sound Transmission (IST) measurements following diffuse field model of the receiving room which is not valid below Schr&ouml;der frequency neither it facilitates auralization. This master thesis aims to create a method to estimate low frequency (LF) sound pressure in the receiving room below Schr&ouml;der frequency based on modal sum theory in room acoustics followed by measurement of IST, impulse response of the receiving room and acceleration of the main floor at two fixed position in two vertically adjacent laboratories.\n  \n\n\n  \n    \n    \n      Designing Gesture-based Interactive Museum Exhibit\n      masters-thesis\n      \n        Jun 20, 2021 • Simon Sandvik\n      A short summary of my Masters Thesis on Gesture-based interaction for museum exhibits.\n  \n\n\n  \n    \n    \n      Theory Controller: A Silent IMS\n      masters-thesis\n      \n        Jun 3, 2021 • Thibault Jaccard\n      Interactive system to control music theory\n  \n\n\n  \n    \n    \n      Toward a Telepresence of Sound: Video Conferencing in Spatial Audio\n      masters-thesis\n      \n        May 18, 2021 • Jackson Goode\n      Teleconferencing in spatial audio with the help of Jitsi Meet and Web Audio\n  \n\n\n  \n    \n    \n      The Portable Portal: An Ecological Approach to Technology-Enhanced Learning in Bangladesh\n      masters-thesis\n      \n        May 15, 2021 • Paul Koenig\n      Working toward a cogent ecological framework for technological-aid development, or: We are our technologies, they are Us\n  \n\n\n  \n    \n    \n      Reinforcement learning for use in cross-adaptive audio processing\n      masters-thesis\n      \n        May 15, 2021 • Ulrik Halmøy\n      This thesis is a study of reinforcement learning as a possible method for finding mappings in cross-adaptive audio effects\n  \n\n\n  \n    \n    \n      Exploring Hardanger Fiddle Performance Patterns Through Interactive Computational Tools\n      masters-thesis\n      \n        May 14, 2021 • Aleksander Tidemann\n      This thesis presents the development and evaluation of two software applications that integrate contemporary research perspectives on the complex rhythmical structuring of Hardanger fiddle performances.\n  \n\n\n  \n    \n    \n      HEARING NOMONO: Our Journey into Audio Branding and Feedback Sounds\n      applied-project\n      \n        May 11, 2021 • Abhishek Choubey, Lindsay Charles, Willie Mandeville, Wenbo Yi\n      Audio branding and audio feedback are everywhere. This semester, we tried our hand at designing some for a young, Trondheim-based audio technology company.\n  \n\n\n  \n    \n    \n      Designing a hybrid conference\n      applied-project\n      \n        May 11, 2021 • Joni Mok, Pedro Lucas, Rayam Luna, Stephen Gardener\n      How do you design conference that brings together both virtual and physical participants? This was the problem we explored for our Applied Project.\n  \n\n\n  \n    \n    \n      SMC Portal II - The Dungeon\n      applied-project\n      \n        May 10, 2021 • Henrik Sveen, Anders Lidal, Leigh Murray, Alena Clim\n      First time we entered the videolab, it was basically a storage room, full of outdated audio equipment and also hardware we would use. The ceiling lights didn’t work, and the cleaning personal hadn’t been there for quite a while.\n  \n\n\n  \n    \n    \n      Interconnecting Modular Synthesizers Using the Web\n      masters-thesis\n      \n        Dec 15, 2020 • Eigil Aandahl\n       In my thesis project, I present an approach to interconnecting modular synthesizer systems using a prototype multi-channel audio network solution made with Max/MSP. The research explores emergent affordances of such a system in the context of telematics and network music performance. At the bottom of this post there is a video demonstrating the prototype in action.  \n  \n\n\n  \n    \n    \n      Sound Design for IMTEL\n      applied-project\n      \n        Dec 10, 2020 • Simon Rønsholm Sandvik, Thibault Jaccard\n      For the applied project 2, we worked on enhancing the sound components of a VR language learning application in Unity\n  \n\n\n  \n    \n    \n      Expanding Collaboration in Pedál\n      applied-project\n      \n        Dec 10, 2020 • Ulrik Halmøy, Paul Koenig, Tom Ignatius, Jackson Goode\n      For our external partner, Pedál, we wanted to document and expand how multi-user interactions took place.  \n  \n\n\n  \n    \n    \n      Algorytme\n      applied-project\n      \n        Dec 10, 2020 • Thomas Anda, Aleksander Tidemann, Gaute Wardenær, Mari Lesteberg\n      A proposed application for generating personalized playlists based on emotional state.\n  \n\n\n  \n    \n    \n      The design and evaluation of the Gyroshuffle\n      masters-thesis\n      \n        Jul 31, 2020 • Sam Roman\n      The aim of this study is to develop and evaluate the Gyroshuffle, a real time rhythmic instrument played with body movement. It is theorised that moving to the rhythm, whilst controlling the rhythm is possible with the Gyroshuffle, blurring the lines between dancing and producing music in real time.\n  \n\n\n  \n    \n    \n      Gatekeepers by design? Gender HCI for Audio and Music Hardware\n      masters-thesis\n      \n        Jul 4, 2020 • Karolina Jawad\n      This dissertation looks into investigating the design of hardware for audio and music which is commonly associated with the term ‘music technology’ under the aspect of Gender-HCI, studies on science and technology as well as design research.\n  \n\n\n  \n    \n    \n      Harmonic interaction for monophonic instruments through musical phrase to scale recognition\n      masters-thesis\n      \n        Jun 23, 2020 • Guy Sion\n      Introducing a novel approach for the augmentation of acoustic instrument by providing musicians playing monophonic instruments the ability produce and control the harmonic outcome of their performance. This approach is integrated in an interactive music system that tracks the notes played by the instrument, analyzes an improvised melodic phrase, and identifies the harmonic environment in which the phrase is played. This information is then used as the input of a sound generating module which generate harmonic textures in accordance with the identified scale. \n  \n\n\n  \n    \n    \n      Motivato\n      masters-thesis\n      \n        Jun 15, 2020 • Elias Andersen\n      Motivato: A standalone music selection system for seamless technology-mediated audience participation.\n  \n\n\n  \n    \n    \n      Multimedia Slideshow Maker\n      masters-thesis\n      \n        Jun 13, 2020 • Jørgen Varpe\n      During my master's thesis, I have designed and developed a tech platform where a mobile application creates slideshows from multimedia content uploaded in a web application titled “Multimedia Slideshow Maker” (MSM). The project is carried out for an external partner, Alight AS, for a project called Alight. Alight is a mobile tech platform aiding caregivers in sending personalised video sessions to patients with dementia. This thesis aims to determine to what degree MSM can be used independently by a caregiver, without instructions from others or prior experience in video editing.  \n  \n\n\n  \n    \n    \n      The Notion of Dialogue in the Interactive Dance\n      masters-thesis\n      \n        Jun 10, 2020 • Sepehr Haghighi\n      The constituent elements of interactive dance are human and computer, which in a human-computer interaction, create a feedback loop, and present the work of art. Considering that matter, each of the opponents in this interaction has their part and space and there is an aesthetic relationship ongoing, defining the quality and amount of each opponent's part and space. In this thesis, this ongoing matter is referred to as the notion of dialogue. To create this sense, the key element that will be discussed is surprise. In order to do that, following a certain design strategy, a practical system will be designed and executed to fortitude the logical argument that is presented in this research. In that performance, by the creative use of the body, space, time, popular art forms (i.e., Hip-Hop music and dance), and with the focus on the subject of sea-level rise, the research argument will be put in practice and further on evaluated. After the evaluation process, in conclusion - despite its limitation - it may be concluded that the use of surprise, will allow the computer to have an active role and possess a significant part in the interaction and convey a sense of dialogue in it. \n  \n\n\n  \n    \n    \n      Sonification of Standstill Recordings \n      masters-thesis\n      \n        May 13, 2020 • Ashane Silva\n      The goal of this thesis was to develop and experiment with a set of sonification tools to explore participant data from standstill competitions. Using data from the 2012 Norwegian Championship of Standstill, three sonification models were developed using the Max/MSP programming environment. The first section of the thesis introduces sonification as a method for data exploration and discusses different sonification strategies. Momentary Displacement of the position was derived from the position data and parameter mapping methods were used to map the data features with sound parameters. The displacement of position in the XY plane or the position changes along the Z-Axis can be mapped either to white-noise or to a sine tone. The data variables control the amplitude and a filter cut-off frequency of the white noise or the amplitude and frequency of the sine tone. Moreover, using sound spatialization together with sonification was explored by mapping position coordinates to spatial parameters of a sine tone. A “falling” effect of the standing posture was identified through the sonification. Also audible were the participants’ breathing patterns and postural adjustments. All in all, the implemented sonification methods can be effectively used to get an overview of the standstill dataset.\n  \n\n\n  \n    \n    \n      NTNU Oceans\n      applied-project\n      \n        May 3, 2020 • Gaute Wardenær, Jarle Steinhovden, Thomas Anda, Ulrik Halmøy\n      An immersive installation on mercury pollution in the ocean. The aim of this project is to conceptualize and implement an installation in which visitors can interactively “see” and “hear” the status of oceans and seas worldwide.\n  \n\n\n  \n    \n    \n      Soundscapes for Dream Nest\n      applied-project\n      \n        May 3, 2020 • Jackson Goode, Paul Koenig, Tom Ignatius\n      For the spring applied project we created music for a hardware specific device to relax colicy babies in a collaboration with our external partner, Dream Nest. Our final product is a six track EP, engineered to put your baby to sleep (we hope!)\n  \n\n\n  \n    \n    \n      MotionComposer\n      applied-project\n      \n        May 2, 2020 • Aleksander Tidemann, Rayam Luna, Simon Sandvik, Thibault Jaccard\n      MotionComposer is a motion capture device that lets people make music with gestures. This is the presentation of our applied project, where we worked on building a new instrument for this device.\n  \n\n\n  \n    \n    \n      Trippi BySykkel Sounds\n      applied-project\n      \n        Dec 15, 2019 • Eigil Aandahl, Elias Andersen, Karolina Jawad, Sam Roman\n      Portray city bike user data into a sonified, interactive display by making use of public space and public data.’\n  \n\n\n  \n    \n    \n      Tree as Speakers\n      applied-project\n      \n        Dec 15, 2019 • Ashane Silva, Shreejay Shrestha, Jørgen Varpe\n      A project in collaboration with ÅF engineering. The goal of the project was to create a non-intrusive soundscape and/or noise-masking installations in an outdoor public space by using trees as speakers, installing audio exciters on trees.\n  \n\n\n  \n    \n    \n      Picture Carmen\n      applied-project\n      \n        Dec 11, 2019 • Guy Sion, Espen Wik, Sepehr Haghighi\n      Promotional video for a new chamber production of the Opera ‘Carmen’\n  \n\n\n  \n    \n    \n      Seismerssion: Retrospectives on an Audio-visual installations\n      applied-project\n      \n        Jul 14, 2019 • Karolina Jawad, Espen Wik, Sepehr Haghighi, Shreejay Shrestha, Jørgen Varpe\n      Seismerssion is the title we gave our Applied Project in the context of the SMC spring semester 2019. This audio-visual installation is dedicated to the widely unknown issue of sound pollution in the ocean. In collaboration with NTNU Oceans, an intership was established to develop and implement a public installation concept for 2 different venues.\n  \n\n\n",
    "url": "/projects/"
  },
  
  {
    "title": "Search",
    "author": "\n",
    "excerpt": "\n",
    "content": "\n",
    "url": "/search/"
  },
  
  {
    "title": "Sonification",
    "author": "\n",
    "excerpt": "\n",
    "content": "The blog posts of this section relate to the Sonification module in the course SMC4001 Introduction to Music, Communication and Technology (from 2022) and to the discontinued course SMC4046 Sonification and Sound Design (2019 to 2021). The aim of the course is to develop knowledge of and practical experience with sonification, including sound design, interface implementation and perceptual analysis. The interface design process extends from requirements in the problem domain to evaluation of the auditory display.\n\n{%- if site.posts.size &gt; 0 -%}\n\n\n\n    {%- for post in site.categories.sonification -%}\n\n    {%- include list-body.html -%}\n\n    {%- endfor -%}\n\n  \n{%- endif -%}\n\n\n\n\n  \n    \n      {% capture difference %} {{ site.posts\n      size\n      minus:1 }} {% endcapture %}\n    \n  \n\n\n\n  \n   \n  \n",
    "url": "/sonification/"
  },
  
  {
    "title": "Sound Programming",
    "author": "\n",
    "excerpt": "\n",
    "content": "The blog posts of this section relate to the course SMC4001 Sound and Music Programming (from 2021), and to the discontinued course SMC4048 Audio Programming (2019 to 2021). This section also includes posts from the Digital Audio and Scientific Computing modules of the discontinued course SMC4000 Introduction to Music, Communication and Technology (from 2018 to 2020). The aim of these course is to develop audio signal processing and sound design knowledge as well as skills with general-purpose and audio-specific programming.\n\n{%- if site.posts.size &gt; 0 -%}\n\n\n\n    {%- for post in site.categories.sound-programming -%}\n\n    {%- include list-body.html -%}\n\n    {%- endfor -%}\n\n  \n{%- endif -%}\n\n\n\n\n  \n    \n      {% capture difference %} {{ site.posts\n      size\n      minus:1 }} {% endcapture %}\n    \n  \n\n\n\n\n\n\n",
    "url": "/sound-programming/"
  },
  
  {
    "title": "Spatial Audio",
    "author": "\n",
    "excerpt": "\n",
    "content": "The blog posts of this section relate to the Spatial Audio module in the course MCT4025 – Physical-Virtual Communication and Music 2 (from 2022), and to the discontinued course MCT4044 Spatial Audio (2019 to 2021). The aim of these courses is to develop knowledge of and practical experience with spatial audio production technologies, including binaural and multichannel rendering of both channel-based and object-based audio. Topics include virtual acoustics, auralization and spatial audio perception.\n\n{%- if site.posts.size &gt; 0 -%}\n\n\n\n    {%- for post in site.categories.spatial-audio -%}\n\n    {%- include list-body.html -%}\n\n    {%- endfor -%}\n\n  \n{%- endif -%}\n",
    "url": "/spatial-audio/"
  }
  
]
