<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The Shapeshifter | The SMC Blog</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="The Shapeshifter" />
<meta name="author" content="Hugh Alexander von Arnim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Co-constructing the body with optical, marker-based motion capture in live dance performance" />
<meta property="og:description" content="Co-constructing the body with optical, marker-based motion capture in live dance performance" />
<link rel="canonical" href="https://smc-aau-cph.github.io/smc-cph.github.io/masters-thesis/2023/12/12/hughav-the-shapeshifter.html" />
<meta property="og:url" content="https://smc-aau-cph.github.io/smc-cph.github.io/masters-thesis/2023/12/12/hughav-the-shapeshifter.html" />
<meta property="og:site_name" content="The SMC Blog" />
<meta property="og:image" content="https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2023_12_15_hughav_cubicspace.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-12-12T10:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2023_12_15_hughav_cubicspace.png" />
<meta property="twitter:title" content="The Shapeshifter" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Hugh Alexander von Arnim"},"dateModified":"2023-12-12T10:00:00+00:00","datePublished":"2023-12-12T10:00:00+00:00","description":"Co-constructing the body with optical, marker-based motion capture in live dance performance","headline":"The Shapeshifter","image":"https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2023_12_15_hughav_cubicspace.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://smc-aau-cph.github.io/smc-cph.github.io/masters-thesis/2023/12/12/hughav-the-shapeshifter.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2022_07_20_stefanof_SMC_logo_grey.png"},"name":"Hugh Alexander von Arnim"},"url":"https://smc-aau-cph.github.io/smc-cph.github.io/masters-thesis/2023/12/12/hughav-the-shapeshifter.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://smc-aau-cph.github.io/smc-cph.github.io/feed.xml" title="The SMC Blog" /></head>
<body><header class="site-header" role="banner"><div class="hamburger-menu-container">
  <nav>
    <ul>
      
      <li class="page-link-hamburger">
        

        <a href="#" class="drop-btn-hamburger" onclick="handleMenuClick(this)"
          >Topics</a
        >
        <ul class="dropdown-hamburger">
          
          <li><a href="/interactive-music/">New Interfaces for Musical Expression (NIME)</a></li>
          
          <li><a href="/machine-learning/">Machine Learning for Media Experiences</a></li>
          
          <li><a href="/motion-capture/">Embodied Interaction</a></li>
          
          <li><a href="/networked-music/">Networked Music</a></li>
          
          <li><a href="/sonification/">Sonification</a></li>
          
          <li><a href="/sound-programming/">Sound Processing</a></li>
          
          <li><a href="/spatial-audio/">Spatial User Interfaces</a></li>
          
          <li><a href="/other/">Other</a></li>
          
          <li><a href="/alltopics/">All Topics</a></li>
          
        </ul>

        
      </li>
      
      <li class="page-link-hamburger">
        

        <a href="#" class="drop-btn-hamburger" onclick="handleMenuClick(this)"
          >Projects</a
        >
        <ul class="dropdown-hamburger">
          
          <li><a href="/mini-projects/">Course Mini Projects</a></li>
          
          <li><a href="/applied-projects/">Semester Projects</a></li>
          
          <li><a href="/masters-thesis/">Master's Theses</a></li>
          
          <li><a href="/projects/">All Projects</a></li>
          
        </ul>

        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/people/">People</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/about/">About</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/Guides/">Guides</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/search/">Search</a>
        
      </li>
      
    </ul>
  </nav>
</div>
<div class="wrapper">
        <div class="title-and-logo-wrapper">
            <a href="/">
                <img class="site-logo" src="/assets/image/2022_07_20_stefanof_SMC_logo_grey.png" />
            </a>
            <p class="site-title">
            The SMC Blog
            </p>
        </div>

        <nav class="site-nav">
        <ul>
            
            <li class="page-link">
            

                <a href='#' class="drop-btn" onclick="handleMenuClick(this)">Topics</a>
                <ul class="dropdown">
                
                <li><a href="/interactive-music/">New Interfaces for Musical Expression (NIME)</a></li>
                
                <li><a href="/machine-learning/">Machine Learning for Media Experiences</a></li>
                
                <li><a href="/motion-capture/">Embodied Interaction</a></li>
                
                <li><a href="/networked-music/">Networked Music</a></li>
                
                <li><a href="/sonification/">Sonification</a></li>
                
                <li><a href="/sound-programming/">Sound Processing</a></li>
                
                <li><a href="/spatial-audio/">Spatial User Interfaces</a></li>
                
                <li><a href="/other/">Other</a></li>
                
                <li><a href="/alltopics/">All Topics</a></li>
                
                </ul>

            
            </li>
            
            <li class="page-link">
            

                <a href='#' class="drop-btn" onclick="handleMenuClick(this)">Projects</a>
                <ul class="dropdown">
                
                <li><a href="/mini-projects/">Course Mini Projects</a></li>
                
                <li><a href="/applied-projects/">Semester Projects</a></li>
                
                <li><a href="/masters-thesis/">Master's Theses</a></li>
                
                <li><a href="/projects/">All Projects</a></li>
                
                </ul>

            
            </li>
            
            <li class="page-link">
            
                <a href="/people/">People</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/about/">About</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/Guides/">Guides</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/search/">Search</a>
            
            </li>
            
        </ul>
        </nav>
        <nav class="hamburger-icon-nav"><div class="hamburger-icon-container" onclick="handleHamburgerClick(this)">
  <div class="hamburger-icon-div hamburger-bar1"></div>
  <div class="hamburger-icon-div hamburger-bar2"></div>
  <div class="hamburger-icon-div hamburger-bar3"></div>
</div>
</nav>
    </div>

  <script src="/utils/jquery.js"></script>
  <script src="/utils/hamburger-nav.js"></script>
  <script src="/utils/nav-dropdown-click-handler.js"></script>
</header>
<main class="page-content" aria-label="Content">

      <div class="wrapper">
        <article
  class="post h-entry"
  itemscope
  itemtype="http://schema.org/BlogPosting"
>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline" align="left">
      The Shapeshifter
    </h1>
    <p class="post-meta">
      <time
        class="dt-published"
        datetime="2023-12-12T10:00:00+00:00"
        itemprop="datePublished"
      >Dec 12, 2023 •
      </time>
         
      <a href="/authors/hughalexandervonarnim.html">Hugh Alexander von Arnim</a>
        </p>
  </header>

  <div class="post-content" itemprop="articleBody"><figure>
  <img src="/assets/image/2023_12_15_hughav_header.png" width="100%" allign="middle" />
  <figcaption>Oskar Schlemmer's depiction of the laws of cubic space. From (Schlemmer, 1987).</figcaption>
</figure>

<div style="text-align: center;">
  <b>The full text of this thesis is available <a href="https://www.uio.no/english/studies/programmes/SMC-master/blog/assets/document/2023_12_15_hughav_shapeshifterthesis.pdf"> here.</a> All source code developed can be accessed at <a href="https://github.com/Hughav92/The-Shapeshifter">this GitHub repository.</a></b>
</div>
<p><br />
When we use motion capture or tracking (MoCap) systems to capture human body motion for artistic purposes, it can be easy to fall into ways of thinking that conceptually merge the physical body and the representation constructed from the captured or tracked data. A recent, high-profile example of this can be found in the rhetoric surrounding ABBA’s comeback <a href="https://abbavoyage.com/">Voyage</a> tour, which features avatars of the members of ABBA as they appeared in the 1970s created through the use of large scale motion capture, with the show’s producer, Ludvig Andersson, employing strong terms to elide the bodies of ABBA and their <em>ABBA-tars</em>, stating that “when you see this show it is not a version of, or a copy of, or four people pretending to be ABBA, it is actually them” (ABBA Voyage, 2021).</p>

<figure>
  <img src="/assets/image/2023_12_15_hughav_abba.png" width="50%" allign="middle" />
  <figcaption><em>ABBA-tars</em> in performance. From (‘The bigger picture: ABBA Voyage’, 2022).</figcaption>
</figure>

<p>However, many actors and technological systems contributed to the production of the <em>ABBA-tars</em>. A team of engineers and animators were involved in the process of constructing the body from the motion data. The technologies involved implied ways of working suggesting the forms that the body can take and what it can do. A number of younger stand-ins even provided the motion from which the virtual body was constructed (Plaete et al., 2022). Although choreographer Wayne McGregor noted that it was a “technical and emotional challenge” to get the members of ABBA “back into their bodies” (ABBA Voyage, 2021), the bodies visible in the show are, in effect, the result of a co-constructive process including all actors involved. In this context, eliding the physical body with representation can lead to the obscuring of assumptions about the body that all these actors bring.</p>

<p>This idea is the central to the research aim of this thesis:</p>

<div style="text-align: center;">
  <b>to critically explore the co-construction of a virtual representation of the body through the use of <a href="https://en.wikipedia.org/wiki/Motion_capture#Optical_systems"> optical, marker-based motion capture </a> in live performance, specifically focusing on dance.</b>
</div>
<p><br />
This aim is formalised in the following research questions:</p>

<div style="text-align: center;">
  <b>RQ1. How does the use of a motion capture system co-construct a virtual representation of the body in live performance, and which assumptions about the body does it make?</b>
</div>
<p><br /></p>
<div style="text-align: center;">
  <b>RQ2. How can a multi-modal interactive system be iteratively designed from a perspective which foregrounds motion capture as co-constructing a representation of the body?</b>
</div>
<p><br /></p>
<div style="text-align: center;">
  <b>RQ3. How does a performer experience their body in relation to the technological components in performance with a system for interactive dance?</b>
</div>
<p><br />
Accordingly, this thesis offers three main contributions:</p>

<div style="text-align: center;">
  <b>1. A theoretical framework which models the use of motion capture technologies to create a virtual representation of a physical body as a co-constructive process.</b>
</div>
<p><br /></p>
<div style="text-align: center;">
  <b><em>2. The Shapeshifter</em>, a performance and multi-modal interactive system.</b>
</div>
<p><br /></p>
<div style="text-align: center;">
  <b>3. A qualitative analysis of the embodied experience of the project's collaborator of performing the work and their relationship to the technological components of the system from an embodied perspective.</b>
</div>
<p><br /></p>

<p>Although addressing each of the research questions required a diverse set of theoretical, qualitative, and quantitative methods, this thesis is encapsulated in a wider research-creation project. This is a methodological approach focused the combination of research methods and creative practices in a causal manner which leads to academic and artefactual results (Stévance and Lacasse, p. 123). This approach is well-suited for collaboration, and accordingly the work done for this thesis was carried out in collaboration with a performer who specialises in <a href="https://en.wikipedia.org/wiki/Physical_theatre">physical theatre</a> and dance.</p>

<h1 id="the-co-construction-model">The Co-Construction Model</h1>

<p>The main theoretical contribution of this thesis is a model of MoCap in live dance performance as a co-constructive process.</p>

<figure>
  <img src="/assets/image/2023_12_15_hughav_coconstruction.png" width="100%" allign="middle" />
  <figcaption>The co-construction model of MoCap in live dance performance.</figcaption>
</figure>

<p>This model was developed to encourage a way of thinking about performance involving the use of motion capture that centres upon critical reflection on the assumptions and values that are embedded in the representation of the body created through the technology’s use. The model depicts this occurring across five layers. These should not be understood as distinct and separated, but rather as nested within one another. They depict the increasing concretisation of the form of the virtual body, until this is realised in performance in relation to the physical body of the performer. Crucially, the flow of the embedding of assumptions and values is not depicted as unidirectional, starting in the outer-most layer and flowing in towards the performance, but rather shows the propagation of assumptions throughout the entirety of the model. Through systematically assessing the locations of actors and technological systems, the assumptions and values that they bring, and how these manifest and interact in the representations that are co-constructed.</p>

<p>The co-construction model was developed upon a methodological foundation based upon the concepts of <em>normalcy</em>, a key concept relating to theorising the body in disability studies (Davis, 1995; Garland-Thomson, 1997), <em>mediate auscultation</em>, in which sensing devices conceptually stand in for that which is being sense (Sterne, 2001), and the propagation of aesthetic and ethical values through the <em>appropriation of sensing devices for artistic practice</em> (Naccarato and MacCallum, 2017).</p>

<p>A full description of the model, how it can be applied as a framework for system design or analysis, and its methodological basis can be found in chapter three of the thesis manuscript.</p>

<h1 id="the-shapeshifter">The Shapeshifter</h1>

<p><br /></p>
<figure style="float: none">
<iframe width="800" height="500" src="https://www.youtube.com/embed/brycv823l5Y?si=tkdxy4xNYugcXTYB" title="YouTube video player" frameborder="0" allow="accelerometer;
100%play;
clipboard-write;
encrypted-media;
gyroscope;
picture-in-picture" allowfullscreen="">
</iframe>
<figcaption><i>A demonstration of <em>The Shapeshifter</em></i>. The video contains a binaural audio mix, so should be viewed with headphones.</figcaption>
</figure>
<p><br /></p>

<p>In view of the co-construction model, together with the research-creation collaborator, we developed <em>The Shapeshifter</em>, an interactive system and peformance to explore the ways in which optical, marker-based motion capture systems construct the representation of the body. With this, we aimed to likewise investigate the performer’s relationship to the representation, as well as the technological components of the system.</p>

<p><em>The Shapeshifter</em> is an improvisatory dance work for a single performer. Prior to the performance, the performer positions up to 30 position markers wherever they please, either on the body, attached to other objects, or placed within the environment. During the performance, they are also free to reposition these whenever and wherever they wish. A performance consists of nine phases, during each of which the performer improvises a motion pattern and accompanying vocalisations. To trigger the end of a phase, each of the position markers must be located within a corresponding physical space (a bounding box) in the physical performance area. Each phase presents a different visualisation style both for the virtual representation of the position marker and any connections drawn between markers as well as distinct limitations on how the visualisation of each marker can move. At the end of the nine phases, the cycle begins again. During the second run-through of the phases, the performer’s vocalisations for each phase from the previous run-through are looped within the corresponding phase in the current run-through. Starting in the third run-through of the motion phases, the representations of the markers and connections and their motion limitations begin to shift, interpolating between combinations of the representations of all nine phases. The interpolation is based upon several factors, relating to the similarity of the performer’s motion and vocalisations to the motion patterns and vocalisations performed in the previous run-throughs. Likewise, the looped vocalisations begin to twist and distort away from the original recordings. When the performer vocalises during the current run-through, both the audible and visual representations are pulled back into their original state from the first run-through. As the number of repetitions increases, it becomes increasingly difficult for the performer to purposefully control the representations, building to a climax in the seventh and final run-through of the nine motion patterns. A performance takes place with the performer facing a video wall which mirrors the physical capture volume with a virtual capture volume. The audience is also positioned within the performance space, with the performer moving around the audience. To support the idea of the audience being within the performance space, the looped vocalisations are played back over a spatial audio system in two manners. The first is an underlying sound bed that slowly envelops the performance area over the course of the performance. The second positions each vocalisation at the position of the performer at the time that the vocalisation was recorded. The playback shifts between the two techniques based upon whether the performer is currently vocalising. The first technique is used to playback the looped vocalisations with the parameters of the current interpolation applied. The second, to reproduce the vocalisations in their original state.</p>

<p>We employed several quantitative and qualitative evaluations of the system and the collaborator’s performance. A full description of these evalutions, as well as a full technical description of <em>The Shapeshifter</em> system, can be found in chapters 4 to 8 of the thesis manuscript.</p>

<h1 id="works-cited">Works Cited</h1>

<p>ABBA Voyage. (2021, October 13). ABBA Voyage: How ABBA used motion capture to create their avatars. Facebook. https://www.facebook.com/ABBAVoyage/videos/abba-voyage-how-abba-used-motion-capture-to-create-their-avatars/399264848445591/</p>

<p>Davis, L. J. (1995). Enforcing Normalcy: Disability, Deafness, and the Body. Verso.
Garland-Thomson, R. (1997). Extraordinary Bodies: Figuring Physical Disability in American Culture and Literature. Columbia University Press.</p>

<p>Naccarato, T. J., &amp; MacCallum, J. (2017). Critical Appropriations of Biosensors in Artistic Practice. Proceedings of the 4th International Conference on Movement Computing, 1–7. https://doi.org/10.1145/3077981.3078053</p>

<p>Plaete, J., Bradley, D., Warner, P., &amp; Zwartouw, A. (2022). ABBA voyage: High volume facial likeness and performance pipeline. ACM SIGGRAPH 2022 Talks. https://doi.org/10.1145/3532836.3536260</p>

<p>Schlemmer, O., Moholy-Nagy, L., &amp; Molnár, F. (1987). The theater of the Bauhaus (W. Gropius &amp; A. S. Wensinger, Eds.). Wesleyan university press.</p>

<p>Sterne, J. (2001). Mediate Auscultation, the Stethoscope, and the “Autopsy of the Living”: Medicine’s Acoustic Culture. Journal of Medical Humanities, 22(2), 115–136. https://doi.org/10.1023/A:1009067628620</p>

<p>Stévance, S., &amp; Lacasse, S. (2018). Research-creation in music: Towards a collaborative interdiscipline. Routledge.
The bigger picture: ABBA Voyage. (2022). Engineering &amp; Technology, 17(6), 14–15. https://doi.org/10.1049/et.2022.0622</p>
</div>

  

  <a class="u-url" href="/masters-thesis/2023/12/12/hughav-the-shapeshifter.html" hidden></a>
</article>

<script src="/utils/slideshow.js"></script>
<script src="https://unpkg.com/wavesurfer.js@5.0.1/dist/wavesurfer.js"></script>
<script src="/utils/waveform.js"></script>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The SMC Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The SMC Blog</li><li><a class="u-email" href="mailto:cer@create.aau.dk">cer@create.aau.dk</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/smc-aau-cph"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">smc-aau-cph</span></a></li><li><a href="https://www.twitter.com/smc-aau-cph"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">smc-aau-cph</span></a></li><li><a href="https://youtube.com/c/smc-aau-cphr/videos"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#youtube"></use></svg> <span class="username">SMC_master</span></a></li><li><a href="/feed.xml"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#rss"></use></svg> <span>RSS feed</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>The student-led blog of the Aalborg University Copenhagen (AAU-CPH) master&#39;s programme in Sound and Music Computing (SMC).</p>
      </div>
    </div>

  </div>

</footer>


    <!-- include comments here -->

  </body>

</html>
