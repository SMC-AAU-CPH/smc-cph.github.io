<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Piano Accompaniment Generation Using Deep Neural Networks | The SMC Blog</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Piano Accompaniment Generation Using Deep Neural Networks" />
<meta name="author" content="Kristian Wentzel" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How I made use of Fourier Transforms in deep learning to generate expressive MIDI piano accompaniments." />
<meta property="og:description" content="How I made use of Fourier Transforms in deep learning to generate expressive MIDI piano accompaniments." />
<link rel="canonical" href="https://smc-aau-cph.github.io/smc-cph.github.io/machine-learning/2022/05/20/kriswent-generating-piano-accompaniments-using-fourier-transforms.html" />
<meta property="og:url" content="https://smc-aau-cph.github.io/smc-cph.github.io/machine-learning/2022/05/20/kriswent-generating-piano-accompaniments-using-fourier-transforms.html" />
<meta property="og:site_name" content="The SMC Blog" />
<meta property="og:image" content="https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2022_05_20_kriswent_piano_robot.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-20T15:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2022_05_20_kriswent_piano_robot.jpeg" />
<meta property="twitter:title" content="Piano Accompaniment Generation Using Deep Neural Networks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Kristian Wentzel"},"dateModified":"2022-05-20T15:00:00+00:00","datePublished":"2022-05-20T15:00:00+00:00","description":"How I made use of Fourier Transforms in deep learning to generate expressive MIDI piano accompaniments.","headline":"Piano Accompaniment Generation Using Deep Neural Networks","image":"https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2022_05_20_kriswent_piano_robot.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"https://smc-aau-cph.github.io/smc-cph.github.io/machine-learning/2022/05/20/kriswent-generating-piano-accompaniments-using-fourier-transforms.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2022_07_20_stefanof_SMC_logo_grey.png"},"name":"Kristian Wentzel"},"url":"https://smc-aau-cph.github.io/smc-cph.github.io/machine-learning/2022/05/20/kriswent-generating-piano-accompaniments-using-fourier-transforms.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://smc-aau-cph.github.io/smc-cph.github.io/feed.xml" title="The SMC Blog" /></head>
<body><header class="site-header" role="banner"><div class="hamburger-menu-container">
  <nav>
    <ul>
      
      <li class="page-link-hamburger">
        

        <a href="#" class="drop-btn-hamburger" onclick="handleMenuClick(this)"
          >Topics</a
        >
        <ul class="dropdown-hamburger">
          
          <li><a href="/interactive-music/">New Interfaces for Musical Expression (NIME)</a></li>
          
          <li><a href="/machine-learning/">Machine Learning for Media Experiences</a></li>
          
          <li><a href="/motion-capture/">Embodied Interaction</a></li>
          
          <li><a href="/networked-music/">Networked Music</a></li>
          
          <li><a href="/sonification/">Sonification</a></li>
          
          <li><a href="/sound-programming/">Sound Processing</a></li>
          
          <li><a href="/spatial-audio/">Spatial User Interfaces</a></li>
          
          <li><a href="/other/">Other</a></li>
          
          <li><a href="/alltopics/">All Topics</a></li>
          
        </ul>

        
      </li>
      
      <li class="page-link-hamburger">
        

        <a href="#" class="drop-btn-hamburger" onclick="handleMenuClick(this)"
          >Projects</a
        >
        <ul class="dropdown-hamburger">
          
          <li><a href="/mini-projects/">Course Mini Projects</a></li>
          
          <li><a href="/applied-projects/">Semester Projects</a></li>
          
          <li><a href="/masters-thesis/">Master's Theses</a></li>
          
          <li><a href="/projects/">All Projects</a></li>
          
        </ul>

        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/people/">People</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/about/">About</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/Guides/">Guides</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/search/">Search</a>
        
      </li>
      
    </ul>
  </nav>
</div>
<div class="wrapper">
        <div class="title-and-logo-wrapper">
            <a href="/">
                <img class="site-logo" src="/assets/image/2022_07_20_stefanof_SMC_logo_grey.png" />
            </a>
            <p class="site-title">
            The SMC Blog
            </p>
        </div>

        <nav class="site-nav">
        <ul>
            
            <li class="page-link">
            

                <a href='#' class="drop-btn" onclick="handleMenuClick(this)">Topics</a>
                <ul class="dropdown">
                
                <li><a href="/interactive-music/">New Interfaces for Musical Expression (NIME)</a></li>
                
                <li><a href="/machine-learning/">Machine Learning for Media Experiences</a></li>
                
                <li><a href="/motion-capture/">Embodied Interaction</a></li>
                
                <li><a href="/networked-music/">Networked Music</a></li>
                
                <li><a href="/sonification/">Sonification</a></li>
                
                <li><a href="/sound-programming/">Sound Processing</a></li>
                
                <li><a href="/spatial-audio/">Spatial User Interfaces</a></li>
                
                <li><a href="/other/">Other</a></li>
                
                <li><a href="/alltopics/">All Topics</a></li>
                
                </ul>

            
            </li>
            
            <li class="page-link">
            

                <a href='#' class="drop-btn" onclick="handleMenuClick(this)">Projects</a>
                <ul class="dropdown">
                
                <li><a href="/mini-projects/">Course Mini Projects</a></li>
                
                <li><a href="/applied-projects/">Semester Projects</a></li>
                
                <li><a href="/masters-thesis/">Master's Theses</a></li>
                
                <li><a href="/projects/">All Projects</a></li>
                
                </ul>

            
            </li>
            
            <li class="page-link">
            
                <a href="/people/">People</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/about/">About</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/Guides/">Guides</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/search/">Search</a>
            
            </li>
            
        </ul>
        </nav>
        <nav class="hamburger-icon-nav"><div class="hamburger-icon-container" onclick="handleHamburgerClick(this)">
  <div class="hamburger-icon-div hamburger-bar1"></div>
  <div class="hamburger-icon-div hamburger-bar2"></div>
  <div class="hamburger-icon-div hamburger-bar3"></div>
</div>
</nav>
    </div>

  <script src="/utils/jquery.js"></script>
  <script src="/utils/hamburger-nav.js"></script>
  <script src="/utils/nav-dropdown-click-handler.js"></script>
</header>
<main class="page-content" aria-label="Content">

      <div class="wrapper">
        <article
  class="post h-entry"
  itemscope
  itemtype="http://schema.org/BlogPosting"
>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline" align="left">
      Piano Accompaniment Generation Using Deep Neural Networks
    </h1>
    <p class="post-meta">
      <time
        class="dt-published"
        datetime="2022-05-20T15:00:00+00:00"
        itemprop="datePublished"
      >May 20, 2022 •
      </time>
         
      <a href="/authors/kristianwentzel.html">Kristian Wentzel</a>
        </p>
  </header>

  <div class="post-content" itemprop="articleBody"><p>How can you generate piano accompaniments to a given melody using machine learning (ML)? That was my question. The answer, my friend, seemed not to be blowing in the wind, but revealed itself after extensive googling, trial, and error. My goal was to somehow generate piano accompaniments by conditioning the ML model on melodies. To do so, I had to look into the field of Natural Language Processing (NLP) and deep neural networks. I settled on working with symbolic music representation in the form of MIDI for my project, not raw audio. There has been a lot of research in translation applications using ML. Let’s pretend that melodies are Chinese, and accompaniments are Swedish. Now we simply need to translate the Chinese melody into a Swedish accompaniment. Easy, right? Not when you’re completely new to the field, as I was. But I slowly and steadily managed to learn some basics of ML translation, and a lot of general ML theory along the way.</p>

<h3 id="sequence-to-sequence">Sequence to Sequence</h3>

<p>Translational tasks are usually tackled by using sequence to sequence (seq2seq) models. This architecture takes one sequence in and spits another sequence out. One of many challenges is that the sequences probably won’t be of the same length. And especially for my use case, where there should be several more notes (analogous to words) in the generated accompaniment than there will be in the incoming melody. One ML model design which has gained much attention (pun intended) the last years, is the Transformer model [1]. This model uses self-attention mechanism to learn the relationship between notes and can potentially learn long-term and high-level structures and phrases in music. This model performs even better than the Long Short-Term Memory (LSTM) layers, which previously was used by Google Translate, among others.</p>

<figure style="float: none">
   <img src="/assets/image/2022_05_20_kriswent_transformer.png" alt="Transformer architecture" title="" height="auto" width="70%" />
   <figcaption><i>The Transformer architecture, sourced from a book [2] I couldn't do without this spring.</i></figcaption>
</figure>

<h3 id="fnet">FNet</h3>

<p>For my project, I chose to use the FNet architecture [3], which is a modification of the Transformer. Here, the attention layer of the encoder is exchanged with a Fourier transform layer to project the input sequence into the Frequency domain. This makes the model much faster, as multiplication in the frequency domain is equivalent to convolution in the time domain, while the performance is only slightly worse. I’m not too concerned about the accuracy scores anyway when working with a generative model like this.</p>

<h3 id="model-architecture">Model Architecture</h3>

<p>Seq2seq models are made up of an encoder block and a decoder block – in my case, the melody (source language) is fed into the encoder, and the accompaniment (translated language) is fed into the decoder when training the model. The encoder output is connected to a multihead attention layer in the decoder, to condition the accompaniment generation on the melody, while the decoders output is the predicted accompaniment. When the model is used for inference/generation, you’ll still feed the encoder with the melody, but the decoder will only receive its last predicted output token at any given step. (You can also feed the decoder a portion of the original accompaniment to give it a flying start, as I did when prediction my accompaniments.) Output token you say, what’s a token?</p>

<figure style="float: none">
   <img src="/assets/image/2022_05_20_kriswent_fnet_transformer.png" alt="Fnet vs Transformer" title="" height="auto" width="70%" />
   <figcaption><i>The Transformer (left) and FNet (right) encoder blocks</i></figcaption>
</figure>

<h3 id="tokens">Tokens</h3>

<p>The food for seq2seq models is tokens. Tokens would usually be a word representation in an NLP task. For symbolic music translation, this could be a note. There are several ways to tokenize music, and I chose to use a modified version of the design found in both Google Magenta’s <a href="https://magenta.tensorflow.org/performance-rnn">Performance RNN</a> and in the research done by the creators of my dataset [4]. I encoded MIDI notes into the following four categories:</p>

<ul>
  <li>128 note-on events (One for each of the 128 MIDI pitches – to start a note)</li>
  <li>128 note-off events (One for each of the 128 MIDI pitches – to release a note)</li>
  <li>32 time-shift events (Making 32nd notes the shortest possible time-shift)</li>
  <li>32 velocity events (MIDI velocities quantized into 32 different values)</li>
</ul>

<h3 id="dataset-and-processing">Dataset and processing</h3>

<p>As my dataset, I chose the mentioned POP909 dataset, consisting of 909 Chinese pop songs. I wanted to work with popular or groove-based music, as much research has been done using classical piano music. This dataset conveniently had MIDI files with separated tracks for melody and accompaniment, which made the whole process easier for me. I chose to quantize the MIDI files into a 32nd second note resolution and chop the files into 32 bar segments to expand my dataset. Now, I only needed a model to train!</p>

<h3 id="model-training">Model training</h3>

<p>As my tool for building a machine learning model, I used the <a href="https://keras.io/">Keras</a> ML framework, which is a part of the <a href="https://www.tensorflow.org/">TensorFlow</a> library. I spent a <em>lot</em> of time in building the model and tweaking different hyperparameters to make it perform in the best way possible. In the start, I used my poor old laptop, and kept it running overnight. (Disclaimer: Do not try this at home! I was a bit worried if the laptop would heat up too much during sleepy time.) After a while, I switched to using UiO’s <a href="https://www.uio.no/tjenester/it/forskning/kompetansehuber/uio-ai-hub-node-project/it-resources/ml-nodes/">ML Nodes</a>. This, in the end, made it possible to fit my model in time. The final training of 695 epochs took about 6 hours and 45 minutes using 2 GPUs on the ML Nodes, while it would’ve approximately taken 10 days and 15 hours on my laptops CPU. (20 seconds per epoch vs 22 minutes pr epoch.) In the end, I chose to also train a vanilla Transformer model, to compare with the FNet.</p>

<h3 id="accompaniment-generation">Accompaniment generation</h3>

<p>In the end, I was quite surprised how the generated accompaniments turned out. It played in the same key as the melody, but the timing and rhythm was oftentimes a bit off. The model seemed to be able to generate accompaniments with some musical phrases and elements audibly present. My virtual pianist is not replacing a professional accompanist anytime soon and will not always sound completely sober. But hey, it works better than I expected it would. Below, you can take a listen for yourself. Impressive or not, this has been a nice entry point for me into deep learning and music generation. I hope to expand my knowledge and experience in this field in future projects.</p>

<h2 id="audio-examples">Audio Examples</h2>

<p>The piano predictions below have the 256 first of 1024 tokens feed as an input condition. The first example was made by me recording “Make You Feel My Love”, to see how well my models performed on a completely different file with complex harmony.</p>

<h3 id="make-you-feel-my-love--bob-dylan">“Make You Feel My Love” – Bob Dylan</h3>

<p>Original Accompaniment</p>
<audio controls="">

  <source src="https://www.uio.no/english/studies/programmes/SMC-master/blog/assets/audio/2022_05_20_kriswent_make_u_feel_original.mp3" type="audio/mpeg" />
</audio>

<p>FNet Predicted Accompaniment</p>
<audio controls="">

  <source src="https://www.uio.no/english/studies/programmes/SMC-master/blog/assets/audio/2022_05_20_kriswent_make_u_feel_fnet.mp3" type="audio/mpeg" />
</audio>

<p>Transformer Predicted Accompaniment</p>
<audio controls="">

  <source src="https://www.uio.no/english/studies/programmes/SMC-master/blog/assets/audio/2022_05_20_kriswent_make_u_feel_transformer.mp3" type="audio/mpeg" />
</audio>

<h3 id="song-186--segment-5--pop909-dataset">“Song 186 – Segment 5” – POP909 dataset</h3>

<p>Original Accompaniment</p>
<audio controls="">

  <source src="https://www.uio.no/english/studies/programmes/SMC-master/blog/assets/audio/2022_05_20_kriswent_185_c4_original.mp3" type="audio/mpeg" />
</audio>

<p>FNet Predicted Accompaniment</p>
<audio controls="">

  <source src="https://www.uio.no/english/studies/programmes/SMC-master/blog/assets/audio/2022_05_20_kriswent_185_c4_fnet.mp3" type="audio/mpeg" />
</audio>

<p>Transformer Predicted Accompaniment</p>
<audio controls="">

  <source src="https://www.uio.no/english/studies/programmes/SMC-master/blog/assets/audio/2022_05_20_kriswent_185_c4_transformer.mp3" type="audio/mpeg" />
</audio>

<h2 id="references">References</h2>

<p>[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in Neural In- formation Processing Systems 30: Annual Conference on Neural Information Processing Systems (NeurIPS), 2017, pp. 5998–6008.</p>

<p>[2] F. Chollet, Deep learning with Python, Second edition. Shelter Island: Manning Publications, 2021.</p>

<p>[3] J. Lee-Thorp, J. Ainslie, I. Eckstein, and S. Onta-non, “FNet: Mixing Tokens with Fourier Trans-forms,” arXiv:2105.03824 [cs], Sep. 2021, Ac-cessed: May 04, 2022. [Online]. Available: http://arxiv.org/abs/2105.03824</p>

<p>[4] Z. Wang et al., “POP909: A Pop-song Dataset for Music Arrangement Generation,” in Proc. of the 21st Int. Society for Music Information Retrieval Conf., Montréal, Canada, 2020</p>
</div>

  

  <a class="u-url" href="/machine-learning/2022/05/20/kriswent-generating-piano-accompaniments-using-fourier-transforms.html" hidden></a>
</article>

<script src="/utils/slideshow.js"></script>
<script src="https://unpkg.com/wavesurfer.js@5.0.1/dist/wavesurfer.js"></script>
<script src="/utils/waveform.js"></script>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The SMC Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The SMC Blog</li><li><a class="u-email" href="mailto:cer@create.aau.dk">cer@create.aau.dk</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/smc-aau-cph"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">smc-aau-cph</span></a></li><li><a href="https://www.twitter.com/smc-aau-cph"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">smc-aau-cph</span></a></li><li><a href="https://youtube.com/c/smc-aau-cphr/videos"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#youtube"></use></svg> <span class="username">SMC_master</span></a></li><li><a href="/feed.xml"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#rss"></use></svg> <span>RSS feed</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>The student-led blog of the Aalborg University Copenhagen (AAU-CPH) master&#39;s programme in Sound and Music Computing (SMC).</p>
      </div>
    </div>

  </div>

</footer>


    <!-- include comments here -->

  </body>

</html>
