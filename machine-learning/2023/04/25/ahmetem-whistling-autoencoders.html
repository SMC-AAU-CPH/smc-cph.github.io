<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>The whistle of the autoencoder | The SMC Blog</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="The whistle of the autoencoder" />
<meta name="author" content="Emin Memis" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How I used autoencoders to create whistling." />
<meta property="og:description" content="How I used autoencoders to create whistling." />
<link rel="canonical" href="https://smc-aau-cph.github.io/smc-cph.github.io/machine-learning/2023/04/25/ahmetem-whistling-autoencoders.html" />
<meta property="og:url" content="https://smc-aau-cph.github.io/smc-cph.github.io/machine-learning/2023/04/25/ahmetem-whistling-autoencoders.html" />
<meta property="og:site_name" content="The SMC Blog" />
<meta property="og:image" content="https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2023_04_25_ahmetem_vae_whistling_bird.jpeg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-04-25T23:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2023_04_25_ahmetem_vae_whistling_bird.jpeg" />
<meta property="twitter:title" content="The whistle of the autoencoder" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Emin Memis"},"dateModified":"2023-04-25T23:00:00+00:00","datePublished":"2023-04-25T23:00:00+00:00","description":"How I used autoencoders to create whistling.","headline":"The whistle of the autoencoder","image":"https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2023_04_25_ahmetem_vae_whistling_bird.jpeg","mainEntityOfPage":{"@type":"WebPage","@id":"https://smc-aau-cph.github.io/smc-cph.github.io/machine-learning/2023/04/25/ahmetem-whistling-autoencoders.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://smc-aau-cph.github.io/smc-cph.github.io/assets/image/2022_07_20_stefanof_SMC_logo_grey.png"},"name":"Emin Memis"},"url":"https://smc-aau-cph.github.io/smc-cph.github.io/machine-learning/2023/04/25/ahmetem-whistling-autoencoders.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://smc-aau-cph.github.io/smc-cph.github.io/feed.xml" title="The SMC Blog" /></head>
<body><header class="site-header" role="banner"><div class="hamburger-menu-container">
  <nav>
    <ul>
      
      <li class="page-link-hamburger">
        

        <a href="#" class="drop-btn-hamburger" onclick="handleMenuClick(this)"
          >Topics</a
        >
        <ul class="dropdown-hamburger">
          
          <li><a href="/interactive-music/">New Interfaces for Musical Expression (NIME)</a></li>
          
          <li><a href="/machine-learning/">Machine Learning for Media Experiences</a></li>
          
          <li><a href="/motion-capture/">Embodied Interaction</a></li>
          
          <li><a href="/networked-music/">Networked Music</a></li>
          
          <li><a href="/sonification/">Sonification</a></li>
          
          <li><a href="/sound-programming/">Sound Processing</a></li>
          
          <li><a href="/spatial-audio/">Spatial User Interfaces</a></li>
          
          <li><a href="/other/">Other</a></li>
          
          <li><a href="/alltopics/">All Topics</a></li>
          
        </ul>

        
      </li>
      
      <li class="page-link-hamburger">
        

        <a href="#" class="drop-btn-hamburger" onclick="handleMenuClick(this)"
          >Projects</a
        >
        <ul class="dropdown-hamburger">
          
          <li><a href="/mini-projects/">Course Mini Projects</a></li>
          
          <li><a href="/applied-projects/">Semester Projects</a></li>
          
          <li><a href="/masters-thesis/">Master's Theses</a></li>
          
          <li><a href="/projects/">All Projects</a></li>
          
        </ul>

        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/people/">People</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/about/">About</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/Guides/">Guides</a>
        
      </li>
      
      <li class="page-link-hamburger">
        
        <a href="/search/">Search</a>
        
      </li>
      
    </ul>
  </nav>
</div>
<div class="wrapper">
        <div class="title-and-logo-wrapper">
            <a href="/">
                <img class="site-logo" src="/assets/image/2022_07_20_stefanof_SMC_logo_grey.png" />
            </a>
            <p class="site-title">
            The SMC Blog
            </p>
        </div>

        <nav class="site-nav">
        <ul>
            
            <li class="page-link">
            

                <a href='#' class="drop-btn" onclick="handleMenuClick(this)">Topics</a>
                <ul class="dropdown">
                
                <li><a href="/interactive-music/">New Interfaces for Musical Expression (NIME)</a></li>
                
                <li><a href="/machine-learning/">Machine Learning for Media Experiences</a></li>
                
                <li><a href="/motion-capture/">Embodied Interaction</a></li>
                
                <li><a href="/networked-music/">Networked Music</a></li>
                
                <li><a href="/sonification/">Sonification</a></li>
                
                <li><a href="/sound-programming/">Sound Processing</a></li>
                
                <li><a href="/spatial-audio/">Spatial User Interfaces</a></li>
                
                <li><a href="/other/">Other</a></li>
                
                <li><a href="/alltopics/">All Topics</a></li>
                
                </ul>

            
            </li>
            
            <li class="page-link">
            

                <a href='#' class="drop-btn" onclick="handleMenuClick(this)">Projects</a>
                <ul class="dropdown">
                
                <li><a href="/mini-projects/">Course Mini Projects</a></li>
                
                <li><a href="/applied-projects/">Semester Projects</a></li>
                
                <li><a href="/masters-thesis/">Master's Theses</a></li>
                
                <li><a href="/projects/">All Projects</a></li>
                
                </ul>

            
            </li>
            
            <li class="page-link">
            
                <a href="/people/">People</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/about/">About</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/Guides/">Guides</a>
            
            </li>
            
            <li class="page-link">
            
                <a href="/search/">Search</a>
            
            </li>
            
        </ul>
        </nav>
        <nav class="hamburger-icon-nav"><div class="hamburger-icon-container" onclick="handleHamburgerClick(this)">
  <div class="hamburger-icon-div hamburger-bar1"></div>
  <div class="hamburger-icon-div hamburger-bar2"></div>
  <div class="hamburger-icon-div hamburger-bar3"></div>
</div>
</nav>
    </div>

  <script src="/utils/jquery.js"></script>
  <script src="/utils/hamburger-nav.js"></script>
  <script src="/utils/nav-dropdown-click-handler.js"></script>
</header>
<main class="page-content" aria-label="Content">

      <div class="wrapper">
        <article
  class="post h-entry"
  itemscope
  itemtype="http://schema.org/BlogPosting"
>
  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline" align="left">
      The whistle of the autoencoder
    </h1>
    <p class="post-meta">
      <time
        class="dt-published"
        datetime="2023-04-25T23:00:00+00:00"
        itemprop="datePublished"
      >Apr 25, 2023 •
      </time>
         
      <a href="/authors/eminmemis.html">Emin Memis</a>
        </p>
  </header>

  <div class="post-content" itemprop="articleBody"><p>I trained variational autoencoders to generate a range of novel and authentic human whistling sounds. Surprisingly, while the capability of human whistling has been widely studied, it has not been focused that much using the power of machine learning until now. I was eager to see if this technology could capture the essence of whistling and create something truly unique. And the results did not disappoint! Join me in exploring the primitive beauty of whistling through the eyes of machine learning!</p>

<figure style="float: none">
   <img src="/assets/image/2023_04_25_ahmetem_vae_whistling_spec1.png" width="80%" />
   <figcaption>Example whistling spectrogram from the dataset.</figcaption>
</figure>

<h1 id="data">Data</h1>
<p>I used <a href="https://www.kaggle.com/datasets/jesusrequena/mlend-hums-and-whistles">MLend’s Hums and Whistles</a> dataset, which contains recordings of people whistling different songs. I preprocessed the the dataset to get ready for the training.</p>

<p>First things first, I min-max normalized the amplitude of the recordings. Then removed the silent portions that are over 500 ms, using the <a href="https://pypi.org/project/pydub/">pydub</a> library. To ensure smooth transitions, I added 50 samples fade in-out at the joining points. Next, I chopped each recordings into 65280-sample-long audio pieces, and disregarded the remaining slice. This awkward number is to make sure that resultant dataset has a power-of-two shape. I extracted spectrogram of each sample using <a href="https://librosa.org/doc/latest/index.html">librosa</a>, on an fft window of 512 sliding 256 samples. Then, converted them to magnitude values.</p>

<figure style="float: none">
   <img src="/assets/image/2023_04_25_ahmetem_vae_whistling_meanspec.png" width="60%" />
   <figcaption></figcaption>
</figure>
<p>
</p>
<figure style="float: none">
   <img src="/assets/image/2023_04_25_ahmetem_vae_whistling_psd.png" width="60%" />
   <figcaption></figcaption>
</figure>

<p>Next, I applied frequency filtering to the spectrograms to reduce their dimentionality. To do that, I started with analyzing the spectral density of the dataset to have an idea of the overall frequency behaviour of the whistlings. In that analysis, above 5400 Hz indicated no significant power and I decided to filter the frequencies that are not significantly used when whistling. The mentioned point luckily is very close to the mid-point of the frequency range, and I put a global threshold at the middle point (5469.4) to make sure things are in powers of two. Above I plot the spectral average and the spectral power distribution of the dataset, along with the filtering threshold.</p>

<p>These steps resulted in ~12000 spectrogram representations with a shape of (128, 256). I splitted them into training and validation sets (80:20), and I was ready for training! See below the examples from the preprocessed dataset to have an idea of their shape and spectral behaviour.</p>

<figure style="float: none">
   <img src="/assets/image/2023_04_25_ahmetem_vae_whistling_prepspec.png" width="80%" />
   <figcaption>Preprocessed whistling samples.</figcaption>
</figure>

<h1 id="model-and-training">Model and Training</h1>
<p>After several experiments with different shapes and hyperparameters, here is the details of my final model desing.</p>

<h3 id="encoder-and-decoder">Encoder and Decoder</h3>
<p>Encoder part has 5 convoltuional layers with 512, …, 32 filters. Encoder encodes the input spectrograms into a 64-dimensional latent space, by learning the characteristics of the spectrograms and mapping them as a probability distribution.</p>

<p>The latent space that encoder creates is a 64-dimensional array with spectrogram encryptions are scattered here and there. See below the UMAP latent space visualisation after the training.</p>

<p>Decoder part is a mirrored encoder, taking the encoded representations to reconstruct a spectrogram. Last layer of the decoder has a sigmoid function that produces a spectrogram output with values between 0 and 1.</p>

<figure style="float: none">
   <img src="/assets/image/2023_04_25_ahmetem_vae_whistling_diagram.jpeg" width="80%" />
   <figcaption>Model diagram</figcaption>
</figure>

<h3 id="loss-functions">Loss Functions</h3>
<p>I used a combintaion of <a href="https://www.jstor.org/stable/2236703">Kullback-Leibler Divergence</a> (KL) and Mean Squared Error (MSE). MSE aimed to reduce the reconstruction error between the input and the output, while KL aimed to establish optimal probability distribution. I weighted the KL by $10^-6$ to balance their importance in the loss function.</p>

<h3 id="training">Training</h3>
<p>With all these, I trained the model with 10000 samples for 178 epochs with a batch size of 64. I performed the training on Google Colab, and it took around 3 hours with premium GPUs.</p>

<h1 id="generating-new-samples">Generating New Samples</h1>
<p>After training the model, I used the decoder part to generate new samples simply by randomly picking a sample from the latent space. Then, the decoder reconstructed a new spectrogram based on whatever it has in his mind(!) about the selected point. And that’s the final result, ALMOST!</p>

<h2 id="postprocessing">Postprocessing</h2>
<p>First I had to put the filtered spectrogram part back - I simply filled the missing parts with zeros. Secondly, I denormalized the amplitudes back to the normal range. Then I converted the spectrograms into audio using the <a href="https://ieeexplore-ieee-org.ezproxy.uio.no/document/1164317">Griffin-Lim algorithm</a>. It is now a final result!</p>

<h2 id="so-what">So what?</h2>
<p>It went well! I think things worked and the resultant spectrograms are very close to human-made ones. Altough some noise in the background and some artifacts here and there, I could barely distinguish them from the originals. Also, worth mentioning that not all the generated spectrograms are that good - some were even terrible, sounding like an alien whistling, or someone practicing whistling while shaving.</p>

<p>Here are some generated whistling samples.</p>

<p><audio controls="controls">
  <source src="http://docs.google.com/uc?export=open&amp;id=1lN21VHc5gpbzwc3Vq9EmMWnHJ902IUEy" />
</audio></p>

<p><audio controls="controls">
  <source src="http://docs.google.com/uc?export=open&amp;id=1s4WiTePftqctnkyWeYCUXXczXG_u33OI" />
</audio></p>

<p><audio controls="controls">
  <source src="http://docs.google.com/uc?export=open&amp;id=1aO2zXEMB0L0v3QH44upNu-1JGPD9U66e" />
</audio></p>

<p><audio controls="controls">
  <source src="http://docs.google.com/uc?export=open&amp;id=1NhDzItT6sSZs_yCr2W_08jhyx_hkSImY" />
</audio></p>

<p><audio controls="controls">
  <source src="http://docs.google.com/uc?export=open&amp;id=1Llz-wFUBUss8DMAqtAEpvfMBKDwjAOPA" />
</audio></p>

<p>I will also conduct a user study with a pool of model-generated and original whistling samples. I will ask participants to rate the whistlings by their <em>realism</em> (in terms of <em>audio-quality</em> and <em>expressiveness</em>) to compare the model whistlings with the real ones. This way I hope to find out how unreal the model-generated whistles really are. I will add the results here when ready!</p>

<p><em>Keep on whistling and stay tuned!</em></p>
</div>

  

  <a class="u-url" href="/machine-learning/2023/04/25/ahmetem-whistling-autoencoders.html" hidden></a>
</article>

<script src="/utils/slideshow.js"></script>
<script src="https://unpkg.com/wavesurfer.js@5.0.1/dist/wavesurfer.js"></script>
<script src="/utils/waveform.js"></script>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">The SMC Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">The SMC Blog</li><li><a class="u-email" href="mailto:cer@create.aau.dk">cer@create.aau.dk</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/smc-aau-cph"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">smc-aau-cph</span></a></li><li><a href="https://www.twitter.com/smc-aau-cph"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">smc-aau-cph</span></a></li><li><a href="https://youtube.com/c/smc-aau-cphr/videos"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#youtube"></use></svg> <span class="username">SMC_master</span></a></li><li><a href="/feed.xml"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#rss"></use></svg> <span>RSS feed</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>The student-led blog of the Aalborg University Copenhagen (AAU-CPH) master&#39;s programme in Sound and Music Computing (SMC).</p>
      </div>
    </div>

  </div>

</footer>


    <!-- include comments here -->

  </body>

</html>
